[{"title":"一致性 Hash 算法分析","url":"/2018/01/08/Consistent-Hash/","content":"\n当我们在做数据库分库分表或者是分布式缓存时，不可避免的都会遇到一个问题:\n如何将数据均匀的分散到各个节点中，并且尽量的在加减节点时能使受影响的数据最少。\nHash 取模随机放置就不说了，会带来很多问题。通常最容易想到的方案就是 hash 取模了。\n可以将传入的 Key 按照 index = hash(key) % N 这样来计算出需要存放的节点。其中 hash 函数是一个将字符串转换为正整数的哈希映射方法，N 就是节点的数量。\n这样可以满足数据的均匀分配，但是这个算法的容错性和扩展性都较差。\n比如增加或删除了一个节点时，所有的 Key 都需要重新计算，显然这样成本较高，为此需要一个算法满足分布均匀同时也要有良好的容错性和拓展性。\n\n\n一致 Hash 算法一致 Hash 算法是将所有的哈希值构成了一个环，其范围在 0 ~ 2^32-1。如下图：\n\n之后将各个节点散列到这个环上，可以用节点的 IP、hostname 这样的唯一性字段作为 Key 进行 hash(key)，散列之后如下：\n\n之后需要将数据定位到对应的节点上，使用同样的 hash 函数 将 Key 也映射到这个环上。\n\n这样按照顺时针方向就可以把 k1 定位到 N1节点，k2 定位到 N3节点，k3 定位到 N2节点。\n容错性这时假设 N1 宕机了：\n\n依然根据顺时针方向，k2 和 k3 保持不变，只有 k1 被重新映射到了 N3。这样就很好的保证了容错性，当一个节点宕机时只会影响到少少部分的数据。\n拓展性当新增一个节点时:\n\n在 N2 和 N3 之间新增了一个节点 N4 ，这时会发现受印象的数据只有 k3，其余数据也是保持不变，所以这样也很好的保证了拓展性。\n虚拟节点到目前为止该算法依然也有点问题:\n当节点较少时会出现数据分布不均匀的情况：\n\n这样会导致大部分数据都在 N1 节点，只有少量的数据在 N2 节点。\n为了解决这个问题，一致哈希算法引入了虚拟节点。将每一个节点都进行多次 hash，生成多个节点放置在环上称为虚拟节点:\n\n计算时可以在 IP 后加上编号来生成哈希值。\n这样只需要在原有的基础上多一步由虚拟节点映射到实际节点的步骤即可让少量节点也能满足均匀性。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["算法"]},{"title":"LinkedHashMap 底层分析","url":"/2018/02/06/LinkedHashMap/","content":"\n众所周知 HashMap 是一个无序的 Map，因为每次根据 key 的 hashcode 映射到 Entry 数组上，所以遍历出来的顺序并不是写入的顺序。\n因此 JDK 推出一个基于 HashMap 但具有顺序的 LinkedHashMap 来解决有排序需求的场景。\n它的底层是继承于 HashMap 实现的，由一个双向链表所构成。\nLinkedHashMap 的排序方式有两种：\n\n根据写入顺序排序。\n根据访问顺序排序。\n\n其中根据访问顺序排序时，每次 get 都会将访问的值移动到链表末尾，这样重复操作就能的到一个按照访问顺序排序的链表。\n\n\n数据结构@Testpublic void test()&#123;\tMap&lt;String, Integer&gt; map = new LinkedHashMap&lt;String, Integer&gt;();\tmap.put(&quot;1&quot;,1) ;\tmap.put(&quot;2&quot;,2) ;\tmap.put(&quot;3&quot;,3) ;\tmap.put(&quot;4&quot;,4) ;\tmap.put(&quot;5&quot;,5) ;\tSystem.out.println(map.toString());&#125;\n\n调试可以看到 map 的组成：\n\n打开源码可以看到：\n/** * The head of the doubly linked list. */private transient Entry&lt;K,V&gt; header;/** * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */private final boolean accessOrder;private static class Entry&lt;K,V&gt; extends HashMap.Entry&lt;K,V&gt; &#123;    // These fields comprise the doubly linked list used for iteration.    Entry&lt;K,V&gt; before, after;    Entry(int hash, K key, V value, HashMap.Entry&lt;K,V&gt; next) &#123;        super(hash, key, value, next);    &#125;&#125;  \n\n其中 Entry 继承于 HashMap 的 Entry，并新增了上下节点的指针，也就形成了双向链表。\n还有一个 header 的成员变量，是这个双向链表的头结点。 \n上边的 demo 总结成一张图如下：\n\n第一个类似于 HashMap 的结构，利用 Entry 中的 next 指针进行关联。\n下边则是 LinkedHashMap 如何达到有序的关键。\n就是利用了头节点和其余的各个节点之间通过 Entry 中的 after 和 before 指针进行关联。\n其中还有一个 accessOrder 成员变量，默认是 false，默认按照插入顺序排序，为 true 时按照访问顺序排序，也可以调用:\npublic LinkedHashMap(int initialCapacity,                     float loadFactor,                     boolean accessOrder) &#123;    super(initialCapacity, loadFactor);    this.accessOrder = accessOrder;&#125;\n\n这个构造方法可以显示的传入 accessOrder 。\n构造方法LinkedHashMap 的构造方法:\npublic LinkedHashMap() &#123;    super();    accessOrder = false;&#125;\n\n其实就是调用的 HashMap 的构造方法:\nHashMap 实现：\npublic HashMap(int initialCapacity, float loadFactor) &#123;    if (initialCapacity &lt; 0)        throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; +                                           initialCapacity);    if (initialCapacity &gt; MAXIMUM_CAPACITY)        initialCapacity = MAXIMUM_CAPACITY;    if (loadFactor &lt;= 0 || Float.isNaN(loadFactor))        throw new IllegalArgumentException(&quot;Illegal load factor: &quot; +                                           loadFactor);    this.loadFactor = loadFactor;    threshold = initialCapacity;    //HashMap 只是定义了改方法，具体实现交给了 LinkedHashMap    init();&#125;\n\n可以看到里面有一个空的 init()，具体是由 LinkedHashMap 来实现的：\n@Overridevoid init() &#123;    header = new Entry&lt;&gt;(-1, null, null, null);    header.before = header.after = header;&#125;\n其实也就是对 header 进行了初始化。\nput 方法看 LinkedHashMap 的 put() 方法之前先看看 HashMap 的 put 方法：\npublic V put(K key, V value) &#123;    if (table == EMPTY_TABLE) &#123;        inflateTable(threshold);    &#125;    if (key == null)        return putForNullKey(value);    int hash = hash(key);    int i = indexFor(hash, table.length);    for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123;        Object k;        if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123;            V oldValue = e.value;            e.value = value;            //空实现，交给 LinkedHashMap 自己实现            e.recordAccess(this);            return oldValue;        &#125;    &#125;    modCount++;    // LinkedHashMap 对其重写    addEntry(hash, key, value, i);    return null;&#125;// LinkedHashMap 对其重写void addEntry(int hash, K key, V value, int bucketIndex) &#123;    if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123;        resize(2 * table.length);        hash = (null != key) ? hash(key) : 0;        bucketIndex = indexFor(hash, table.length);    &#125;    createEntry(hash, key, value, bucketIndex);&#125;// LinkedHashMap 对其重写void createEntry(int hash, K key, V value, int bucketIndex) &#123;    Entry&lt;K,V&gt; e = table[bucketIndex];    table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e);    size++;&#125;       \n\n主体的实现都是借助于 HashMap 来完成的，只是对其中的 recordAccess(), addEntry(), createEntry() 进行了重写。\nLinkedHashMap 的实现：\n    //就是判断是否是根据访问顺序排序，如果是则需要将当前这个 Entry 移动到链表的末尾    void recordAccess(HashMap&lt;K,V&gt; m) &#123;        LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m;        if (lm.accessOrder) &#123;            lm.modCount++;            remove();            addBefore(lm.header);        &#125;    &#125;        //调用了 HashMap 的实现，并判断是否需要删除最少使用的 Entry(默认不删除)    void addEntry(int hash, K key, V value, int bucketIndex) &#123;    super.addEntry(hash, key, value, bucketIndex);    // Remove eldest entry if instructed    Entry&lt;K,V&gt; eldest = header.after;    if (removeEldestEntry(eldest)) &#123;        removeEntryForKey(eldest.key);    &#125;&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123;    HashMap.Entry&lt;K,V&gt; old = table[bucketIndex];    Entry&lt;K,V&gt; e = new Entry&lt;&gt;(hash, key, value, old);    //就多了这一步，将新增的 Entry 加入到 header 双向链表中    table[bucketIndex] = e;    e.addBefore(header);    size++;&#125;    //写入到双向链表中    private void addBefore(Entry&lt;K,V&gt; existingEntry) &#123;        after  = existingEntry;        before = existingEntry.before;        before.after = this;        after.before = this;    &#125;      \n\nget 方法LinkedHashMap 的 get() 方法也重写了：\npublic V get(Object key) &#123;    Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key);    if (e == null)        return null;            //多了一个判断是否是按照访问顺序排序，是则将当前的 Entry 移动到链表头部。    e.recordAccess(this);    return e.value;&#125;void recordAccess(HashMap&lt;K,V&gt; m) &#123;    LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m;    if (lm.accessOrder) &#123;        lm.modCount++;                //删除        remove();        //添加到头部        addBefore(lm.header);    &#125;&#125;    \n\nclear() 清空就要比较简单了：\n//只需要把指针都指向自己即可，原本那些 Entry 没有引用之后就会被 JVM 自动回收。public void clear() &#123;    super.clear();    header.before = header.after = header;&#125;\n\n\n总结总的来说 LinkedHashMap 其实就是对 HashMap 进行了拓展，使用了双向链表来保证了顺序性。\n因为是继承与 HashMap 的，所以一些 HashMap 存在的问题 LinkedHashMap 也会存在，比如不支持并发等。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["Java 进阶"],"tags":["Java","LinkedHashMap"]},{"title":"Linux（一）常用命令","url":"/2016/04/10/Linux-normal/","content":"前言\n由于现在JAVA开发的很多应用都是部署到Linux系统上的，因此了解和掌握一些Linux的常用命令是非常有必要的，以下就是在Java开发过程中一些常用的命令。\n\n\n常用命令\n查找文件find / -name log.txt根据名称查找在 &#x2F;目录下的 log.txt文件。\n\nfind .-name &quot;*.xml&quot;递归查找所有的xml文件。\nfind .-name &quot;*.xml&quot;|xargs grep &quot;hello&quot;递归查找所有包含hello的xml文件。\nls -l grep &#39;jar&#39;查找当前目录中的所有jar文件。 \n\n\n检查一个文件是否运行ps –ef|grep tomecate检查所有有关tomcat的进程。\n\n终止线程kill -9 19979 终止线程号为19979的线程\n\n查看文件，包括隐藏文件。ls -al\n\n查看当前工作目录。pwd\n\n复制文件包括其子文件到指定目录cp -r source target复制source文件到target目录中。\n\n创建一个目录mkdir new创建一个new的目录\n\n删除目录(前提是此目录是空目录)rmdir source删除source目录。\n\n删除文件 包括其子文件rm -rf file删除file文件和其中的子文件。-r表示向下递归，不管有多少目录一律删除-f表示强制删除，不做任何提示。\n\n移动文件mv /temp/movefile  /target\n\n切换用户su -username\n\n查看ipifconfig注意是 ifconfig 不是windows中的ipconfig\n\n\n总结以上就是在Linux下开发Java应用常用的Linux命令，如有遗漏请在评论处补充，我将不定期添加。\n","categories":["Linux笔记"],"tags":["Linux"]},{"title":"Linux（二）服务器运行环境配置","url":"/2016/09/20/Linux-normal2/","content":"\n前言Linux相信对大多数程序员来说都不陌生，毕竟在服务器端依然还是霸主地位而且丝毫没有退居二线的意思，以至于现在几乎每一个软件开发的相关人员都得或多或少的知道一些Linux的相关内容，本文将介绍如何在刚拿到一台云服务器(采用centos)来进行运行环境的搭建，包括JDK、Mysql、Tomcat以及nginx。相信对于小白来说很有必要的，也是我个人的一个记录。\n\n该服务器的用途是用于部署JavaEE项目。部署之后的效果图如下:\n\n\n\nJDK安装由于我们之后需要部署的是JavaEE项目，所以首先第一步就是安装JDK了。\n卸载自带的openJDK现在的服务器拿来之后一般都是默认给我们安装一个openJDK，首先我们需要卸载掉。\n\n使用rpm -qa | grep java命令查看系统中是否存在有Java。\n使用rpm -e --nodeps 相关应用名称来进行卸载。(相关应用名称就是上一个命令中显示出来的名称复制到这里卸载即可)。\n\n下载并安装JDK\n之后是下载ORACLE所提供的JDK，传送门根据自己系统的情况下载对应版本即可。笔者使用的是jdk-8u101-linux-x64.rpm版本。\n然后使用FTP工具上传到/usr/java目录下即可，没有java目录新建一个即可。\n然后使用rpm -ivh jdk-8u101-linux-x64.rpm命令进行解压安装。\n\nprofile文件配置安装完成之后使用vi /etc/profile命令编辑profile文件(注意该文件路径是指根目录下的etc文件夹不要找错了)。在该文件中加入以下内容：\nexport JAVA_HOME=/usr/java/jdk-8u101-linux-x64export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin\n保存之后运行source /etc/profile使配置生效。\n验证是否安装成功之后我们使用在windows平台也有的命令java -version，如果输出如图：表示安装成功。\nMySQL安装卸载自带的Mysql首先第一步还是要卸载掉自带的mysql。rpm -e --nodeps mysql命令和之前一样只是把应用名称换成mysql了而已。\n使用yum来安装mysql之后我们采用yum来安装mysql。这样的方式最简单便捷。yum install -y mysql-server mysql mysql-deve执行该命令直到出现Complete!提示之后表示安装成功。rpm -qi mysql-server之后使用该命令可以查看我们安装的mysql信息。\nmysql相关配置使用service mysqld start来启动mysql服务(第一次会输出很多信息)，之后就不会了。然后我们可以使用chkconfig mysqld on命令将mysql设置为开机启动。输入chkconfig --list | grep mysql命令显示如下图：表示设置成功。使用mysqladmin -u root password &#39;root&#39;为root账户设置密码。\n设置远程使用grant all privileges on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27; with grant option;# root是用户名，%代表任意主机，&#x27;123456&#x27;指定的登录密码（这个和本地的root密码可以设置不同的，互不影响）flush privileges; # 重载系统权限exit;\n\n验证使用使用mysql -u root -proot来登录mysql。如果出现以下界面表示设置成功。\nTomcat安装Tomcat也是我们运行JavaEE项目必备的一个中间件。\n\n第一步需要下载linux的Tomcat，传送门。根据自己系统版本进行下载即可。之后将apache-tomcat-8.5.5.tar.gz上传到/usr/local目录中。\n解压该压缩包tar -zxv -f apache-tomcat-8.5.5.tar.gz,再使用mv apache-tomcat-8.5.5  tomcat将解压的Tomcat移动到外层的Tomcat目录中。\n进入/usr/local/tomcat/apache-tomcat-8.5.5/bin目录使用./startup.bat命令启动tomcat。\n因为tomcat使用的默认端口是8080，linux防火墙默认是不能访问的，需要手动将其打开。使用vi + /etc/sysconfig/iptables编辑iptables(注意etc目录是根目录下的)，加入以下代码:-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -j ACCEPT-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT\n这里我们开放了8080和80端口，之后安装nginx就不用在开放了。\nps:这里用到了简单的vim命令。按i进入插入模式，输入上面两段代码。之后按esc退出插入模式。再按:wq保存关闭即可。之后使用service iptables restart命令重启防火墙即可。在浏览器输入服务器的ip+8080如果出现Tomcat的欢迎页即表明Tomcat安装成功。\n\n\n\nnginx安装最后是安装nginx，这里我们还是使用最简单的yum的方式来进行安装。\n\n首先使用以下几个命令安装必备的几个库：yum -y install pcre*yum -y install openssl*yum -y install gcc\n之后安装nginx。cd /usr/local/wget http://nginx.org/download/nginx-1.4.2.tar.gztar -zxvf nginx-1.4.2.tar.gzcd nginx-1.4.2  ./configure --prefix=/usr/local/nginx --with-http_stub_status_modulemakemake install\n之后就可以使用/usr/local/nginx/sbin/nginx命令来启动nginx了。输入服务器的IP地址，如果出现nginx的欢迎界面表示安装成功了。\n\nnginx配置这里我就简单贴以下我的配置，主要就是配置一个upstream,之后在server中引用配置的那个upstream即可。\n#user  nobody;worker_processes  1;#error_log  logs/error.log;#error_log  logs/error.log  notice;#error_log  logs/error.log  info;#pid        logs/nginx.pid;events &#123;    worker_connections  1024;&#125;http &#123;    include       mime.types;    default_type  application/octet-stream;    #log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;    #                  &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;    #                  &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;    #access_log  logs/access.log  main;    sendfile        on;    #tcp_nopush     on;    #keepalive_timeout  0;    keepalive_timeout  65;    #gzip  on;    upstream crossover_main &#123;        server 127.0.0.1:8080;    &#125;    server &#123;        listen       80;        server_name  www.crossoverjie.top;        #charset koi8-r;        #access_log  logs/host.access.log  main;        location  / &#123;             proxy_pass http://crossover_main/examples/;             proxy_set_header Host $http_host;             proxy_set_header X-Forwarded-For $remote_addr;             index  index.jsp;        &#125;        #error_page  404              /404.html;        # redirect server error pages to the static page /50x.html        #        error_page   500 502 503 504  /50x.html;        location = /50x.html &#123;            root   html;        &#125;        # proxy the PHP scripts to Apache listening on 127.0.0.1:80        #        #location ~ \\.php$ &#123;        #    proxy_pass   http://127.0.0.1;        #&#125;        # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000        #        #location ~ \\.php$ &#123;        #    root           html;        #    fastcgi_pass   127.0.0.1:9000;        #    fastcgi_index  index.php;        #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;        #    include        fastcgi_params;        #&#125;        # deny access to .htaccess files, if Apache&#x27;s document root        # concurs with nginx&#x27;s one        #        #location ~ /\\.ht &#123;        #    deny  all;        #&#125;    &#125;    # another virtual host using mix of IP-, name-, and port-based configuration    #    #server &#123;    #    listen       8000;    #    listen       somename:8080;    #    server_name  somename  alias  another.alias;    #    location / &#123;    #        root   html;    #        index  index.html index.htm;    #    &#125;    #&#125;    # HTTPS server    #    #server &#123;    #    listen       443;    #    server_name  localhost;    #    ssl                  on;    #    ssl_certificate      cert.pem;    #    ssl_certificate_key  cert.key;    #    ssl_session_timeout  5m;    #    ssl_protocols  SSLv2 SSLv3 TLSv1;    #    ssl_ciphers  HIGH:!aNULL:!MD5;    #    ssl_prefer_server_ciphers   on;    #    location / &#123;    #        root   html;    #        index  index.html index.htm;    #    &#125;    #&#125;&#125;\n之后我们在地址栏输入服务器的IP地址(如果有域名解析了服务器的IP可以直接输入域名)就会进入我们在upstream中配置的地址加上在server中的地址。根据我这里的配置最后解析地址就是http://127.0.0.1:8080/examples应该是很好理解的。最终的结果是我在片头放的那张截图一样。\n总结这是一个简单的基于centOS的运行环境配置，对于小白练手应该是够了，有不清楚和错误的地方欢迎指出反正我也不会回复。\n\n个人博客地址：http://crossoverjie.top。\n\n\nGitHub地址：https://github.com/crossoverJie。\n\n","categories":["Linux笔记"],"tags":["Linux","centos","nginx"]},{"title":"Markdown小计","url":"/2016/05/06/Markdown%E5%B0%8F%E8%AE%A1/","content":"# 标题 \n表示标题 一个#号代表一级标题，以此类推。 * 无序列表\n\n无序列表\n\n&gt; 引用  \n\n引用\n\n[http://www.baidu.com](http://www.baidu.com &quot;百度&quot;)\n百度\n\n\n![艾弗森](http://i.imgur.com/TLnZ2S6.jpg)插入图片\n\n    \n","categories":["blog"],"tags":["Markdown"]},{"title":"ReentrantLock 实现原理","url":"/2018/01/25/ReentrantLock/","content":"\n使用 synchronize 来做同步处理时，锁的获取和释放都是隐式的，实现的原理是通过编译后加上不同的机器指令来实现。\n而 ReentrantLock 就是一个普通的类，它是基于 AQS(AbstractQueuedSynchronizer)来实现的。\n是一个重入锁：一个线程获得了锁之后仍然可以反复的加锁，不会出现自己阻塞自己的情况。\n\nAQS 是 Java 并发包里实现锁、同步的一个重要的基础框架。\n\n锁类型ReentrantLock 分为公平锁和非公平锁，可以通过构造方法来指定具体类型：\n//默认非公平锁public ReentrantLock() &#123;    sync = new NonfairSync();&#125;//公平锁public ReentrantLock(boolean fair) &#123;    sync = fair ? new FairSync() : new NonfairSync();&#125;\n\n默认一般使用非公平锁，它的效率和吞吐量都比公平锁高的多(后面会分析具体原因)。\n\n\n获取锁通常的使用方式如下:\nprivate ReentrantLock lock = new ReentrantLock();public void run() &#123;    lock.lock();    try &#123;        //do bussiness    &#125; catch (InterruptedException e) &#123;        e.printStackTrace();    &#125; finally &#123;        lock.unlock();    &#125;&#125;\n\n公平锁获取锁首先看下获取锁的过程：\npublic void lock() &#123;    sync.lock();&#125;\n\n可以看到是使用 sync的方法，而这个方法是一个抽象方法，具体是由其子类(FairSync)来实现的，以下是公平锁的实现:\n   final void lock() &#123;       acquire(1);   &#125;      //AbstractQueuedSynchronizer 中的 acquire()   public final void acquire(int arg) &#123;   if (!tryAcquire(arg) &amp;&amp;       acquireQueued(addWaiter(Node.EXCLUSIVE), arg))       selfInterrupt();&#125;\n\n第一步是尝试获取锁(tryAcquire(arg)),这个也是由其子类实现：\n    protected final boolean tryAcquire(int acquires) &#123;        final Thread current = Thread.currentThread();        int c = getState();        if (c == 0) &#123;            if (!hasQueuedPredecessors() &amp;&amp;                compareAndSetState(0, acquires)) &#123;                setExclusiveOwnerThread(current);                return true;            &#125;        &#125;        else if (current == getExclusiveOwnerThread()) &#123;            int nextc = c + acquires;            if (nextc &lt; 0)                throw new Error(&quot;Maximum lock count exceeded&quot;);            setState(nextc);            return true;        &#125;        return false;    &#125;&#125;\n\n首先会判断 AQS 中的 state 是否等于 0，0 表示目前没有其他线程获得锁，当前线程就可以尝试获取锁。\n注意:尝试之前会利用 hasQueuedPredecessors() 方法来判断 AQS 的队列中中是否有其他线程，如果有则不会尝试获取锁(这是公平锁特有的情况)。\n如果队列中没有线程就利用 CAS 来将 AQS 中的 state 修改为1，也就是获取锁，获取成功则将当前线程置为获得锁的独占线程(setExclusiveOwnerThread(current))。\n如果 state 大于 0 时，说明锁已经被获取了，则需要判断获取锁的线程是否为当前线程(ReentrantLock 支持重入)，是则需要将 state + 1，并将值更新。\n写入队列如果 tryAcquire(arg) 获取锁失败，则需要用 addWaiter(Node.EXCLUSIVE) 将当前线程写入队列中。\n写入之前需要将当前线程包装为一个 Node 对象(addWaiter(Node.EXCLUSIVE))。\n\nAQS 中的队列是由 Node 节点组成的双向链表实现的。\n\n包装代码:\nprivate Node addWaiter(Node mode) &#123;    Node node = new Node(Thread.currentThread(), mode);    // Try the fast path of enq; backup to full enq on failure    Node pred = tail;    if (pred != null) &#123;        node.prev = pred;        if (compareAndSetTail(pred, node)) &#123;            pred.next = node;            return node;        &#125;    &#125;    enq(node);    return node;&#125;\n\n首先判断队列是否为空，不为空时则将封装好的 Node 利用 CAS 写入队尾，如果出现并发写入失败就需要调用 enq(node); 来写入了。\nprivate Node enq(final Node node) &#123;    for (;;) &#123;        Node t = tail;        if (t == null) &#123; // Must initialize            if (compareAndSetHead(new Node()))                tail = head;        &#125; else &#123;            node.prev = t;            if (compareAndSetTail(t, node)) &#123;                t.next = node;                return t;            &#125;        &#125;    &#125;&#125;\n\n这个处理逻辑就相当于自旋加上 CAS 保证一定能写入队列。\n挂起等待线程写入队列之后需要将当前线程挂起(利用acquireQueued(addWaiter(Node.EXCLUSIVE), arg))：\nfinal boolean acquireQueued(final Node node, int arg) &#123;    boolean failed = true;    try &#123;        boolean interrupted = false;        for (;;) &#123;            final Node p = node.predecessor();            if (p == head &amp;&amp; tryAcquire(arg)) &#123;                setHead(node);                p.next = null; // help GC                failed = false;                return interrupted;            &#125;            if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;                parkAndCheckInterrupt())                interrupted = true;        &#125;    &#125; finally &#123;        if (failed)            cancelAcquire(node);    &#125;&#125;\n\n首先会根据 node.predecessor() 获取到上一个节点是否为头节点，如果是则尝试获取一次锁，获取成功就万事大吉了。\n如果不是头节点，或者获取锁失败，则会根据上一个节点的 waitStatus 状态来处理(shouldParkAfterFailedAcquire(p, node))。\nwaitStatus 用于记录当前节点的状态，如节点取消、节点等待等。\nshouldParkAfterFailedAcquire(p, node) 返回当前线程是否需要挂起，如果需要则调用 parkAndCheckInterrupt()：\nprivate final boolean parkAndCheckInterrupt() &#123;    LockSupport.park(this);    return Thread.interrupted();&#125;\n\n他是利用 LockSupport 的 part 方法来挂起当前线程的，直到被唤醒。\n非公平锁获取锁公平锁与非公平锁的差异主要在获取锁：\n公平锁就相当于买票，后来的人需要排到队尾依次买票，不能插队。\n而非公平锁则没有这些规则，是抢占模式，每来一个人不会去管队列如何，直接尝试获取锁。\n非公平锁:\nfinal void lock() &#123;    //直接尝试获取锁    if (compareAndSetState(0, 1))        setExclusiveOwnerThread(Thread.currentThread());    else        acquire(1);&#125;\n\n公平锁:\nfinal void lock() &#123;    acquire(1);&#125;\n\n还要一个重要的区别是在尝试获取锁时tryAcquire(arg)，非公平锁是不需要判断队列中是否还有其他线程，也是直接尝试获取锁：\nfinal boolean nonfairTryAcquire(int acquires) &#123;    final Thread current = Thread.currentThread();    int c = getState();    if (c == 0) &#123;        //没有 !hasQueuedPredecessors() 判断        if (compareAndSetState(0, acquires)) &#123;            setExclusiveOwnerThread(current);            return true;        &#125;    &#125;    else if (current == getExclusiveOwnerThread()) &#123;        int nextc = c + acquires;        if (nextc &lt; 0) // overflow            throw new Error(&quot;Maximum lock count exceeded&quot;);        setState(nextc);        return true;    &#125;    return false;&#125;\n\n释放锁公平锁和非公平锁的释放流程都是一样的：\npublic void unlock() &#123;    sync.release(1);&#125;public final boolean release(int arg) &#123;    if (tryRelease(arg)) &#123;        Node h = head;        if (h != null &amp;&amp; h.waitStatus != 0)        \t   //唤醒被挂起的线程            unparkSuccessor(h);        return true;    &#125;    return false;&#125;//尝试释放锁protected final boolean tryRelease(int releases) &#123;    int c = getState() - releases;    if (Thread.currentThread() != getExclusiveOwnerThread())        throw new IllegalMonitorStateException();    boolean free = false;    if (c == 0) &#123;        free = true;        setExclusiveOwnerThread(null);    &#125;    setState(c);    return free;&#125;        \n\n首先会判断当前线程是否为获得锁的线程，由于是重入锁所以需要将 state 减到 0 才认为完全释放锁。\n释放之后需要调用 unparkSuccessor(h) 来唤醒被挂起的线程。\n总结由于公平锁需要关心队列的情况，得按照队列里的先后顺序来获取锁(会造成大量的线程上下文切换)，而非公平锁则没有这个限制。\n所以也就能解释非公平锁的效率会被公平锁更高。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["Java 进阶"],"tags":["Java","ReentrantLock"]},{"title":"SSM(一)框架的整合","url":"/2016/06/28/SSM1/","content":"\n前言最近这几年JetBrains公司开发的IDEA是越来越流行了，甚至Google的官方IDE都是IDEA来定制的，可见IDEA的发展趋势是越来越好，由于博主接触IDEA的时间也不长，所以有关IDEA和Eclipse的区别和优劣势请自行百度了。借此机会我就使用IDEA来整合一下SSM，针对于初学者(初次使用IDEA和JAVAEE初学者)还是有帮助的。\n\n新建SSM项目哦对了，关于IDEA的版本问题强烈建议使用旗舰版，有条件的就购买，没条件的嘛。。天朝你懂的。在欢迎界面点击Create New Project。\n\n之后选择Maven(新建JAVAEE项目是需要安装JDK的，这个就不在这里讲解了。)选好之后点击下一步。之后填入GroupID和ArtifactID这里尽量按照Maven的命名规范来即可。之后点击下一步，填入项目名称，这里我建议和之前填写的ArtifactID名称一样即可。点击Finish完成项目的创建。之后尽量不要做其他操作，让IDEA完成索引创建。\n完善目录结构首先观察一下IDEA给我们生成的目录结构，这是一个标准的Maven目录。但是其中少了一个webapp目录用于存放jsp、css、js、图片之类的文件。之后还需要完善我们的目录结构，如下图：以上的命名都是我们开发过程中常用的命名规则，不一定按照我这样来，但是最好是有一定的规范。\n\nPOM.xmlpom.xml是整个maven的核心配置文件，里面有对项目的描述和项目所需要的依赖。哦对了，在修改pom.xml文件之前我们最好先设置一下该项目的Maven设置(IDEA对每个项目的maven设置和Eclipse不一样，不是设置一次就可了，如果今后还要新建项目那就还需要设置，同时按住ctrl,alt,s是打开设置的快捷键，更多有关IDEA的操作今后会更新相关博文。)\nIDEA的Maven设置在Eclipse中用过Maven的都应该知道，这里是将项目的Maven换成我们自己安装的Maven，下面两个目录是选择Maven配置文件，不知道是什么原因在Eclipse中选择了配置文件之后会自动的将Maven本地厂库的路径更改为你settings.xml中配置的路径。既然这里没有自动选中那我们就手动修改即可，尽量不要放在C盘，一是用久之后本地厂库占用的空间会比较大，二是万一系统崩溃的话还有可能找回来。\n修改pom.xml以下是我的pom.xml文件：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--suppress MavenModelInspection --&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;    &lt;artifactId&gt;SSM&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;source&gt;1.6&lt;/source&gt;                    &lt;target&gt;1.6&lt;/target&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;spring.version&gt;4.1.4.RELEASE&lt;/spring.version&gt;        &lt;jackson.version&gt;2.5.0&lt;/jackson.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;3.8.1&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;!-- spring --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;            &lt;artifactId&gt;spring-core&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;            &lt;artifactId&gt;spring-beans&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;            &lt;artifactId&gt;spring-context&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;            &lt;artifactId&gt;spring-tx&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;            &lt;artifactId&gt;spring-web&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;            &lt;artifactId&gt;spring-test&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;!-- 使用SpringMVC需配置 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;            &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- 关系型数据库整合时需配置 如hibernate jpa等 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework&lt;/groupId&gt;            &lt;artifactId&gt;spring-orm&lt;/artifactId&gt;            &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.mybatis&lt;/groupId&gt;            &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;            &lt;version&gt;1.2.4&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- log4j --&gt;        &lt;dependency&gt;            &lt;groupId&gt;log4j&lt;/groupId&gt;            &lt;artifactId&gt;log4j&lt;/artifactId&gt;            &lt;version&gt;1.2.17&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- mysql连接 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;5.1.34&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.mybatis&lt;/groupId&gt;            &lt;artifactId&gt;mybatis&lt;/artifactId&gt;            &lt;version&gt;3.3.1&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;druid&lt;/artifactId&gt;            &lt;version&gt;1.0.18&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- json --&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;            &lt;version&gt;1.2.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;            &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt;            &lt;version&gt;$&#123;jackson.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;            &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;            &lt;version&gt;$&#123;jackson.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;            &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;            &lt;version&gt;$&#123;jackson.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- aop --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.aspectj&lt;/groupId&gt;            &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt;            &lt;version&gt;1.8.4&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- servlet --&gt;        &lt;dependency&gt;            &lt;groupId&gt;javax.servlet&lt;/groupId&gt;            &lt;artifactId&gt;servlet-api&lt;/artifactId&gt;            &lt;version&gt;3.0-alpha-1&lt;/version&gt;            &lt;scope&gt;provided&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;javax.servlet&lt;/groupId&gt;            &lt;artifactId&gt;jstl&lt;/artifactId&gt;            &lt;version&gt;1.2&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;            &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;            &lt;version&gt;3.4&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- 上传文件 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;commons-fileupload&lt;/groupId&gt;            &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt;            &lt;version&gt;1.3.1&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/project&gt;\n关于maven的知识点我就不细讲了，毕竟这是一个整合教程。\n\nspring-mvc.xml这个配置文件是springMVC的配置文件：里面的我都写有注释，应该都能看懂。\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans                        http://www.springframework.org/schema/beans/spring-beans-3.1.xsd                        http://www.springframework.org/schema/context                        http://www.springframework.org/schema/context/spring-context-3.1.xsd                        http://www.springframework.org/schema/mvc                        http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd&quot;&gt;    &lt;!-- 自动扫描该包，使SpringMVC认为包下用了@controller注解的类是控制器 --&gt;    &lt;context:component-scan base-package=&quot;com.crossoverJie.controller&quot; /&gt;    &lt;!--避免IE执行AJAX时，返回JSON出现下载文件 --&gt;    &lt;!--&lt;bean id=&quot;mappingJacksonHttpMessageConverter&quot;          class=&quot;org.springframework.http.converter.json.MappingJacksonHttpMessageConverter&quot;&gt;        &lt;property name=&quot;supportedMediaTypes&quot;&gt;            &lt;list&gt;                &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt;            &lt;/list&gt;        &lt;/property&gt;    &lt;/bean&gt;--&gt;    &lt;mvc:annotation-driven/&gt;    &lt;!-- 启动SpringMVC的注解功能，完成请求和注解POJO的映射    --&gt;    &lt;!-- 定义跳转的文件的前后缀 ，视图模式配置--&gt;    &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt;        &lt;!-- 这里的配置我的理解是自动给后面action的方法return的字符串加上前缀和后缀，变成一个 可用的url地址 --&gt;        &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot; /&gt;        &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot; /&gt;    &lt;/bean&gt;    &lt;!-- 配置文件上传，如果没有使用文件上传可以不用配置，当然如果不配，那么配置文件中也不必引入上传组件包 --&gt;    &lt;bean id=&quot;multipartResolver&quot;          class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt;        &lt;!-- 默认编码 --&gt;        &lt;property name=&quot;defaultEncoding&quot; value=&quot;utf-8&quot; /&gt;        &lt;!-- 文件大小最大值 --&gt;        &lt;property name=&quot;maxUploadSize&quot; value=&quot;10485760000&quot; /&gt;        &lt;!-- 内存中的最大值 --&gt;        &lt;property name=&quot;maxInMemorySize&quot; value=&quot;40960&quot; /&gt;    &lt;/bean&gt;    &lt;!-- 配置拦截器 --&gt;    &lt;!--&lt;mvc:interceptors&gt;        &lt;mvc:interceptor&gt;            &amp;lt;!&amp;ndash; &lt;mvc:mapping path=&quot;/**&quot;/&gt;拦截所有 &amp;ndash;&amp;gt;            &lt;mvc:mapping path=&quot;/user/**&quot;/&gt;            &lt;mvc:mapping path=&quot;/role/**&quot;/&gt;            &lt;mvc:mapping path=&quot;/function/**&quot;/&gt;            &lt;mvc:mapping path=&quot;/news/**&quot;/&gt;            &lt;mvc:mapping path=&quot;/img/**&quot;/&gt;            &lt;bean class=&quot;com.crossoverJie.intercept.Intercept&quot;&gt;&lt;/bean&gt;        &lt;/mvc:interceptor&gt;    &lt;/mvc:interceptors&gt;--&gt;&lt;/beans&gt;\n关于上面拦截器注释掉的那里，配置是没有问题的，因为这是一个整合项目，所以里边也没有用到拦截器，为了防止运行报错所以就先注释掉了。如果后续需要增加拦截器，可以参考这里的配置。\n\nspring-mybatis.xml这个是spring和mybatis的整合配置文件，其中还有Druid连接池的配置。\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans                        http://www.springframework.org/schema/beans/spring-beans-3.1.xsd                        http://www.springframework.org/schema/context                        http://www.springframework.org/schema/context/spring-context-3.1.xsd&quot;&gt;    &lt;!-- 自动扫描 --&gt;    &lt;context:component-scan base-package=&quot;com.crossoverJie&quot; /&gt;    &lt;!-- 引入配置文件 --&gt;    &lt;bean id=&quot;propertyConfigurer&quot;          class=&quot;org.springframework.beans.factory.config.PropertyPlaceholderConfigurer&quot;&gt;        &lt;property name=&quot;location&quot; value=&quot;classpath:jdbc.properties&quot; /&gt;    &lt;/bean&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;          init-method=&quot;init&quot; destroy-method=&quot;close&quot;&gt;        &lt;!-- 指定连接数据库的驱动 --&gt;        &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driverClass&#125;&quot; /&gt;        &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot; /&gt;        &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.user&#125;&quot; /&gt;        &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot; /&gt;        &lt;!-- 配置初始化大小、最小、最大 --&gt;        &lt;property name=&quot;initialSize&quot; value=&quot;3&quot; /&gt;        &lt;property name=&quot;minIdle&quot; value=&quot;3&quot; /&gt;        &lt;property name=&quot;maxActive&quot; value=&quot;20&quot; /&gt;        &lt;!-- 配置获取连接等待超时的时间 --&gt;        &lt;property name=&quot;maxWait&quot; value=&quot;60000&quot; /&gt;        &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt;        &lt;property name=&quot;timeBetweenEvictionRunsMillis&quot; value=&quot;60000&quot; /&gt;        &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt;        &lt;property name=&quot;minEvictableIdleTimeMillis&quot; value=&quot;300000&quot; /&gt;        &lt;property name=&quot;validationQuery&quot; value=&quot;SELECT &#x27;x&#x27;&quot; /&gt;        &lt;property name=&quot;testWhileIdle&quot; value=&quot;true&quot; /&gt;        &lt;property name=&quot;testOnBorrow&quot; value=&quot;false&quot; /&gt;        &lt;property name=&quot;testOnReturn&quot; value=&quot;false&quot; /&gt;        &lt;!-- 打开PSCache，并且指定每个连接上PSCache的大小 --&gt;        &lt;property name=&quot;poolPreparedStatements&quot; value=&quot;true&quot; /&gt;        &lt;property name=&quot;maxPoolPreparedStatementPerConnectionSize&quot;                  value=&quot;20&quot; /&gt;        &lt;!-- 配置监控统计拦截的filters，去掉后监控界面sql无法统计 --&gt;        &lt;property name=&quot;filters&quot; value=&quot;stat&quot; /&gt;    &lt;/bean&gt;    &lt;!-- spring和MyBatis完美整合，不需要mybatis的配置映射文件 --&gt;    &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;        &lt;!-- 自动扫描mapping.xml文件 --&gt;        &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:mapping/*.xml&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;    &lt;!-- DAO接口所在包名，Spring会自动查找其下的类 --&gt;    &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt;        &lt;property name=&quot;basePackage&quot; value=&quot;com.crossoverJie.dao&quot; /&gt;        &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;    &lt;!-- (事务管理)transaction manager, use JtaTransactionManager for global tx --&gt;    &lt;bean id=&quot;transactionManager&quot;          class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;    &lt;/bean&gt;&lt;/beans&gt;\n以上两个就是最重要的配置文件了，只要其中的包名和配置文件中的名字一样就不会出问题。关于xxMpper.xml以及实体类的生成，我们可以借助mybatis-generator自动生成工具来生成，方便快捷。\n\nIDEA配置Tomcat关于Tomcat的下载与安装我这里就不多介绍了。按照下图选择：在name中为这个Tomcat输入一个名字。之后选择你本地Tomcat的目录点击Ok即可。点击apply和保存之后就返回首页即可看到Tomcat的标识。根据需要点击Run和Debug即可运行。\n运行结果如下：点击上图的2,3,4可看到不同用户的结果，如果你走到这一步，那么恭喜你整合成功。\n\n总结以上源码都在github上。项目地址：SSM欢迎拍砖。\n","categories":["SSM"],"tags":["Java","Spring","SpringMVC","Mybatis","IDEA"]},{"title":"SSM(十) 项目重构-互联网项目的Maven结构","url":"/2017/03/04/SSM10/","content":"\n前言很久没有更新博客了，之前定下周更逐渐成了月更。怎么感觉像我追过的一部动漫。这个博文其实很早就想写了。之前所有的代码都是在一个模块里面进行开发，这和maven的理念是完全不相符的，最近硬是抽了一个时间来对项目的结构进行了一次重构。\n\n先来看看这次重构之后的目录结构\n\n\n\n\n为什么需要分模块\n至于为什么要分模块呢？\n\n我们设想一个这样的场景：在现在的互联网开发中，会把一个很大的系统拆分成各个子系统用于降低他们之间的耦合度。\n在一个子项目中通常都会为API、WEB、Service等模块。而且当项目够大时，这些通常都不是一个人能完成的工作，需要一个团队来各司其职。\n想象一下：当之前所有的项目都在一个模块的时候，A改动了API，需要Deploy代码。而B也改动了service的代码，但并没有完全做完。所以A在提交build的时候就会报错\n而且在整个项目足够大的时候，这个build的时间也是很影响效率的。\n但让我将各个模块之间分开之后效果就不一样了。我修改了API我就只需要管我的就行，不需要整个项目进行build。\n而且当有其他项目需要依赖我这个API的时候也只需要依赖API即可，不用整个项目都依赖过去。\n各个模块的作用来看下这次我所分的模块。\nROOT这是整个项目的根节点。先看一下其中的pom.xml：\n&lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;    &lt;artifactId&gt;SSM&lt;/artifactId&gt;    &lt;packaging&gt;pom&lt;/packaging&gt;    &lt;version&gt;2.0.0&lt;/version&gt;    &lt;modules&gt;        &lt;module&gt;SSM-API&lt;/module&gt;        &lt;module&gt;SSM-BOOT&lt;/module&gt;        &lt;module&gt;SSM-SERVICE&lt;/module&gt;        &lt;module&gt;SSM-WEB&lt;/module&gt;    &lt;/modules&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;spring.version&gt;4.1.4.RELEASE&lt;/spring.version&gt;        &lt;jackson.version&gt;2.5.0&lt;/jackson.version&gt;        &lt;lucene.version&gt;6.0.1&lt;/lucene.version&gt;    &lt;/properties&gt;    &lt;dependencyManagement&gt;        &lt;dependencies&gt;            &lt;dependency&gt;                &lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;                &lt;artifactId&gt;SSM-API&lt;/artifactId&gt;                &lt;version&gt;2.0.0&lt;/version&gt;            &lt;/dependency&gt;         &lt;/dependencies&gt;    &lt;/dependencyManagement&gt;\n\n我截取了其中比较重点的配置。\n由于这是父节点，所以我的packag类型使用的是pom。其中分别有着四个子模块。\n其中重点看下&lt;dependencyManagement&gt;这个标签。如果使用的是IDEA这个开发工具的话是可以看到如下图：\n标红的有一个向下的箭头，点一下就可以进入子模块中相同的依赖。这样子模块就不需要配置具体的版本了，统一由父模块来进行维护，对之后的版本升级也带来了好处。\nSSM-API接下来看下API这个模块：\n通常这个模块都是用于定义外部接口的，以及改接口所依赖的一些DTO类。一般这个模块都是拿来给其他项目进行依赖，并和本项目进行数据交互的。\nSSM-BOOTBOOT这个模块比较特殊。可以看到这里没有任何代码，只有一个rpc的配置文件。通常这个模块是用于给我们内部项目进行依赖的，并不像上面的API模块一样给其他部门或者是项目进行依赖的。\n因为在我们的RPC调用的时候，用dubbo来举例，是需要配置所依赖的consumer。\n但如果是我们自己内部调用的话我们就可以把需要调用自己的dubbo服务提供者配置在这里，这样的话我们自己调用就只需要依赖这个BOOT就可以进行调用了。\n哦对了，BOOT同时还会依赖API，这样才实现了只依赖BOOT就可以调用自己内部的dubbo服务了。如下所示：\n&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;junit&lt;/groupId&gt;        &lt;artifactId&gt;junit&lt;/artifactId&gt;        &lt;scope&gt;test&lt;/scope&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;        &lt;artifactId&gt;SSM-API&lt;/artifactId&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;\n\nSSM-SERVICESERVICE模块就比较好理解了。是处理具体业务逻辑的地方，也是对之前的API的实现。\n通常这也是一个web模块，所以我的pom类型是WAR。\nSSM-WEB其实WEB模块和SERVICE模块有点重合了。通常来说这个模块一般在一个对外提供http访问接口的项目中。\n这里只是为了展示项目结构，所以也写在了这里。\n他的作用和service差不多，都是WAR的类型。\n总结这次没有实现什么特别的功能，只是对一些还没有接触过这种项目结构开发的童鞋能起到一些引导作用。\n具体源码还请关注我的github。\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客地址：http://crossoverjie.top。\n\n\nGitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["Maven","重构"]},{"title":"SSM(十一) 基于dubbo的分布式架构","url":"/2017/04/07/SSM11/","content":"\n前言现在越来越多的互联网公司还是将自己公司的项目进行服务化，这确实是今后项目开发的一个趋势，就这个点再凭借之前的SSM项目来让第一次接触的同学能快速上手。\n浅谈分布式架构分布式架构单看这个名字给人的感觉就是高逼格，但其实从历史的角度来分析一下就比较明了了。\n\n我们拿一个电商系统来说：\n\n单系统对于一个刚起步的创业公司项目肯定是追求越快完成功能越好，并且用户量也不大。\n\n\n这时候所有的业务逻辑都是在一个项目中就可以满足。\n垂直拆分-多应用当业务量和用户量发展到一定地步的时候，这时一般会将应用同时部署到几台服务器上，在用户访问的时候使用Nginx进行反向代理和简单的负载均衡。\nSOA服务化当整个系统以及发展的足够大的时候，比如一个电商系统中存在有：\n\n用户系统\n订单系统\n支付系统\n物流系统\n\n等系统。如果每次修改了其中一个系统就要重新发布上线的话那么耦合就太严重了。\n\n所以需要将整个项目拆分成若干个独立的应用，可以进行独立的开发上线实现快速迭代。\n\n\n\n如上图所示每个应用之间相互独立,每个应用可以消费其他应用暴露出来的服务，同时也对外提供服务。\n\n从架构的层面简单的理解了，接下来看看如何编码实现。\n基于dubbo的实现dubbo应该算是国内使用最多的分布式服务框架，基于此来实现对新入门的同学应该很有帮助。\n\n其中有涉及到安装dubbo服务的注册中心zookeeper等相关知识点可以自行查看官方文档，这里就不单独讲了。\n\n对外提供服务首先第一步需要在SSM-API模块中定义一个接口，这里就搞了一个用户查询的接口\n/** * Function:用户API * @author chenjiec * Date: 2017/4/4 下午9:46 * @since JDK 1.7 */public interface UserInfoApi &#123;    /**     * 获取用户信息     * @param userId     * @return     * @throws Exception     */    public UserInfoRsp getUserInfo(int userId) throws Exception;&#125;\n\n接着在SSM-SERVICE模块中进行实现：\nimport com.alibaba.dubbo.config.annotation.Service;/** * Function: * @author chenjiec * Date: 2017/4/4 下午9:51 * @since JDK 1.7 */@Servicepublic class UserInfoApiImpl implements UserInfoApi &#123;    private static Logger logger = LoggerFactory.getLogger(UserInfoApiImpl.class);    @Autowired    private T_userService t_userService ;    /**     * 获取用户信息     *     * @param userId     * @return     * @throws Exception     */    @Override    public UserInfoRsp getUserInfo(int userId) throws Exception &#123;        logger.info(&quot;用户查询Id=&quot;+userId);        //返回对象        UserInfoRsp userInfoRsp = new UserInfoRsp() ;        T_user t_user = t_userService.selectByPrimaryKey(userId) ;        //构建        buildUserInfoRsp(userInfoRsp,t_user) ;        return userInfoRsp;    &#125;    /**     * 构建返回     * @param userInfoRsp     * @param t_user     */    private void buildUserInfoRsp(UserInfoRsp userInfoRsp, T_user t_user) &#123;        if (t_user ==  null)&#123;            t_user = new T_user() ;        &#125;        CommonUtil.setLogValueModelToModel(t_user,userInfoRsp);    &#125;&#125;\n这些都是通用的代码，但值得注意的一点是这里使用的dubbo框架所提供的@service注解。作用是声明需要暴露的服务接口。\n再之后就是几个dubbo相关的配置文件了。\nspring-dubbo-config.xml&lt;dubbo:application name=&quot;ssm-service&quot; owner=&quot;crossoverJie&quot;\torganization=&quot;ssm-crossoverJie&quot; logger=&quot;slf4j&quot;/&gt;&lt;dubbo:registry id=&quot;dubbo-registry&quot; address=&quot;zookeeper://192.168.0.188:2181&quot;\tfile=&quot;/tmp/dubbo.cachr&quot; /&gt;&lt;dubbo:monitor protocol=&quot;registry&quot; /&gt;&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot; /&gt;&lt;dubbo:provider timeout=&quot;15000&quot; retries=&quot;0&quot; delay=&quot;-1&quot; /&gt;&lt;dubbo:consumer check=&quot;false&quot; timeout=&quot;15000&quot; /&gt;\n其实就是配置我们服务注册的zk地址，以及服务名称、超时时间等配置。\nspring-dubbo-provider.xml&lt;dubbo:annotation package=&quot;com.crossoverJie.api.impl&quot; /&gt;\n这个配置扫描注解包的位置，一般配置到接口实现包即可。\nspring-dubbo-consumer.xml这个是消费者配置项，表明我们需要依赖的其他应用。这里我们在SSM-BOOT项目中进行配置：\n&lt;dubbo:reference id=&quot;userInfoApi&quot;\t\tinterface=&quot;com.crossoverJie.api.UserInfoApi&quot; /&gt;\n直接就是配置的刚才我们提供的那个用户查询的接口，这样当我们自己的内部项目需要使用到这个服务只需要依赖SSM-BOOT即可，不需要单独的再去配置consumer。这个我有在上一篇SSM(十) 项目重构-互联网项目的Maven结构中也有提到。\n安装管理控制台还有一个需要做的就是安装管理控制台，这里可以看到我们有多少服务、调用情况是怎么样等作用。\n这里我们可以将dubbo的官方源码下载下来，对其中的dubbo-admin模块进行打包，将生成的WAR包放到Tomcat中运行起来即可。\n但是需要注意一点的是：需要将其中的dubbo.properties的zk地址修改为自己的即可。\ndubbo.registry.address=zookeeper://127.0.0.1:2181dubbo.admin.root.password=rootdubbo.admin.guest.password=guest\n到时候登陆的话使用root，密码也是root。使用guest，密码也是guest。\n登陆界面如下图：\n其中我们可以看到有两个服务以及注册上去了，但是没有消费者。\n消费服务为了能够更直观的体验到消费服务，我新建了一个项目：https://github.com/crossoverJie/SSM-CONSUMER。\n其中在SSM-CONSUMER-API中我也定义了一个接口：\n/** * Function:薪资API * @author chenjiec * Date: 2017/4/4 下午9:46 * @since JDK 1.7 */public interface SalaryInfoApi &#123;    /**     * 获取薪资     * @param userId     * @return     * @throws Exception     */    public SalaryInfoRsp getSalaryInfo(int userId) throws Exception;&#125;\n因为作为消费者的同时我们也对外提供了一个获取薪资的一个服务。\n在SSM-CONSUMER-SERVICE模块中进行了实现：\n/** * Function: * @author chenjiec * Date: 2017/4/4 下午9:51 * @since JDK 1.7 */@Servicepublic class SalaryInfoApiImpl implements SalaryInfoApi &#123;    private static Logger logger = LoggerFactory.getLogger(SalaryInfoApiImpl.class);    @Autowired    UserInfoApi userInfoApi ;    /**     * 获取用户信息     *     * @param userId     * @return     * @throws Exception     */    @Override    public SalaryInfoRsp getSalaryInfo(int userId) throws Exception &#123;        logger.info(&quot;薪资查询Id=&quot;+userId);        //返回对象        SalaryInfoRsp salaryInfoRsp = new SalaryInfoRsp() ;                //调用远程服务        UserInfoRsp userInfo = userInfoApi.getUserInfo(userId);                salaryInfoRsp.setUsername(userInfo.getUserName());        return salaryInfoRsp;    &#125;&#125;\n其中就可以直接使用userInfoApi调用之前的个人信息服务。\n再调用之前需要注意的有点是，我们只需要依赖SSM-BOOT这个模块即可进行调用，因为SSM-BOOT模块已经为我们配置了消费者之类的操作了：\n&lt;dependency&gt;    &lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;    &lt;artifactId&gt;SSM-BOOT&lt;/artifactId&gt;&lt;/dependency&gt;\n\n还有一点是在配置SSM-BOOT中的spring-dubbo-cosumer.xml配置文件的时候，路径要和我们初始化spring配置文件时的路径一致：\n&lt;!-- Spring和mybatis的配置文件 --&gt;&lt;context-param&gt;    &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;    &lt;param-value&gt;classpath*:spring/*.xml&lt;/param-value&gt;&lt;/context-param&gt;\n\n接下来跑个单测试一下能否调通：\n/** * Function: * * @author chenjiec *         Date: 2017/4/5 下午10:41 * @since JDK 1.7 */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &#123; &quot;classpath*:/spring/*.xml&quot; &#125;)public class SalaryInfoApiImplTest &#123;    @Autowired    private SalaryInfoApi salaryInfoApi ;    @Test    public void getSalaryInfo() throws Exception &#123;        SalaryInfoRsp salaryInfo = salaryInfoApi.getSalaryInfo(1);        System.out.println(JSON.toJSONString(salaryInfo));    &#125;&#125;\n消费者\n提供者可以看到确实是调用成功了的。\n接下来将消费者项目也同时启动在来观察管理控制台有什么不一样：会看到多了一个消费者所提供的服务com.crossoverjie.consumer.api.SalaryInfoApi,同时com.crossoverJie.api.UserInfoApi服务已经正常，说明已经有消费者了。\n点进去便可查看具体的消费者。\n总结这样一个基于dubbo的分布式服务已经讲的差不多了，在实际的开发中我们便会开发一个大系统中的某一个子应用，这样就算一个子应用出问题了也不会影响到整个大的项目。\n再提一点：在实际的生产环境一般同一个服务我们都会有一个master,slave的主从服务，这样在上线的过程中不至于整个应用出现无法使用的尴尬情况。\n谈到了SOA的好处，那么自然也有相对于传统模式的不方便之处：\n\n拆分一个大的项目为成百上千的子应用就不可能手动上线了，即需要自动化的部署上线，如Jenkins。\n还有一个需要做到的就是监控，需要一个单独的监控平台来帮我们实时查看各个服务的运行情况以便于及时定位和解决问题。\n日志查看分析，拆分之后不可能再去每台服务器上查看日志，需要一个单独的日志查看分析工具如elk。\n\n以上就是我理解的，如有差错欢迎指正。\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客地址：http://crossoverjie.top。\n\n\nGitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["dubbo","分布式"]},{"title":"SSM(十二) dubbo日志插件","url":"/2017/04/25/SSM12/","content":"\n前言在之前dubbo分布式框架中讲到了如何利用dubbo来搭建一个微服务项目。其中还有一些值得优化提高开发效率的地方，比如日志：\n\n当我们一个项目拆分为N多个微服务之后，当其中一个调用另一个服务出现了问题，首先第一步自然是查看日志。 \n\n\n出现问题的有很多情况，如提供方自身代码的问题，调用方的姿势不对等。\n\n\n自身的问题这个管不了，但是我们可以对每一个入参、返回都加上日志，这样首先就可以判断调用方是否姿势不对了。\n\n\n为了规范日志已经后续的可扩展，我们可以单独提供一个插件给每个项目使用即可。\n\n效果如下：\n2017-04-25 15:15:38,968 DEBUG [com.alibaba.dubbo.remoting.transport.DecodeHandler] -  [DUBBO] Decode decodeable message com.alibaba.dubbo.rpc.protocol.dubbo.DecodeableRpcInvocation, dubbo version: 2.5.3, current host: 127.0.0.12017-04-25 15:15:39,484 DEBUG [com.crossoverJie.dubbo.filter.DubboTraceFilter] - dubbo请求数据:&#123;&quot;args&quot;:[1],&quot;interfaceName&quot;:&quot;com.crossoverJie.api.UserInfoApi&quot;,&quot;methodName&quot;:&quot;getUserInfo&quot;&#125;2017-04-25 15:15:39,484 INFO [com.crossoverJie.api.impl.UserInfoApiImpl] - 用户查询Id=12017-04-25 15:15:39,505 DEBUG [org.mybatis.spring.SqlSessionUtils] - Creating a new SqlSession2017-04-25 15:15:39,525 DEBUG [org.mybatis.spring.SqlSessionUtils] - SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@6f56b29] was not registered for synchronization because synchronization is not active2017-04-25 15:15:39,549 DEBUG [org.mybatis.spring.transaction.SpringManagedTransaction] - JDBC Connection [com.alibaba.druid.proxy.jdbc.ConnectionProxyImpl@778b3121] will not be managed by Spring2017-04-25 15:15:39,555 DEBUG [com.crossoverJie.api.dubbo.dao.T_userDao.selectByPrimaryKey] - ==&gt;  Preparing: select id, username, password,roleId from t_user where id = ? 2017-04-25 15:15:39,591 DEBUG [com.crossoverJie.api.dubbo.dao.T_userDao.selectByPrimaryKey] - ==&gt; Parameters: 1(Integer)2017-04-25 15:15:39,616 DEBUG [com.crossoverJie.api.dubbo.dao.T_userDao.selectByPrimaryKey] - &lt;==      Total: 12017-04-25 15:15:39,616 DEBUG [com.alibaba.druid.pool.PreparedStatementPool] - &#123;conn-10003, pstmt-20000&#125; enter cache2017-04-25 15:15:39,617 DEBUG [org.mybatis.spring.SqlSessionUtils] - Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@6f56b29]2017-04-25 15:15:45,473 INFO [com.crossoverJie.dubbo.filter.DubboTraceFilter] - dubbo执行成功2017-04-25 15:15:45,476 DEBUG [com.crossoverJie.dubbo.filter.DubboTraceFilter] - dubbo返回数据&#123;&quot;args&quot;:[&#123;&quot;id&quot;:1,&quot;password&quot;:&quot;123456&quot;,&quot;roleId&quot;:1,&quot;userName&quot;:&quot;crossoverJie&quot;&#125;],&quot;interfaceName&quot;:&quot;com.crossoverJie.api.UserInfoApi&quot;,&quot;methodName&quot;:&quot;getUserInfo&quot;&#125;\n\n\n\ndubbo filter拓展参考官方文档，我们可以通过com.alibaba.dubbo.rpc.Filter进行拓展。\n定义实体首先定义一个实体类用于保存调用过程中的一些数据：\npublic class FilterDesc &#123;    private String interfaceName ;//接口名    private String methodName ;//方法名    private Object[] args ;//参数    //省略getter setter&#125;\n\nDubboTraceFilter具体拦截逻辑@Activate(group = Constants.PROVIDER, order = -999)public class DubboTraceFilter implements Filter&#123;    private static final Logger logger = LoggerFactory.getLogger(DubboTraceFilter.class);    public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123;        try &#123;            FilterDesc filterReq = new FilterDesc() ;            filterReq.setInterfaceName(invocation.getInvoker().getInterface().getName());            filterReq.setMethodName(invocation.getMethodName()) ;            filterReq.setArgs(invocation.getArguments());            logger.debug(&quot;dubbo请求数据:&quot;+JSON.toJSONString(filterReq));            Result result = invoker.invoke(invocation);            if (result.hasException() &amp;&amp; invoker.getInterface() != GenericService.class)&#123;                logger.error(&quot;dubbo执行异常&quot;,result.getException());            &#125;else &#123;                logger.info(&quot;dubbo执行成功&quot;);                FilterDesc filterRsp = new FilterDesc() ;                filterRsp.setMethodName(invocation.getMethodName());                filterRsp.setInterfaceName(invocation.getInvoker().getInterface().getName());                filterRsp.setArgs(new Object[]&#123;result.getValue()&#125;);                logger.debug(&quot;dubbo返回数据&quot;+JSON.toJSONString(filterRsp));            &#125;            return result ;        &#125;catch (RuntimeException e)&#123;            logger.error(&quot;dubbo未知异常&quot; + RpcContext.getContext().getRemoteHost()                    + &quot;. service: &quot; + invoker.getInterface().getName() + &quot;, method: &quot; + invocation.getMethodName()                    + &quot;, exception: &quot; + e.getClass().getName() + &quot;: &quot; + e.getMessage(), e);            throw e ;        &#125;    &#125;&#125;\n逻辑非常简单，只是对调用过程、异常、成功之后打印相应的日志而已。\n但是有个地方要注意一下：需要在resource目录下加上META-INF.dubbo/com.alibaba.dubbo.rpc.Filter文件。\ndubboTraceFilter=com.crossoverJie.dubbo.filter.DubboTraceFilter\n目录结构如下：\nsrc |-main    |-java        |-com            |-xxx                |-XxxFilter.java (实现Filter接口)    |-resources        |-META-INF            |-dubbo                |-com.alibaba.dubbo.rpc.Filter (纯文本文件，内容为：xxx=com.xxx.XxxFilter)\n\n\n\n总结该项目已经托管到GitHub：https://github.com/crossoverJie/SSM-DUBBO-FILTER\n使用方法安装cd /SSM-DUBBO-FILTER\n\nmvn clean\n\nmvn install\n\n使用在服务提供的项目中加上依赖，这样每次调用都会打上日志。\n&lt;dependency&gt;    &lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;    &lt;artifactId&gt;SSM-TRACE-FILTER&lt;/artifactId&gt;    &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt;\n\n在拦截器中最好不要加上一些耗时任务，需要考虑到性能问题。\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客地址：http://crossoverjie.top。\n\n\nGitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["dubbo","日志"]},{"title":"SSM(十四) 基于annotation的http防重插件","url":"/2017/05/24/SSM14/","content":"\n前言针对于我们现在常用的RESTful API通常我们需要对请求进行唯一标识，也就是每次都要带上一个请求号,如reqNO。\n对于入库这种操作数据库的请求我们一般要保证他的唯一性，一个请求号通常只能用一次，所以需要我们对这种请求加上校验机制。\n\n该需求的实现思路是通过自定义annotation，只给需要进行校验的接口加上注解。然后通过切面使用了注解的接口将每次请求号存进Redis，每次都进行判断是否存在这个请求号即可。\n\n来看下加上本次插件的实际效果：\n\n\n自定义注解首先我们要自定义一个注解：\n@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface CheckReqNo &#123;    String desc() default &quot;&quot;;&#125;\n(ps:这里并不过多的讲解注解相关的知识)。\n首先使用@interface来声明一个注解。接着利用Java为我们提供的三个元注解来定义CheckReqNo注解。\n其中@Target表明这个注解被用于什么地方，使用ElementType.METHOD表明被应用到方法上，还有一些其他值可以查看java.lang.annotation.ElementType这个枚举类型。\n@Retention注解表明我们的注解在什么范围内有效，这里配置的RetentionPolicy.RUNTIME表明在运行时可以通过反射来获取。\n@Documented看字面意思应该也能猜到是用于生成JavaDoc文档的。\n其中定义了一个desc()的方法其实并没有用到，但如果需要在使用注解的时候需要自定义一些filed(域)的需求可以按照这样的方式写到这里，通过反射都可以获取到具体的值。如： @CheckReqNo(desc = &quot;abc&quot;)就可以获取到&quot;abc&quot;的值。\n切面注解按照之前的想法是在对所有使用了该注解的方法进行切面：\n@Aspect@Componentpublic class ReqNoDrcAspect &#123;\tprivate static Logger logger = LoggerFactory.getLogger(ReqNoDrcAspect.class);\t@Value(&quot;$&#123;redis.prefixReq:reqNo&#125;&quot;)\tprivate String prefixReq ;\t@Value(&quot;$&#123;redis.day:1&#125;&quot;)\tprivate long day ;\t@Autowired\tprivate RedisTemplate&lt;String, String&gt; redisTemplate;\t\t@PostConstruct\tpublic void init() throws Exception &#123;\t\tlogger.info(&quot;SSM-REQUEST-CHECK init......&quot;);\t&#125;\t\t@Pointcut(&quot;@annotation(com.crossoverJie.request.anotation.CheckReqNo)&quot;)\tpublic void checkRepeat()&#123;\t\t\t&#125;\t@Before(&quot;checkRepeat()&quot;)\tpublic void before(JoinPoint joinPoint) throws Exception &#123;\t\tBaseRequest request;\t\trequest = getBaseRequest(joinPoint);\t\tif(request != null)&#123;\t\t\tfinal String reqNo = request.getReqNo();\t\t\tif(StringUtil.isEmpty(reqNo))&#123;\t\t\t\tthrow new RuntimeException(&quot;reqNo不能为空&quot;);\t\t\t&#125;else&#123;\t\t\t\ttry &#123;\t\t\t\t\tString tempReqNo = redisTemplate.opsForValue().get(prefixReq +reqNo);\t\t\t\t\tlogger.debug(&quot;tempReqNo=&quot;+tempReqNo);\t\t\t\t\tif((StringUtil.isEmpty(tempReqNo)))&#123;\t\t\t\t\t\tredisTemplate.opsForValue().set(prefixReq + reqNo, reqNo, day, TimeUnit.DAYS);\t\t\t\t\t&#125;else&#123;\t\t\t\t\t\tthrow new RuntimeException(&quot;请求号重复,reqNo=&quot;+reqNo);\t\t\t\t\t&#125;\t\t\t\t&#125; catch (RedisConnectionFailureException e)&#123;\t\t\t\t\tlogger.error(&quot;redis操作异常&quot;,e);\t\t\t\t\tthrow new RuntimeException(&quot;need redisService&quot;) ;\t\t\t\t&#125;\t\t\t&#125;\t\t&#125;\t\t\t\t&#125;\t\t\t \t public static BaseRequest getBaseRequest(JoinPoint joinPoint) throws Exception &#123;\t\t BaseRequest returnRequest = null;\t\t Object[] arguments = joinPoint.getArgs();\t\t if(arguments != null &amp;&amp; arguments.length &gt; 0)&#123;\t\t\t returnRequest = (BaseRequest) arguments[0];\t\t &#125;\t     return returnRequest;\t &#125;&#125;\n\n使用@Aspect来定义了一个切面。其中prefixReq,day域可以自定义缓存请求号时的key前缀以及缓存的时间。\n最关键的一点是用@Pointcut(&quot;@annotation(com.crossoverJie.request.anotation.CheckReqNo)&quot;)定义了一个切入点，这样所有使用@CheckReqNo的注解都会被拦截。\n接下来的逻辑就比较简单了，在每次请求之前进行拦截。\n先去Redis中查看这个请求号(ps:反射获取)是否存在，如果不存在则通过并将本次的请求号缓存起来。如果存在则抛出异常。\n使用注解可以在jdbc.properties配置文件中自定义前缀和缓存时间\n#redis前缀redis.prefixReq=reqNo#redis缓存时间 默认单位为天redis.day=1\n不定义也可以，会使用默认值。\n由于该注解是需要加到controller层,因此我们得使用CGLIB代理。这里有一个坑，需要将开启CGLIB 的配置配置到我们web.xml中的\n&lt;!-- Spring MVC servlet --&gt;    &lt;servlet&gt;        &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt;        &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;            &lt;param-value&gt;classpath:spring-mvc.xml&lt;/param-value&gt;        &lt;/init-param&gt;        &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;        &lt;async-supported&gt;true&lt;/async-supported&gt;    &lt;/servlet&gt;\n这里所定义的spring-mvc.xml文件中，不然springMVC所在的子容器是无法被父容器所加载的。\n使用实例：\n@CheckReqNo@RequestMapping(value = &quot;/createRedisContent&quot;,method = RequestMethod.POST)@ResponseBodypublic BaseResponse&lt;NULLBody&gt; createRedisContent(@RequestBody RedisContentReq redisContentReq)&#123;    BaseResponse&lt;NULLBody&gt; response = new BaseResponse&lt;NULLBody&gt;() ;    Rediscontent rediscontent = new Rediscontent() ;    try &#123;        CommonUtil.setLogValueModelToModel(redisContentReq,rediscontent);        rediscontentMapper.insertSelective(rediscontent) ;        response.setReqNo(redisContentReq.getReqNo());        response.setCode(StatusEnum.SUCCESS.getCode());        response.setMessage(StatusEnum.SUCCESS.getMessage());    &#125;catch (Exception e)&#123;        logger.error(&quot;system error&quot;,e);        response.setReqNo(response.getReqNo());        response.setCode(StatusEnum.FAIL.getCode());        response.setMessage(StatusEnum.FAIL.getMessage());    &#125;    return response ;&#125;\n\n统一异常controller/** * * ClassName: ErrorController &lt;br/&gt; * Function: 错误异常统一处理. &lt;br/&gt; * @author crossoverJie * @version * @since JDK 1.7 */@ControllerAdvicepublic class ErrorController &#123;    private Logger logger = LoggerFactory.getLogger(this.getClass());    @ExceptionHandler(Exception.class)    @ResponseStatus(HttpStatus.OK)    @ResponseBody    public Object processUnauthenticatedException(NativeWebRequest request, Exception e) &#123;        logger.error(&quot;请求出现异常:&quot;, e);        BaseResponse&lt;NULLBody&gt; response = new BaseResponse&lt;NULLBody&gt;();        response.setCode(StatusEnum.FAIL.getCode());        if (e instanceof RuntimeException)&#123;            response.setMessage(e.getMessage());        &#125; else &#123;            response.setMessage(StatusEnum.FAIL.getMessage());        &#125;        return response ;    &#125;&#125;\n\n这样当controller层出现异常之后都会进入这里进行统一的返回。\n总结至此整个插件的流程已经全部OK，从中可以看出Spring AOP在实际开发中的各种好处。之前的几篇文章也有应用到：\n\n在JavaWeb应用中使用Redis\n动态切换数据源\n\n不知不觉这个小白入门的SSM系列已经更新了14篇了，在GitHub也有了500多颗星了，期间也和不少朋友有过交流、探讨，感谢大家的支持。\n接下来可能不太会更新这个系列了，由于博主现在所在的项目组采用的是目前比较流行的SpringBoot+SpringCloud和Docker的方式来进行架构的，所以之后的重心肯定会移到这方面，用过SpringBoot之后相信大家肯定也回不去了。\n所以之后我会继续更新SpringBoot+SpringCloud相关的文章，欢迎持续关注，持续拍砖(ps:这个插件也会用springBoot重写一遍)\n\n插件地址：https://github.com/crossoverJie/SSM-REQUEST-CHECK.git\n\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客地址：http://crossoverjie.top。\n\n","categories":["SSM"],"tags":["HTTP","annotation","AOP"]},{"title":"SSM(十三) 将dubbo暴露出HTTP服务","url":"/2017/05/07/SSM13/","content":"\n前言通常来说一个dubbo服务都是对内给内部调用的，但也有可能一个服务就是需要提供给外部使用，并且还不能有使用语言的局限性。\n比较标准的做法是对外的服务我们统一提供一个openAPI，这样的调用方需要按照标准提供相应的appID以及密钥来进行验签才能使用。这样固然是比较规范和安全，但复杂度也不亚于开发一个单独的系统了。\n这里所讲到的没有那么复杂，就只是把一个不需要各种权限检验的dubbo服务对外提供为HTTP服务。\n调用示例:\n\n\n准备工作以下是本文所涉及到的一些知识点：\n\nSpring相关知识。\nJava反射相关知识。\nSpringMVC相关知识。\n\n\n其实思路很简单，就是利用SpringMVC提供一个HTTP接口。在该接口中通过入参进行反射找到具体的dubbo服务实现进行调用。\n\nHttpProviderConf配置类首先需要定义一个HttpProviderConf类用于保存声明需要对外提供服务的包名，毕竟我们反射时需要用到一个类的全限定名：\npublic class HttpProviderConf &#123;    /**     * 提供http访问的包     */    private List&lt;String&gt; usePackage ;    //省略getter setter方法&#125;\n就只有一个usePackage成员变量，用于存放需要包名。至于用List的原因是允许有多个。\n请求响应入参、出参HttpRequest入参public class HttpRequest &#123;    private String param ;//入参    private String service ;//请求service    private String method ;//请求方法    //省略getter setter方法&#125;\n其中param是用于存放真正调用dubbo服务时的入参，传入json在调用的时候解析成具体的参数对象。\nservice存放dubbo服务声明的interface API的包名。\nmethod则是真正调用的方法名称。\nHttpResponse 响应public class HttpResponse implements Serializable&#123;    private static final long serialVersionUID = -552828440320737814L;    private boolean success;//成功标志    private String code;//信息码    private String description;//描述    //省略getter setter方法&#125;\n\n这里只是封装了常用的HTTP服务的响应数据。\n暴露服务controller最重要的则是controller里的实现代码了。\n先贴代码：\n@Controller@RequestMapping(&quot;/dubboAPI&quot;)public class DubboController implements ApplicationContextAware&#123;    private final static Logger logger = LoggerFactory.getLogger(DubboController.class);    @Autowired    private HttpProviderConf httpProviderConf;    //缓存作用的map    private final Map&lt;String, Class&lt;?&gt;&gt; cacheMap = new HashMap&lt;String, Class&lt;?&gt;&gt;();    protected ApplicationContext applicationContext;    @ResponseBody    @RequestMapping(value = &quot;/&#123;service&#125;/&#123;method&#125;&quot;,method = RequestMethod.POST)    public String api(HttpRequest httpRequest, HttpServletRequest request,                      @PathVariable String service,                      @PathVariable String method) &#123;        logger.debug(&quot;ip:&#123;&#125;-httpRequest:&#123;&#125;&quot;,getIP(request), JSON.toJSONString(httpRequest));        String invoke = invoke(httpRequest, service, method);        logger.debug(&quot;callback :&quot;+invoke) ;        return invoke ;    &#125;    private String invoke(HttpRequest httpRequest,String service,String method)&#123;        httpRequest.setService(service);        httpRequest.setMethod(method);        HttpResponse response = new HttpResponse() ;        logger.debug(&quot;input param:&quot;+JSON.toJSONString(httpRequest));        if (!CollectionUtils.isEmpty(httpProviderConf.getUsePackage()))&#123;            boolean isPac = false ;            for (String pac : httpProviderConf.getUsePackage()) &#123;                if (service.startsWith(pac))&#123;                    isPac = true ;                    break ;                &#125;            &#125;            if (!isPac)&#123;                //调用的是未经配置的包                logger.error(&quot;service is not correct,service=&quot;+service);                response.setCode(&quot;2&quot;);                response.setSuccess(false);                response.setDescription(&quot;service is not correct,service=&quot;+service);            &#125;        &#125;        try &#123;            Class&lt;?&gt; serviceCla = cacheMap.get(service);            if (serviceCla == null)&#123;                serviceCla = Class.forName(service) ;                logger.debug(&quot;serviceCla:&quot;+JSON.toJSONString(serviceCla));                //设置缓存                cacheMap.put(service,serviceCla) ;            &#125;            Method[] methods = serviceCla.getMethods();            Method targetMethod = null ;            for (Method m : methods) &#123;                if (m.getName().equals(method))&#123;                    targetMethod = m ;                    break ;                &#125;            &#125;            if (method == null)&#123;                logger.error(&quot;method is not correct,method=&quot;+method);                response.setCode(&quot;2&quot;);                response.setSuccess(false);                response.setDescription(&quot;method is not correct,method=&quot;+method);            &#125;            Object bean = this.applicationContext.getBean(serviceCla);            Object result = null ;            Class&lt;?&gt;[] parameterTypes = targetMethod.getParameterTypes();            if (parameterTypes.length == 0)&#123;                //没有参数                result = targetMethod.invoke(bean);            &#125;else if (parameterTypes.length == 1)&#123;                Object json = JSON.parseObject(httpRequest.getParam(), parameterTypes[0]);                result = targetMethod.invoke(bean,json) ;            &#125;else &#123;                logger.error(&quot;Can only have one parameter&quot;);                response.setSuccess(false);                response.setCode(&quot;2&quot;);                response.setDescription(&quot;Can only have one parameter&quot;);            &#125;            return JSON.toJSONString(result) ;        &#125;catch (ClassNotFoundException e)&#123;            logger.error(&quot;class not found&quot;,e);            response.setSuccess(false);            response.setCode(&quot;2&quot;);            response.setDescription(&quot;class not found&quot;);        &#125; catch (InvocationTargetException e) &#123;            logger.error(&quot;InvocationTargetException&quot;,e);            response.setSuccess(false);            response.setCode(&quot;2&quot;);            response.setDescription(&quot;InvocationTargetException&quot;);        &#125; catch (IllegalAccessException e) &#123;            logger.error(&quot;IllegalAccessException&quot;,e);            response.setSuccess(false);            response.setCode(&quot;2&quot;);            response.setDescription(&quot;IllegalAccessException&quot;);        &#125;        return JSON.toJSONString(response) ;    &#125;    /**     * 获取IP     * @param request     * @return     */    private String getIP(HttpServletRequest request) &#123;        if (request == null)            return null;        String s = request.getHeader(&quot;X-Forwarded-For&quot;);        if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) &#123;            s = request.getHeader(&quot;Proxy-Client-IP&quot;);        &#125;        if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) &#123;            s = request.getHeader(&quot;WL-Proxy-Client-IP&quot;);        &#125;        if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) &#123;            s = request.getHeader(&quot;HTTP_CLIENT_IP&quot;);        &#125;        if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) &#123;            s = request.getHeader(&quot;HTTP_X_FORWARDED_FOR&quot;);        &#125;        if (s == null || s.length() == 0 || &quot;unknown&quot;.equalsIgnoreCase(s)) &#123;            s = request.getRemoteAddr();        &#125;        if (&quot;127.0.0.1&quot;.equals(s) || &quot;0:0:0:0:0:0:0:1&quot;.equals(s))            try &#123;                s = InetAddress.getLocalHost().getHostAddress();            &#125; catch (UnknownHostException unknownhostexception) &#123;                return &quot;&quot;;            &#125;        return s;    &#125;    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123;        this.applicationContext = applicationContext;    &#125;\n\n先一步一步的看：\n\n首先是定义了一个DubboController,并使用了SpringMVC的注解对外暴露HTTP服务。\n\n实现了org.springframework.context.ApplicationContextAware类，实现了setApplicationContext()方法用于初始化Spring上下文对象，在之后可以获取到容器里的相应对象。\n\n核心的invoke()方法。\n\n调用时：http://127.0.0.1:8080/SSM-SERVICE/dubboAPI/com.crossoverJie.api.UserInfoApi/getUserInfo。\n\n具体如上文的调用实例。先将com.crossoverJie.api.UserInfoApi、getUserInfo赋值到httpRequest入参中。\n\n判断传入的包是否是对外提供的。如下配置：\n&lt;!--dubbo服务暴露为http服务--&gt;&lt;bean class=&quot;com.crossoverJie.dubbo.http.conf.HttpProviderConf&quot;&gt;    &lt;property name=&quot;usePackage&quot;&gt;        &lt;list&gt;        \t   &lt;!--需要暴露服务的接口包名，可多个--&gt;            &lt;value&gt;com.crossoverJie.api&lt;/value&gt;        &lt;/list&gt;    &lt;/property&gt;&lt;/bean&gt;&lt;!--扫描暴露包--&gt;&lt;context:component-scan base-package=&quot;com.crossoverJie.dubbo.http&quot;/&gt;\n其中的com.crossoverJie.api就是自己需要暴露的包名，可以多个。\n\n接着在缓存map中取出反射获取到的接口类类型，如果获取不到则通过反射获取，并将值设置到缓存map中，这样不用每次都反射获取，可以节省系统开销(反射很耗系统资源)。\n\n接着也是判断该接口中是否有传入的getUserInfo方法。\n\n取出该方法的参数列表，如果没有参数则直接调用。\n\n如果有参数，判断个数。这里最多只运行一个参数。也就是说在真正的dubbo调用的时候只能传递一个BO类型，具体的参数列表可以写到BO中。因为如果有多个在进行json解析的时候是无法赋值到两个参数对象中去的。\n\n之后进行调用，将调用返回的数据进行返回即可。\n\n\n总结通常来说这样提供的HTTP接口再实际中用的不多，但是很方便调试。\n比如写了一个dubbo的查询接口，在测试环境或者是预发布环境中就可以直接通过HTTP请求的方式进行简单的测试，或者就是查询数据。比在Java中写单测来测试或查询快的很多。\n安装git clone https://github.com/crossoverJie/SSM-DUBBO-HTTP.git\n\ncd SSM-DUBBO-HTTP\n\nmvn clean\n\nmvn install\n\n\n使用&lt;dependency&gt;    &lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;    &lt;artifactId&gt;SSM-HTTP-PROVIDER&lt;/artifactId&gt;    &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt;\n\nspring配置&lt;!--dubbo服务暴露为http服务--&gt;&lt;bean class=&quot;com.crossoverJie.dubbo.http.conf.HttpProviderConf&quot;&gt;    &lt;property name=&quot;usePackage&quot;&gt;        &lt;list&gt;        \t   &lt;!--需要暴露服务的接口包名，可多个--&gt;            &lt;value&gt;com.crossoverJie.api&lt;/value&gt;        &lt;/list&gt;    &lt;/property&gt;&lt;/bean&gt;&lt;!--扫描暴露包--&gt;&lt;context:component-scan base-package=&quot;com.crossoverJie.dubbo.http&quot;/&gt;\n\n\n 插件地址：https://github.com/crossoverJie/SSM-DUBBO-HTTP\n\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客地址：http://crossoverjie.top。\n\n\nGitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["Java","dubbo","HTTP"]},{"title":"SSM(十五) 乐观锁与悲观锁的实际应用","url":"/2017/07/09/SSM15/","content":"\n前言随着互联网的兴起，现在三高(高可用、高性能、高并发)项目是越来越流行。\n本次来谈谈高并发。首先假设一个业务场景：数据库中有一条数据，需要获取到当前的值，在当前值的基础上+10，然后再更新回去。如果此时有两个线程同时并发处理，第一个线程拿到数据是10，+10&#x3D;20更新回去。第二个线程原本是要在第一个线程的基础上再+20=40,结果由于并发访问取到更新前的数据为10，+20=30。\n这就是典型的存在中间状态，导致数据不正确。来看以下的例子：\n并发所带来的问题和上文提到的类似，这里有一张price表，表结构如下：\nCREATE TABLE `price` (  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;主键&#x27;,  `total` decimal(12,2) DEFAULT &#x27;0.00&#x27; COMMENT &#x27;总值&#x27;,  `front` decimal(12,2) DEFAULT &#x27;0.00&#x27; COMMENT &#x27;消费前&#x27;,  `end` decimal(12,2) DEFAULT &#x27;0.00&#x27; COMMENT &#x27;消费后&#x27;,  PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1268 DEFAULT CHARSET=utf8\n\n\n\n我这里写了一个单测：就一个主线程，循环100次，每次把front的值减去10，再写入一次流水记录，正常情况是写入的每条记录都会每次减去10。\n/** * 单线程消费 */@Testpublic void singleCounsumerTest1()&#123;    for (int i=0 ;i&lt;100 ;i++)&#123;        Price price = priceMapper.selectByPrimaryKey(1);        int ron = 10 ;        price.setFront(price.getFront().subtract(new BigDecimal(ron)));        price.setEnd(price.getEnd().add(new BigDecimal(ron)));        price.setTotal(price.getFront().add(price.getEnd()));        priceMapper.updateByPrimaryKey(price) ;        price.setId(null);        priceMapper.insertSelective(price) ;    &#125;&#125;\n执行结果如下：\n可以看到确实是每次都递减10。\n但是如果是多线程的情况下会是如何呢：\n\n我这里新建了一个PriceController\n\n /** * 线程池 无锁 * @param redisContentReq * @return */@RequestMapping(value = &quot;/threadPrice&quot;,method = RequestMethod.POST)@ResponseBodypublic BaseResponse&lt;NULLBody&gt; threadPrice(@RequestBody RedisContentReq redisContentReq)&#123;    BaseResponse&lt;NULLBody&gt; response = new BaseResponse&lt;NULLBody&gt;() ;    try &#123;        for (int i=0 ;i&lt;10 ;i++)&#123;            Thread t = new Thread(new Runnable() &#123;                @Override                public void run() &#123;                    Price price = priceMapper.selectByPrimaryKey(1);                    int ron = 10 ;                    price.setFront(price.getFront().subtract(new BigDecimal(ron)));                    price.setEnd(price.getEnd().add(new BigDecimal(ron)));                    priceMapper.updateByPrimaryKey(price) ;                    price.setId(null);                    priceMapper.insertSelective(price) ;                &#125;            &#125;);            config.submit(t);        &#125;        response.setReqNo(redisContentReq.getReqNo());        response.setCode(StatusEnum.SUCCESS.getCode());        response.setMessage(StatusEnum.SUCCESS.getMessage());    &#125;catch (Exception e)&#123;        logger.error(&quot;system error&quot;,e);        response.setReqNo(response.getReqNo());        response.setCode(StatusEnum.FAIL.getCode());        response.setMessage(StatusEnum.FAIL.getMessage());    &#125;    return response ;&#125;\n其中为了节省资源使用了一个线程池:\n@Componentpublic class ThreadPoolConfig &#123;    private static final int MAX_SIZE = 10 ;    private static final int CORE_SIZE = 5;    private static final int SECOND = 1000;    private ThreadPoolExecutor executor ;    public ThreadPoolConfig()&#123;        executor = new ThreadPoolExecutor(CORE_SIZE,MAX_SIZE,SECOND, TimeUnit.MICROSECONDS,new LinkedBlockingQueue&lt;Runnable&gt;()) ;    &#125;    public void submit(Thread thread)&#123;        executor.submit(thread) ;    &#125;&#125;\n关于线程池的使用今后会仔细探讨。这里就简单理解为有10个线程并发去处理上面单线程的逻辑，来看看结果怎么样：\n\n会看到明显的数据错误，导致错误的原因自然就是有线程读取到了中间状态进行了错误的更新。\n进而有了以下两种解决方案：悲观锁和乐观锁。\n悲观锁简单理解下悲观锁：当一个事务锁定了一些数据之后，只有当当前锁提交了事务，释放了锁，其他事务才能获得锁并执行操作。\n使用方式如下：首先要关闭MySQL的自动提交：set autocommit = 0;\nbigen --开启事务select id, total, front, end from price where id=1 for update insert into price values(?,?,?,?,?)commit --提交事务\n\n这里使用select for update的方式利用数据库开启了悲观锁，锁定了id&#x3D;1的这条数据(注意:这里除非是使用了索引会启用行级锁，不然是会使用表锁，将整张表都锁住。)。之后使用commit提交事务并释放锁，这样下一个线程过来拿到的就是正确的数据。\n悲观锁一般是用于并发不是很高，并且不允许脏读等情况。但是对数据库资源消耗较大。\n乐观锁那么有没有性能好，支持的并发也更多的方式呢？\n那就是乐观锁。\n乐观锁是首先假设数据冲突很少，只有在数据提交修改的时候才进行校验，如果冲突了则不会进行更新。\n通常的实现方式增加一个version字段，为每一条数据加上版本。每次更新的时候version+1，并且更新时候带上版本号。实现方式如下：\n新建了一张price_version表：\nCREATE TABLE `price_version` (  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;主键&#x27;,  `total` decimal(12,2) DEFAULT &#x27;0.00&#x27; COMMENT &#x27;总值&#x27;,  `front` decimal(12,2) DEFAULT &#x27;0.00&#x27; COMMENT &#x27;消费前&#x27;,  `end` decimal(12,2) DEFAULT &#x27;0.00&#x27; COMMENT &#x27;消费后&#x27;,  `version` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;并发版本控制&#x27;,  PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1268 DEFAULT CHARSET=utf8\n\n更新数据的SQL：\n&lt;update id=&quot;updateByVersion&quot; parameterType=&quot;com.crossoverJie.pojo.PriceVersion&quot;&gt;    UPDATE price_version    SET front = #&#123;front,jdbcType=DECIMAL&#125;,        version= version + 1    WHERE id = #&#123;id,jdbcType=INTEGER&#125;    AND version = #&#123;version,jdbcType=INTEGER&#125;  &lt;/update&gt;\n\n调用方式：\n/** * 线程池，乐观锁 * @param redisContentReq * @return */@RequestMapping(value = &quot;/threadPriceVersion&quot;,method = RequestMethod.POST)@ResponseBodypublic BaseResponse&lt;NULLBody&gt; threadPriceVersion(@RequestBody RedisContentReq redisContentReq)&#123;    BaseResponse&lt;NULLBody&gt; response = new BaseResponse&lt;NULLBody&gt;() ;    try &#123;        for (int i=0 ;i&lt;3 ;i++)&#123;            Thread t = new Thread(new Runnable() &#123;                @Override                public void run() &#123;                    PriceVersion priceVersion = priceVersionMapper.selectByPrimaryKey(1);                    int ron = new Random().nextInt(20);                    logger.info(&quot;本次消费=&quot;+ron);                    priceVersion.setFront(new BigDecimal(ron));                    int count = priceVersionMapper.updateByVersion(priceVersion);                    if (count == 0)&#123;                        logger.error(&quot;更新失败&quot;);                    &#125;else &#123;                        logger.info(&quot;更新成功&quot;);                    &#125;                &#125;            &#125;);            config.submit(t);        &#125;        response.setReqNo(redisContentReq.getReqNo());        response.setCode(StatusEnum.SUCCESS.getCode());        response.setMessage(StatusEnum.SUCCESS.getMessage());    &#125;catch (Exception e)&#123;        logger.error(&quot;system error&quot;,e);        response.setReqNo(response.getReqNo());        response.setCode(StatusEnum.FAIL.getCode());        response.setMessage(StatusEnum.FAIL.getMessage());    &#125;    return response ;&#125;\n\n处理逻辑：开了三个线程生成了20以内的随机数更新到front字段。\n当调用该接口时日志如下：\n\n可以看到线程1、4、5分别生成了15，2，11三个随机数。最后线程4、5都更新失败了，只有线程1更新成功了。\n查看数据库：\n\n发现也确实是更新的15。\n乐观锁在实际应用相对较多，它可以提供更好的并发访问，并且数据库开销较少，但是有可能存在脏读的情况。\n总结以上两种各有优劣，大家可以根据具体的业务场景来判断具体使用哪种方式来保证数据的一致性。\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客：http://crossoverjie.top。\n\n","categories":["SSM"],"tags":["lock","mysql"]},{"title":"SSM(十六) 曲线救国-Kafka消费异常","url":"/2017/09/05/SSM16/","content":"\n前言最近线上遇到一个问题:在消费kafka消息的时候如果长时间(大概半天到一天的时间)队列里没有消息就可能再也消费不了。针对这个问题我们反复调试多次。线下模拟，调整代码，但貌似还是没有找到原因。但是只要重启消费进程就又可以继续消费。\n解决方案由于线上业务非常依赖kafka的消费，但一时半会也没有找到原因，所以最后只能想一个临时的替换方案：\n\n基于重启就可以消费这个特点，我们在每次消费的时候都记下当前的时间点，当这个时间点在十分钟之内都没有更新我们就认为当前队列中没有消息了，就需要重启下消费进程。\n\n既然是需要重启，由于目前还没有上分布式调度中心所以需要crontab来配合调度：每隔一分钟会调用一个shell脚本，该脚本会判断当前进程是否存在，如果存在则什么都不作，不存在则启动消费进程。\n\n\n具体实现消费程序:\n/** * kafka消费 * * @author crossoverJie * @date 2017年6月19日 下午3:15:16 */public class KafkaMsgConsumer &#123;    private static final Logger LOGGER = LoggerFactory.getLogger(KafkaMsgConsumer.class);    private static final int CORE_POOL_SIZE = 4;    private static final int MAXIMUM_POOL_SIZE = 4;    private static final int BLOCKING_QUEUE_CAPACITY = 4000;    private static final String KAFKA_CONFIG = &quot;kafkaConfig&quot;;    private static final ExecutorService fixedThreadPool = new ThreadPoolExecutor(CORE_POOL_SIZE, MAXIMUM_POOL_SIZE, 0L, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(BLOCKING_QUEUE_CAPACITY));    //最后更新时间    private static AtomicLong LAST_MESSAGE_TIME = new AtomicLong(DateUtil.getLongTime());    private static MsgIterator iter = null;    private static String topic;//主题名称    static &#123;        Properties properties = new Properties();        String path = System.getProperty(KAFKA_CONFIG);        checkArguments(!StringUtils.isBlank(path), &quot;启动参数中没有配置kafka_easyframe_msg参数来指定kafka启动参数，请使用-DkafkaConfig=/path/fileName/easyframe-msg.properties&quot;);        try &#123;            properties.load(new FileInputStream(new File(path)));        &#125; catch (IOException e) &#123;            LOGGER.error(&quot;IOException&quot; ,e);        &#125;        EasyMsgConfig.setProperties(properties);    &#125;    private static void iteratorTopic() &#123;        if (iter == null) &#123;            iter = MsgUtil.consume(topic);        &#125;        long i = 0L;        while (iter.hasNext()) &#123;            i++;            if (i % 10000 == 0) &#123;                LOGGER.info(&quot;consume i:&quot; + i);            &#125;            try &#123;                String message = iter.next();                if (StringUtils.isEmpty(message)) &#123;                    continue;                &#125;                LAST_MESSAGE_TIME = new AtomicLong(DateUtil.getLongTime());                //处理消息                LOGGER.debug(&quot;msg = &quot; + JSON.toJSONString(message));            &#125; catch (Exception e) &#123;                LOGGER.error(&quot;KafkaMsgConsumer err:&quot;, e);                try &#123;                    Thread.sleep(1000);                &#125; catch (InterruptedException e1) &#123;                    LOGGER.error(&quot;Thread InterruptedException&quot;, e1);                &#125;                break;            &#125;        &#125;    &#125;    public static void main(String[] args) &#123;        topic = System.getProperty(&quot;topic&quot;);        checkArguments(!StringUtils.isBlank(topic), &quot;system property topic or log_path is must!&quot;);        while (true) &#123;            try &#123;                iteratorTopic();            &#125; catch (Exception e) &#123;                MsgUtil.shutdownConsummer();                iter = null;                LOGGER.error(&quot;KafkaMsgConsumer err:&quot;, e);                try &#123;                    Thread.sleep(1000);                &#125; catch (InterruptedException e1) &#123;                    LOGGER.error(&quot;Thread InterruptedException&quot;, e1);                &#125;            &#125; finally &#123;                //此处关闭之后，由crontab每分钟检查一次，挂掉的话会重新拉起来                if (DateUtil.getLongTime() - LAST_MESSAGE_TIME.get() &gt; 10 * 60) &#123; //10分钟                    fixedThreadPool.shutdown();                    LOGGER.info(&quot;线程池是否关闭：&quot; + fixedThreadPool.isShutdown());                    try &#123;                        //当前线程阻塞10ms后，去检测线程池是否终止，终止则返回true                        while (!fixedThreadPool.awaitTermination(10, TimeUnit.MILLISECONDS)) &#123;                            LOGGER.info(&quot;检测线程池是否终止：&quot; + fixedThreadPool.isTerminated());                        &#125;                    &#125; catch (InterruptedException e) &#123;                        LOGGER.error(&quot;等待线程池关闭错误&quot;, e);                    &#125;                    LOGGER.info(&quot;线程池是否终止：&quot; + fixedThreadPool.isTerminated());                    LOGGER.info(&quot;in 10 min dont have data break&quot;);                    break;                &#125;            &#125;        &#125;        LOGGER.info(&quot;app shutdown&quot;);        System.exit(0);    &#125;&#125;\n在线代码\n需要配合以下这个shell脚本运行:\n#!/bin/sh#crontab# * * * * * sh /data/schedule/kafka/run-kafka-consumer.sh &gt;&gt;/data/schedule/kafka/run-sms-log.log# 如果进程存在就不启动a1=`ps -ef|grep &#x27;KafkaMsgConsumer&#x27;|grep -v grep|wc -l`if [ $a1 -gt 0  ];then        echo &quot;=======     `date +&#x27;%Y-%m-%d %H:%M:%S&#x27;` KafkaMsgConsumer  is EXIT...=======     &quot;        exitfiLANG=&quot;zh_CN.UTF-8&quot;nohup /opt/java/jdk1.7.0_80/bin/java -d64 -Djava.security.egd=file:/dev/./urandom-Djava.ext.dirs=/opt/tomcat/webapps/ROOT/WEB-INF/lib-Dtopic=TOPIC_A-Dlogback.configurationFile=/data/schedule/kafka/logback.xml-DkafkaConfig=/opt/tomcat/iopconf/easyframe-msg.properties-classpath /opt/tomcat/webapps/ROOT/WEB-INF/classes com.crossoverJie.kafka.SMSMsgConsumer &gt;&gt; /data/schedule/kafka/smslog/kafka.log 2&gt;&amp;1 &amp;echo &quot;`date +&#x27;%Y-%m-%d %H:%M:%S&#x27;`  KafkaMsgConsumer running....&quot;\n在线代码\n再配合crontab的调度:\n* * * * * sh /data/schedule/kafka/run-kafka-consumer.sh &gt;&gt;/data/schedule/kafka/run-sms-log.log\n即可。\n总结虽说处理起来很简单，但依然是治标不治本，依赖的东西比较多(shell脚本，调度)。所以也问问各位有没有什么思路：\n\n消费程序用的:https://github.com/linzhaoming/easyframe-msg\n\n生产配置:\n\n三台kafka、ZK组成的集群。\n\n其中也有其他团队的消费程序在正常运行，应该和kafka的配置没有关系。\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客：http://crossoverjie.top。\n\n","categories":["SSM"],"tags":["Java","Kafka","shell"]},{"title":"SSM(十七) MQ应用","url":"/2017/10/20/SSM17/","content":"\n前言写这篇文章的起因是由于之前的一篇关于Kafka异常消费，当时为了解决问题不得不使用临时的方案。\n总结起来归根结底还是对Kafka不熟悉导致的，加上平时工作的需要，之后就花些时间看了Kafka相关的资料。\n何时使用MQ谈到Kafka就不得不提到MQ，是属于消息队列的一种。作为一种基础中间件在互联网项目中有着大量的使用。\n一种技术的产生自然是为了解决某种需求，通常来说是以下场景：\n\n\n需要跨进程通信：B系统需要A系统的输出作为输入参数。\n当A系统的输出能力远远大于B系统的处理能力。\n\n\n针对于第一种情况有两种方案:\n\n使用RPC远程调用,A直接调用B。\n使用MQ,A发布消息到MQ,B订阅该消息。\n\n当我们的需求是:A调用B实时响应，并且实时关心响应结果则使用RPC，这种情况就得使用同步调用。\n\n\n反之当我们并不关心调用之后的执行结果，并且有可能被调用方的执行非常耗时，这种情况就非常适合用MQ来达到异步调用目的。\n比如常见的登录场景就只能用同步调用的方式，因为这个过程需要实时的响应结果，总不能在用户点了登录之后排除网络原因之外再额外的等几秒吧。\n但类似于用户登录需要奖励积分的情况则使用MQ会更好，因为登录并不关系积分的情况，只需要发个消息到MQ,处理积分的服务订阅处理即可，这样还可以解决积分系统故障带来的雪崩效应。\nMQ还有一个基础功能则是限流削峰，这对于大流量的场景如果将请求直接调用到B系统则非常有可能使B系统出现不可用的情况。这种场景就非常适合将请求放入MQ，不但可以利用MQ削峰还尽可能的保证系统的高可用。\nKafka简介本次重点讨论下Kafka。简单来说Kafka是一个支持水平扩展，高吞吐率的分布式消息系统。\nKafka的常用知识:\n\nTopic:生产者和消费者的交互都是围绕着一个Topic进行的，通常来说是由业务来进行区分，由生产消费者协商之后进行创建。\n\nPartition(分区):是Topic下的组成，通常一个Topic下有一个或多个分区，消息生产之后会按照一定的算法负载到每个分区，所以分区也是Kafka性能的关键。当发现性能不高时便可考虑新增分区。\n\n\n结构图如下:\n创建TopicKafka的安装官网有非常详细的讲解。这里谈一下在日常开发中常见的一些操作，比如创建Topic：\nsh bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic `test`\n创建了三个分区的test主题。\n使用\nsh bin/kafka-topics.sh --list --zookeeper localhost:2181\n可以列出所有的Topic。\nKafka生产者使用kafka官方所提供的Java API来进行消息生产，实际使用中编码实现更为常用:\n/** Kafka生产者 * @author crossoverJie */public class Producer &#123;    private static final Logger LOGGER = LoggerFactory.getLogger(Producer.class);    /**     * 消费配置文件     */    private static String consumerProPath;    public static void main(String[] args) throws IOException &#123;        // set up the producer        consumerProPath = System.getProperty(&quot;product_path&quot;);        KafkaProducer&lt;String, String&gt; producer = null;        try &#123;            FileInputStream inputStream = new FileInputStream(new File(consumerProPath));            Properties properties = new Properties();            properties.load(inputStream);            producer = new KafkaProducer&lt;String, String&gt;(properties);        &#125; catch (IOException e) &#123;            LOGGER.error(&quot;load config error&quot;, e);        &#125;        try &#123;            // send lots of messages            for (int i=0 ;i&lt;100 ; i++)&#123;                producer.send(new ProducerRecord&lt;String, String&gt;(                        &quot;topic_optimization&quot;, i+&quot;&quot;, i+&quot;&quot;));            &#125;        &#125; catch (Throwable throwable) &#123;            System.out.printf(&quot;%s&quot;, throwable.getStackTrace());        &#125; finally &#123;            producer.close();        &#125;    &#125;&#125;\n再配合以下启动参数即可发送消息:\n-Dproduct_path=/xxx/producer.properties\n以及生产者的配置文件:\n#集群地址，可以多个bootstrap.servers=10.19.13.51:9094acks=allretries=0batch.size=16384auto.commit.interval.ms=1000linger.ms=0key.serializer=org.apache.kafka.common.serialization.StringSerializervalue.serializer=org.apache.kafka.common.serialization.StringSerializerblock.on.buffer.full=true\n具体的配置说明详见此处:https://kafka.apache.org/0100/documentation.html#theproducer\n流程非常简单，其实就是一些API的调用。\n消息发完之后可以通过以下命令查看队列内的情况:\nsh kafka-consumer-groups.sh --bootstrap-server localhost:9094 --describe --group group1 \n其中的lag便是队列里的消息数量。\nKafka消费者有了生产者自然也少不了消费者，这里首先针对单线程消费:\n/** * Function:kafka官方消费 * * @author crossoverJie *         Date: 2017/10/19 01:11 * @since JDK 1.8 */public class KafkaOfficialConsumer &#123;    private static final Logger LOGGER = LoggerFactory.getLogger(KafkaOfficialConsumer.class);    /**     * 日志文件地址     */    private static String logPath;    /**     * 主题名称     */    private static String topic;    /**     * 消费配置文件     */    private static String consumerProPath ;    /**     * 初始化参数校验     * @return     */    private static boolean initCheck() &#123;        topic = System.getProperty(&quot;topic&quot;) ;        logPath = System.getProperty(&quot;log_path&quot;) ;        consumerProPath = System.getProperty(&quot;consumer_pro_path&quot;) ;        if (StringUtil.isEmpty(topic) || logPath.isEmpty()) &#123;            LOGGER.error(&quot;system property topic ,consumer_pro_path, log_path is required !&quot;);            return true;        &#125;        return false;    &#125;    /**     * 初始化kafka配置     * @return     */    private static KafkaConsumer&lt;String, String&gt; initKafkaConsumer() &#123;        KafkaConsumer&lt;String, String&gt; consumer = null;        try &#123;            FileInputStream inputStream = new FileInputStream(new File(consumerProPath)) ;            Properties properties = new Properties();            properties.load(inputStream);            consumer = new KafkaConsumer&lt;String, String&gt;(properties);            consumer.subscribe(Arrays.asList(topic));        &#125; catch (IOException e) &#123;            LOGGER.error(&quot;加载consumer.props文件出错&quot;, e);        &#125;        return consumer;    &#125;    public static void main(String[] args) &#123;        if (initCheck())&#123;            return;        &#125;        int totalCount = 0 ;        long totalMin = 0L ;        int count = 0;        KafkaConsumer&lt;String, String&gt; consumer = initKafkaConsumer();        long startTime = System.currentTimeMillis() ;        //消费消息        while (true) &#123;            ConsumerRecords&lt;String, String&gt; records = consumer.poll(200);            if (records.count() &lt;= 0)&#123;                continue ;            &#125;            LOGGER.debug(&quot;本次获取:&quot;+records.count());            count += records.count() ;            long endTime = System.currentTimeMillis() ;            LOGGER.debug(&quot;count=&quot; +count) ;            if (count &gt;= 10000 )&#123;                totalCount += count ;                LOGGER.info(&quot;this consumer &#123;&#125; record，use &#123;&#125; milliseconds&quot;,count,endTime-startTime);                totalMin += (endTime-startTime) ;                startTime = System.currentTimeMillis() ;                count = 0 ;            &#125;            LOGGER.debug(&quot;end totalCount=&#123;&#125;,min=&#123;&#125;&quot;,totalCount,totalMin);            /*for (ConsumerRecord&lt;String, String&gt; record : records) &#123;                record.value() ;                JsonNode msg = null;                try &#123;                    msg = mapper.readTree(record.value());                &#125; catch (IOException e) &#123;                    LOGGER.error(&quot;消费消息出错&quot;, e);                &#125;                LOGGER.info(&quot;kafka receive = &quot;+msg.toString());            &#125;*/        &#125;    &#125;&#125;\n配合以下启动参数:\n-Dlog_path=/log/consumer.log -Dtopic=test -Dconsumer_pro_path=consumer.properties\n其中采用了轮询的方式获取消息，并且记录了消费过程中的数据。\n消费者采用的配置:\nbootstrap.servers=192.168.1.2:9094group.id=group1# 自动提交enable.auto.commit=truekey.deserializer=org.apache.kafka.common.serialization.StringDeserializervalue.deserializer=org.apache.kafka.common.serialization.StringDeserializer# fast session timeout makes it more fun to play with failoversession.timeout.ms=10000# These buffer sizes seem to be needed to avoid consumer switching to# a mode where it processes one bufferful every 5 seconds with multiple# timeouts along the way.  No idea why this happens.fetch.min.bytes=50000receive.buffer.bytes=262144max.partition.fetch.bytes=2097152\n为了简便我采用的是自动提交offset。\n消息存放机制谈到offset就必须得谈谈Kafka的消息存放机制.\nKafka的消息不会因为消费了就会立即删除，所有的消息都会持久化到日志文件，并配置有过期时间，到了时间会自动删除过期数据，并且不会管其中的数据是否被消费过。\n由于这样的机制就必须的有一个标志来表明哪些数据已经被消费过了，offset(偏移量)就是这样的作用，它类似于指针指向某个数据，当消费之后offset就会线性的向前移动，这样一来的话消息是可以被任意消费的，只要我们修改offset的值即可。\n消费过程中还有一个值得注意的是:\n\n同一个consumer group(group.id相等)下只能有一个消费者可以消费，这个刚开始确实会让很多人踩坑。\n\n多线程消费针对于单线程消费实现起来自然是比较简单，但是效率也是要大打折扣的。\n为此我做了一个测试，使用之前的单线程消费120009条数据的结果如下:\n总共花了12450毫秒。\n那么换成多线程消费怎么实现呢？\n我们可以利用partition的分区特性来提高消费能力，单线程的时候等于是一个线程要把所有分区里的数据都消费一遍，如果换成多线程就可以让一个线程只消费一个分区,这样效率自然就提高了，所以线程数coreSize&lt;=partition。\n首先来看下入口:\npublic class ConsumerThreadMain &#123;    private static String brokerList = &quot;localhost:9094&quot;;    private static String groupId = &quot;group1&quot;;    private static String topic = &quot;test&quot;;    /**     * 线程数量     */    private static int threadNum = 3;    public static void main(String[] args) &#123;        ConsumerGroup consumerGroup = new ConsumerGroup(threadNum, groupId, topic, brokerList);        consumerGroup.execute();    &#125;&#125;\n其中的ConsumerGroup类:\npublic class ConsumerGroup &#123;    private static Logger LOGGER = LoggerFactory.getLogger(ConsumerGroup.class);    /**     * 线程池     */    private ExecutorService threadPool;    private List&lt;ConsumerCallable&gt; consumers ;    public ConsumerGroup(int threadNum, String groupId, String topic, String brokerList) &#123;        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder()                .setNameFormat(&quot;consumer-pool-%d&quot;).build();        threadPool = new ThreadPoolExecutor(threadNum, threadNum,                0L, TimeUnit.MILLISECONDS,                new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy());        consumers = new ArrayList&lt;ConsumerCallable&gt;(threadNum);        for (int i = 0; i &lt; threadNum; i++) &#123;            ConsumerCallable consumerThread = new ConsumerCallable(brokerList, groupId, topic);            consumers.add(consumerThread);        &#125;    &#125;    /**     * 执行任务     */    public void execute() &#123;        long startTime = System.currentTimeMillis() ;        for (ConsumerCallable runnable : consumers) &#123;            Future&lt;ConsumerFuture&gt; future = threadPool.submit(runnable) ;        &#125;        if (threadPool.isShutdown())&#123;            long endTime = System.currentTimeMillis() ;            LOGGER.info(&quot;main thread use &#123;&#125; Millis&quot; ,endTime -startTime) ;        &#125;        threadPool.shutdown();    &#125;&#125;\n最后真正的执行逻辑ConsumerCallable:\npublic class ConsumerCallable implements Callable&lt;ConsumerFuture&gt; &#123;    private static Logger LOGGER = LoggerFactory.getLogger(ConsumerCallable.class);    private AtomicInteger totalCount = new AtomicInteger() ;    private AtomicLong totalTime = new AtomicLong() ;    private AtomicInteger count = new AtomicInteger() ;    /**     * 每个线程维护KafkaConsumer实例     */    private final KafkaConsumer&lt;String, String&gt; consumer;    public ConsumerCallable(String brokerList, String groupId, String topic) &#123;        Properties props = new Properties();        props.put(&quot;bootstrap.servers&quot;, brokerList);        props.put(&quot;group.id&quot;, groupId);        //自动提交位移        props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);        props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);        props.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);        this.consumer = new KafkaConsumer&lt;&gt;(props);        consumer.subscribe(Arrays.asList(topic));    &#125;    /**     * Computes a result, or throws an exception if unable to do so.     *     * @return computed result     * @throws Exception if unable to compute a result     */    @Override    public ConsumerFuture call() throws Exception &#123;        boolean flag = true;        int failPollTimes = 0 ;        long startTime = System.currentTimeMillis() ;        while (flag) &#123;            // 使用200ms作为获取超时时间            ConsumerRecords&lt;String, String&gt; records = consumer.poll(200);            if (records.count() &lt;= 0)&#123;                failPollTimes ++ ;                if (failPollTimes &gt;= 20)&#123;                    LOGGER.debug(&quot;达到&#123;&#125;次数，退出 &quot;,failPollTimes);                    flag = false ;                &#125;                continue ;            &#125;            //获取到之后则清零            failPollTimes = 0 ;            LOGGER.debug(&quot;本次获取:&quot;+records.count());            count.addAndGet(records.count()) ;            totalCount.addAndGet(count.get()) ;            long endTime = System.currentTimeMillis() ;            if (count.get() &gt;= 10000 )&#123;                LOGGER.info(&quot;this consumer &#123;&#125; record，use &#123;&#125; milliseconds&quot;,count,endTime-startTime);                totalTime.addAndGet(endTime-startTime) ;                startTime = System.currentTimeMillis() ;                count = new AtomicInteger();            &#125;            LOGGER.debug(&quot;end totalCount=&#123;&#125;,min=&#123;&#125;&quot;,totalCount,totalTime);            /*for (ConsumerRecord&lt;String, String&gt; record : records) &#123;                // 简单地打印消息                LOGGER.debug(record.value() + &quot; consumed &quot; + record.partition() +                        &quot; message with offset: &quot; + record.offset());            &#125;*/        &#125;        ConsumerFuture consumerFuture = new ConsumerFuture(totalCount.get(),totalTime.get()) ;        return consumerFuture ;    &#125;&#125;\n理一下逻辑:\n\n其实就是初始化出三个消费者实例，用于三个线程消费。其中加入了一些统计，最后也是消费120009条数据结果如下。\n\n\n由于是并行运行，可见消费120009条数据可以提高2秒左右，当数据以更高的数量级提升后效果会更加明显。\n但这也有一些弊端:\n\n灵活度不高，当分区数量变更之后不能自适应调整。\n消费逻辑和处理逻辑在同一个线程，如果处理逻辑较为复杂会影响效率，耦合也较高。当然这个处理逻辑可以再通过一个内部队列发出去由另外的程序来处理也是可以的。\n\n总结Kafka的知识点还是较多，Kafka的使用也远不这些。之后会继续分享一些关于Kafka监控等相关内容。\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客：http://crossoverjie.top。\n\n","categories":["SSM"],"tags":["Java","Kafka"]},{"title":"SSM(二)Lucene全文检索","url":"/2016/07/06/SSM2/","content":"\n前言\n大家平时肯定都有用过全文检索工具，最常用的百度谷歌就是其中的典型。如果自己能够做一个那是不是想想就逼格满满呢。Apache就为我们提供了这样一个框架，以下就是在实际开发中加入Lucene的一个小Demo。\n\n\n获取Maven依赖首先看一下实际运行的效果图：\n\n这个项目是基于之前使用IDEA搭建的SSM的基础上进行增加的，建议小白先看下一我。上一篇博客，以及共享在Github上的源码。以下是Lucene所需要的依赖：\n&lt;!--加入lucene--&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-core --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;            &lt;artifactId&gt;lucene-core&lt;/artifactId&gt;            &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-queryparser --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;            &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt;            &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-analyzers-common --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;            &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt;            &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--lucene中文分词--&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-analyzers-smartcn --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;            &lt;artifactId&gt;lucene-analyzers-smartcn&lt;/artifactId&gt;            &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt;        &lt;/dependency&gt;        &lt;!--lucene高亮--&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.lucene/lucene-highlighter --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;            &lt;artifactId&gt;lucene-highlighter&lt;/artifactId&gt;            &lt;version&gt;$&#123;lucene.version&#125;&lt;/version&gt;        &lt;/dependency&gt;\n具体的用途我都写有注释。在IDEA中修改了Pom.xml文件之后只需要点击如图所示的按钮即可重新获取依赖：\n\n编写Lucene工具类这个工具类中的具体代码我就不单独提出来说了，每个关键的地方我都写有注释，不清楚的再讨论。\npackage com.crossoverJie.lucene;import com.crossoverJie.pojo.User;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.cn.smart.SmartChineseAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.StringField;import org.apache.lucene.document.TextField;import org.apache.lucene.index.*;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.*;import org.apache.lucene.search.highlight.*;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import java.io.StringReader;import java.nio.file.Paths;import java.util.LinkedList;import java.util.List;import com.crossoverJie.util.*;/** * 博客索引类 * @author Administrator * */public class LuceneIndex &#123;\tprivate Directory dir=null;\t/**\t * 获取IndexWriter实例\t * @return\t * @throws Exception\t */\tprivate IndexWriter getWriter()throws Exception&#123;\t\t/**\t\t * 生成的索引我放在了C盘，可以根据自己的需要放在具体位置\t\t */\t\tdir= FSDirectory.open(Paths.get(&quot;C://lucene&quot;));\t\tSmartChineseAnalyzer analyzer=new SmartChineseAnalyzer();\t\tIndexWriterConfig iwc=new IndexWriterConfig(analyzer);\t\tIndexWriter writer=new IndexWriter(dir, iwc);\t\treturn writer;\t&#125;\t/**\t * 添加博客索引\t * @param user\t */\tpublic void addIndex(User user)throws Exception&#123;\t\tIndexWriter writer=getWriter();\t\tDocument doc=new Document();\t\tdoc.add(new StringField(&quot;id&quot;,String.valueOf(user.getUserId()), Field.Store.YES));\t\t/**\t\t * yes是会将数据存进索引，如果查询结果中需要将记录显示出来就要存进去，如果查询结果\t\t * 只是显示标题之类的就可以不用存，而且内容过长不建议存进去\t\t * 使用TextField类是可以用于查询的。\t\t */\t\tdoc.add(new TextField(&quot;username&quot;, user.getUsername(), Field.Store.YES));\t\tdoc.add(new TextField(&quot;description&quot;,user.getDescription(), Field.Store.YES));\t\twriter.addDocument(doc);\t\twriter.close();\t&#125;\t/**\t * 更新博客索引\t * @param user\t * @throws Exception\t */\tpublic void updateIndex(User user)throws Exception&#123;\t\tIndexWriter writer=getWriter();\t\tDocument doc=new Document();\t\tdoc.add(new StringField(&quot;id&quot;,String.valueOf(user.getUserId()), Field.Store.YES));\t\tdoc.add(new TextField(&quot;username&quot;, user.getUsername(), Field.Store.YES));\t\tdoc.add(new TextField(&quot;description&quot;,user.getDescription(), Field.Store.YES));\t\twriter.updateDocument(new Term(&quot;id&quot;, String.valueOf(user.getUserId())), doc);\t\twriter.close();\t&#125;\t/**\t * 删除指定博客的索引\t * @param userId\t * @throws Exception\t */\tpublic void deleteIndex(String userId)throws Exception&#123;\t\tIndexWriter writer=getWriter();\t\twriter.deleteDocuments(new Term(&quot;id&quot;, userId));\t\twriter.forceMergeDeletes(); // 强制删除\t\twriter.commit();\t\twriter.close();\t&#125;\t/**\t * 查询用户\t * @param q 查询关键字\t * @return\t * @throws Exception\t */\tpublic List&lt;User&gt; searchBlog(String q)throws Exception&#123;\t\t/**\t\t * 注意的是查询索引的位置得是存放索引的位置，不然会找不到。\t\t */\t\tdir= FSDirectory.open(Paths.get(&quot;C://lucene&quot;));\t\tIndexReader reader = DirectoryReader.open(dir);\t\tIndexSearcher is=new IndexSearcher(reader);\t\tBooleanQuery.Builder booleanQuery = new BooleanQuery.Builder();\t\tSmartChineseAnalyzer analyzer=new SmartChineseAnalyzer();\t\t/**\t\t * username和description就是我们需要进行查找的两个字段\t\t * 同时在存放索引的时候要使用TextField类进行存放。\t\t */\t\tQueryParser parser=new QueryParser(&quot;username&quot;,analyzer);\t\tQuery query=parser.parse(q);\t\tQueryParser parser2=new QueryParser(&quot;description&quot;,analyzer);\t\tQuery query2=parser2.parse(q);\t\tbooleanQuery.add(query, BooleanClause.Occur.SHOULD);\t\tbooleanQuery.add(query2, BooleanClause.Occur.SHOULD);\t\tTopDocs hits=is.search(booleanQuery.build(), 100);\t\tQueryScorer scorer=new QueryScorer(query);\t\tFragmenter fragmenter = new SimpleSpanFragmenter(scorer);\t\t/**\t\t * 这里可以根据自己的需要来自定义查找关键字高亮时的样式。\t\t */\t\tSimpleHTMLFormatter simpleHTMLFormatter=new SimpleHTMLFormatter(&quot;&lt;b&gt;&lt;font color=&#x27;red&#x27;&gt;&quot;,&quot;&lt;/font&gt;&lt;/b&gt;&quot;);\t\tHighlighter highlighter=new Highlighter(simpleHTMLFormatter, scorer);\t\thighlighter.setTextFragmenter(fragmenter);\t\tList&lt;User&gt; userList=new LinkedList&lt;User&gt;();\t\tfor(ScoreDoc scoreDoc:hits.scoreDocs)&#123;\t\t\tDocument doc=is.doc(scoreDoc.doc);\t\t\tUser user=new User();\t\t\tuser.setUserId(Integer.parseInt(doc.get((&quot;id&quot;))));\t\t\tuser.setDescription(doc.get((&quot;description&quot;)));\t\t\tString username=doc.get(&quot;username&quot;);\t\t\tString description=doc.get(&quot;description&quot;);\t\t\tif(username!=null)&#123;\t\t\t\tTokenStream tokenStream = analyzer.tokenStream(&quot;username&quot;, new StringReader(username));\t\t\t\tString husername=highlighter.getBestFragment(tokenStream, username);\t\t\t\tif(StringUtil.isEmpty(husername))&#123;\t\t\t\t\tuser.setUsername(username);\t\t\t\t&#125;else&#123;\t\t\t\t\tuser.setUsername(husername);\t\t\t\t&#125;\t\t\t&#125;\t\t\tif(description!=null)&#123;\t\t\t\tTokenStream tokenStream = analyzer.tokenStream(&quot;description&quot;, new StringReader(description));\t\t\t\tString hContent=highlighter.getBestFragment(tokenStream, description);\t\t\t\tif(StringUtil.isEmpty(hContent))&#123;\t\t\t\t\tif(description.length()&lt;=200)&#123;\t\t\t\t\t\tuser.setDescription(description);\t\t\t\t\t&#125;else&#123;\t\t\t\t\t\tuser.setDescription(description.substring(0, 200));\t\t\t\t\t&#125;\t\t\t\t&#125;else&#123;\t\t\t\t\tuser.setDescription(hContent);\t\t\t\t&#125;\t\t\t&#125;\t\t\tuserList.add(user);\t\t&#125;\t\treturn userList;\t&#125;&#125;\n查询Controller的编写接下来是查询Controller：\n@RequestMapping(&quot;/q&quot;)public String search(@RequestParam(value = &quot;q&quot;, required = false,defaultValue = &quot;&quot;) String q,                     @RequestParam(value = &quot;page&quot;, required = false, defaultValue = &quot;1&quot;) String page,                     Model model,                     HttpServletRequest request) throws Exception &#123;    LuceneIndex luceneIndex = new LuceneIndex() ;    List&lt;User&gt; userList = luceneIndex.searchBlog(q);    /**     * 关于查询之后的分页我采用的是每次分页发起的请求都是将所有的数据查询出来，     * 具体是第几页再截取对应页数的数据，典型的拿空间换时间的做法，如果各位有什么     * 高招欢迎受教。     */    Integer toIndex = userList.size() &gt;= Integer.parseInt(page) * 5 ? Integer.parseInt(page) * 5 : userList.size();    List&lt;User&gt; newList = userList.subList((Integer.parseInt(page) - 1) * 5, toIndex);    model.addAttribute(&quot;userList&quot;,newList) ;    String s = this.genUpAndDownPageCode(Integer.parseInt(page), userList.size(), q, 5, request.getServletContext().            getContextPath());    model.addAttribute(&quot;pageHtml&quot;,s) ;    model.addAttribute(&quot;q&quot;,q) ;    model.addAttribute(&quot;resultTotal&quot;,userList.size()) ;    model.addAttribute(&quot;pageTitle&quot;,&quot;搜索关键字&#x27;&quot; + q + &quot;&#x27;结果页面&quot;) ;    return &quot;queryResult&quot;;&#125;\n其中有用到一个genUpAndDownPageCode()方法来生成分页的Html代码，如下：\n/** * 查询之后的分页 * @param page * @param totalNum * @param q * @param pageSize * @param projectContext * @return */private String genUpAndDownPageCode(int page,Integer totalNum,String q,Integer pageSize,String projectContext)&#123;    long totalPage=totalNum%pageSize==0?totalNum/pageSize:totalNum/pageSize+1;    StringBuffer pageCode=new StringBuffer();    if(totalPage==0)&#123;        return &quot;&quot;;    &#125;else&#123;        pageCode.append(&quot;&lt;nav&gt;&quot;);        pageCode.append(&quot;&lt;ul class=&#x27;pager&#x27; &gt;&quot;);        if(page&gt;1)&#123;            pageCode.append(&quot;&lt;li&gt;&lt;a href=&#x27;&quot;+projectContext+&quot;/q?page=&quot;+(page-1)+&quot;&amp;q=&quot;+q+&quot;&#x27;&gt;上一页&lt;/a&gt;&lt;/li&gt;&quot;);        &#125;else&#123;            pageCode.append(&quot;&lt;li class=&#x27;disabled&#x27;&gt;&lt;a href=&#x27;#&#x27;&gt;上一页&lt;/a&gt;&lt;/li&gt;&quot;);        &#125;        if(page&lt;totalPage)&#123;            pageCode.append(&quot;&lt;li&gt;&lt;a href=&#x27;&quot;+projectContext+&quot;/q?page=&quot;+(page+1)+&quot;&amp;q=&quot;+q+&quot;&#x27;&gt;下一页&lt;/a&gt;&lt;/li&gt;&quot;);        &#125;else&#123;            pageCode.append(&quot;&lt;li class=&#x27;disabled&#x27;&gt;&lt;a href=&#x27;#&#x27;&gt;下一页&lt;/a&gt;&lt;/li&gt;&quot;);        &#125;        pageCode.append(&quot;&lt;/ul&gt;&quot;);        pageCode.append(&quot;&lt;/nav&gt;&quot;);    &#125;    return pageCode.toString();&#125;\n代码比较简单，就是根据的页数、总页数来生成分页代码，对了我前端采用的是现在流行的Bootstrap，这个有不会的可以去他官网看看，比较简单易上手。接下来只需要编写显示界面就大功告成了。\n\n显示界面我只贴关键代码，具体的可以去Github上查看。\n&lt;c:choose&gt;                    &lt;c:when test=&quot;$&#123;userList.size()==0 &#125;&quot;&gt;                        &lt;div align=&quot;center&quot; style=&quot;padding-top: 20px&quot;&gt;&lt;font color=&quot;red&quot;&gt;$&#123;q&#125;&lt;/font&gt;未查询到结果，请换个关键字试试！&lt;/div&gt;                    &lt;/c:when&gt;                    &lt;c:otherwise&gt;                        &lt;div align=&quot;center&quot; style=&quot;padding-top: 20px&quot;&gt;                            查询&lt;font color=&quot;red&quot;&gt;$&#123;q&#125;&lt;/font&gt;关键字，约$&#123;resultTotal&#125;条记录！                        &lt;/div&gt;                        &lt;c:forEach var=&quot;u&quot; items=&quot;$&#123;userList &#125;&quot; varStatus=&quot;status&quot;&gt;                            &lt;div class=&quot;panel-heading &quot;&gt;                                &lt;div class=&quot;row&quot;&gt;                                    &lt;div class=&quot;col-md-6&quot;&gt;                                        &lt;div class=&quot;row&quot;&gt;                                            &lt;div class=&quot;col-md-12&quot;&gt;                                                &lt;b&gt;                                                    &lt;a href=&quot;&lt;%=path %&gt;/user/showUser/$&#123;u.userId&#125;&quot;&gt;$&#123;u.username&#125;&lt;/a&gt;                                                &lt;/b&gt;                                                &lt;br/&gt;                                                    $&#123;u.description&#125;                                            &lt;/div&gt;                                        &lt;/div&gt;                                    &lt;/div&gt;                                    &lt;div class=&quot;col-md-4 col-md-offset-2&quot;&gt;                                        &lt;p class=&quot;text-muted text-right&quot;&gt;                                                $&#123;u.password&#125;                                        &lt;/p&gt;                                    &lt;/div&gt;                                &lt;/div&gt;                            &lt;/div&gt;                            &lt;div class=&quot;panel-footer&quot;&gt;                                &lt;p class=&quot;text-right&quot;&gt;\t\t\t\t\t\t\t&lt;span class=&quot;label label-default&quot;&gt;\t\t\t\t\t\t\t&lt;span class=&quot;glyphicon glyphicon-comment&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt;\t\t\t\t\t\t\t $&#123;u.password&#125;\t\t\t\t\t\t\t&lt;/span&gt;                                &lt;/p&gt;                            &lt;/div&gt;                        &lt;/c:forEach&gt;                    &lt;/c:otherwise&gt;                &lt;/c:choose&gt;\n利用JSTL标签即可将数据循环展示出来，关键字就不需要单独做处理了，在后台查询的时候已经做了修改了。\n\n总结关于全文检索的框架不止Lucene还有solr，具体谁好有什么区别我也不太清楚，准备下来花点时间研究下。哦对了，最近又有点想做Android开发了，感觉做点东西能够实实在在的摸得到逼格确实要高些(现在主要在做后端开发)，感兴趣的朋友可以关注下。哦对了，直接运行我代码的朋友要下注意：\n\n首先要将数据库倒到自己的MySQL上\n之后在首次运行的时候需要点击重新生成索引按钮生成一遍索引之后才能进行搜索，因为现在的数据是直接存到数据库中的，并没有在新增的时候就增加索引，在实际开发的时候需要在新增数据那里再生成一份索引，就直接调用LuceneIndex类中的addIndex方法传入实体即可，再做更新、删除操作的时候也同样需要对索引做操作。\n\n","categories":["SSM"],"tags":["Java","IDEA","Lucene"]},{"title":"SSM(三)Shiro使用详解","url":"/2016/07/15/SSM3/","content":"\n前言相比有做过企业级开发的童鞋应该都有做过权限安全之类的功能吧，最先开始我采用的是建用户表,角色表,权限表，之后在拦截器中对每一个请求进行拦截，再到数据库中进行查询看当前用户是否有该权限，这样的设计能满足大多数中小型系统的需求。不过这篇所介绍的Shiro能满足之前的所有需求，并且使用简单，安全性高，而且现在越来越的多企业都在使用Shiro，这应该是一个收入的你的技能库。\n\n创建自定义MyRealm类有关Shiro的基础知识我这里就不过多介绍了，直接来干货，到最后会整合Spring来进行权限验证。首先在使用Shiro的时候我们要考虑在什么样的环境下使用：\n\n登录的验证\n对指定角色的验证\n对URL的验证\n\n\n\n\n基本上我们也就这三个需求，所以同时我们也需要三个方法：\n\nfindUserByUserName(String username)根据username查询用户，之后Shiro会根据查询出来的User的密码来和提交上来的密码进行比对。\nfindRoles(String username)根据username查询该用户的所有角色，用于角色验证。\nfindPermissions(String username)根据username查询他所拥有的权限信息，用于权限判断。\n\n下面我贴一下我的mapper代码(PS:该项目依然是基于之前的SSM，不太清楚整合的请看SSM一)。\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot; &gt;&lt;mapper namespace=&quot;com.crossoverJie.dao.T_userDao&quot; &gt;    &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.crossoverJie.pojo.T_user&quot; &gt;        &lt;result property=&quot;id&quot; column=&quot;id&quot;/&gt;        &lt;result property=&quot;userName&quot; column=&quot;userName&quot;/&gt;        &lt;result property=&quot;password&quot; column=&quot;password&quot;/&gt;        &lt;result property=&quot;roleId&quot; column=&quot;roleId&quot;/&gt;    &lt;/resultMap&gt;    &lt;sql id=&quot;Base_Column_List&quot; &gt;        id, username, password,roleId    &lt;/sql&gt;    &lt;select id=&quot;findUserByUsername&quot; parameterType=&quot;String&quot; resultMap=&quot;BaseResultMap&quot;&gt;        select &lt;include refid=&quot;Base_Column_List&quot;/&gt;        from t_user where userName=#&#123;userName&#125;    &lt;/select&gt;    &lt;select id=&quot;findRoles&quot; parameterType=&quot;String&quot; resultType=&quot;String&quot;&gt;        select r.roleName from t_user u,t_role r where u.roleId=r.id and u.userName=#&#123;userName&#125;    &lt;/select&gt;    &lt;select id=&quot;findPermissions&quot; parameterType=&quot;String&quot; resultType=&quot;String&quot;&gt;        select p.permissionName from t_user u,t_role r,t_permission p        where u.roleId=r.id and p.roleId=r.id and u.userName=#&#123;userName&#125;    &lt;/select&gt;&lt;/mapper&gt;\n很简单只有三个方法，分别对应上面所说的三个方法。对sql稍微熟悉点的童鞋应该都能看懂，不太清楚就拷到数据库中执行一下就行了，数据库的Sql也在我的github上。实体类就比较简单了，就只有四个字段以及get,set方法。我就这里就不贴了，具体可以去github上fork我的源码。\n现在就需要创建自定义的MyRealm类，这个还是比较重要的。继承至Shiro的AuthorizingRealm类，用于处理自己的验证逻辑，下面贴一下我的代码：\npackage com.crossoverJie.shiro;import com.crossoverJie.pojo.T_user;import com.crossoverJie.service.T_userService;import org.apache.shiro.authc.AuthenticationException;import org.apache.shiro.authc.AuthenticationInfo;import org.apache.shiro.authc.AuthenticationToken;import org.apache.shiro.authc.SimpleAuthenticationInfo;import org.apache.shiro.authz.AuthorizationInfo;import org.apache.shiro.authz.SimpleAuthorizationInfo;import org.apache.shiro.realm.AuthorizingRealm;import org.apache.shiro.subject.PrincipalCollection;import javax.annotation.Resource;import java.util.Set;/** * Created with IDEA * Created by $&#123;jie.chen&#125; on 2016/7/14. * Shiro自定义域 */public class MyRealm extends AuthorizingRealm &#123;    @Resource    private T_userService t_userService;    /**     * 用于的权限的认证。     * @param principalCollection     * @return     */    @Override    protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) &#123;        String username = principalCollection.getPrimaryPrincipal().toString() ;        SimpleAuthorizationInfo info = new SimpleAuthorizationInfo() ;        Set&lt;String&gt; roleName = t_userService.findRoles(username) ;        Set&lt;String&gt; permissions = t_userService.findPermissions(username) ;        info.setRoles(roleName);        info.setStringPermissions(permissions);        return info;    &#125;    /**     * 首先执行这个登录验证     * @param token     * @return     * @throws AuthenticationException     */    @Override    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token)            throws AuthenticationException &#123;        //获取用户账号        String username = token.getPrincipal().toString() ;        T_user user = t_userService.findUserByUsername(username) ;        if (user != null)&#123;            //将查询到的用户账号和密码存放到 authenticationInfo用于后面的权限判断。第三个参数随便放一个就行了。            AuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo(user.getUserName(),user.getPassword(),                    &quot;a&quot;) ;            return authenticationInfo ;        &#125;else&#123;            return  null ;        &#125;    &#125;&#125;\n继承AuthorizingRealm类之后就需要覆写它的两个方法，doGetAuthorizationInfo,doGetAuthenticationInfo，这两个方法的作用我都有写注释，逻辑也比较简单。doGetAuthenticationInfo是用于登录验证的，在登录的时候需要将数据封装到Shiro的一个token中，执行shiro的login()方法，之后只要我们将MyRealm这个类配置到Spring中，登录的时候Shiro就会自动的调用doGetAuthenticationInfo()方法进行验证。哦对了，忘了贴下登录的Controller了：\npackage com.crossoverJie.controller;import com.crossoverJie.pojo.T_user;import com.crossoverJie.service.T_userService;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.UsernamePasswordToken;import org.apache.shiro.subject.Subject;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import javax.annotation.Resource;/** * Created with IDEA * Created by $&#123;jie.chen&#125; on 2016/7/14. * 后台Controller */@Controller@RequestMapping(&quot;/&quot;)public class T_userController &#123;    @Resource    private T_userService t_userService ;    @RequestMapping(&quot;/loginAdmin&quot;)    public String login(T_user user, Model model)&#123;        Subject subject = SecurityUtils.getSubject() ;        UsernamePasswordToken token = new UsernamePasswordToken(user.getUserName(),user.getPassword()) ;        try &#123;            subject.login(token);            return &quot;admin&quot; ;        &#125;catch (Exception e)&#123;            //这里将异常打印关闭是因为如果登录失败的话会自动抛异常//            e.printStackTrace();            model.addAttribute(&quot;error&quot;,&quot;用户名或密码错误&quot;) ;            return &quot;../../login&quot; ;        &#125;    &#125;    @RequestMapping(&quot;/admin&quot;)    public String admin()&#123;        return &quot;admin&quot;;    &#125;    @RequestMapping(&quot;/student&quot;)    public String student()&#123;        return &quot;admin&quot; ;    &#125;    @RequestMapping(&quot;/teacher&quot;)    public String teacher()&#123;        return &quot;admin&quot; ;    &#125;&#125;\n\n主要就是login()方法。逻辑比较简单，只是登录验证的时候不是像之前那样直接查询数据库然后返回是否有用户了，而是调用subject的login()方法,就是我上面提到的，调用login()方法时Shiro会自动调用我们自定义的MyRealm类中的doGetAuthenticationInfo()方法进行验证的，验证逻辑是先根据用户名查询用户，如果查询到的话再将查询到的用户名和密码放到SimpleAuthenticationInfo对象中，Shiro会自动根据用户输入的密码和查询到的密码进行匹配，如果匹配不上就会抛出异常，匹配上之后就会执行doGetAuthorizationInfo()进行相应的权限验证。doGetAuthorizationInfo()方法的处理逻辑也比较简单，根据用户名获取到他所拥有的角色以及权限，然后赋值到SimpleAuthorizationInfo对象中即可，Shiro就会按照我们配置的XX角色对应XX权限来进行判断，这个配置在下面的整合中会讲到。\n\n整合Spring接下来应该是大家比较关系的一步：整合Spring。我是在之前的Spring SpringMVC Mybatis的基础上进行整合的。\nweb.xml配置首先我们需要在web.xml进行配置Shiro的过滤器。我只贴Shiro部分的，其余的和之前配置是一样的。\n&lt;!-- shiro过滤器定义 --&gt;&lt;filter&gt;    &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt;    &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;    &lt;init-param&gt;        &lt;!-- 该值缺省为false,表示生命周期由SpringApplicationContext管理,设置为true则表示由ServletContainer管理 --&gt;        &lt;param-name&gt;targetFilterLifecycle&lt;/param-name&gt;        &lt;param-value&gt;true&lt;/param-value&gt;    &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt;    &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt;    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;\n配置还是比较简单的，这样会过滤所有的请求。之后我们还需要在Spring中配置一个shiroFilter的bean。\nspring-mybatis.xml配置由于这里配置较多，我就全部贴一下：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans                        http://www.springframework.org/schema/beans/spring-beans-3.1.xsd                        http://www.springframework.org/schema/context                        http://www.springframework.org/schema/context/spring-context-3.1.xsd&quot;&gt;    &lt;!-- 自动扫描 --&gt;    &lt;context:component-scan base-package=&quot;com.crossoverJie&quot; /&gt;    &lt;!-- 引入配置文件 --&gt;    &lt;bean id=&quot;propertyConfigurer&quot;          class=&quot;org.springframework.beans.factory.config.PropertyPlaceholderConfigurer&quot;&gt;        &lt;property name=&quot;location&quot; value=&quot;classpath:jdbc.properties&quot; /&gt;    &lt;/bean&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;          init-method=&quot;init&quot; destroy-method=&quot;close&quot;&gt;        &lt;!-- 指定连接数据库的驱动 --&gt;        &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driverClass&#125;&quot; /&gt;        &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot; /&gt;        &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.user&#125;&quot; /&gt;        &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot; /&gt;        &lt;!-- 配置初始化大小、最小、最大 --&gt;        &lt;property name=&quot;initialSize&quot; value=&quot;3&quot; /&gt;        &lt;property name=&quot;minIdle&quot; value=&quot;3&quot; /&gt;        &lt;property name=&quot;maxActive&quot; value=&quot;20&quot; /&gt;        &lt;!-- 配置获取连接等待超时的时间 --&gt;        &lt;property name=&quot;maxWait&quot; value=&quot;60000&quot; /&gt;        &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt;        &lt;property name=&quot;timeBetweenEvictionRunsMillis&quot; value=&quot;60000&quot; /&gt;        &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt;        &lt;property name=&quot;minEvictableIdleTimeMillis&quot; value=&quot;300000&quot; /&gt;        &lt;property name=&quot;validationQuery&quot; value=&quot;SELECT &#x27;x&#x27;&quot; /&gt;        &lt;property name=&quot;testWhileIdle&quot; value=&quot;true&quot; /&gt;        &lt;property name=&quot;testOnBorrow&quot; value=&quot;false&quot; /&gt;        &lt;property name=&quot;testOnReturn&quot; value=&quot;false&quot; /&gt;        &lt;!-- 打开PSCache，并且指定每个连接上PSCache的大小 --&gt;        &lt;property name=&quot;poolPreparedStatements&quot; value=&quot;true&quot; /&gt;        &lt;property name=&quot;maxPoolPreparedStatementPerConnectionSize&quot;                  value=&quot;20&quot; /&gt;        &lt;!-- 配置监控统计拦截的filters，去掉后监控界面sql无法统计 --&gt;        &lt;property name=&quot;filters&quot; value=&quot;stat&quot; /&gt;    &lt;/bean&gt;    &lt;!-- spring和MyBatis完美整合，不需要mybatis的配置映射文件 --&gt;    &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;        &lt;!-- 自动扫描mapping.xml文件 --&gt;        &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:mapping/*.xml&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;    &lt;!-- DAO接口所在包名，Spring会自动查找其下的类 --&gt;    &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt;        &lt;property name=&quot;basePackage&quot; value=&quot;com.crossoverJie.dao&quot; /&gt;        &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;    &lt;!-- (事务管理)transaction manager, use JtaTransactionManager for global tx --&gt;    &lt;bean id=&quot;transactionManager&quot;          class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;    &lt;/bean&gt;    &lt;!-- 配置自定义Realm --&gt;    &lt;bean id=&quot;myRealm&quot; class=&quot;com.crossoverJie.shiro.MyRealm&quot;/&gt;    &lt;!-- 安全管理器 --&gt;    &lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt;        &lt;property name=&quot;realm&quot; ref=&quot;myRealm&quot;/&gt;    &lt;/bean&gt;    &lt;!-- Shiro过滤器 核心--&gt;    &lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt;        &lt;!-- Shiro的核心安全接口,这个属性是必须的 --&gt;        &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt;        &lt;!-- 身份认证失败，则跳转到登录页面的配置 --&gt;        &lt;property name=&quot;loginUrl&quot; value=&quot;/login.jsp&quot;/&gt;        &lt;!-- 权限认证失败，则跳转到指定页面 --&gt;        &lt;property name=&quot;unauthorizedUrl&quot; value=&quot;/nopower.jsp&quot;/&gt;        &lt;!-- Shiro连接约束配置,即过滤链的定义 --&gt;        &lt;property name=&quot;filterChainDefinitions&quot;&gt;            &lt;value&gt;                &lt;!--anon 表示匿名访问，不需要认证以及授权--&gt;                /loginAdmin=anon                &lt;!--authc表示需要认证 没有进行身份认证是不能进行访问的--&gt;                /admin*=authc                /student=roles[teacher]                /teacher=perms[&quot;user:create&quot;]            &lt;/value&gt;        &lt;/property&gt;    &lt;/bean&gt;    &lt;!-- 保证实现了Shiro内部lifecycle函数的bean执行 --&gt;    &lt;bean id=&quot;lifecycleBeanPostProcessor&quot; class=&quot;org.apache.shiro.spring.LifecycleBeanPostProcessor&quot;/&gt;    &lt;!-- 开启Shiro注解 --&gt;    &lt;bean class=&quot;org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator&quot;          depends-on=&quot;lifecycleBeanPostProcessor&quot;/&gt;    &lt;bean class=&quot;org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt;        &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt;    &lt;/bean&gt;&lt;/beans&gt;\n在这里我们配置了上文中所提到的自定义myRealm,这样Shiro就可以按照我们自定义的逻辑来进行权限验证了。其余的都比较简单，看注释应该都能明白。着重讲解一下：\n&lt;property name=&quot;filterChainDefinitions&quot;&gt;    &lt;value&gt;        &lt;!--anon 表示匿名访问，不需要认证以及授权--&gt;        /loginAdmin=anon        &lt;!--authc表示需要认证 没有进行身份认证是不能进行访问的--&gt;        /admin*=authc        /student=roles[teacher]        /teacher=perms[&quot;user:create&quot;]    &lt;/value&gt;&lt;/property&gt;\n\n&#x2F;loginAdmin&#x3D;anon的意思的意思是，发起&#x2F;loginAdmin这个请求是不需要进行身份认证的，这个请求在这次项目中是一个登录请求，一般对于这样的请求都是不需要身份认证的。\n&#x2F;admin*&#x3D;authc表示 &#x2F;admin,&#x2F;admin1,&#x2F;admin2这样的请求都是需要进行身份认证的，不然是不能访问的。\n&#x2F;student&#x3D;roles[teacher]表示访问&#x2F;student请求的用户必须是teacher角色，不然是不能进行访问的。\n&#x2F;teacher&#x3D;perms[“user:create”]表示访问&#x2F;teacher请求是需要当前用户具有user:create权限才能进行访问的。更多相关权限过滤的资料可以访问shiro的官方介绍：传送门\n\n\n使用Shiro标签库Shiro还有着强大标签库，可以在前端帮我获取信息和做判断。我贴一下我这里登录完成之后显示的界面：\n&lt;%--  Created by IntelliJ IDEA.  User: Administrator  Date: 2016/7/14  Time: 13:17  To change this template use File | Settings | File Templates.--%&gt;&lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;&lt;%@ taglib prefix=&quot;shiro&quot; uri=&quot;http://shiro.apache.org/tags&quot; %&gt;&lt;html&gt;&lt;head&gt;    &lt;title&gt;后台&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;shiro:hasRole name=&quot;admin&quot;&gt;    这是admin角色登录：&lt;shiro:principal&gt;&lt;/shiro:principal&gt;&lt;/shiro:hasRole&gt;&lt;shiro:hasPermission name=&quot;user:create&quot;&gt;    有user:create权限信息&lt;/shiro:hasPermission&gt;&lt;br&gt;登录成功&lt;/body&gt;&lt;/html&gt;\n要想使用Shiro标签，只需要引入一下标签即可：&lt;%@ taglib prefix=&quot;shiro&quot; uri=&quot;http://shiro.apache.org/tags&quot; %&gt;其实英语稍微好点的童鞋应该都能看懂。下面我大概介绍下一些标签的用法：\n\n&lt;shiro:hasRole name&#x3D;”admin”&gt;具有admin角色才会显示标签内的信息。\nshiro:principal获取用户信息。默认调用 Subject.getPrincipal()获取，即 Primary Principal。\n&lt;shiro:hasPermission name&#x3D;”user:create”&gt; 用户拥有user:create这个权限才回显示标签内的信息。更多的标签可以查看官网：传送门\n\n\n整体测试\n这是我的测试数据。首先来验证一下登录：先输入一个错误的账号和密码：\n接下来输入一个正确的：\n可以看到我登录的用户是crossoverJie他是有admin的角色，并且拥有user:*(ps:系统数据详见上面的数据库截图)的权限，所以在这里：\n&lt;shiro:hasRole name=&quot;admin&quot;&gt;       这是admin角色登录：&lt;shiro:principal&gt;&lt;/shiro:principal&gt;&lt;/shiro:hasRole&gt;&lt;shiro:hasPermission name=&quot;user:create&quot;&gt;    有user:create权限信息&lt;/shiro:hasPermission&gt;\n是能显示出标签内的信息，并把用户信息也显示出来了。接着我们来访问一下/student这个请求，因为在Spring的配置文件中：\n&lt;property name=&quot;filterChainDefinitions&quot;&gt;    &lt;value&gt;        &lt;!--anon 表示匿名访问，不需要认证以及授权--&gt;        /loginAdmin=anon        &lt;!--authc表示需要认证 没有进行身份认证是不能进行访问的--&gt;        /admin*=authc        /student=roles[teacher]        /teacher=perms[&quot;user:create&quot;]    &lt;/value&gt;&lt;/property&gt;\n只有teacher角色才能访问/student这个请求的：\n果然，Shiro做了安全控制是不能进行访问的。然后我们换aaa用户登录，他正好是teacher角色，看能不能访问/student。\n\n果然是能访问的。因为我在控制器里访问/student返回的是同一个界面所以看到的还是这个界面。\n@RequestMapping(&quot;/teacher&quot;)public String teacher()&#123;    return &quot;admin&quot; ;&#125;\n并且没有显示之前Shiro标签内的内容。其他的我就不测了，大家可以自己在数据库里加一些数据，或者是改下拦截的权限多试试，这样对Shiro的理解就会更加深刻。\n\nMD5加密Shiro还封装了一个我认为非常不错的功能，那就是MD5加密，代码如下：\npackage com.crossoverJie.shiro;import org.apache.shiro.crypto.hash.Md5Hash;/** * Created with IDEA * 基于Shiro的MD5加密 * Created by $&#123;jie.chen&#125; on 2016/7/13. */public class MD5Util &#123;    public static String md5(String str,String salt)&#123;        return new Md5Hash(str,salt).toString() ;    &#125;    public static void main(String[] args) &#123;        String md5 = md5(&quot;abc123&quot;,&quot;crossoverjie&quot;) ;        System.out.println(md5);    &#125;&#125;\n代码非常简单，只需要调用Md5Hash(str,salt)方法即可，这里多了一个参数，第一个参数不用多解释，是需要加密的字符串。第二个参数salt中文翻译叫盐，加密的时候我们传一个字符串进去，只要这个salt不被泄露出去，那原则上加密之后是无法被解密的，在存用户密码的时候可以使用，感觉还是非常屌的。\n\n总结以上就是Shiro实际使用的案例，将的比较初略，但是关于Shiro的核心东西都在里面了。大家可以去我的github上下载源码，只要按照我给的数据库就没有问题，项目跑起来之后试着改下里面的东西可以加深对Shiro的理解。\n\n项目地址：https://github.com/crossoverJie/SSM.git个人博客地址：http://crossoverjie.top。GitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["Java","IDEA","Shiro"]},{"title":"SSM(四)WebService入门详解","url":"/2016/08/02/SSM4/","content":"\n前言webservice这个不知道大家首次接触的时候是怎么理解的，反正我记得我当时第一次接触这个东西的时候以为又是一个XX框架，觉得还挺高大上。然而这一切在之后我使用过后才发现这些全都是YY。那么webservice到底是什么呢，根据我自己的理解：简单来说就像是一个公开的接口，其他系统不管你是用什么语言来编写的都可以调用这个接口，并可以返回相应的数据给你。就像是现在很多的天气应用，他们肯定不会自己去搞一个气象局之类的部门去监测天气，大多都是直接调用一个天气接口，然后返回天气数据，相关应用就可以将这些信息展示给用户了。通常来说发布这类接口的应用都是用一两种语言来编写即可，但是调用这个接口应用可能会是各种语言来编写的，为了满足这样的需求webservice出现了。\n\n简单来说webservice就是为了满足以上需求而定义出来的规范。\n\n\nSpring整合CXF在Java中实现webservice有多种方法，java本身在jdk1.7之后也对webservice有了默认的实现，但是在我们实际开发中一般还是会使用框架来，比如这里所提到的CXF就有着广泛的应用。废话我就不多说了，直接讲Spring整合CXF，毕竟现在的JavaEE开发是离不开Spring了。该项目还是基于之前的SSM进行开发的。\n加入maven依赖第一步肯定是要加入maven依赖：\n&lt;!--cxf--&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.cxf/cxf-rt-frontend-jaxws --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.cxf&lt;/groupId&gt;    &lt;artifactId&gt;cxf-rt-frontend-jaxws&lt;/artifactId&gt;    &lt;version&gt;3.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.cxf/cxf-core --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.cxf&lt;/groupId&gt;    &lt;artifactId&gt;cxf-core&lt;/artifactId&gt;    &lt;version&gt;3.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.cxf/cxf-rt-transports-http --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.apache.cxf&lt;/groupId&gt;    &lt;artifactId&gt;cxf-rt-transports-http&lt;/artifactId&gt;    &lt;version&gt;3.1.6&lt;/version&gt;&lt;/dependency&gt;\n\n\nweb.xml配置接着我们需要配置一个CXF的servlet：\n&lt;!--定义一个cxf的servlet--&gt;&lt;servlet&gt;    &lt;servlet-name&gt;CXFServlet&lt;/servlet-name&gt;    &lt;servlet-class&gt;org.apache.cxf.transport.servlet.CXFServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt;    &lt;servlet-name&gt;CXFServlet&lt;/servlet-name&gt;    &lt;url-pattern&gt;/webservice/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;\n之后只要我们访问webservice&#x2F;*这个地址就会进入CXF的servlet中。\n整合Spring配置接下来是最重要的一部，用Spring整合CXF：在这之前我有新建一个CXF的包，如下图：\n\n这里有两个主要类\n\nHelloWorld接口。\n实现HelloWorld接口的HelloWorldImpl类。代码如下：HelloWorld.javapackage com.crossoverJie.cxf;import javax.jws.WebService;@WebServicepublic interface HelloWorld &#123;\tpublic String say(String str);&#125;\n其中就只定义了一个简单的say()方法。HelloWorldImpl.javapackage com.crossoverJie.cxf.impl;import com.crossoverJie.cxf.HelloWorld;import org.springframework.stereotype.Component;import javax.jws.WebService;@Component(&quot;helloWorld&quot;)@WebServicepublic class HelloWorldImpl implements HelloWorld &#123;\tpublic String say(String str) &#123;\t\treturn &quot;Hello&quot;+str;\t&#125;&#125;\n这里就是对say()方法的简单实现。接下来就是整合Spring了，由于需要使用到CXF的标签，所以我们需要添加额外的命名路径如下：\n\n&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xmlns:p=&quot;http://www.springframework.org/schema/p&quot;       xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xmlns:jee=&quot;http://www.springframework.org/schema/jee&quot;       xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot;       xmlns:jaxws=&quot;http://cxf.apache.org/jaxws&quot;       xsi:schemaLocation=&quot;        http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd        http://www.springframework.org/schema/jee http://www.springframework.org/schema/jee/spring-jee-4.0.xsd        http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd        http://cxf.apache.org/jaxws http://cxf.apache.org/schemas/jaxws.xsd&quot;&gt;    &lt;import resource=&quot;classpath:META-INF/cxf/cxf.xml&quot;/&gt;    &lt;import resource=&quot;classpath:META-INF/cxf/cxf-servlet.xml&quot;/&gt;    &lt;!-- 自动扫描webService --&gt;    &lt;context:component-scan base-package=&quot;com.crossoverJie.cxf&quot; /&gt;    &lt;!-- 定义webservice的发布接口  --&gt;    &lt;jaxws:endpoint            implementor=&quot;#helloWorld&quot;            address=&quot;/HelloWorld&quot;&lt;/beans&gt;\n\n更加具体的配置可以查看官方给出的文档:http://cxf.apache.org/docs/how-do-i-develop-a-service.html。#helloWorld指的是我们在HelloWorldImpl类中所自定义的名字，/HelloWorld则是我们需要访问的地址。之后我们运行项目输入该地址：http://127.0.0.1:8080/ssm/webservice/HelloWorld?wsdl如果出现如下界面：\n\n则说明我们的webservice发布成功了。接下来只需要通过客户端调用这个接口即可获得返回结果了。\n\n总结以上就是一个简单的webservice入门实例，更多的关于CXF拦截器，客户端调用就没有做过多介绍，后续有时间的话再接着更新。\n\n项目地址：https://github.com/crossoverJie/SSM.git个人博客地址：http://crossoverjie.top。GitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["Java","IDEA","CXF"]},{"title":"SSM(五)基于webSocket的聊天室","url":"/2016/09/04/SSM5/","content":"\n前言不知大家在平时的需求中有没有遇到需要实时处理信息的情况，如站内信，订阅，聊天之类的。在这之前我们通常想到的方法一般都是采用轮训的方式每隔一定的时间向服务器发送请求从而获得最新的数据，但这样会浪费掉很多的资源并且也不是实时的，于是随着HTML5的推出带来了websocket可以根本的解决以上问题实现真正的实时传输。\nwebsocket是什么？至于websocket是什么、有什么用这样的问题一Google一大把，这里我就简要的说些websocket再本次实例中的作用吧。由于在本次实例中需要实现的是一个聊天室，一个实时的聊天室。如下图：\n\n\n\n采用websocket之后可以让前端和和后端像C&#x2F;S模式一样实时通信，不再需要每次单独发送请求。由于是基于H5的所以对于老的浏览器如IE7、IE8之类的就没办法了，不过H5是大势所趋这点不用担心。\n后端既然推出了websocket，作为现在主流的Java肯定也有相应的支持，所以在JavaEE7之后也对websocket做出了规范，所以本次的代码理论上是要运行在Java1.7+和Tomcat7.0+之上的。看过我前面几篇文章的朋友应该都知道本次实例也是运行在之前的SSM之上的，所以这里就不再赘述了。首先第一步需要加入websocket的依赖：\n&lt;!-- https://mvnrepository.com/artifact/javax.websocket/javax.websocket-api --&gt;&lt;dependency&gt;    &lt;groupId&gt;javax.websocket&lt;/groupId&gt;    &lt;artifactId&gt;javax.websocket-api&lt;/artifactId&gt;    &lt;version&gt;1.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.springframework&lt;/groupId&gt;    &lt;artifactId&gt;spring-websocket&lt;/artifactId&gt;    &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;\n以上就是使用websocket所需要用到的包。spring-websocket这个主要是在之后需要在websocket的后端注入service所需要的。之后再看一下后端的核心代码MyWebSocket.java\npackage com.crossoverJie.controller;/** * Created by Administrator on 2016/8/7. */import com.crossoverJie.pojo.Content;import com.crossoverJie.service.ContentService;import org.apache.camel.BeanInject;import org.apache.camel.EndpointInject;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import org.springframework.stereotype.Controller;import org.springframework.web.context.support.SpringBeanAutowiringSupport;import org.springframework.web.socket.server.standard.SpringConfigurator;import java.io.IOException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.concurrent.CopyOnWriteArraySet;import javax.annotation.PostConstruct;import javax.websocket.OnClose;import javax.websocket.OnError;import javax.websocket.OnMessage;import javax.websocket.OnOpen;import javax.websocket.Session;import javax.websocket.server.ServerEndpoint;//该注解用来指定一个URI，客户端可以通过这个URI来连接到WebSocket。/**  类似Servlet的注解mapping。无需在web.xml中配置。 * configurator = SpringConfigurator.class是为了使该类可以通过Spring注入。 */@ServerEndpoint(value = &quot;/websocket&quot;,configurator = SpringConfigurator.class)public class MyWebSocket &#123;    //静态变量，用来记录当前在线连接数。应该把它设计成线程安全的。    private static int onlineCount = 0;    public MyWebSocket() &#123;    &#125;    @Autowired    private ContentService contentService ;    //concurrent包的线程安全Set，用来存放每个客户端对应的MyWebSocket对象。    // 若要实现服务端与单一客户端通信的话，可以使用Map来存放，其中Key可以为用户标识    private static CopyOnWriteArraySet&lt;MyWebSocket&gt; webSocketSet = new CopyOnWriteArraySet&lt;MyWebSocket&gt;();    //与客户端的连接会话，需要通过它来给客户端发送数据    private Session session;    /**     * 连接建立成功调用的方法     * @param session  可选的参数。session为与某个客户端的连接会话，需要通过它来给客户端发送数据     */    @OnOpen    public void onOpen(Session session)&#123;        this.session = session;        webSocketSet.add(this);     //加入set中        addOnlineCount();           //在线数加1        System.out.println(&quot;有新连接加入！当前在线人数为&quot; + getOnlineCount());    &#125;    /**     * 连接关闭调用的方法     */    @OnClose    public void onClose()&#123;        webSocketSet.remove(this);  //从set中删除        subOnlineCount();           //在线数减1        System.out.println(&quot;有一连接关闭！当前在线人数为&quot; + getOnlineCount());    &#125;    /**     * 收到客户端消息后调用的方法     * @param message 客户端发送过来的消息     * @param session 可选的参数     */    @OnMessage    public void onMessage(String message, Session session) &#123;        System.out.println(&quot;来自客户端的消息:&quot; + message);        //群发消息        for(MyWebSocket item: webSocketSet)&#123;            try &#123;                item.sendMessage(message);            &#125; catch (IOException e) &#123;                e.printStackTrace();                continue;            &#125;        &#125;    &#125;    /**     * 发生错误时调用     * @param session     * @param error     */    @OnError    public void onError(Session session, Throwable error)&#123;        System.out.println(&quot;发生错误&quot;);        error.printStackTrace();    &#125;    /**     * 这个方法与上面几个方法不一样。没有用注解，是根据自己需要添加的方法。     * @param message     * @throws IOException     */    public void sendMessage(String message) throws IOException&#123;        //保存数据到数据库        Content content = new Content() ;        content.setContent(message);        SimpleDateFormat sm = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:dd&quot;) ;        content.setCreateDate(sm.format(new Date()));        contentService.insertSelective(content) ;        this.session.getBasicRemote().sendText(message);        //this.session.getAsyncRemote().sendText(message);    &#125;    public static synchronized int getOnlineCount() &#123;        return onlineCount;    &#125;    public static synchronized void addOnlineCount() &#123;        MyWebSocket.onlineCount++;    &#125;    public static synchronized void subOnlineCount() &#123;        MyWebSocket.onlineCount--;    &#125;&#125;\n这就是整个websocket的后端代码。看起来也比较简单主要就是使用那几个注解。每当有一个客户端连入、关闭、发送消息都会调用各自注解的方法。这里我讲一下sendMessage()这个方法。\nwebsocket绕坑在sendMessage()方法中我只想实现一个简单的功能，就是将每次的聊天记录都存到数据库中。看似一个简单的功能硬是花了我半天的时间。我先是按照以前的惯性思维只需要在这个类中注入service即可。但是无论怎么弄每次都注入不进来都是null。最后没办法只有google了，最后终于在神级社区StackOverFlow中找到了答案，就是前边所说的需要添加的第二个\tmaven依赖，然后加入@ServerEndpoint(value = &quot;/websocket&quot;,configurator = SpringConfigurator.class)这个注解即可利用Spring注入了。接着就可以做消息的保存了。\n前端前端我采用了Bootstrap做的，不太清楚Bootstrap的童鞋建议先看下官方文档也比较简单。还是先贴一下代码：\n&lt;%@ page language=&quot;java&quot; import=&quot;java.util.*&quot; pageEncoding=&quot;UTF-8&quot; %&gt;&lt;%    String path = request.getContextPath();    String basePath = request.getScheme() + &quot;://&quot; + request.getServerName() + &quot;:&quot; + request.getServerPort() + path + &quot;/&quot;;%&gt;&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt;    &lt;base href=&quot;&lt;%=basePath%&gt;&quot;&gt;    &lt;!-- Bootstrap --&gt;    &lt;link rel=&quot;stylesheet&quot;          href=&quot;http://cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css&quot;&gt;    &lt;!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries --&gt;    &lt;!-- WARNING: Respond.js doesn&#x27;t work if you view the page via file:// --&gt;    &lt;!--[if lt IE 9]&gt;    &lt;script src=&quot;//cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js&quot;&gt;&lt;/script&gt;    &lt;script src=&quot;//cdn.bootcss.com/respond.js/1.4.2/respond.min.js&quot;&gt;&lt;/script&gt;    &lt;![endif]--&gt;    &lt;script type=&quot;text/javascript&quot; charset=&quot;utf-8&quot; src=&quot;&lt;%=path%&gt;/ueditor/ueditor.config.js&quot;&gt;&lt;/script&gt;    &lt;script type=&quot;text/javascript&quot; charset=&quot;utf-8&quot; src=&quot;&lt;%=path%&gt;/ueditor/ueditor.all.min.js&quot;&gt; &lt;/script&gt;    &lt;!--建议手动加在语言，避免在ie下有时因为加载语言失败导致编辑器加载失败--&gt;    &lt;!--这里加载的语言文件会覆盖你在配置项目里添加的语言类型，比如你在配置项目里配置的是英文，这里加载的中文，那最后就是中文--&gt;    &lt;script type=&quot;text/javascript&quot; charset=&quot;utf-8&quot; src=&quot;&lt;%=path%&gt;/ueditor/lang/zh-cn/zh-cn.js&quot;&gt;&lt;/script&gt;    &lt;title&gt;聊天室&lt;/title&gt;&lt;/head&gt;&lt;body data=&quot;/ssm&quot;&gt;&lt;input id=&quot;text&quot; type=&quot;text&quot;/&gt;&lt;button onclick=&quot;send()&quot;&gt;发送&lt;/button&gt;&lt;button onclick=&quot;closeWebSocket()&quot;&gt;关闭连接&lt;/button&gt;&lt;div id=&quot;message&quot;&gt;&lt;/div&gt;&lt;div class=&quot;container-fluid&quot;&gt;    &lt;div class=&quot;row&quot;&gt;        &lt;div class=&quot;col-md-12&quot;&gt;            &lt;div class=&quot;panel panel-primary&quot;&gt;                &lt;div class=&quot;panel-heading&quot;&gt;聊天室&lt;/div&gt;                &lt;div id=&quot;msg&quot; class=&quot;panel-body&quot;&gt;                &lt;/div&gt;                &lt;div class=&quot;panel-footer&quot;&gt;                    在线人数&lt;span id=&quot;onlineCount&quot;&gt;1&lt;/span&gt;人                &lt;/div&gt;            &lt;/div&gt;        &lt;/div&gt;    &lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;container-fluid&quot;&gt;    &lt;div class=&quot;row&quot;&gt;        &lt;div class=&quot;col-md-12&quot;&gt;            &lt;script id=&quot;editor&quot; type=&quot;text/plain&quot; style=&quot;width:1024px;height:200px;&quot;&gt;&lt;/script&gt;        &lt;/div&gt;    &lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;container-fluid&quot;&gt;    &lt;div class=&quot;row&quot;&gt;        &lt;div class=&quot;col-md-12&quot;&gt;            &lt;p class=&quot;text-right&quot;&gt;            &lt;button onclick=&quot;sendMsg();&quot; class=&quot;btn btn-success&quot;&gt;发送&lt;/button&gt;            &lt;/p&gt;        &lt;/div&gt;    &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;script type=&quot;text/javascript&quot;&gt;    var ue = UE.getEditor(&#x27;editor&#x27;);    var websocket = null;    //判断当前浏览器是否支持WebSocket    if (&#x27;WebSocket&#x27; in window) &#123;        websocket = new WebSocket(&quot;ws://192.168.0.102:8080/ssm/websocket&quot;);    &#125;    else &#123;        alert(&quot;对不起！你的浏览器不支持webSocket&quot;)    &#125;    //连接发生错误的回调方法    websocket.onerror = function () &#123;        setMessageInnerHTML(&quot;error&quot;);    &#125;;    //连接成功建立的回调方法    websocket.onopen = function (event) &#123;        setMessageInnerHTML(&quot;加入连接&quot;);    &#125;;    //接收到消息的回调方法    websocket.onmessage = function (event) &#123;        setMessageInnerHTML(event.data);    &#125;;    //连接关闭的回调方法    websocket.onclose = function () &#123;        setMessageInnerHTML(&quot;断开连接&quot;);    &#125;;    //监听窗口关闭事件，当窗口关闭时，主动去关闭websocket连接，    // 防止连接还没断开就关闭窗口，server端会抛异常。    window.onbeforeunload = function () &#123;        var is = confirm(&quot;确定关闭窗口？&quot;);        if (is)&#123;            websocket.close();        &#125;    &#125;;    //将消息显示在网页上    function setMessageInnerHTML(innerHTML) &#123;        $(&quot;#msg&quot;).append(innerHTML+&quot;&lt;br/&gt;&quot;)    &#125;;    //关闭连接    function closeWebSocket() &#123;        websocket.close();    &#125;    //发送消息    function send() &#123;        var message = $(&quot;#text&quot;).val() ;        websocket.send(message);        $(&quot;#text&quot;).val(&quot;&quot;) ;    &#125;    function sendMsg()&#123;        var msg = ue.getContent();        websocket.send(msg);        ue.setContent(&#x27;&#x27;);    &#125;&lt;/script&gt;&lt;!-- jQuery (necessary for Bootstrap&#x27;s JavaScript plugins) --&gt;&lt;script src=&quot;http://cdn.bootcss.com/jquery/1.11.3/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;!-- Include all compiled plugins (below), or include individual files as needed --&gt;&lt;script src=&quot;http://cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;&lt;%=path%&gt;/js/Globals.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;&lt;%=path%&gt;/js/websocket.js&quot;&gt;&lt;/script&gt;&lt;/html&gt;\n其实其中重要的就是那几个JS方法，都写有注释。需要注意的是这里\n//判断当前浏览器是否支持WebSocketif (&#x27;WebSocket&#x27; in window) &#123;    websocket = new WebSocket(&quot;ws://192.168.0.102:8080/ssm/websocket&quot;);&#125;else &#123;    alert(&quot;对不起！你的浏览器不支持webSocket&quot;)&#125;\n当项目跑起来之后需要将这里的地址改为你项目的地址即可。哦对了，我在这里采用了百度的一个Ueditor的富文本编辑器(虽然百度搜索我现在很少用了，但是这个编辑器确实还不错)，这个编辑器也比较简单只需要个性化的配置一下个人的需求即可。\nUeditor相关配置直接使用我项目运行的童鞋就不需要重新下载了，我将资源放在了webapp目录下的ueditor文件夹下面的。值得注意的是我们首先需要将jsp--&gt;lib下的jar包加入到项目中。加好之后会出现一个想下的箭头表示已经引入成功。，之后修改该目录下的config.json文件，主要修改以下内容即可：\n&quot;imageAllowFiles&quot;: [&quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.gif&quot;, &quot;.bmp&quot;], /* 上传图片格式显示 */&quot;imageCompressEnable&quot;: true, /* 是否压缩图片,默认是true */&quot;imageCompressBorder&quot;: 1600, /* 图片压缩最长边限制 */&quot;imageInsertAlign&quot;: &quot;none&quot;, /* 插入的图片浮动方式 */&quot;imageUrlPrefix&quot;: &quot;http://192.168.0.102:8080/ssm&quot;, /* 图片访问路径前缀 */&quot;imagePathFormat&quot;: &quot;/ueditor/jsp/upload/image/&#123;yyyy&#125;&#123;mm&#125;&#123;dd&#125;/&#123;time&#125;&#123;rand:6&#125;&quot;,\n这里主要是要修改imageUrlPrefix为你自己的项目地址就可以了。ueditor一个我认为很不错的就是他支持图片、多图、截图上传，而且都不需要手动编写后端接口，所有上传的文件、图片都会保存到项目发布出去的jsp--&gt;upload文件夹下一看就明白了。更多关于ueditor的配置可以查看官网。\n\n其中值得注意一点的是，由于项目采用了Spring MVC并拦截了所有的请求，导致静态资源不能访问，如果是需要用到上传txt文件之类的需求可以参照web.xml中修改，如下:\n\n&lt;servlet-mapping&gt;    &lt;servlet-name&gt;default&lt;/servlet-name&gt;    &lt;url-pattern&gt;*.txt&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;\n这样就可以访问txt文件了，如果还需要上传PPT之类的就以此类推。\n总结这样一个简单的基于websocket的聊天室就算完成了，感兴趣的朋友可以将项目部署到外网服务器上这样好基友之间就可以愉快的聊(zhuang)天(bi)了。当然这只是一个简单的项目，感兴趣的朋友再这基础之上加入实时在线人数，用户名和IP之类的。\n\n项目地址：https://github.com/crossoverJie/SSM.git个人博客地址：http://crossoverjie.top。GitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["Java","websocket","HTML5","ueditor"]},{"title":"SSM(六)跨域传输","url":"/2016/10/18/SSM6/","content":"\n前言不知大家在平时的开发过程中有没有遇到过跨域访问资源的问题，我不巧在上周就碰到一个这样的问题，幸运的是在公司前端同学的帮忙下解决了该问题。\n什么是跨域问题？\n只要协议、域名、端口有任何一个不同，都被当作是不同的域\n只要是在不同域中是无法进行通信的。\n\n\n\n基于以上的的出发点，我们又有跨域共享资源的需求(譬如现在流行的前后端分离之后分别部署的情况)，本文所采用的解决办法是JSONP，说到JSONP就会首先想到JSON。虽然只有一字之差但意义却完全不一样，首先科普一下JSON。\nJSON\n其实现在JSON已经是相当流行了，只要涉及到前后端的数据交互大都都是采用的JSON(不管是web还是android和IOS)，所以我这里就举一个例子，就算是没有用过的同学也能很快明白其中的意思。\n\nPostMan首先给大家安利一款后端开发的利器PostMan,可以用于模拟几乎所有的HTTP请求，在开发阶段调试后端接口非常有用。这是一个Chrome插件，可以直接在google商店搜索直接下载(当然前提你懂得)。之后界面就如下：\n\n界面非常简洁，有点开发经验的童鞋应该都会使用，不太会用的直接google下就可以了比较简单。接着我们就可以利用PostMan来发起一次请求获取JSON了。这里以我SSM项目为例,也正好有暴露一个JSON的接口。地址如下:http://www.crossoverjie.top/SSM/content_load。直接在POSTMAN中的地址栏输入该地址，采用GET的方式请求，之后所返回的就是JSON格式的字符串。由于Javascript原生的就支持JSON，所以解析起来非常方便。\nJSONP好了，终于可以谈谈JSONP了。之前说道JSONP是用来解决跨域问题的，那么他是如何解决的呢。经过我们开发界的前辈们发现，HTML中拥有SRC属性的标签都不受跨域的影响，比如：&lt;script&gt;、&lt;img&gt;、&lt;iframe&gt;标签。由于JS原生支持JSON的解析，于是我们采用&lt;script&gt;的方式来处理跨域解析，代码如下一看就明白。web端:\n&lt;html lang=&quot;zh&quot;&gt;&lt;head&gt;    &lt;script type=&quot;text/javascript&quot;&gt;        $(document).ready(function()&#123;            $.ajax(&#123;                type: &quot;get&quot;,                async: false,                url: &quot;http://www.crossoverjie.top/SSM/jsonpInfo?callback=getUser&amp;userId=3&quot;,                dataType: &quot;jsonp&quot;,                jsonp: &quot;callback&quot;,//一般默认为:callback                jsonpCallback:&quot;getUser&quot;,//自定义的jsonp回调函数名称，默认为jQuery自动生成的随机函数名，也可以写&quot;?&quot;，jQuery会自动为你处理数据                success: function(json)&#123;                    /**                     * 获得服务器返回的信息。                     * 可以做具体的业务处理。                     */                    alert(&#x27;用户信息：ID： &#x27; + json.userId + &#x27; ，姓名： &#x27; + json.username + &#x27;。&#x27;);                &#125;,                error: function()&#123;                    alert(&#x27;fail&#x27;);                &#125;            &#125;);        &#125;);    &lt;/script&gt;&lt;/head&gt;&lt;body oncontextmenu=&quot;return false&quot;&gt;&lt;/body&gt;&lt;/html&gt;\n其中我们采用了JQuery给我封装好的函数，这样就可以自动帮我们解析了。首先我们来看下代码中的http://www.crossoverjie.top/SSM/jsonpInfo?callback=getUser&userId=3这个地址返回的是什么内容，还是放到POSTMAN中执行如下：。可以看到我们所传递的callback参数带着查询的数据又原封不动的返回给我们了，这样的话即使我们不使用JQuery给我封装好的函数，我们自定义一个和callback名称一样的函数一样是可以解析其中的数据的，只是Jquery帮我们做了而已。\n前端没问题了，那么后端又是如何实现的呢？也很简单，如下：\n@RequestMapping(value = &quot;/jsonpInfo&quot;,method = &#123; RequestMethod.GET &#125;)@ResponseBodypublic Object jsonpInfo(String callback,Integer userId) throws IOException &#123;    User user = userService.getUserById(userId);    JSONPObject jsonpObject = new JSONPObject(callback,user) ;    return jsonpObject ;&#125;\n后端采用了jackson中的JSONPObject这个类的一个构造方法，只需要将callback字段和需要转成JSON字符串的对象放进去即可。需要主要的是需要使用@ResponseBody注解才能成功返回。\n总结其实网上还有其他的方法来处理跨域问题，不过我觉得这样的方式最为简单。同样JSONP也是有缺点的，比如：只支持GET方式的HTTP请求。以上代码依然在博主的SSM项目中，如有需要可以直接FORK。\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客地址：http://crossoverjie.top。\n\n\nGitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["Java","JSONP","JSON"]},{"title":"SSM(七)在JavaWeb应用中使用Redis","url":"/2016/12/18/SSM7/","content":"\n前言由于最近换(mang)了(de)家(yi)公(bi)司接触了新的东西所以很久没有更新了。这次谈谈Redis，关于Redis应该很多朋友就算没有用过也听过，算是这几年最流行的NoSql之一了。Redis的应用场景非常多这里就不一一列举了，这次就以一个最简单的也最常用的 缓存数据 来举例。先来看一张效果图：\n\n\n作用就是在每次查询接口的时候首先判断Redis中是否有缓存，有的话就读取，没有就查询数据库并保存到Redis中，下次再查询的话就会直接从缓存中读取了。Redis中的结果：\n\n之后查询redis发现确实是存进来了。\nRedis安装与使用首先第一步自然是安装Redis。我是在我VPS上进行安装的，操作系统是CentOS6.5。\n\n下载Redishttps://redis.io/download，我机器上安装的是3.2.5\n\n将下载下来的’reidis-3.2.5-tar.gz’上传到usr/local这个目录进行解压。\n\n进入该目录。\n\n\n\n\n编译安装\nmakemake install\n\n修改redis.conf配置文件。\n\n\n这里我只是简单的加上密码而已。\nvi redis.confrequirepass 你的密码\n\n启动Redis\n\n启动时候要选择我们之前修改的配置文件才能使配置文件生效。\n进入src目录cd /usr/local/redis-3.2.5/src启动服务./redis-server ../redis.conf\n\n\n登陆redis./redis-cli -a 你的密码\n\nSpring整合Redis这里我就直接开始用Spring整合毕竟在实际使用中都是和Spring一起使用的。\n\n修改Spring配置文件加入以下内容：&lt;!-- jedis 配置 --&gt;    &lt;bean id=&quot;poolConfig&quot; class=&quot;redis.clients.jedis.JedisPoolConfig&quot;&gt;        &lt;property name=&quot;maxIdle&quot; value=&quot;$&#123;redis.maxIdle&#125;&quot;/&gt;        &lt;property name=&quot;maxWaitMillis&quot; value=&quot;$&#123;redis.maxWait&#125;&quot;/&gt;        &lt;property name=&quot;testOnBorrow&quot; value=&quot;$&#123;redis.testOnBorrow&#125;&quot;/&gt;    &lt;/bean&gt;    &lt;!-- redis服务器中心 --&gt;    &lt;bean id=&quot;connectionFactory&quot; class=&quot;org.springframework.data.redis.connection.jedis.JedisConnectionFactory&quot;&gt;        &lt;property name=&quot;poolConfig&quot; ref=&quot;poolConfig&quot;/&gt;        &lt;property name=&quot;port&quot; value=&quot;$&#123;redis.port&#125;&quot;/&gt;        &lt;property name=&quot;hostName&quot; value=&quot;$&#123;redis.host&#125;&quot;/&gt;        &lt;property name=&quot;password&quot; value=&quot;$&#123;redis.password&#125;&quot;/&gt;        &lt;property name=&quot;timeout&quot; value=&quot;$&#123;redis.timeout&#125;&quot;&gt;&lt;/property&gt;    &lt;/bean&gt;    &lt;bean id=&quot;redisTemplate&quot; class=&quot;org.springframework.data.redis.core.RedisTemplate&quot;&gt;        &lt;property name=&quot;connectionFactory&quot; ref=&quot;connectionFactory&quot;/&gt;        &lt;property name=&quot;keySerializer&quot;&gt;            &lt;bean class=&quot;org.springframework.data.redis.serializer.StringRedisSerializer&quot;/&gt;        &lt;/property&gt;        &lt;property name=&quot;valueSerializer&quot;&gt;            &lt;bean class=&quot;org.springframework.data.redis.serializer.JdkSerializationRedisSerializer&quot;/&gt;        &lt;/property&gt;    &lt;/bean&gt;    &lt;!-- cache配置 --&gt;    &lt;bean id=&quot;methodCacheInterceptor&quot; class=&quot;com.crossoverJie.intercept.MethodCacheInterceptor&quot;&gt;        &lt;property name=&quot;redisUtil&quot; ref=&quot;redisUtil&quot;/&gt;    &lt;/bean&gt;    &lt;bean id=&quot;redisUtil&quot; class=&quot;com.crossoverJie.util.RedisUtil&quot;&gt;        &lt;property name=&quot;redisTemplate&quot; ref=&quot;redisTemplate&quot;/&gt;    &lt;/bean&gt;    &lt;!--配置切面拦截方法 --&gt;    &lt;aop:config proxy-target-class=&quot;true&quot;&gt;        &lt;!--将com.crossoverJie.service包下的所有select开头的方法加入拦截        去掉select则加入所有方法w        --&gt;        &lt;aop:pointcut id=&quot;controllerMethodPointcut&quot; expression=&quot;        execution(* com.crossoverJie.service.*.select*(..))&quot;/&gt;        &lt;aop:pointcut id=&quot;selectMethodPointcut&quot; expression=&quot;        execution(* com.crossoverJie.dao..*Mapper.select*(..))&quot;/&gt;        &lt;aop:advisor advice-ref=&quot;methodCacheInterceptor&quot; pointcut-ref=&quot;controllerMethodPointcut&quot;/&gt;    &lt;/aop:config&gt;\n更多的配置可以直接在源码里面查看：https://github.com/crossoverJie/SSM/blob/master/src/main/resources/spring-mybatis.xml。以上都写有注释，也都是一些简单的配置相信都能看懂。下面我会着重说下如何配置缓存的。\n\nSpring切面使用缓存Spring的AOP真是是一个好东西，还不太清楚是什么的同学建议先自行Google下吧。在不使用切面的时候如果我们想给某个方法加入缓存的话肯定是在方法返回之前就要加入相应的逻辑判断，只有一个或几个倒还好，如果有几十上百个的话那GG了，而且维护起来也特别麻烦。\n\n好在Spring的AOP可以帮我们解决这个问题。这次就在我们需要加入缓存方法的切面加入这个逻辑，并且只需要一个配置即可搞定，就是上文中所提到的配置文件，如下：\n\n&lt;!--配置切面拦截方法 --&gt;&lt;aop:config proxy-target-class=&quot;true&quot;&gt;    &lt;!--将com.crossoverJie.service包下的所有select开头的方法加入拦截    去掉select则加入所有方法w    --&gt;    &lt;aop:pointcut id=&quot;controllerMethodPointcut&quot; expression=&quot;    execution(* com.crossoverJie.service.*.select*(..))&quot;/&gt;    &lt;aop:pointcut id=&quot;selectMethodPointcut&quot; expression=&quot;    execution(* com.crossoverJie.dao..*Mapper.select*(..))&quot;/&gt;    &lt;aop:advisor advice-ref=&quot;methodCacheInterceptor&quot; pointcut-ref=&quot;controllerMethodPointcut&quot;/&gt;&lt;/aop:config&gt;\n这里我们使用表达式execution(* com.crossoverJie.service.*.select*(..))来拦截service中所有以select开头的方法。这样只要我们要将加入的缓存的方法以select命名开头的话每次进入方法之前都会进入我们自定义的MethodCacheInterceptor拦截器。这里贴一下MethodCacheInterceptor中处理逻辑的核心方法：\n@Override    public Object invoke(MethodInvocation invocation) throws Throwable &#123;        Object value = null;        String targetName = invocation.getThis().getClass().getName();        String methodName = invocation.getMethod().getName();        // 不需要缓存的内容        //if (!isAddCache(StringUtil.subStrForLastDot(targetName), methodName)) &#123;        if (!isAddCache(targetName, methodName)) &#123;            // 执行方法返回结果            return invocation.proceed();        &#125;        Object[] arguments = invocation.getArguments();        String key = getCacheKey(targetName, methodName, arguments);        logger.debug(&quot;redisKey: &quot; + key);        try &#123;            // 判断是否有缓存            if (redisUtil.exists(key)) &#123;                return redisUtil.get(key);            &#125;            // 写入缓存            value = invocation.proceed();            if (value != null) &#123;                final String tkey = key;                final Object tvalue = value;                new Thread(new Runnable() &#123;                    @Override                    public void run() &#123;                        if (tkey.startsWith(&quot;com.service.impl.xxxRecordManager&quot;)) &#123;                            redisUtil.set(tkey, tvalue, xxxRecordManagerTime);                        &#125; else if (tkey.startsWith(&quot;com.service.impl.xxxSetRecordManager&quot;)) &#123;                            redisUtil.set(tkey, tvalue, xxxSetRecordManagerTime);                        &#125; else &#123;                            redisUtil.set(tkey, tvalue, defaultCacheExpireTime);                        &#125;                    &#125;                &#125;).start();            &#125;        &#125; catch (Exception e) &#123;            e.printStackTrace();            if (value == null) &#123;                return invocation.proceed();            &#125;        &#125;        return value;    &#125;\n\n先是查看了当前方法是否在我们自定义的方法中，如果不是的话就直接返回，不进入拦截器。\n之后利用反射获取的类名、方法名、参数生成redis的key。\n用key在redis中查询是否已经有缓存。\n有缓存就直接返回缓存内容，不再继续查询数据库。\n如果没有缓存就查询数据库并将返回信息加入到redis中。\n\n使用PageHelper这次为了分页方便使用了比较流行的PageHelper来帮我们更简单的进行分页。首先是新增一个mybatis的配置文件mybatis-config：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration        PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot;        &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt;    &lt;settings&gt;        &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt;        &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;true&quot;/&gt;        &lt;setting name=&quot;multipleResultSetsEnabled&quot; value=&quot;true&quot;/&gt;        &lt;setting name=&quot;useColumnLabel&quot; value=&quot;true&quot;/&gt;        &lt;setting name=&quot;useGeneratedKeys&quot; value=&quot;false&quot;/&gt;        &lt;setting name=&quot;autoMappingBehavior&quot; value=&quot;PARTIAL&quot;/&gt;        &lt;setting name=&quot;defaultExecutorType&quot; value=&quot;SIMPLE&quot;/&gt;        &lt;setting name=&quot;defaultStatementTimeout&quot; value=&quot;25&quot;/&gt;        &lt;setting name=&quot;safeRowBoundsEnabled&quot; value=&quot;false&quot;/&gt;        &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;false&quot;/&gt;        &lt;setting name=&quot;localCacheScope&quot; value=&quot;SESSION&quot;/&gt;        &lt;setting name=&quot;jdbcTypeForNull&quot; value=&quot;OTHER&quot;/&gt;        &lt;setting name=&quot;lazyLoadTriggerMethods&quot; value=&quot;equals,clone,hashCode,toString&quot;/&gt;    &lt;/settings&gt;    &lt;plugins&gt;        &lt;!-- com.github.pagehelper为PageHelper类所在包名 --&gt;        &lt;plugin interceptor=&quot;com.github.pagehelper.PageHelper&quot;&gt;            &lt;property name=&quot;dialect&quot; value=&quot;mysql&quot;/&gt;            &lt;!-- 该参数默认为false --&gt;            &lt;!-- 设置为true时，会将RowBounds第一个参数offset当成pageNum页码使用 --&gt;            &lt;!-- 和startPage中的pageNum效果一样 --&gt;            &lt;property name=&quot;offsetAsPageNum&quot; value=&quot;true&quot;/&gt;            &lt;!-- 该参数默认为false --&gt;            &lt;!-- 设置为true时，使用RowBounds分页会进行count查询 --&gt;            &lt;property name=&quot;rowBoundsWithCount&quot; value=&quot;true&quot;/&gt;            &lt;!-- 设置为true时，如果pageSize=0或者RowBounds.limit = 0就会查询出全部的结果 --&gt;            &lt;!-- （相当于没有执行分页查询，但是返回结果仍然是Page类型） &lt;property name=&quot;pageSizeZero&quot; value=&quot;true&quot;/&gt; --&gt;            &lt;!-- 3.3.0版本可用 - 分页参数合理化，默认false禁用 --&gt;            &lt;!-- 启用合理化时，如果pageNum&lt;1会查询第一页，如果pageNum&gt;pages会查询最后一页 --&gt;            &lt;!-- 禁用合理化时，如果pageNum&lt;1或pageNum&gt;pages会返回空数据 --&gt;            &lt;property name=&quot;reasonable&quot; value=&quot;true&quot;/&gt;            &lt;!-- 3.5.0版本可用 - 为了支持startPage(Object params)方法 --&gt;            &lt;!-- 增加了一个`params`参数来配置参数映射，用于从Map或ServletRequest中取值 --&gt;            &lt;!-- 可以配置pageNum,pageSize,count,pageSizeZero,reasonable,不配置映射的用默认值 --&gt;            &lt;!-- 不理解该含义的前提下，不要随便复制该配置 --&gt;            &lt;property name=&quot;params&quot; value=&quot;pageNum=start;pageSize=limit;&quot;/&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/configuration&gt;\n接着在mybatis的配置文件中引入次配置文件：\n&lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt;    &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot; /&gt;    &lt;!-- 自动扫描mapping.xml文件 --&gt;    &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath:mapping/*.xml&quot;&gt;&lt;/property&gt;    &lt;!--加入PageHelper--&gt;    &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatis-config.xml&quot;/&gt;&lt;/bean&gt;\n接着在service方法中：\n@Overridepublic PageEntity&lt;Rediscontent&gt; selectByPage(Integer pageNum, Integer pageSize) &#123;    PageHelper.startPage(pageNum, pageSize);    //因为是demo，所以这里默认没有查询条件。    List&lt;Rediscontent&gt; rediscontents = rediscontentMapper.selectByExample(new RediscontentExample());    PageEntity&lt;Rediscontent&gt; rediscontentPageEntity = new PageEntity&lt;Rediscontent&gt;();    rediscontentPageEntity.setList(rediscontents);    int size = rediscontentMapper.selectByExample(new RediscontentExample()).size();    rediscontentPageEntity.setCount(size);    return rediscontentPageEntity;&#125;\n只需要使用PageHelper.startPage(pageNum, pageSize);方法就可以帮我们简单的分页了。这里我自定义了一个分页工具类PageEntity来更方便的帮我们在之后生成JSON数据。\npackage com.crossoverJie.util;import java.io.Serializable;import java.util.List;/** * 分页实体 * * @param &lt;T&gt; */public class PageEntity&lt;T&gt; implements Serializable &#123;    private List&lt;T&gt; list;// 分页后的数据    private Integer count;    public Integer getCount() &#123;        return count;    &#125;    public void setCount(Integer count) &#123;        this.count = count;    &#125;    public List&lt;T&gt; getList() &#123;        return list;    &#125;    public void setList(List&lt;T&gt; list) &#123;        this.list = list;    &#125;&#125;\n更多PageHelper的使用请查看一下链接：https://github.com/pagehelper/Mybatis-PageHelper\n前端联调接下来看下控制层RedisController:\npackage com.crossoverJie.controller;import com.crossoverJie.pojo.Rediscontent;import com.crossoverJie.service.RediscontentService;import com.crossoverJie.util.CommonUtil;import com.crossoverJie.util.PageEntity;import com.github.pagehelper.PageHelper;import net.sf.json.JSONArray;import net.sf.json.JSONObject;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import javax.servlet.http.HttpServletResponse;@Controller@RequestMapping(&quot;/redis&quot;)public class RedisController &#123;    private static Logger logger = LoggerFactory.getLogger(RedisController.class);    @Autowired    private RediscontentService rediscontentService;    @RequestMapping(&quot;/redis_list&quot;)    public void club_list(HttpServletResponse response,                          @RequestParam(value = &quot;page&quot;, defaultValue = &quot;0&quot;) int page,                          @RequestParam(value = &quot;pageSize&quot;, defaultValue = &quot;0&quot;) int pageSize) &#123;        JSONObject jsonObject = new JSONObject();        JSONObject jo = new JSONObject();        try &#123;            JSONArray ja = new JSONArray();            PageHelper.startPage(1, 10);            PageEntity&lt;Rediscontent&gt; rediscontentPageEntity = rediscontentService.selectByPage(page, pageSize);            for (Rediscontent rediscontent : rediscontentPageEntity.getList()) &#123;                JSONObject jo1 = new JSONObject();                jo1.put(&quot;rediscontent&quot;, rediscontent);                ja.add(jo1);            &#125;            jo.put(&quot;redisContents&quot;, ja);            jo.put(&quot;count&quot;, rediscontentPageEntity.getCount());            jsonObject = CommonUtil.parseJson(&quot;1&quot;, &quot;成功&quot;, jo);        &#125; catch (Exception e) &#123;            jsonObject = CommonUtil.parseJson(&quot;2&quot;, &quot;操作异常&quot;, &quot;&quot;);            logger.error(e.getMessage(), e);        &#125;        //构建返回        CommonUtil.responseBuildJson(response, jsonObject);    &#125;&#125;\n这里就不做过多解释了，就是从redis或者是service中查询出数据并返回。\n前端的显示界面在https://github.com/crossoverJie/SSM/blob/master/src/main/webapp/redis/showRedis.jsp中(并不是前端，将就看)。其中核心的redis_list.js的代码如下：\nvar page = 1,    rows = 10;$(document).ready(function () &#123;    initJqPaginator();    //加载    load_redis_list();    $(&quot;.query_but&quot;).click(function () &#123;//查询按钮        page = 1;        load_redis_list();    &#125;);&#125;);//初始化分页function initJqPaginator() &#123;    $.jqPaginator(&#x27;#pagination&#x27;, &#123;        totalPages: 100,        visiblePages: 10,        currentPage: 1,        first: &#x27;&lt;li class=&quot;prev&quot;&gt;&lt;a href=&quot;javascript:;&quot;&gt;首页&lt;/a&gt;&lt;/li&gt;&#x27;,        last: &#x27;&lt;li class=&quot;prev&quot;&gt;&lt;a href=&quot;javascript:;&quot;&gt;末页&lt;/a&gt;&lt;/li&gt;&#x27;,        prev: &#x27;&lt;li class=&quot;prev&quot;&gt;&lt;a href=&quot;javascript:;&quot;&gt;上一页&lt;/a&gt;&lt;/li&gt;&#x27;,        next: &#x27;&lt;li class=&quot;next&quot;&gt;&lt;a href=&quot;javascript:;&quot;&gt;下一页&lt;/a&gt;&lt;/li&gt;&#x27;,        page: &#x27;&lt;li class=&quot;page&quot;&gt;&lt;a href=&quot;javascript:;&quot;&gt;&#123;&#123;page&#125;&#125;&lt;/a&gt;&lt;/li&gt;&#x27;,        onPageChange: function (num, type) &#123;            page = num;            if (type == &quot;change&quot;) &#123;                load_redis_list();            &#125;        &#125;    &#125;);&#125;//列表function create_club_list(redisContens) &#123;    var phone = 0;    var html = &#x27;&lt;div class=&quot;product_box&quot;&gt;&#x27;        + &#x27;&lt;div class=&quot;br&quot;&gt;&#x27;        + &#x27;&lt;div class=&quot;product_link&quot;&gt;&#x27;        + &#x27;&lt;div class=&quot;product_phc&quot;&gt;&#x27;        + &#x27;&lt;img class=&quot;phc&quot; src=&quot;&quot; &gt;&#x27;        + &#x27;&lt;/div&gt;&#x27;        + &#x27;&lt;span class=&quot;product_name&quot;&gt;&#x27; + redisContens.id + &#x27;&lt;/span&gt;&lt;/div&gt;&#x27;        + &#x27;&lt;div class=&quot;product_link toto&quot;&gt;&#x27; + redisContens.content + &#x27;&lt;/div&gt;&#x27;        + &#x27;&lt;div class=&quot;product_link toto&quot;&gt;&#x27;        + &#x27;&lt;span&gt;&#x27; + &quot;&quot; + &#x27;&lt;/span&gt;&#x27;        + &#x27;&lt;/div&gt;&#x27;        + &#x27;&lt;div class=&quot;product_link toto&quot;&gt;&#x27;        + &#x27;&lt;span&gt;&#x27; + phone + &#x27;&lt;/span&gt;&lt;/div&gt;&#x27;        + &#x27;&lt;div class=&quot;product_link toto&quot;&gt;&#x27;        + &#x27;&lt;span&gt;&#x27; + 0 + &#x27;&lt;/span&gt;&lt;/div&gt;&#x27;        + &#x27;&lt;div class=&quot;product_link toto product_operation&quot;&gt;&#x27;        + &#x27;&lt;span onclick=&quot;edit_club(&#x27; + 0 + &#x27;)&quot;&gt;编辑&lt;/span&gt;&#x27;        + &#x27;&lt;span onclick=&quot;edit_del(&#x27; + 0 + &#x27;)&quot;&gt;删除&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&#x27;        + &#x27;&lt;/div&gt;&#x27;;    return html;&#125;//加载列表function load_redis_list() &#123;    var name = $(&quot;#name&quot;).val();    $.ajax(&#123;        type: &#x27;POST&#x27;,        url: getPath() + &#x27;/redis/redis_list&#x27;,        async: false,        data: &#123;name: name, page: page, pageSize: rows&#125;,        datatype: &#x27;json&#x27;,        success: function (data) &#123;            if (data.result == 1) &#123;                $(&quot;.product_length_number&quot;).html(data.data.count);                var html = &quot;&quot;;                var count = data.data.count;                for (var i = 0; i &lt; data.data.redisContents.length; i++) &#123;                    var redisContent = data.data.redisContents[i];                    html += create_club_list(redisContent.rediscontent);                &#125;                $(&quot;.product_content&quot;).html(html);                //这里是分页的插件                $(&#x27;#pagination&#x27;).jqPaginator(&#x27;option&#x27;, &#123;                    totalPages: (Math.ceil(count / rows) &lt; 1 ? 1 : Math.ceil(count / rows)),                    currentPage: page                &#125;);            &#125; else &#123;                alert(data.msg);            &#125;        &#125;    &#125;);    $(&quot;.product_box:even&quot;).css(&quot;background&quot;, &quot;#e6e6e6&quot;);//隔行变色&#125;\n其实就是一个简单的请求接口，并根据返回数据动态生成Dom而已。\n总结以上就是一个简单的redis的应用。redis的应用场景还非常的多，比如现在我所在做的一个项目就有用来处理短信验证码的业务场景，之后有时间可以写一个demo。\n\n项目地址：https://github.com/crossoverJie/SSM.gitGitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["Java","Redis"]},{"title":"SSM(八)动态切换数据源","url":"/2017/01/05/SSM8/","content":"\n前言\n在现在开发的过程中应该大多数朋友都有遇到过切换数据源的需求。比如现在常用的数据库读写分离，或者就是有两个数据库的情况，这些都需要用到切换数据源。\n\n手动切换数据源使用Spring的AbstractRoutingDataSource类来进行拓展多数据源。\n该类就相当于一个dataSource的路由，用于根据key值来进行切换对应的dataSource。\n下面简单来看下AbstractRoutingDataSource类的几段关键源码：\n@Overridepublic Connection getConnection() throws SQLException &#123;\treturn determineTargetDataSource().getConnection();&#125;@Overridepublic Connection getConnection(String username, String password) throws SQLException &#123;\treturn determineTargetDataSource().getConnection(username, password);&#125;/** * Retrieve the current target DataSource. Determines the * &#123;@link #determineCurrentLookupKey() current lookup key&#125;, performs * a lookup in the &#123;@link #setTargetDataSources targetDataSources&#125; map, * falls back to the specified * &#123;@link #setDefaultTargetDataSource default target DataSource&#125; if necessary. * @see #determineCurrentLookupKey() */protected DataSource determineTargetDataSource() &#123;\tAssert.notNull(this.resolvedDataSources, &quot;DataSource router not initialized&quot;);\tObject lookupKey = determineCurrentLookupKey();\tDataSource dataSource = this.resolvedDataSources.get(lookupKey);\tif (dataSource == null &amp;&amp; (this.lenientFallback || lookupKey == null)) &#123;\t\tdataSource = this.resolvedDefaultDataSource;\t&#125;\tif (dataSource == null) &#123;\t\tthrow new IllegalStateException(&quot;Cannot determine target DataSource for lookup key [&quot; + lookupKey + &quot;]&quot;);\t&#125;\treturn dataSource;&#125;/** * Determine the current lookup key. This will typically be * implemented to check a thread-bound transaction context. * &lt;p&gt;Allows for arbitrary keys. The returned key needs * to match the stored lookup key type, as resolved by the * &#123;@link #resolveSpecifiedLookupKey&#125; method. */protected abstract Object determineCurrentLookupKey();\n可以看到其中获取链接的方法getConnection()调用的determineTargetDataSource则是关键方法。该方法用于返回我们使用的数据源。\n\n\n其中呢又是determineCurrentLookupKey()方法来返回当前数据源的key值。之后通过该key值在resolvedDataSources这个map中找到对应的value(该value就是数据源)。\nresolvedDataSources这个map则是在：\n@Overridepublic void afterPropertiesSet() &#123;\tif (this.targetDataSources == null) &#123;\t\tthrow new IllegalArgumentException(&quot;Property &#x27;targetDataSources&#x27; is required&quot;);\t&#125;\tthis.resolvedDataSources = new HashMap&lt;Object, DataSource&gt;(this.targetDataSources.size());\tfor (Map.Entry&lt;Object, Object&gt; entry : this.targetDataSources.entrySet()) &#123;\t\tObject lookupKey = resolveSpecifiedLookupKey(entry.getKey());\t\tDataSource dataSource = resolveSpecifiedDataSource(entry.getValue());\t\tthis.resolvedDataSources.put(lookupKey, dataSource);\t&#125;\tif (this.defaultTargetDataSource != null) &#123;\t\tthis.resolvedDefaultDataSource = resolveSpecifiedDataSource(this.defaultTargetDataSource);\t&#125;&#125;\n这个方法通过targetDataSources这个map来进行赋值的。targetDataSources则是我们在配置文件中进行赋值的，下面会讲到。\n再来看看determineCurrentLookupKey()方法，从protected来修饰就可以看出是需要我们来进行重写的。\nDynamicDataSource 和 DataSourceHolder于是我新增了DynamicDataSource类，代码如下：\npackage com.crossoverJie.util;import org.springframework.jdbc.datasource.lookup.AbstractRoutingDataSource;/** * Function: * * @author chenjiec *         Date: 2017/1/2 上午12:22 * @since JDK 1.7 */public class DynamicDataSource extends AbstractRoutingDataSource &#123;    @Override    protected Object determineCurrentLookupKey() &#123;        return DataSourceHolder.getDataSources();    &#125;&#125;\n\n代码很简单，继承了AbstractRoutingDataSource类并重写了其中的determineCurrentLookupKey()方法。\n这里直接用DataSourceHolder返回了一个数据源。\nDataSourceHolder代码如下：\npackage com.crossoverJie.util;/** * Function:动态数据源 * * @author chenjiec *         Date: 2017/1/2 上午12:19 * @since JDK 1.7 */public class DataSourceHolder &#123;    private static final ThreadLocal&lt;String&gt; dataSources = new ThreadLocal&lt;String&gt;();    public static void setDataSources(String dataSource) &#123;        dataSources.set(dataSource);    &#125;    public static String getDataSources() &#123;        return dataSources.get();    &#125;&#125;\n这里我使用了ThreadLocal来保存了数据源，关于ThreadLocal的知识点可以查看以下这篇文章：解密ThreadLocal\n之后在Spring的配置文件中配置我们的数据源，就是上文讲到的为targetDataSources赋值：\n&lt;bean id=&quot;ssm1DataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;          init-method=&quot;init&quot; destroy-method=&quot;close&quot;&gt;        &lt;!-- 指定连接数据库的驱动 --&gt;        &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driverClass&#125;&quot; /&gt;        &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot; /&gt;        &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.user&#125;&quot; /&gt;        &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot; /&gt;        &lt;!-- 配置初始化大小、最小、最大 --&gt;        &lt;property name=&quot;initialSize&quot; value=&quot;3&quot; /&gt;        &lt;property name=&quot;minIdle&quot; value=&quot;3&quot; /&gt;        &lt;property name=&quot;maxActive&quot; value=&quot;20&quot; /&gt;        &lt;!-- 配置获取连接等待超时的时间 --&gt;        &lt;property name=&quot;maxWait&quot; value=&quot;60000&quot; /&gt;        &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt;        &lt;property name=&quot;timeBetweenEvictionRunsMillis&quot; value=&quot;60000&quot; /&gt;        &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt;        &lt;property name=&quot;minEvictableIdleTimeMillis&quot; value=&quot;300000&quot; /&gt;        &lt;property name=&quot;validationQuery&quot; value=&quot;SELECT &#x27;x&#x27;&quot; /&gt;        &lt;property name=&quot;testWhileIdle&quot; value=&quot;true&quot; /&gt;        &lt;property name=&quot;testOnBorrow&quot; value=&quot;false&quot; /&gt;        &lt;property name=&quot;testOnReturn&quot; value=&quot;false&quot; /&gt;        &lt;!-- 打开PSCache，并且指定每个连接上PSCache的大小 --&gt;        &lt;property name=&quot;poolPreparedStatements&quot; value=&quot;true&quot; /&gt;        &lt;property name=&quot;maxPoolPreparedStatementPerConnectionSize&quot;                  value=&quot;20&quot; /&gt;        &lt;!-- 配置监控统计拦截的filters，去掉后监控界面sql无法统计 --&gt;        &lt;property name=&quot;filters&quot; value=&quot;stat&quot; /&gt;    &lt;/bean&gt;    &lt;bean id=&quot;ssm2DataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;          init-method=&quot;init&quot; destroy-method=&quot;close&quot;&gt;        &lt;!-- 指定连接数据库的驱动 --&gt;        &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driverClass&#125;&quot;/&gt;        &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url2&#125;&quot;/&gt;        &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.user2&#125;&quot;/&gt;        &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password2&#125;&quot;/&gt;        &lt;property name=&quot;initialSize&quot; value=&quot;3&quot;/&gt;        &lt;property name=&quot;minIdle&quot; value=&quot;3&quot;/&gt;        &lt;property name=&quot;maxActive&quot; value=&quot;20&quot;/&gt;        &lt;!-- 配置获取连接等待超时的时间 --&gt;        &lt;property name=&quot;maxWait&quot; value=&quot;60000&quot;/&gt;        &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt;        &lt;property name=&quot;timeBetweenEvictionRunsMillis&quot; value=&quot;60000&quot;/&gt;        &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt;        &lt;property name=&quot;minEvictableIdleTimeMillis&quot; value=&quot;300000&quot;/&gt;        &lt;property name=&quot;validationQuery&quot; value=&quot;SELECT &#x27;x&#x27;&quot;/&gt;        &lt;property name=&quot;testWhileIdle&quot; value=&quot;true&quot;/&gt;        &lt;property name=&quot;testOnBorrow&quot; value=&quot;false&quot;/&gt;        &lt;property name=&quot;testOnReturn&quot; value=&quot;false&quot;/&gt;        &lt;!-- 打开PSCache，并且指定每个连接上PSCache的大小 --&gt;        &lt;property name=&quot;poolPreparedStatements&quot; value=&quot;true&quot;/&gt;        &lt;property name=&quot;maxPoolPreparedStatementPerConnectionSize&quot;                  value=&quot;20&quot;/&gt;        &lt;!-- 配置监控统计拦截的filters，去掉后监控界面sql无法统计 --&gt;        &lt;property name=&quot;filters&quot; value=&quot;stat&quot;/&gt;    &lt;/bean&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;com.crossoverJie.util.DynamicDataSource&quot;&gt;        &lt;property name=&quot;targetDataSources&quot;&gt;            &lt;map key-type=&quot;java.lang.String&quot;&gt;                &lt;entry key=&quot;ssm1DataSource&quot; value-ref=&quot;ssm1DataSource&quot;/&gt;                &lt;entry key=&quot;ssm2DataSource&quot; value-ref=&quot;ssm2DataSource&quot;/&gt;            &lt;/map&gt;        &lt;/property&gt;        &lt;!--默认数据源--&gt;        &lt;property name=&quot;defaultTargetDataSource&quot; ref=&quot;ssm1DataSource&quot;/&gt;    &lt;/bean&gt;\n这里分别配置了两个数据源：ssm1DataSource和ssm2DataSource。之后再通过Spring的依赖注入方式将两个数据源设置进targetDataSources。\n接下来的用法相比大家也应该猜到了。\n\n就是在每次调用数据库之前我们都要先通过DataSourceHolder来设置当前的数据源。看下demo：\n\n@Testpublic void selectByPrimaryKey() throws Exception &#123;    DataSourceHolder.setDataSources(Constants.DATASOURCE_TWO);    Datasource datasource = dataSourceService.selectByPrimaryKey(7);    System.out.println(JSON.toJSONString(datasource));&#125;\n详见我的单测。\n使用起来也是非常简单。但是不知道大家注意到没有，这样的做法槽点很多：\n\n每次使用需要手动切换，总有一些人会忘记写(比如我)。\n如果是后期需求变了，查询其他的表了还得一个个改回来。\n\n那有没有什么方法可以自动的帮我们切换呢？\n肯定是有的，大家应该也想得到。就是利用Spring的AOP了。\n自动切换数据源首先要定义好我们的切面类DataSourceExchange:\npackage com.crossoverJie.util;import org.aspectj.lang.JoinPoint;/** * Function:拦截器方法 * * @author chenjiec *         Date: 2017/1/3 上午12:34 * @since JDK 1.7 */public class DataSourceExchange &#123;        /**     *     * @param point     */    public void before(JoinPoint point) &#123;        //获取目标对象的类类型        Class&lt;?&gt; aClass = point.getTarget().getClass();        //获取包名用于区分不同数据源        String whichDataSource = aClass.getName().substring(25, aClass.getName().lastIndexOf(&quot;.&quot;));        if (&quot;ssmone&quot;.equals(whichDataSource)) &#123;            DataSourceHolder.setDataSources(Constants.DATASOURCE_ONE);        &#125; else &#123;            DataSourceHolder.setDataSources(Constants.DATASOURCE_TWO);        &#125;    &#125;    /**     * 执行后将数据源置为空     */    public void after() &#123;        DataSourceHolder.setDataSources(null);    &#125;&#125;\n\n逻辑也比较简单，就是在执行数据库操作之前做一个切面。\n\n通过JoinPoint对象获取目标对象。\n在目标对象中获取包名来区分不同的数据源。\n根据不同数据源来进行赋值。\n执行完毕之后将数据源清空。\n\n关于一些JoinPoint的API：\npackage org.aspectj.lang;import org.aspectj.lang.reflect.SourceLocation;public interface JoinPoint &#123;    String toString();         //连接点所在位置的相关信息    String toShortString();     //连接点所在位置的简短相关信息    String toLongString();     //连接点所在位置的全部相关信息    Object getThis();         //返回AOP代理对象    Object getTarget();       //返回目标对象    Object[] getArgs();       //返回被通知方法参数列表    Signature getSignature();  //返回当前连接点签名    SourceLocation getSourceLocation();//返回连接点方法所在类文件中的位置    String getKind();        //连接点类型    StaticPart getStaticPart(); //返回连接点静态部分&#125;\n\n为了通过包名来区分不同数据源，我将目录结构稍微调整了下：\n将两个不同的数据源的实现类放到不同的包中，这样今后如果还需要新增其他数据源也可以灵活的切换。\n看下Spring的配置：\n&lt;bean id=&quot;dataSourceExchange&quot; class=&quot;com.crossoverJie.util.DataSourceExchange&quot;/&gt;&lt;!--配置切面拦截方法 --&gt;&lt;aop:config proxy-target-class=&quot;false&quot;&gt;    &lt;!--将com.crossoverJie.service包下的所有select开头的方法加入拦截    去掉select则加入所有方法    --&gt;    &lt;aop:pointcut id=&quot;controllerMethodPointcut&quot; expression=&quot;    execution(* com.crossoverJie.service.*.select*(..))&quot;/&gt;    &lt;aop:pointcut id=&quot;selectMethodPointcut&quot; expression=&quot;    execution(* com.crossoverJie.dao..*Mapper.select*(..))&quot;/&gt;    &lt;aop:advisor advice-ref=&quot;methodCacheInterceptor&quot; pointcut-ref=&quot;controllerMethodPointcut&quot;/&gt;    &lt;!--所有数据库操作的方法加入切面--&gt;    &lt;aop:aspect ref=&quot;dataSourceExchange&quot;&gt;        &lt;aop:pointcut id=&quot;dataSourcePointcut&quot; expression=&quot;execution(* com.crossoverJie.service.*.*(..))&quot;/&gt;        &lt;aop:before pointcut-ref=&quot;dataSourcePointcut&quot; method=&quot;before&quot;/&gt;        &lt;aop:after pointcut-ref=&quot;dataSourcePointcut&quot; method=&quot;after&quot;/&gt;    &lt;/aop:aspect&gt;&lt;/aop:config&gt;\n这是在我们上一篇整合redis缓存的基础上进行修改的。这样缓存和多数据源都满足了。\n实际使用：\n@Testpublic void selectByPrimaryKey() throws Exception &#123;    Rediscontent rediscontent = rediscontentService.selectByPrimaryKey(30);    System.out.println(JSON.toJSONString(rediscontent));&#125;\n\n\n这样看起来就和使用一个数据源这样简单，再也不用关心切换的问题了。\n总结不过按照这样的写法是无法做到在一个事务里控制两个数据源的。这个我还在学习中，有相关经验的大牛不妨指点一下。\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客地址：http://crossoverjie.top。\n\n\nGitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["Java","AOP"]},{"title":"SSM(九) 反射的实际应用 - 构建日志对象","url":"/2017/01/19/SSM9/","content":"\n前言相信做Java的童鞋或多或少都听过反射，这也应该是Java从入门到进阶的必经之路。\n但是在我们的实际开发中直接使用它们的几率貌似还是比较少的，（除了造轮子或者是Spring Mybatis这些框架外）。\n所以这里介绍一个在实际开发中还是小有用处的反射实例。\n传统日志有关反射的一些基本知识就不说了，可以自行Google，也可以看下反射入门。\n日志相信大家都不陌生，在实际开发中一些比较敏感的数据表我们需要对它的每一次操作都记录下来。\n先来看看传统的写法：\n@Testpublic void insertSelective() throws Exception &#123;    Content content = new Content() ;    content.setContent(&quot;asdsf&quot;);    content.setCreatedate(&quot;2016-12-09&quot;);    contentService.insertSelective(content) ;    ContentLog log = new ContentLog();    log.setContentid(content.getContentid());    log.setContent(&quot;asdsf&quot;);    log.setCreatedate(&quot;2016-12-09&quot;);    contentLogService.insertSelective(log);&#125;\n非常简单，就是在保存完数据表之后再把相同的数据保存到日志表中。\n\n但是这样有以下几个问题：\n\n如果数据表的字段较多的话，比如几百个。那么日志表的setter()方法就得写几百次，还得是都写对的情况下。\n如果哪天数据表的字段发生了增加，那么每个写日志的地方都得增加该字段，提高了维护的成本。\n\n针对以上的情况就得需要反射这个主角来解决了。\n利用反射构建日志我们先来先来看下使用反射之后对代码所带来的改变：\n@Testpublic void insertSelective2() throws Exception &#123;    Content content = new Content();    content.setContent(&quot;你好&quot;);    content.setContentname(&quot;1&quot;);    content.setCreatedate(&quot;2016-09-23&quot;);    contentService.insertSelective(content);    ContentLog log = new ContentLog();    CommonUtil.setLogValueModelToModel(content, log);    contentLogService.insertSelective(log);&#125;\n同样的保存日志，不管多少字段，只需要三行代码即可解决。而且就算之后字段发生改变写日志这段代码仍然不需要改动。\n其实这里最主要的一个方法就是CommonUtil.setLogValueModelToModel(content, log);\n来看下是如何实现的;\n/**     * 生成日志实体工具     *     * @param objectFrom     * @param objectTo     */    public static void setLogValueModelToModel(Object objectFrom, Object objectTo) &#123;        Class&lt;? extends Object&gt; clazzFrom = objectFrom.getClass();        Class&lt;? extends Object&gt; clazzTo = objectTo.getClass();        for (Method toSetMethod : clazzTo.getMethods()) &#123;            String mName = toSetMethod.getName();            if (mName.startsWith(&quot;set&quot;)) &#123;                //字段名                String field = mName.substring(3);                //获取from 值                Object value;                try &#123;                    if (&quot;LogId&quot;.equals(field)) &#123;                        continue;                    &#125;                    Method fromGetMethod = clazzFrom.getMethod(&quot;get&quot; + field);                    value = fromGetMethod.invoke(objectFrom);                    //设置值                    toSetMethod.invoke(objectTo, value);                &#125; catch (NoSuchMethodException e) &#123;                    throw new RuntimeException(e);                &#125; catch (InvocationTargetException e) &#123;                    throw new RuntimeException(e);                &#125; catch (IllegalAccessException e) &#123;                    throw new RuntimeException(e);                &#125;            &#125;        &#125;    &#125;\n再使用之前我们首先需要构建好主的数据表，然后new一个日志表的对象。\n在setLogValueModelToModel()方法中：\n\n分别获得数据表和日志表对象的类类型。\n获取到日志对象的所有方法集合。\n遍历该集合，并拿到该方法的名称。\n只取其中set开头的方法，也就是set方法。因为我们需要在循环中为日志对象的每一个字段赋值。 \n之后截取方法名称获得具体的字段名称。\n用之前截取的字段名称，通过getMethod()方法返回数据表中的该字段的getter方法。\n相当于执行了String content = content.getContent();\n执行该方法获得该字段具体的值。\n利用当前循环的setter方法为日志对象的每一个字段赋值。\n相当于执行了log.setContent(&quot;asdsf&quot;);\n\n其中字段名称为LogId时跳出了当前循环，因为LogId是日志表的主键，是不需要赋值的。\n当循环结束时，日志对象也就构建完成了。之后只需要保存到数据库中即可。\n总结反射其实是非常耗资源的，再使用过程中还是要慎用。其中对method、field、constructor等对象做缓存也是很有必要的。\n\n项目地址：https://github.com/crossoverJie/SSM.git\n\n\n个人博客地址：http://crossoverjie.top。\n\n\nGitHub地址：https://github.com/crossoverJie。\n\n","categories":["SSM"],"tags":["Java","Reflect"]},{"title":"synchronized 关键字原理","url":"/2018/01/14/Synchronize/","content":"\n众所周知 synchronized 关键字是解决并发问题常用解决方案，有以下三种使用方式:\n\n同步普通方法，锁的是当前对象。\n同步静态方法，锁的是当前 Class 对象。\n同步块，锁的是 () 中的对象。\n\n实现原理：JVM 是通过进入、退出对象监视器( Monitor )来实现对方法、同步块的同步的。\n具体实现是在编译之后在同步方法调用前加入一个 monitor.enter 指令，在退出方法和异常处插入 monitor.exit 的指令。\n其本质就是对一个对象监视器( Monitor )进行获取，而这个获取过程具有排他性从而达到了同一时刻只能一个线程访问的目的。\n而对于没有获取到锁的线程将会阻塞到方法入口处，直到获取锁的线程 monitor.exit 之后才能尝试继续获取锁。\n流程图如下:\n\n\n通过一段代码来演示:\npublic static void main(String[] args) &#123;    synchronized (Synchronize.class)&#123;        System.out.println(&quot;Synchronize&quot;);    &#125;&#125;\n\n使用 javap -c Synchronize 可以查看编译之后的具体信息。\npublic class com.crossoverjie.synchronize.Synchronize &#123;  public com.crossoverjie.synchronize.Synchronize();    Code:       0: aload_0       1: invokespecial #1                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V       4: return  public static void main(java.lang.String[]);    Code:       0: ldc           #2                  // class com/crossoverjie/synchronize/Synchronize       2: dup       3: astore_1       **4: monitorenter**       5: getstatic     #3                  // Field java/lang/System.out:Ljava/io/PrintStream;       8: ldc           #4                  // String Synchronize      10: invokevirtual #5                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V      13: aload_1      **14: monitorexit**      15: goto          23      18: astore_2      19: aload_1      20: monitorexit      21: aload_2      22: athrow      23: return    Exception table:       from    to  target type           5    15    18   any          18    21    18   any&#125;\n\n可以看到在同步块的入口和出口分别有 monitorenter,monitorexit指令。\n锁优化synchronized  很多都称之为重量锁，JDK1.6 中对 synchronized 进行了各种优化，为了能减少获取和释放锁带来的消耗引入了偏向锁和轻量锁。\n轻量锁当代码进入同步块时，如果同步对象为无锁状态时，当前线程会在栈帧中创建一个锁记录(Lock Record)区域，同时将锁对象的对象头中 Mark Word 拷贝到锁记录中，再尝试使用 CAS 将 Mark Word 更新为指向锁记录的指针。\n如果更新成功，当前线程就获得了锁。\n如果更新失败 JVM 会先检查锁对象的 Mark Word 是否指向当前线程的锁记录。\n如果是则说明当前线程拥有锁对象的锁，可以直接进入同步块。\n不是则说明有其他线程抢占了锁，如果存在多个线程同时竞争一把锁，轻量锁就会膨胀为重量锁。\n解锁轻量锁的解锁过程也是利用 CAS 来实现的，会尝试锁记录替换回锁对象的 Mark Word 。如果替换成功则说明整个同步操作完成，失败则说明有其他线程尝试获取锁，这时就会唤醒被挂起的线程(此时已经膨胀为重量锁)\n轻量锁能提升性能的原因：\n认为大多数锁在整个同步周期都不存在竞争，所以使用 CAS 比使用互斥开销更少。但如果锁竞争激烈，轻量锁就不但有互斥的开销，还有 CAS 的开销，甚至比重量锁更慢。\n偏向锁为了进一步的降低获取锁的代价，JDK1.6 之后还引入了偏向锁。\n偏向锁的特征是:锁不存在多线程竞争，并且应由一个线程多次获得锁。\n当线程访问同步块时，会使用 CAS 将线程 ID 更新到锁对象的 Mark Word 中，如果更新成功则获得偏向锁，并且之后每次进入这个对象锁相关的同步块时都不需要再次获取锁了。\n释放锁当有另外一个线程获取这个锁时，持有偏向锁的线程就会释放锁，释放时会等待全局安全点(这一时刻没有字节码运行)，接着会暂停拥有偏向锁的线程，根据锁对象目前是否被锁来判定将对象头中的 Mark Word 设置为无锁或者是轻量锁状态。\n偏向锁可以提高带有同步却没有竞争的程序性能，但如果程序中大多数锁都存在竞争时，那偏向锁就起不到太大作用。可以使用 -XX:-userBiasedLocking=false 来关闭偏向锁，并默认进入轻量锁。\n其他优化适应性自旋在使用 CAS 时，如果操作失败，CAS 会自旋再次尝试。由于自旋是需要消耗 CPU 资源的，所以如果长期自旋就白白浪费了 CPU。JDK1.6加入了适应性自旋:\n\n如果某个锁自旋很少成功获得，那么下一次就会减少自旋。\n\n总结synchronized 现在已经不像以前那么重了，拿 1.8 中的 ConcurrentHashMap 就可以看出，里面大量的使用了 synchronized 来进行同步。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["Java 进阶"],"tags":["Java","synchronize"]},{"title":"让百度和google收录我们的网站","url":"/2016/05/19/baidu-google/","content":"前言花了几天时间终于把这个看似高大上的博客搞好了，但是发现只能通过在地址栏输入地址进行访问，这很明显和我装X装到底的性格，于是乎在查阅了嘟爷的博客，和我各种百度终于搞出来了。\n\n让谷歌收录让谷歌收录还是比较简单，首先我们肯定是要翻墙的(这个就不仔细说了，具体百度。)由于我这里突然登不上google账号了，所以下次补充截图。同体来说就是以下步骤：\n\n\n下载google的html验证文件放到网站的根目录，使google能够访问得到。\n在谷歌站长工具里加上自己的站点地图。\n\n\n\n\n\n创建站点地图站点地图是一种文件，可以通过该文件列出您网站上的网页，从而将您网站内容的组织架构告知Google和其他搜索引擎，以便更加智能的抓取你的网站信息。首先我们要为Hexo安装谷歌和百度的插件(博主是用Hexo来搭建的博客)，如下：\nnpm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save\n在博客的根目录中的_config.yml文件中加入以下内容：之后部署上去之后如果在地址栏后面加上站点地图如下的话表示部署成功：\n\n让百度收录有三种方式可以让百度收录我们的网站。第一种：主动推送我用Java写了一个小程序，可以手工的自己推送地址给百度。\npackage top.crossoverjie.post;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.io.PrintWriter;import java.net.URL;import java.net.URLConnection;public class Post &#123;\tpublic static void main(String[] args) &#123;\t\t\t\t\t  \t\tString url = &quot;http://data.zz.baidu.com/urls?site=crossoverjie.top&amp;token=1002EzhDReuy34dq&quot;;// 网站的服务器连接\t\tString[] param = &#123; \t\t\t// 需要推送的网址//\t\t\t&quot;http://crossoverjie.top/tags&quot;,//\t\t\t&quot;http://crossoverjie.top/categories&quot;,\t\t\t//&quot;http://crossoverjie.top/about/&quot;\t\t\t&quot;http://crossoverjie.top/2016/05/14/java-thread1&quot;\t\t\t\t\t&#125;;\t\tString json = Post(url, param);// 执行推送方法\t\tSystem.out.println(&quot;结果是&quot; + json); // 打印推送结果\t&#125;\t/**\t * 百度链接实时推送\t * \t * @param PostUrl\t * @param Parameters\t * @return\t */\tpublic static String Post(String PostUrl, String[] Parameters) &#123;\t\tif (null == PostUrl || null == Parameters || Parameters.length == 0) &#123;\t\t\treturn null;\t\t&#125;\t\tString result = &quot;&quot;;\t\tPrintWriter out = null;\t\tBufferedReader in = null;\t\ttry &#123;\t\t\t// 建立URL之间的连接\t\t\tURLConnection conn = new URL(PostUrl).openConnection();\t\t\t// 设置通用的请求属性\t\t\tconn.setRequestProperty(&quot;Host&quot;, &quot;data.zz.baidu.com&quot;);\t\t\tconn.setRequestProperty(&quot;User-Agent&quot;, &quot;curl/7.12.1&quot;);\t\t\tconn.setRequestProperty(&quot;Content-Length&quot;, &quot;83&quot;);\t\t\tconn.setRequestProperty(&quot;Content-Type&quot;, &quot;text/plain&quot;);\t\t\t// 发送POST请求必须设置如下两行\t\t\tconn.setDoInput(true);\t\t\tconn.setDoOutput(true);\t\t\t// 获取conn对应的输出流\t\t\tout = new PrintWriter(conn.getOutputStream());\t\t\t// 发送请求参数\t\t\tString param = &quot;&quot;;\t\t\tfor (String s : Parameters) &#123;\t\t\t\tparam += s + &quot;\\n&quot;;\t\t\t&#125;\t\t\tout.print(param.trim());\t\t\t// 进行输出流的缓冲\t\t\tout.flush();\t\t\t// 通过BufferedReader输入流来读取Url的响应\t\t\tin = new BufferedReader(\t\t\t\t\tnew InputStreamReader(conn.getInputStream()));\t\t\tString line;\t\t\twhile ( (line = in.readLine()) != null) &#123;\t\t\t\tresult += line;\t\t\t&#125;\t\t&#125; catch (Exception e) &#123;\t\t\tSystem.out.println(&quot;发送post请求出现异常！&quot; + e);\t\t\te.printStackTrace();\t\t&#125; finally &#123;\t\t\ttry &#123;\t\t\t\tif (out != null) &#123;\t\t\t\t\tout.close();\t\t\t\t&#125;\t\t\t\tif (in != null) &#123;\t\t\t\t\tin.close();\t\t\t\t&#125;\t\t\t&#125; catch (IOException ex) &#123;\t\t\t\tex.printStackTrace();\t\t\t&#125;\t\t&#125;\t\treturn result;\t&#125;&#125;\n运行之后结果如下：\n结果是&#123;&quot;remain&quot;:499,&quot;success&quot;:1&#125;\nremain表示还有多少可以推送，我这里表示还有499条。success表示成功推送了多少条链接，我这里表示成功推送了一条链接。\n第二种是主动推送，可以按照百度的教程进行配置：\n第三种就是配置站点地图了，按照之前将的将站点地图安装到项目中，参照我的配置即可：如果能像我这个一样状态正常，能获取到URL数量就表示成功了。\n\n总结在整个过程中不是我黑百度，百度的效率真是太低了。我头一天在google提交上去第二天就能收到了，百度是我提交了大概一周多才给我收录进去，这当然肯定也和我的内容有关系。\n","categories":["小技巧"],"tags":["baidu","google"]},{"title":"科普-为自己的博客免费加上小绿锁","url":"/2017/05/07/https/","content":"\n在如今的HTTPS大当其道的情况下自己的博客要是还没有用上。作为互联网的螺丝钉(码农)岂不是很没面子。\n使用CLOUDFLARE这里使用CLOUDFLARE来提供HTTPS服务。\n\n在其官网进行注册，按照提示添加好自己的域名即可。\n之后需要在自己域名的提供商处修改DNS服务器，我是在万网购买的修改后如下图：其中的DNS服务器地址由CLOUDFLARE是提供的。修改完成之后通常需要等待一段时间才能生效。\n接着在CLOUDFLARE配置DNS解析：点击CLOUDFLARE顶部的DNS进行如我上图中的配置，和之前的配置没有什么区别。\n\n等待一段时间之后发现使用HTTP,HTTPS都能访问，但是最好还是能在访问HTTP的时候能强制跳转到HTTPS.\n\n在CLOUDFLARE菜单栏点击page-rules之后新建一个page rule：这样整个网站的请求都会强制到请求到HTTPS.\n\n\n\n主题配置由于我才用的是Hexo中的Next主题，其中配置了CNZZ站长统计。其中配置的CNZZ统计JS是才用的HTTP。导致在首页的时候chrome一直提示感叹号。修改站点themes/next/layout/_scripts/third-party/analytics目录下的cnzz-analytics.swig文件\n&#123;% if theme.cnzz_siteid %&#125;  &lt;div style=&quot;display: none;&quot;&gt;    &lt;script src=&quot;https://s6.cnzz.com/stat.php?id=&#123;&#123; theme.cnzz_siteid &#125;&#125;&amp;web_id=&#123;&#123; theme.cnzz_siteid &#125;&#125;&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;  &lt;/div&gt;&#123;% endif %&#125;\n之后再进行构建的时候就会使用HTTPS.\n\n值得注意一点的是之后文章中所使用的图片都要用HTTPS的地址了，不然chrome会提示感叹号。\n\n\n个人博客地址：http://crossoverjie.top。\n\n\nGitHub地址：https://github.com/crossoverJie。\n\n","categories":["科普"],"tags":["HTTP","HTTPS"]},{"title":"Java笔记（一）Java的反射机制","url":"/2016/05/10/java-reflect/","content":"前言java反射机制指的是在java运行过程中，对于任意的类都可以知道他的所有属性以及方法，对于任意一个对象都可以任意的调用他的属性和方法，这种动态获取对象信息和动态调用对象方法的功能称为java反射机制，但是反射使用不当会造成很高的成本。\n简单实例\n反射获取类名称package top.crosssoverjie.study;public class Reflect &#123;    public static void main(String[] args) &#123;        Class&lt;Reflect&gt; c1 = Reflect.class;        System.out.println(c1.getName());                Reflect r1 = new Reflect() ;        Class&lt;Reflect&gt; c2 = (Class&lt;Reflect&gt;) r1.getClass() ;        System.out.println(c2.getName());                try &#123;            Class&lt;Reflect&gt; c3 = (Class&lt;Reflect&gt;) Class.forName(&quot;top.crosssoverjie.study.Reflect&quot;);            System.out.println(c3.getName());        &#125; catch (ClassNotFoundException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125;\n\n输出结果：\ntop.crosssoverjie.study.Reflecttop.crosssoverjie.study.Reflecttop.crosssoverjie.study.Reflect  \n\n以上的 c1,c2,c3是完全一样的，他们都有一个统一的名称：叫做Reflect类的类类型。\n\n反射的用处获取成员方法public Method getDeclaredMethod(String name,Class&lt;?&gt;...parameterTypes)//得到该类的所有方法，但是不包括父类的方法。public Method getMethod(String name,Class&lt;?&gt;...parameterTypes)//获得该类的所有public方法，包括父类的。\n\n通过反射获取成员方法调用的实例:\npackage top.crosssoverjie.study;import java.lang.reflect.Method;public class Person &#123;\tprivate String name=&quot;crossover&quot; ;\tprivate String msg ;\t\tpublic Person(String name, String msg) &#123;\t\tthis.name = name;\t\tthis.msg = msg;\t\tSystem.out.println(name+&quot;的描述是&quot;+msg);\t&#125;\tpublic Person() &#123;\t\tsuper();\t&#125;\tpublic void say(String name ,String msg)&#123;\t\tSystem.out.println(name+&quot;说：&quot;+msg);\t&#125;\t\tpublic String getName() &#123;\t\treturn name;\t&#125;\tpublic void setName(String name) &#123;\t\tthis.name = name;\t&#125;\tpublic String getMsg() &#123;\t\treturn msg;\t&#125;\tpublic void setMsg(String msg) &#123;\t\tthis.msg = msg;\t&#125;\t\tpublic static void main(String[] args) &#123;\t\ttry &#123;\t\t\t//首先获取类类型\t\t\tClass c1 = Class.forName(&quot;top.crosssoverjie.study.Person&quot;) ;\t\t\t\t\t\t//通过newInstance()方法生成一个实例\t\t\tObject o1 = c1.newInstance() ;\t\t\t\t\t\t//获取该类的say方法\t\t\tMethod m1 = c1.getMethod(&quot;say&quot;, String.class,String.class) ;\t\t\t\t\t\t//通过invoke方法调用该方法//\t\t\tm1.invoke(o1, &quot;张三&quot;,&quot;你好啊&quot;) ;\t\t\t\t\t\tMethod[] methods = c1.getDeclaredMethods() ;//\t\t\tfor(Method m : methods)&#123;//\t\t\t\tSystem.out.println(m.getName());//\t\t\t&#125;\t\t\t\t\t\tMethod[] methods2 = c1.getMethods() ;\t\t\tfor (Method method : methods2) &#123;\t\t\t\tSystem.out.println(method.getName());\t\t\t&#125;\t\t\t\t\t&#125; catch (Exception e) &#123;\t\t\te.printStackTrace();\t\t&#125;\t\t\t&#125;&#125;\n输出结果：\n张三说：你好啊\n\n所以我们只要知道类的全限定名就可以任意的调用里面的方法。\n\nMethod[] methods = c1.getDeclaredMethods() ;for(Method m : methods)&#123;\tSystem.out.println(m.getName());&#125;\n\n输出结果：\nmaingetNamesetNamesaygetMsgsetMsg\n\n使用的还是之前那个Person类，所以这里只写了关键代码。这里输出的是Person的所有public方法。\n如果我们调用getMethods()方法会是什么结果呢？\nMethod[] methods2 = c1.getMethods() ;for (Method method : methods2) &#123;\tSystem.out.println(method.getName());&#125;\n\n输出结果:\nmaingetNamesetNamesaygetMsgsetMsgwaitwaitwaithashCodegetClassequalstoStringnotifynotifyAll\n\n这时我们会发现这里输出的结果会比刚才多得多，这时因为getMethods()方法返回的是包括父类的所有方法。\n\n获取成员变量我们还可以通过反射来获取类包括父类的成员变量，主要方法如下：\npublic Field getDeclaredFiled(String name)//获得该类所有的成员变量，但不包括父类的。public Filed getFiled(String name)//获得该类的所有的public变量，包括其父类的。\n\n还是按照之前例子中的Person类举例，他具有两个成员变量：\nprivate String name=&quot;crossover&quot; ;private String msg ;\n我们可以通过以下方法来获取其中的成员变量：\nClass c1 = Class.forName(&quot;top.crosssoverjie.study.Person&quot;) ;Field field = c1.getDeclaredField(&quot;name&quot;);//获取该类所有的成员属性\n\n通过以下例子可以获取指定对象上此field的值：\npackage top.crosssoverjie.study;import java.io.File;import java.lang.reflect.Field;public class Reflect &#123;\tpublic static void main(String[] args) &#123;\t\ttry &#123;\t\t\tClass c1 = Class.forName(&quot;top.crosssoverjie.study.Person&quot;);\t\t\tField field = c1.getDeclaredField(&quot;name&quot;) ;\t\t\tObject o1 = c1.newInstance() ;\t\t\t/**\t\t\t * 由于Person类中的name变量是private修饰的，\t\t\t * 所以需要手动开启允许访问，是public修饰的就不需要设置了\t\t\t */\t\t\tfield.setAccessible(true);\t\t\tObject name = field.get(o1) ;\t\t\tSystem.out.println(name);\t\t&#125; catch (Exception e) &#123;\t\t\te.printStackTrace() ;\t\t&#125;//\t\tClass&lt;Reflect&gt; c1 = Reflect.class;//\t\tSystem.out.println(c1.getName());//\t\t//\t\tReflect r1 = new Reflect() ;//\t\tClass&lt;Reflect&gt; c2 = (Class&lt;Reflect&gt;) r1.getClass() ;//\t\tSystem.out.println(c2.getName());//\t\t//\t\ttry &#123;//\t\t\tClass&lt;Reflect&gt; c3 = (Class&lt;Reflect&gt;) Class.forName(&quot;top.crosssoverjie.study.Reflect&quot;);//\t\t\tSystem.out.println(c3.getName());//\t\t&#125; catch (ClassNotFoundException e) &#123;//\t\t\te.printStackTrace();//\t\t&#125;\t&#125;&#125;\n输出结果：\ncrossover\n\n我们也可以通过方法getDeclaredFieds()方法来获取所有的成员变量，返回是是一个Field[]数组，只需要遍历这个数组即可获所有的成员变量。例子如下：\nField[] fields = c1.getDeclaredFields() ;for(Field f :fields)&#123;\tSystem.out.println(f.getName());&#125;\n输出结果如下：\nnamemsg\n\n获取构造方法可以通过以下两个方法来获取构造方法：\npublic Constructor getDeclaredConstructor(Class&lt;?&gt;...parameterTypes)//获取该类的所有构造方法，不包括父类的。public Constructor getConstructor(Class&lt;?&gt;...parameterTypes)//获取该类的所有public修饰的构造方法，包括父类的。\n在之前的Person类中有以下的构造方法：\npublic Person(String name, String msg) &#123;\tthis.name = name;\tthis.msg = msg;&#125;\n我们可以通过以下方法来获取Person类的构造方法：\nConstructor dc1 = c1.getDeclaredConstructor(String.class,String.class) ;\n具体代码如下：\nConstructor dc1 = c1.getDeclaredConstructor(String.class,String.class) ;dc1.setAccessible(true);dc1.newInstance(&quot;小明&quot;,&quot;很帅&quot;) ;\ndc1.newInstance(&quot;小明&quot;,&quot;很帅&quot;);方法调用了Person类中的：\npublic Person(String name, String msg) &#123;\tthis.name = name;\tthis.msg = msg;\tSystem.out.println(name+&quot;的描述是&quot;+msg);&#125;\n这个构造方法，如果不传参数的话，那么调用的就是无参的构造方法。输出结果为:\n小明的描述是很帅\n\n\n通过反射了解集合泛型的本质通过以下例子程序可以看出：\npackage top.crosssoverjie.study;import java.lang.reflect.Method;import java.util.ArrayList;import java.util.List;public class GenericEssence &#123;\tpublic static void main(String[] args) &#123;\t\t//声明两个list，一个有泛型，一个没有泛型\t\tList list1 = new ArrayList() ;\t\tList&lt;String&gt; list2 = new ArrayList&lt;String&gt;() ;\t\t\t\tlist2.add(&quot;你好&quot;) ;//\t\tlist2.add(11) ;加上泛型之后在编译期间只能添加String，不然会报错。\t\tSystem.out.println(&quot;list2的长度是：&quot;+list2.size());\t\t\t\t\t\tClass c1 = list1.getClass();\t\tClass c2 = list2.getClass() ;\t\tSystem.out.print(&quot;c1,c2是否相等:&quot;);\t\tSystem.out.println(c1==c2);\t\t\t\ttry &#123;\t\t\t//通过反射绕过编译器动态调用add方法，可能否加入非String类型的元素\t\t\tMethod method = c2.getDeclaredMethod(&quot;add&quot;, Object.class) ;\t\t\tmethod.invoke(list2, 123) ;//在这里加入int类型，在上面如果加入int会出现编译报错。\t\t\t\t\t\t//list2的长度增加了，说明添加成功了\t\t\tSystem.out.println(&quot;现在list2的长度是:&quot;+list2.size());\t\t\t\t\t\t/**\t\t\t * 所以可以看出，泛型只是在编译期间起作用，在经过编译进入运行期间是不起作用的。\t\t\t * 就算是不是泛型要求的类型也是可以插入的。\t\t\t */\t\t\t\t\t&#125; catch (Exception e) &#123;\t\t\te.printStackTrace() ;\t\t&#125;\t\t\t&#125;&#125;\n\n所以可以看出，泛型只是在编译期间起作用，在经过编译进入运行期间是不起作用的。就算是不是泛型要求的类型也是可以插入的。\n\n反射知识点\n总结\n泛型的应用比较多：\n\nspring的IOC&#x2F;DI。 \nJDBC中的中的加载驱动\n\n\n\n参考\njava中的反射机制\n反射机制是什么\n\n","categories":["Java笔记"],"tags":["Java","Reflect"]},{"title":"java多线程（一）多线程基础","url":"/2016/05/14/java-thread1/","content":"前言本文主要讲解java多线程的基础，以及一些常用方法。关于线程同步、ExecutorService框架我会放到后续的文章进行讲解。\n\n进程与线程的区别进程进程简单的来说就是在内存中运行的应用程序，一个进程可以启动多个线程。比如在windows中一个运行EXE文件就是一个进程。\n线程同一个线程中的进程共用相同的地址空间，同时共享进程所拥有的内存和其他资源。\n\n\n\n线程Demo-继承Thread类首先我们我们继承java.lang.Thread类来创建线程。\npackage top.crosssoverjie.study.Thread;public class TestThread &#123;\tpublic static void main(String[] args) &#123;\t\tSystem.out.println(&quot;主线程ID是：&quot; + Thread.currentThread().getId());\t\tMyThread my = new MyThread(&quot;线程1&quot;);\t\tmy.start() ;\t\t\t\tMyThread my2 = new MyThread(&quot;线程2&quot;) ;\t\t/**\t\t * 这里直接调用my2的run()方法。\t\t */\t\tmy2.run() ;\t&#125;&#125;class MyThread extends Thread &#123;\tprivate String name;\tpublic MyThread(String name) &#123;\t\tthis.name = name;\t&#125;\t@Override\tpublic void run() &#123;\t\tSystem.out.println(&quot;名字：&quot; + name + &quot;的线程ID是=&quot;\t\t\t\t+ Thread.currentThread().getId());\t&#125;&#125;\n输出结果:\n主线程ID是：1名字：线程2的线程ID是=1名字：线程1的线程ID是=9\n由输出结果我们可以得出以下结论：\n\n\nmy和my2的线程ID不相同，my2和主线程ID相同。说明直接调用run()方法不会创建新的线程，而是在主线程中直接调用的run()方法,和普通的方法调用没有区别。\n虽然my的start()方法是在my2的run()方法之前调用，但是却是后输出内容，说明新建的线程并不会影响主线程的执行。\n\n\n\n线程Demo-实现Runnable接口除了继承java.lang.Thread类之外，我们还可以实现java.lang.Runnable接口来创建线程。\npackage top.crosssoverjie.study.Thread;public class TestRunnable &#123;\tpublic static void main(String[] args) &#123;\t\tSystem.out.println(&quot;主线程的线程ID是&quot;+Thread.currentThread().getId());\t\tMyThread2 my = new MyThread2(&quot;线程1&quot;) ;\t\tThread t = new Thread(my) ;\t\tt.start() ;\t\t\t\tMyThread2 my2 = new MyThread2(&quot;线程2&quot;) ;\t\tThread t2 = new Thread(my2) ;\t\t/**\t\t * 方法调用，并不会创建线程，依然是主线程\t\t */\t\tt2.run() ;\t&#125;&#125;class MyThread2 implements Runnable&#123;\tprivate String name ;\tpublic MyThread2(String name)&#123;\t\tthis.name = name ;\t&#125;\t@Override\tpublic void run() &#123;\t\tSystem.out.println(&quot;线程&quot;+name+&quot;的线程ID是&quot;+Thread.currentThread().getId());\t&#125;\t\t&#125;\n输出结果:\n主线程的线程ID是1线程线程2的线程ID是1线程线程1的线程ID是9\nnotes:\n\n\n实现Runnable的方式需要将实现Runnable接口的类作为参数传递给Thread，然后通过Thread类调用Start()方法来创建线程。\n这两种方式都可以来创建线程，至于选择哪一种要看自己的需求。直接继承Thread类的话代码要简洁一些，但是由于java只支持单继承，所以如果要继承其他类的同时需要实现线程那就只能实现Runnable接口了，这里更推荐实现Runnable接口。\n\n\n实际上如果我们查看Thread类的源码我们会发现Thread是实现了Runnable接口的：\n\n线程中常用的方法\n\n\n序号\n方法\n介绍\n\n\n\n1\npublic void start()\n使该线程执行，java虚拟机会调用该线程的run()方法。\n\n\n2\npublic final void setName(String name)\n修改线程名称。\n\n\n3\npublic final void setPriority(int privority)\n修改线程的优先级。\n\n\n4\npublic final void setDaemon(false on)\n将该线程标记为守护线程或用户线程，当正在运行线程都是守护线程时，java虚拟机退出，该方法必须在启动线程前调用。\n\n\n5\npublic final void join(long mills)\n等待该线程的终止时间最长为mills毫秒。\n\n\n6\npublic void interrupt()\n中断线程。\n\n\n7\npublic static boolean isAlive()\n测试线程是否处于活动状态。如果该线程已经启动尚未终止，则为活动状态。\n\n\n8\npublic static void yield()\n暂停当前线程执行的对象，并执行其他线程。\n\n\n9\npublic static void sleep(long mills)\n在指定毫秒数内，让当前执行的线程休眠(暂停)。\n\n\n10\npublic static Thread currentThread()\n返回当前线程的引用。\n\n\n方法详解- public static void sleep(long mills)package top.crosssoverjie.study.Thread;public class TestSleep &#123;\tprivate int i = 10 ;\tprivate Object ob = new Object() ;\t\tpublic static void main(String[] args) &#123;\t\tTestSleep t = new TestSleep() ;\t\tMyThread3 thread1 = t.new MyThread3() ;\t\tMyThread3 thread2 = t.new MyThread3() ;\t\tthread1.start() ;\t\tthread2.start() ;\t&#125;\t\tclass MyThread3 extends Thread&#123;\t\t@Override\t\tpublic void run() &#123;\t\t\tsynchronized (ob) &#123;\t\t\t\ti++ ;\t\t\t\tSystem.out.println(&quot;i的值：&quot;+i);\t\t\t\tSystem.out.println(&quot;线程：&quot;+Thread.currentThread().getName()+&quot;进入休眠状态&quot;);\t\t\t\ttry &#123;\t\t\t\t\tThread.currentThread().sleep(1000) ;\t\t\t\t&#125; catch (Exception e) &#123;\t\t\t\t\te.printStackTrace();\t\t\t\t&#125;\t\t\t\tSystem.out.println(&quot;线程：&quot;+Thread.currentThread().getName()+&quot;休眠结束&quot;);\t\t\t\ti++;\t\t\t\tSystem.out.println(&quot;i的值&gt;：&quot;+i);\t\t\t&#125;\t\t&#125;\t&#125;\t&#125;\n输出结果：\ni的值：11线程：Thread-0进入休眠状态线程：Thread-0休眠结束i的值&gt;：12i的值：13线程：Thread-1进入休眠状态线程：Thread-1休眠结束i的值&gt;：14\n由输出结果我们可以得出：\n\n\n当Thread0进入休眠状态时，Thread1并没有继续执行，而是等待Thread0休眠结束释放了对象锁，Thread1才继续执行。当调用sleep()方法时，必须捕获异常或者向上层抛出异常。当线程休眠时间满时，并不一定会马上执行，因为此时有可能CPU正在执行其他的任务，所以调用了sleep()方法相当于线程进入了阻塞状态。\n\n\n方法详解- public static void yield()package top.crosssoverjie.study.Thread;public class Testyield &#123;\tpublic static void main(String[] args) &#123;\t\tMyThread4 my = new MyThread4() ;\t\tmy.start() ;\t&#125;&#125;class MyThread4 extends Thread&#123;\t@Override\tpublic void run() &#123;\t\tlong open = System.currentTimeMillis();\t\tint count= 0 ;\t\tfor(int i=0 ;i&lt;1000000;i++)&#123;\t\t\tcount= count+(i+1);//\t\t\tThread.yield() ;\t\t&#125;\t\tlong end = System.currentTimeMillis();\t\tSystem.out.println(&quot;用时：&quot;+(end-open)+&quot;毫秒&quot;);\t&#125;&#125;\n输出结果:用时：1毫秒如果将 Thread.yield()注释取消掉，输出结果:用时：116毫秒\n\n\n调用yield()方法是为了让当前线程交出CPU权限，让CPU去执行其他线程。它和sleep()方法类似同样是不会释放锁。但是yield()不能控制具体的交出CUP的时间。并且它只能让相同优先级的线程获得CPU执行时间的机会。\n调用yield()方法不会让线程进入阻塞状态，而是进入就绪状态，它只需要等待重新获取CPU的时间，这一点和sleep()方法是不一样的。\n\n\n方法详解- public final void join()在很多情况下我们需要在子线程中执行大量的耗时任务，但是我们主线程又必须得等待子线程执行完毕之后才能结束，这就需要用到 join()方法了。join()方法的作用是等待线程对象销毁，如果子线程执行了这个方法，那么主线程就要等待子线程执行完毕之后才会销毁，请看下面这个例子：\npackage top.crosssoverjie.study.Thread;public class Testjoin &#123;\tpublic static void main(String[] args) throws InterruptedException &#123;\t\tnew MyThread5(&quot;t1&quot;).start() ;\t\tfor (int i = 0; i &lt; 10; i++) &#123;\t\t\tif(i == 5)&#123;\t\t\t\tMyThread5 my =new MyThread5(&quot;t2&quot;) ;\t\t\t\tmy.start() ;\t\t\t\tmy.join() ;\t\t\t&#125;\t\t\tSystem.out.println(&quot;main当前线程：&quot;+Thread.currentThread().getName()+&quot; &quot;+i);\t\t&#125;\t&#125;&#125;class MyThread5 extends Thread&#123;\t\tpublic MyThread5(String name)&#123;\t\tsuper(name) ;\t&#125;\t@Override\tpublic void run() &#123;\t\tfor (int i = 0; i &lt; 5; i++) &#123;\t\t\tSystem.out.println(&quot;当前线程：&quot;+Thread.currentThread().getName()+&quot; &quot;+i);\t\t&#125;\t&#125;&#125;\n输出结果：\nmain当前线程：main 0当前线程：t1 0当前线程：t1 1main当前线程：main 1当前线程：t1 2main当前线程：main 2当前线程：t1 3main当前线程：main 3当前线程：t1 4main当前线程：main 4当前线程：t2 0当前线程：t2 1当前线程：t2 2当前线程：t2 3当前线程：t2 4main当前线程：main 5main当前线程：main 6main当前线程：main 7main当前线程：main 8main当前线程：main 9\n如果我们把join()方法注释掉之后：\nmain当前线程：main 0当前线程：t1 0main当前线程：main 1当前线程：t1 1main当前线程：main 2当前线程：t1 2main当前线程：main 3当前线程：t1 3main当前线程：main 4当前线程：t1 4main当前线程：main 5main当前线程：main 6main当前线程：main 7main当前线程：main 8main当前线程：main 9当前线程：t2 0当前线程：t2 1当前线程：t2 2当前线程：t2 3当前线程：t2 4\n由上我们可以得出以下结论：\n\n\n在使用了join()方法之后主线程会等待子线程结束之后才会结束。\n\n\n方法详解- setDaemon(boolean on),getDaemon()用来设置是否为守护线程和判断是否为守护线程。notes：\n\n\n守护线程依赖于创建他的线程，而用户线程则不需要。如果在main()方法中创建了一个守护线程，那么当main方法执行完毕之后守护线程也会关闭。而用户线程则不会，在JVM中垃圾收集器的线程就是守护线程。\n\n\n\n优雅的终止线程有三种方法可以终止线程，如下：\n\n使用退出标识，使线程正常的退出，也就是当run()方法完成后线程终止。\n使用stop()方法强行关闭，这个方法现在已经被废弃，不推荐使用\n使用interrupt()方法终止线程。\n\n具体的实现代码我将在下一篇博文中将到。。\n线程的优先级在操作系统中线程是分优先级的，优先级高的线程CPU将会提供更多的资源，在java中我们可以通过setPriority(int newPriority)方法来更改线程的优先级。在java中分为1~10这个十个优先级，设置不在这个范围内的优先级将会抛出IllegalArgumentException异常。java中有三个预设好的优先级：\n\n\npublic final static int MIN_PRIORITY = 1;\npublic final static int NORM_PRIORITY = 5;\npublic final static int MAX_PRIORITY = 10;\n\n\n\n参考\n\n如何终止线程\njava多线程学习\n\n\njava多线程思维图\n\n总结以上就是我总结的java多线程基础知识，后续会补充线程关闭、线程状态、线程同步和有返回结果的多线程。\n","categories":["java多线程"],"tags":["Java","Thread","Runnable"]},{"title":"java多线程（二）有返回值的多线程","url":"/2016/05/27/java-thread2/","content":"前言之前我们使用多线程要么是继承Thread类，要么是实现Runnable接口，然后重写一下run()方法即可。但是只有的话如果有死锁、对共享资源的访问和随时监控线程状态就不行了，于是在Java5之后就有了Callable接口。\n\n简单的实现有返回值的线程代码如下：CallableFuture类\npackage top.crosssoverjie.study.Thread;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Future;public class CallableFuture &#123;\tpublic static void main(String[] args) &#123;\t\t//创建一个线程池\t\tExecutorService pool = Executors.newFixedThreadPool(3) ;\t\t\t\t//创建三个有返回值的任务\t\tCallableTest2 c1 = new CallableTest2(&quot;线程1&quot;) ;\t\tCallableTest2 c2 = new CallableTest2(&quot;线程2&quot;) ;\t\tCallableTest2 c3 = new CallableTest2(&quot;线程3&quot;) ;\t\t\t\tFuture f1 = pool.submit(c1) ;\t\tFuture f2 = pool.submit(c2) ;\t\tFuture f3 = pool.submit(c3) ;\t\t\t\ttry &#123;\t\t\tSystem.out.println(f1.get().toString());\t\t\tSystem.out.println(f2.get().toString());\t\t\tSystem.out.println(f3.get().toString());\t\t&#125; catch (InterruptedException e) &#123;\t\t\te.printStackTrace();\t\t&#125; catch (ExecutionException e) &#123;\t\t\te.printStackTrace();\t\t&#125;finally&#123;\t\t\tpool.shutdown();\t\t&#125;\t\t\t&#125;&#125;\n\n·CallableTest2·类：\npackage top.crosssoverjie.study.Thread;import java.util.concurrent.Callable;public class CallableTest2 implements Callable &#123;\tprivate String name ;\tpublic CallableTest2(String name) &#123;\t\tthis.name = name;\t&#125;\t@Override\tpublic Object call() throws Exception &#123;\t\treturn name+&quot;返回了东西&quot;;\t&#125;\t\t&#125;\n运行结果：\n线程1返回了东西线程2返回了东西线程3返回了东西\n\n\n总结以上就是一个简单的例子，需要了解更多详情可以去看那几个类的API。\n","categories":["java多线程"],"tags":["Java","Callable","ExecutorService","Future","Executors"]},{"title":"对象的创建与内存分配","url":"/2018/01/18/newObject/","content":"\n创建对象当 JVM 收到一个 new 指令时，会检查指令中的参数在常量池是否有这个符号的引用，还会检查该类是否已经被加载过了，如果没有的话则要进行一次类加载。\n接着就是分配内存了，通常有两种方式：\n\n指针碰撞\n空闲列表\n\n使用指针碰撞的前提是堆内存是完全工整的，用过的内存和没用的内存各在一边每次分配的时候只需要将指针向空闲内存一方移动一段和内存大小相等区域即可。\n当堆中已经使用的内存和未使用的内存互相交错时，指针碰撞的方式就行不通了，这时就需要采用空闲列表的方式。虚拟机会维护一个空闲的列表，用于记录哪些内存是可以进行分配的，分配时直接从可用内存中直接分配即可。\n堆中的内存是否工整是有垃圾收集器来决定的，如果带有压缩功能的垃圾收集器就是采用指针碰撞的方式来进行内存分配的。\n\n\n分配内存时也会出现并发问题:\n这样可以在创建对象的时候使用 CAS 这样的乐观锁来保证。\n也可以将内存分配安排在每个线程独有的空间进行，每个线程首先在堆内存中分配一小块内存，称为本地分配缓存(TLAB : Thread Local Allocation Buffer)。\n分配内存时，只需要在自己的分配缓存中分配即可，由于这个内存区域是线程私有的，所以不会出现并发问题。\n可以使用 -XX:+/-UseTLAB 参数来设定 JVM 是否开启 TLAB 。\n内存分配之后需要对该对象进行设置，如对象头。对象头的一些应用可以查看 Synchronize 关键字原理。\n对象访问一个对象被创建之后自然是为了使用，在 Java 中是通过栈来引用堆内存中的对象来进行操作的。\n对于我们常用的 HotSpot 虚拟机来说，这样引用关系是通过直接指针来关联的。\n如图:\n\n这样的好处就是：在 Java 里进行频繁的对象访问可以提升访问速度(相对于使用句柄池来说)。\n内存分配Eden 区分配简单的来说对象都是在堆内存中分配的，往细一点看则是优先在 Eden 区分配。\n这里就涉及到堆内存的划分了，为了方便垃圾回收，JVM 将对内存分为新生代和老年代。\n而新生代中又会划分为 Eden 区，from Survivor、to Survivor 区。\n其中 Eden 和 Survivor 区的比例默认是 8:1:1，当然也支持参数调整 -XX:SurvivorRatio=8。\n当在 Eden 区分配内存不足时，则会发生 minorGC ，由于 Java 对象多数是朝生夕灭的特性，所以 minorGC 通常会比较频繁，效率也比较高。\n当发生 minorGC 时，JVM 会根据复制算法将存活的对象拷贝到另一个未使用的 Survivor 区，如果 Survivor 区内存不足时，则会使用分配担保策略将对象移动到老年代中。\n谈到 minorGC 时，就不得不提到 fullGC(majorGC) ，这是指发生在老年代的 GC ，不论是效率还是速度都比 minorGC  慢的多，回收时还会发生 stop the world 使程序发生停顿，所以应当尽量避免发生 fullGC 。\n老年代分配也有一些情况会导致对象直接在老年代分配，比如当分配一个大对象时(大的数组，很长的字符串)，由于 Eden 区没有足够大的连续空间来分配时，会导致提前触发一次 GC，所以尽量别频繁的创建大对象。\n因此 JVM 会根据一个阈值来判断大于该阈值对象直接分配到老年代，这样可以避免在新生代频繁的发生 GC。\n对于一些在新生代的老对象 JVM 也会根据某种机制移动到老年代中。\nJVM 是根据记录对象年龄的方式来判断该对象是否应该移动到老年代，根据新生代的复制算法，当一个对象被移动到 Survivor 区之后 JVM 就给该对象的年龄记为1，每当熬过一次 minorGC 后对象的年龄就 +1 ，直到达到阈值(默认为15)就移动到老年代中。\n\n可以使用 -XX:MaxTenuringThreshold=15 来配置这个阈值。\n\n总结虽说这些内容略显枯燥，但当应用发生不正常的 GC 时，可以方便更快的定位问题。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["Java 进阶"]},{"title":"日常记录（一）MySQL被锁解决方案","url":"/2016/06/05/normal-skill1/","content":"前言\n由于前段时间为了让部署在Linux中的项目访问另一台服务器的MySQL，经过各种折腾就把root用户给弄出问题了，导致死活登不上PS:Linux中的项目还是没有连上。。(这是后话了。)。经过各种查阅资料终于找到解决方法了。\n\n报错如下：Access denied for user &#39;root&#39;@&#39;localhost&#39; (using password:YES)\n\n关闭MySQL服务，修改MySQL初始文件打开MySQL目录下的my-default.ini文件，如图：在最后一行加入skip-grant-tables之后保存。然后重启MySQL服务。\n\n\n用命令行登录MySQL修改ROOT账号密码用命令行登录MySQL输入mysql -uroot -p,不用输入密码，直接敲回车即可进入。如下图：之后执行以下语句修改ROOT用户密码：\n\nuse mysql;\nupdate user set password=PASSWORD(&quot;你的密码&quot;) where user=&#39;root&#39;;\n\n还原my-default.ini文件最后还原配置文件，之后重启MySQL服务即可正常登录了。\n","categories":["日常记录"],"tags":["MySQL"]},{"title":"日常记录（二）SpringMvc导出Excel","url":"/2016/06/14/normal-skill2/","content":"前言\n相信很多朋友在实际工作中都会要将数据导出成Excel的需求，通常这样的做法有两种。一是采用JXL来生成Excel，之后保存到服务器，然后在生成页面之后下载该文件。二是使用POI来生成Excel，之后使用Stream的方式输出到前台直接下载(ps:当然也可以生成到服务器中再下载。)。这里我们讨论第二种。*至于两种方式的优缺点请自行百度*。\n\n\nStruts2的方式通常我会将已经生成好的HSSFWorkbook放到一个InputStream中，然后再到xml配置文件中将返回结果更改为stream的方式。如下：\nprivate void responseData(HSSFWorkbook wb) throws IOException &#123;\tByteArrayOutputStream baos = new ByteArrayOutputStream();\twb.write(baos);\tbaos.flush();\tbyte[] aa = baos.toByteArray();\texcelStream = new ByteArrayInputStream(aa, 0, aa.length);\tbaos.close();&#125;\n\n配置文件：\n&lt;action name=&quot;exportXxx&quot; class=&quot;xxxAction&quot; method=&quot;exportXxx&quot;&gt;\t&lt;result name=&quot;exportSuccess&quot; type=&quot;stream&quot;&gt;\t\t&lt;param name=&quot;inputName&quot;&gt;excelStream&lt;/param&gt;    \t&lt;param name=&quot;contentType&quot;&gt;application/vnd.ms-excel&lt;/param&gt;    \t&lt;param name=&quot;contentDisposition&quot;&gt;attachment;filename=&quot;Undefined.xls&quot;&lt;/param&gt;\t&lt;/result&gt;&lt;/action&gt;\n这样即可达到点击链接即可直接下载文件的目的。\n\nSpringMVC的方式先贴代码：\n@RequestMapping(&quot;/exportXxx.action&quot;)public void exportXxx(HttpServletRequest request, HttpServletResponse response,\t\t@RequestParam(value=&quot;scheduleId&quot;, defaultValue=&quot;0&quot;)int scheduleId)&#123;\tHSSFWorkbook wb = createExcel(scheduleId) ;\ttry &#123;\t\tresponse.setHeader(&quot;Content-Disposition&quot;, &quot;attachment; filename=appointmentUser.xls&quot;);\t\tresponse.setContentType(&quot;application/vnd.ms-excel; charset=utf-8&quot;) ;\t\tOutputStream out = response.getOutputStream() ;\t\twb.write(out) ;\t\tout.flush();\t\tout.close();\t&#125; catch (IOException e) &#123;\t\te.printStackTrace();\t&#125; &#125;\n其实springMVC和Struts2的原理上是一样的，只是Struts2是才去配置文件的方式。首先是使用createExcel()这个方法来生成Excel并返回，最后利用rresponse即可向前台输出Excel，这种方法是通用的，也可以试用与Servlet、Struts2等。我们只需要在response的头信息中设置相应的输出信息即可。\n\n总结不管是使用Struts2，还是使用SpringMVC究其根本都是使用的response，所以只要我们把response理解透了不管是下载图片、world、Excel还是其他什么文件都是一样的。\n","categories":["日常记录"],"tags":["Java","poi"]},{"title":"日常记录（三）更换Hexo主题","url":"/2016/06/18/normal-skill3/","content":"前言\n由于博主的喜新厌旧，再经过一段时间对上一个主题的审美疲劳加上我专(zhuang)研(bi)的精神于是就想找一个B格较高的主题。经过一段时间的查找发现NexT这个主题简洁而不失华丽，低调而不失逼格(就不收广告费了)特别适合我，接着就着手开干。\n\n\n安装NexT主题从Git上克隆主题这里我就不介绍有关Hexo的东西了，默认是知道如何搭建Hexo博客的。还不太清楚的请自行百度。首先将NexT主题先克隆到自己电脑上：- cd your-hexo-site- git clone https://github.com/iissnan/hexo-theme-next themes/next。## 安装主题接下来我们只需要将站点下的_config.yml配置文件中的主题配置更换成Next，如下图：其实这样主题就配好了，是不是很简单。NexT主题配置Hexo配置文件相关配置Next主题的个人头像是在Hexo配置文件中的。NexT同样也支持多说配置，我们只需要将你自己的多说账号也配置到Hexo的配置文件中即可。duoshuo_shortname: your name\nNext配置文件相关配置NexT主题非常吸引我的一点就是他支持打赏功能，这让我这种穷逼程序猿又看到了生路(多半也没人会给我打赏)，以下一段配置即可在每篇博文下边开启打赏功能。微信也是可以的，但是我找了半天没有找到生成微信支付码的地方。其他的一些配置我觉得都比较简单，看官方的帮助文档也是完全可以的，有问题的我们可以再讨论。\n\n一个绕坑指南我在换完NexT之后发现在首页这里显示的分类和便签的统计都是对的，但是点进去之后就是空白的。我查看了Hexo和NexT的文档发现我写的没有任何问题，之后就懵逼了。。。各位有碰到这个问题的可以往下看。\n绕坑之后我仔细的查阅了NexT的文档，发现他所使用的tags和categories文件夹下的index.md的格式是这样的：\n---title: tagsdate: 2016-06-16 02:13:06type: &quot;tags&quot;---\n这和我之前使用的JackMan主题是完全不一样的(有关JackMan主题可以自行查阅)。之后我讲categories文件下的index.md文件也换成这样的格式就没有问题了。如果你和我一样眼神不好的话建议配副眼镜。\n总结其实以上的很多东西都是在NexT官方文档里查得到的，接下来我会尝试提一点pull request来更加深入的了解Hexo。\n","categories":["日常记录"],"tags":["Hexo"]},{"title":"sbc(一)SpringBoot+SpringCloud初探","url":"/2017/06/15/sbc1/","content":"前言\n有看过我之前的SSM系列的朋友应该有一点印象是非常深刻的。\n\n那就是需要配置的配置文件非常多，什么Spring、mybatis、redis、mq之类的配置文件非常多，并且还存在各种版本，甚至有些版本还互不兼容。其中有很多可能就是刚开始整合的时候需要配置，之后压根就不会再动了。\n\n鉴于此，Spring又推出了又一神器SpringBoot.\n它可以让我们更加快速的开发Spring应用，甚至做到了开箱即用。由于在实际开发中我们使用SpringBoot+SpringCloud进行了一段时间的持续交付，并在生产环境得到了验证，其中也有不少踩坑的地方，借此机会和大家分享交流一下。\n本篇我们首先会用利用SpringBoot构建出一个简单的REST API.接着会创建另一个SpringBoot项目，基于SpringCloud部署，并在两个应用之间进行调用。\n使用SpringBoot构建REST API我们可以使用Spring官方提供的初始化工具帮我们生成一个基础项目：http://start.spring.io/,如下图所示：\n填入相应信息即可。由于只是要实现REST API所以这里只需要引用web依赖即可。\n\n\n将生成好的项目导入IDE(我使用的是idea)中,目录结构如下;\n\n其中的SbcUserApplication是整个应用的入口。\nresource/application.properties这里是存放整个应用的配置文件。\n其中的static和templates是存放静态资源以及前端模板的地方，由于我们采用了前后端分离，所以这些目录基本上用不上了。\n\n通过运行SbcUserApplication类的main方法可以启动SpringBoot项目。\n接着在PostMan中进行调用，看到以下结果表明启动成功了：\n\n这样一看是不是要比之前用Spring+SpringMVC来整合要方便快捷很多。\n创建另一个SpringBoot项目当我们的项目采用微服务构建之后自然就会被拆分成N多个独立的应用。比如上文中的sbc-user用于用户管理。这里再创建一个sbc-order用户生成订单。\n\n为了方便之后的代码复用，我将common包中的一些枚举值、工具类单独提到sbc-common应用中了，这样有其他应用要使用这些基础类直接引入这个依赖即可。\n\n&lt;dependency&gt;\t&lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;\t&lt;artifactId&gt;sbc-common&lt;/artifactId&gt;\t&lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;\n\n创建步骤和上文差不多，这里就不再赘述了。其中有一个order/getOrderNo的服务，调用结果如下：\n\n之后会利用SpringCloud来将两个服务关联起来，并可以互相调用。\n使用SpringCloud进行分布式调用搭建eureka注册中心既然是要搭建微服务那自然少不了注册中心了，之前讲的dubbo采用的是zookeeper作为注册中心，SpringCloud则采用的是Netflix Eureka来做服务的注册与发现。\n新建一个项目sbc-service,目录结构如下：\n\n核心的pom.xml\n&lt;dependencies&gt;\t&lt;dependency&gt;\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t\t&lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt;\t&lt;/dependency&gt;\t&lt;dependency&gt;\t\t&lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;\t\t&lt;artifactId&gt;logback-classic&lt;/artifactId&gt;\t\t&lt;version&gt;1.2.1&lt;/version&gt;\t&lt;/dependency&gt;&lt;/dependencies&gt;&lt;dependencyManagement&gt;\t&lt;dependencies&gt;\t\t&lt;dependency&gt;\t\t\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t\t\t&lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt;\t\t\t&lt;version&gt;Brixton.RELEASE&lt;/version&gt;\t\t\t&lt;type&gt;pom&lt;/type&gt;\t\t\t&lt;scope&gt;import&lt;/scope&gt;\t\t&lt;/dependency&gt;\t&lt;/dependencies&gt;&lt;/dependencyManagement&gt;\n\n非常easy，只需要引入eureka 的依赖即可。然后在入口类加入一个注解@EnableEurekaServer，即可将该项目作为服务注册中心：\npackage com.crossoverJie.service;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@EnableEurekaServer@SpringBootApplicationpublic class EurekaApplication &#123;\tprivate final static Logger logger = LoggerFactory.getLogger(EurekaApplication.class);\tpublic static void main(String[] args) &#123;\t\tSpringApplication.run(EurekaApplication.class, args);\t\tlogger.info(&quot;SpringBoot Start Success&quot;);\t&#125;&#125;\n\n接着修改配置文件application.properties:\nserver.port=8888# 不向注册中心注册自己eureka.client.register-with-eureka=false# 不需要检索服务eureka.client.fetch-registry=falseeureka.client.serviceUrl.defaultZone=http://localhost:$&#123;server.port&#125;/eureka/\n\n配置一下端口以及注册中心的地址即可。然后按照正常启动springBoot项目一样启动即可。\n在地址栏输入http://localhost:8888看到一下界面：\n\n当然现在在注册中心还看不到任何一个应用，下面需要将上文的sbc-user,sbc-order注册进来。\n向注册中心注册服务提供者只需要在application.properties配置文件中加上注册中心的配置：\neureka.client.serviceUrl.defaultZone=http://localhost:8888/eureka/\n\n并在sbc-order的主类中加入@EnableDiscoveryClient注解即可完成注册服务。\n启动注册中心以及应用，在注册中心看到一下界面则成功注册:\n\n消费注册中心的服务服务是注册上去了，自然是需要消费了，这里就简单模拟了在调用http://localhost:8080/user/getUser这个接口的时候getUser接口会去调用order的getOrder服务。\n这里会用到另一个依赖:\n&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt;&lt;/dependency&gt;\n\n他可以帮我们做到客户端负载，具体使用如下：\n\n加入ribbon依赖。\n在主类中开启@LoadBalanced客户端负载。\n创建restTemplate类的实例\n\n@Bean\t@LoadBalanced\tpublic RestTemplate restTemplate() &#123;\t\treturn new RestTemplate();\t&#125;\n\n\n使用restTemplate 调用远程服务:\n\n@Autowired  private RestTemplate restTemplate;  @RequestMapping(value = &quot;/getUser&quot;,method = RequestMethod.POST)  public UserRes getUser(@RequestBody UserReq userReq)&#123;      OrderNoReq req = new OrderNoReq() ;      req.setReqNo(&quot;1213&quot;);      //调用远程服务      ResponseEntity&lt;Object&gt; res = restTemplate.postForEntity(&quot;http://sbc-order/order/getOrderNo&quot;, req, Object.class);      logger.info(&quot;res=&quot;+JSON.toJSONString(res));      logger.debug(&quot;入参=&quot;+ JSON.toJSONString(userReq));      UserRes userRes = new UserRes() ;      userRes.setUserId(123);      userRes.setUserName(&quot;张三&quot;);      userRes.setReqNo(userReq.getReqNo());      userRes.setCode(StatusEnum.SUCCESS.getCode());      userRes.setMessage(&quot;成功&quot;);      return userRes ;  &#125;\n\n由于我的远程接口是post,所以使用了postForEntity()方法，如果是get就换成getForEntity()即可。\n\n注意这里是使用应用名sbc-order(配置于sbc-order的application.properties中)来进行调用的，并不是一个IP地址。\n\n启动注册中心、两个应用。用PostMan调用getUser接口时控制台打印:\n2017-06-27 00:18:04.534  INFO 63252 --- [nio-8080-exec-3] c.c.sbcuser.controller.UserController    : res=&#123;&quot;body&quot;:&#123;&quot;code&quot;:&quot;4000&quot;,&quot;message&quot;:&quot;appID不能为空&quot;,&quot;reqNo&quot;:&quot;1213&quot;&#125;,&quot;headers&quot;:&#123;&quot;X-Application-Context&quot;:[&quot;sbc-order:8181&quot;],&quot;Content-Type&quot;:[&quot;application/xml;charset=UTF-8&quot;],&quot;Transfer-Encoding&quot;:[&quot;chunked&quot;],&quot;Date&quot;:[&quot;Mon, 26 Jun 2017 16:18:04 GMT&quot;]&#125;,&quot;statusCode&quot;:&quot;OK&quot;,&quot;statusCodeValue&quot;:200&#125;\n由于并没有传递appId所以order服务返回了一个错误，也正说明是远程调用到了该服务。\n总结\nps:这里只是简单使用了ribbon来进行服务调用，但在实际的开发中还是比较少的使用这种方式来调用远程服务，而是使用Feign进行声明式调用，可以简化客户端代码，具体使用方式请持续关注。\n\n本次算是springBoot+springCloud的入门，还有很多东西没有讲到，之后我将会根据实际使用的一些经验继续分享SpringCloud这个新兴框架。\n\n项目：https://github.com/crossoverJie/springboot-cloud\n\n\n博客：http://crossoverjie.top。\n\n","categories":["sbc"],"tags":["Java","SpringBoot","SpringCloud"]},{"title":"sbc(二)高可用Eureka+声明式服务调用","url":"/2017/07/19/sbc2/","content":"\n前言\n上一篇简单入门了SpringBoot+SpringCloud 构建微服务。但只能算是一个demo级别的应用。这次会按照实际生产要求来搭建这套服务。\n\nSwagger应用上次提到我们调用自己的http接口的时候采用的是PostMan来模拟请求，这个在平时调试时自然没有什么问题，但当我们需要和前端联调开发的时候效率就比较低了。\n通常来说现在前后端分离的项目一般都是后端接口先行。\n后端大大们先把接口定义好(入参和出参),前端大大们来确定是否满足要求，可以了之后后端才开始着手写实现，这样整体效率要高上许多。\n但也会带来一个问题:在接口定义阶段频繁变更接口定义而没有一个文档或类似的东西来记录，那么双方的沟通加上前端的调试都是比较困难的。\n基于这个需求网上有各种解决方案，比如阿里的rap就是一个不错的例子。\n但是springCould为我们在提供了一种在开发springCloud项目下更方便的工具swagger。\n实际效果如下:\n\n\n\n\n配置swagger以sbc-order为例我将项目分为了三个模块:\n├── order                                    // Order服务实现  │   ├── src/main├── order-api                                // 对内API│   ├── src/main├── order-client                             // 对外的clientAPI│   ├── src/main├── .gitignore                               ├── LICENSE                ├── README.md               \n\n因为实现都写在order模块中，所以只需要在该模块中配置即可。\n首先需要加入依赖，由于我在order模块中依赖了:\n&lt;dependency&gt;    &lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;    &lt;artifactId&gt;order-api&lt;/artifactId&gt;    &lt;version&gt;$&#123;target.version&#125;&lt;/version&gt;&lt;/dependency&gt;\n\norder-api又依赖了：\n&lt;dependency&gt;    &lt;groupId&gt;io.springfox&lt;/groupId&gt;    &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt;    &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;io.springfox&lt;/groupId&gt;    &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt;    &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;\n\n接着需要配置一个SwaggerConfig\n@Configuration@EnableSwagger2/** 是否打开swagger **/@ConditionalOnExpression(&quot;&#x27;$&#123;swagger.enable&#125;&#x27; == &#x27;true&#x27;&quot;)public class SwaggerConfig &#123;\t    \t@Bean    public Docket createRestApi() &#123;        return new Docket(DocumentationType.SWAGGER_2)                .apiInfo(apiInfo())                .select()                .apis(RequestHandlerSelectors.basePackage(&quot;com.crossoverJie.sbcorder.controller&quot;))                .paths(PathSelectors.any())                .build();    &#125;\t    private ApiInfo apiInfo() &#123;        return new ApiInfoBuilder()                .title(&quot;sbc order api&quot;)                .description(&quot;sbc order api&quot;)                .termsOfServiceUrl(&quot;http://crossoverJie.top&quot;)                .contact(&quot;crossoverJie&quot;)                .version(&quot;1.0.0&quot;)                .build();    &#125;    &#125;\n\n其实就是配置swagger的一些基本信息。之后启动项目，在地址栏输入http://ip:port/swagger-ui.html#/即可进入。可以看到如上图所示的接口列表,点击如下图所示的参数例子即可进行接口调用。\n\n自定义开关Swagger\nswagger的便利能给我们带来很多好处，但稍有不慎也可能出现问题。\n\n比如如果在生产环境还能通过IP访问swagger的话那后果可是不堪设想的。所以我们需要灵活控制swagger的开关。\n这点可以利用spring的条件化配置(条件化配置可以配置存在于应用中,一旦满足一些特定的条件时就取消这些配置)来实现这一功能:\n@ConditionalOnExpression(&quot;&#x27;$&#123;swagger.enable&#125;&#x27; == &#x27;true&#x27;&quot;)\n\n该注解的意思是给定的SpEL表达式计算结果为true时才会创建swagger的bean。\nswagger.enable这个配置则是配置在application.properties中:\n# 是否打开swaggerswagger.enable = true\n\n这样当我们在生产环境时只需要将该配置改为false即可。\nps:更多spring条件化配置:\n@ConditionalOnBean                 //配置了某个特定Bean@ConditionalOnMissingBean          //没有配置特定的Bean@ConditionalOnClass                //Classpath里有指定的类@ConditionalOnMissingClass         //Classpath里缺少指定的类@ConditionalOnExpression           //给定的Spring Expression Language(SpEL)表达式计算结果为true@ConditionalOnJava                 //Java的版本匹配特定值或者一个范围值@ConditionalOnJndi                 //参数中给定的JNDI位置必须存在一个，如果没有给参数，则要有JNDI InitialContext@ConditionalOnProperty             //指定的配置属性要有一个明确的值@ConditionalOnResource             //Classpath里有指定的资源@ConditionalOnWebApplication       //这是一个Web应用程序@ConditionalOnNotWebApplication    //这不是一个Web应用程序(参考SpringBoot实战)\n\n高可用Eureka在上一篇中是用Eureka 来做了服务注册中心，所有的生产者都往它注册服务，消费者又通过它来获取服务。\n但是之前讲到的都是单节点，这在生产环境风险巨大，我们必须做到注册中心的高可用，搭建Eureka 集群。\n这里简单起见就搭建两个Eureka ,思路则是这两个Eureka都把自己当成应用向对方注册，这样就可以构成一个高可用的服务注册中心。\n在实际生产环节中会是每个注册中心一台服务器，为了演示起见，我就在本地启动两个注册中心，但是端口不一样。\n首先需要在本地配置一个host:\n127.0.0.1 node1 node2\n\n这样不论是访问node1 还是node2都可以在本机调用的到(当然不配置host也可以，只是需要通过IP来访问，这样看起来不是那么明显)。\n并给sbc-service新增了两个配置文件:\napplication-node1.properties:\nspring.application.name=sbc-serviceserver.port=8888eureka.instance.hostname=node1## 不向注册中心注册自己#eureka.client.register-with-eureka=false### 不需要检索服务#eureka.client.fetch-registry=falseeureka.client.serviceUrl.defaultZone=http://node2:9999/eureka/\n\napplication-node2.properties:\nspring.application.name=sbc-serviceserver.port=9999eureka.instance.hostname=node2## 不向注册中心注册自己#eureka.client.register-with-eureka=false### 不需要检索服务#eureka.client.fetch-registry=falseeureka.client.serviceUrl.defaultZone=http://node1:8888/eureka/\n\n其中最重要的就是:\neureka.client.serviceUrl.defaultZone=http://node2:9999/eureka/eureka.client.serviceUrl.defaultZone=http://node1:8888/eureka/\n两个应用互相注册。\n启动的时候我们按照:java -jar sbc-service-1.0.0-SNAPSHOT.jar --spring.profiles.active=node1启动，就会按照传入的node1或者是node2去读取application-node1.properties,application-node2.properties这两个配置文件(配置文件必须按照application-&#123;name&#125;.properties的方式命名)。\n分别启动两个注册中心可以看到以下:\n\n\n可以看到两个注册中心以及互相注册了。在服务注册的时候只需要将两个地址都加上即可:eureka.client.serviceUrl.defaultZone=http://node1:8888/eureka/,http://node2:9999/eureka/ \n在服务调用的时候可以尝试关闭其中一个，正常情况下依然是可以调用到服务的。\nFeign声明式调用接下来谈谈服务调用，上次提到可以用ribbon 来进行服务调用，但是明显很不方便，不如像之前rpc调用那样简单直接。\n为此这次使用Feign来进行声明式调用，就像调用一个普通方法那样简单。\norder-client片头说到我将应用分成了三个模块order、order-api、order-client，其中的client模块就是关键。\n来看看其中的内容,只有一个接口:\n@RequestMapping(value=&quot;/orderService&quot;)@FeignClient(name=&quot;sbc-order&quot;)@RibbonClientpublic interface OrderServiceClient extends OrderService&#123;    @ApiOperation(&quot;获取订单号&quot;)    @RequestMapping(value = &quot;/getOrderNo&quot;, method = RequestMethod.POST)    BaseResponse&lt;OrderNoResVO&gt; getOrderNo(@RequestBody OrderNoReqVO orderNoReq) ;&#125;\n@FeignClient这个注解要注意下，其中的name的是自己应用的应用名称，在application.properties中的spring.application.name配置。\n其中继承了一个OrderService在order-api模块中，来看看order-api中的内容。\norder-api其中也只有一个接口:\n@RestController@Api(&quot;订单服务API&quot;)@RequestMapping(value = &quot;/orderService&quot;)@Validatedpublic interface OrderService &#123;    @ApiOperation(&quot;获取订单号&quot;)    @RequestMapping(value = &quot;/getOrderNo&quot;, method = RequestMethod.POST)    BaseResponse&lt;OrderNoResVO&gt; getOrderNo(@RequestBody OrderNoReqVO orderNoReq) ;&#125;\n\n这个接口有两个目的。\n\n给真正的controller来进行实现。\n给client接口进行继承。\n\n类关系如下:\n\n注解这些都没什么好说的，一看就懂。\norderorder则是具体接口实现的模块，就和平时写controller一样。来看看如何使用client进行声明式调用:\n这次看看sbc-user这个项目，在里边调用了sbc-order的服务。其中的user模块依赖了order-client:\n&lt;dependency&gt;    &lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;    &lt;artifactId&gt;order-client&lt;/artifactId&gt;&lt;/dependency&gt;\n\n具体调用:\n@Autowiredprivate OrderServiceClient orderServiceClient ;@Overridepublic BaseResponse&lt;UserResVO&gt; getUserByFeign(@RequestBody UserReqVO userReq) &#123;    //调用远程服务    OrderNoReqVO vo = new OrderNoReqVO() ;    vo.setReqNo(userReq.getReqNo());    BaseResponse&lt;OrderNoResVO&gt; orderNo = orderServiceClient.getOrderNo(vo);    logger.info(&quot;远程返回:&quot;+JSON.toJSONString(orderNo));    UserRes userRes = new UserRes() ;    userRes.setUserId(123);    userRes.setUserName(&quot;张三&quot;);    userRes.setReqNo(userReq.getReqNo());    userRes.setCode(StatusEnum.SUCCESS.getCode());    userRes.setMessage(&quot;成功&quot;);    return userRes ;&#125;\n\n可以看到只需要将order-client包中的Order服务注入进来即可。\n在sbc-client的swagger中进行调用:\n\n\n\n由于我并没传appId所以order服务返回的错误。\n总结\n当一个应用需要对外暴露接口时着需要按照以上方式提供一个client包更消费者使用。\n\n其实应用本身也是需要做高可用的，和Eureka高可用一样，再不同的服务器上再启一个或多个服务并注册到Eureka集群中即可。\n后续还会继续谈到zuul网关，容错，断路器等内容，欢迎拍砖讨论。\n\n项目：https://github.com/crossoverJie/springboot-cloud\n\n\n博客：http://crossoverjie.top。\n\n","categories":["sbc"],"tags":["Java","SpringBoot","SpringCloud","swagger","Eureka"]},{"title":"sbc(三)自定义Starter-SpringBoot重构去重插件","url":"/2017/08/01/sbc3/","content":"\n前言之前看过SSM(十四) 基于annotation的http防重插件的朋友应该记得我后文说过之后要用SpringBoot来进行重构。\n\n这次采用自定义的starter的方式来进行重构。 \n\n关于starter(起步依赖)其实在第一次使用SpringBoot的时候就已经用到了，比如其中的:\n&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;\n我们只需要引入这一个依赖SpringBoot就会把相关的依赖都加入进来，自己也不需要再去担心各个版本之间的兼容问题(具体使用哪个版本由使用的spring-boot-starter-parent版本决定)，这些SpringBoot都已经帮我们做好了。\n\n\n\n\n\nSpring自动化配置先加入需要的一些依赖:\n&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--aop相关--&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--redis相关--&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--配置相关--&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;\t&lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;&lt;!--通用依赖--&gt;&lt;dependency&gt;\t&lt;groupId&gt;com.crossoverJie&lt;/groupId&gt;\t&lt;artifactId&gt;sbc-common&lt;/artifactId&gt;\t&lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;\n\n创建了CheckReqConf配置类用于在应用启动的时候自动配置。当然前提还得在resources目录下创建META-INF/spring.factories配置文件用于指向当前类，才能在应用启动时进行自动配置。\nspring.factories:\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\com.crossoverJie.request.check.conf.CheckReqConf\n\n使用条件化配置试着考虑下如下情况:\n\n因为该插件是使用redis来存储请求信息的，外部就依赖了redis。如果使用了该插件的应用没有配置或者忘了配置redis的一些相关连接，那么在应用使用过程中肯定会出现写入redis异常。\n如果异常没有控制好的话还有可能影响项目的正常运行。\n\n那么怎么解决这个情况呢，可以使用Spring4.0新增的条件化配置来解决。\n解决思路是:可以简单的通过判断应用中是否配置有spring.redis.hostredis连接，如果没有我们的这个配置就会被忽略掉。\n实现代码:\nimport org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Conditional;import org.springframework.context.annotation.Configuration;@Configuration@ComponentScan(&quot;com.crossoverJie.request.check.interceptor,com.crossoverJie.request.check.properties&quot;)//是否有redis配置的校验，如果没有配置则不会加载改配置，也就是当前插件并不会生效@Conditional(CheckReqCondition.class)public class CheckReqConf &#123;&#125;\n\n具体校验的代码CheckReqCondition:\npublic class CheckReqCondition implements Condition &#123;    private static Logger logger = LoggerFactory.getLogger(CheckReqCondition.class);    @Override    public boolean matches(ConditionContext context, AnnotatedTypeMetadata annotatedTypeMetadata) &#123;        //如果没有加入redis配置的就返回false        String property = context.getEnvironment().getProperty(&quot;spring.redis.host&quot;);        if (StringUtils.isEmpty(property))&#123;            logger.warn(&quot;Need to configure redis!&quot;);            return false ;        &#125;else &#123;            return true;        &#125;    &#125;&#125;\n\n只需要实现org.springframework.context.annotation.Condition并重写matches()方法,即可实现个人逻辑。\n\n可以在使用了该依赖的配置文件中配置或者是不配置spring.redis.host这个配置,来看我们的切面类(ReqNoDrcAspect)中53行的日志是否有打印来判断是否生效。\n\n这样只有在存在该key的情况下才会应用这个配置。\n\n当然最好的做法是直接尝试读、写redis,看是否连接畅通来进行判断。\n\nAOP切面最核心的其实就是这个切面类，里边主要逻辑和之前是一模一样的就不在多说,只是这里应用到了自定义配置。\n切面类ReqNoDrcAspect:\n//切面注解@Aspect//扫描@Component//开启cglib代理@EnableAspectJAutoProxy(proxyTargetClass = true)public class ReqNoDrcAspect &#123;    private static Logger logger = LoggerFactory.getLogger(ReqNoDrcAspect.class);    @Autowired    private CheckReqProperties properties ;    private String prefixReq ;    private long day ;    @Autowired    private RedisTemplate&lt;String, String&gt; redisTemplate;    @PostConstruct    public void init() throws Exception &#123;        prefixReq = properties.getRedisKey() == null ? &quot;reqNo&quot; : properties.getRedisKey() ;        day = properties.getRedisTimeout() == null ? 1L : properties.getRedisTimeout() ;        logger.info(&quot;sbc-request-check init......&quot;);        logger.info(String.format(&quot;redis prefix is [%s],timeout is [%s]&quot;, prefixReq, day));    &#125;    /**     * 切面该注解     */    @Pointcut(&quot;@annotation(com.crossoverJie.request.check.anotation.CheckReqNo)&quot;)    public void checkRepeat()&#123;    &#125;    @Before(&quot;checkRepeat()&quot;)    public void before(JoinPoint joinPoint) throws Exception &#123;        BaseRequest request = getBaseRequest(joinPoint);        if(request != null)&#123;            final String reqNo = request.getReqNo();            if(StringUtil.isEmpty(reqNo))&#123;                throw new SBCException(StatusEnum.REPEAT_REQUEST);            &#125;else&#123;                try &#123;                    String tempReqNo = redisTemplate.opsForValue().get(prefixReq +reqNo);                    logger.debug(&quot;tempReqNo=&quot; + tempReqNo);                    if((StringUtil.isEmpty(tempReqNo)))&#123;                        redisTemplate.opsForValue().set(prefixReq + reqNo, reqNo, day, TimeUnit.DAYS);                    &#125;else&#123;                        throw new SBCException(&quot;请求号重复,&quot;+ prefixReq +&quot;=&quot; + reqNo);                    &#125;                &#125; catch (RedisConnectionFailureException e)&#123;                    logger.error(&quot;redis操作异常&quot;,e);                    throw new SBCException(&quot;need redisService&quot;) ;                &#125;            &#125;        &#125;    &#125;    public static BaseRequest getBaseRequest(JoinPoint joinPoint) throws Exception &#123;        BaseRequest returnRequest = null;        Object[] arguments = joinPoint.getArgs();        if(arguments != null &amp;&amp; arguments.length &gt; 0)&#123;            returnRequest = (BaseRequest) arguments[0];        &#125;        return returnRequest;    &#125;&#125;\n\n这里我们的写入rediskey的前缀和过期时间改为从CheckReqProperties类中读取:\n@Component//定义配置前缀@ConfigurationProperties(prefix = &quot;sbc.request.check&quot;)public class CheckReqProperties &#123;    private String redisKey ;//写入redis中的前缀    private Long redisTimeout ;//redis的过期时间 默认是天    public String getRedisKey() &#123;        return redisKey;    &#125;    public void setRedisKey(String redisKey) &#123;        this.redisKey = redisKey;    &#125;    public Long getRedisTimeout() &#123;        return redisTimeout;    &#125;    public void setRedisTimeout(Long redisTimeout) &#123;        this.redisTimeout = redisTimeout;    &#125;    @Override    public String toString() &#123;        return &quot;CheckReqProperties&#123;&quot; +                &quot;redisKey=&#x27;&quot; + redisKey + &#x27;\\&#x27;&#x27; +                &quot;, redisTimeout=&quot; + redisTimeout +                &#x27;&#125;&#x27;;    &#125;&#125;\n\n这样如果是需要很多配置的情况下就可以将内容封装到该对象中，方便维护和读取。\n使用的时候只需要在自己应用的application.properties中加入\n# 去重配置sbc.request.check.redis-key = reqsbc.request.check.redis-timeout= 2\n\n应用插件使用方法也和之前差不多(在sbc-order应用)：\n\n加入依赖：\n\n&lt;!--防重插件--&gt;&lt;dependency&gt;    &lt;groupId&gt;com.crossoverJie.request.check&lt;/groupId&gt;    &lt;artifactId&gt;request-check&lt;/artifactId&gt;    &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt;\n\n\n在接口上加上注解:\n\n@RestController@Api(value = &quot;orderApi&quot;, description = &quot;订单API&quot;, tags = &#123;&quot;订单服务&quot;&#125;)public class OrderController implements OrderService&#123;    private final static Logger logger = LoggerFactory.getLogger(OrderController.class);    @Override    @CheckReqNo    public BaseResponse&lt;OrderNoResVO&gt; getOrderNo(@RequestBody OrderNoReqVO orderNoReq) &#123;        BaseResponse&lt;OrderNoResVO&gt; res = new BaseResponse();        res.setReqNo(orderNoReq.getReqNo());        if (null == orderNoReq.getAppId())&#123;            throw new SBCException(StatusEnum.FAIL);        &#125;        OrderNoResVO orderNoRes = new OrderNoResVO() ;        orderNoRes.setOrderId(DateUtil.getLongTime());        res.setCode(StatusEnum.SUCCESS.getCode());        res.setMessage(StatusEnum.SUCCESS.getMessage());        res.setDataBody(orderNoRes);        return res ;    &#125;&#125;\n\n使用效果如下:\n\n总结注意一点是spring.factories的路径不要搞错了,之前就是因为路径写错了，导致自动配置没有加载，AOP也就没有生效，排查了好久。。\n\n项目：https://github.com/crossoverJie/springboot-cloud\n\n\n博客：http://crossoverjie.top。\n\n","categories":["sbc"],"tags":["Java","重构","AOP","SpringBoot","SpringCloud"]},{"title":"sbc(四)应用限流","url":"/2017/08/11/sbc4/","content":"\n前言\n在一个高并发系统中对流量的把控是非常重要的，当巨大的流量直接请求到我们的服务器上没多久就可能造成接口不可用，不处理的话甚至会造成整个应用不可用。\n\n比如最近就有个这样的需求，我作为客户端要向kafka生产数据，而kafka的消费者则再源源不断的消费数据，并将消费的数据全部请求到web服务器，虽说做了负载(有4台web服务器)但业务数据的量也是巨大的，每秒钟可能有上万条数据产生。如果生产者直接生产数据的话极有可能把web服务器拖垮。\n对此就必须要做限流处理，每秒钟生产一定限额的数据到kafka，这样就能极大程度的保证web的正常运转。\n其实不管处理何种场景，本质都是降低流量保证应用的高可用。\n常见算法对于限流常见有两种算法:\n\n漏桶算法\n令牌桶算法\n\n漏桶算法比较简单，就是将流量放入桶中，漏桶同时也按照一定的速率流出，如果流量过快的话就会溢出(漏桶并不会提高流出速率)。溢出的流量则直接丢弃。\n如下图所示:\n\n\n\n这种做法简单粗暴。\n漏桶算法虽说简单，但却不能应对实际场景，比如突然暴增的流量。\n这时就需要用到令牌桶算法:\n令牌桶会以一个恒定的速率向固定容量大小桶中放入令牌，当有流量来时则取走一个或多个令牌。当桶中没有令牌则将当前请求丢弃或阻塞。\n\n\n相比之下令牌桶可以应对一定的突发流量.\n\nRateLimiter实现对于令牌桶的代码实现，可以直接使用Guava包中的RateLimiter。\n@Overridepublic BaseResponse&lt;UserResVO&gt; getUserByFeignBatch(@RequestBody UserReqVO userReqVO) &#123;    //调用远程服务    OrderNoReqVO vo = new OrderNoReqVO() ;    vo.setReqNo(userReqVO.getReqNo());    RateLimiter limiter = RateLimiter.create(2.0) ;    //批量调用    for (int i = 0 ;i&lt; 10 ; i++)&#123;        double acquire = limiter.acquire();        logger.debug(&quot;获取令牌成功!,消耗=&quot; + acquire);        BaseResponse&lt;OrderNoResVO&gt; orderNo = orderServiceClient.getOrderNo(vo);        logger.debug(&quot;远程返回:&quot;+JSON.toJSONString(orderNo));    &#125;    UserRes userRes = new UserRes() ;    userRes.setUserId(123);    userRes.setUserName(&quot;张三&quot;);    userRes.setReqNo(userReqVO.getReqNo());    userRes.setCode(StatusEnum.SUCCESS.getCode());    userRes.setMessage(&quot;成功&quot;);    return userRes ;&#125;\n\n详见此。\n调用结果如下:\n\n代码可以看出以每秒向桶中放入两个令牌，请求一次消耗一个令牌。所以每秒钟只能发送两个请求。按照图中的时间来看也确实如此(返回值是获取此令牌所消耗的时间，差不多也是每500ms一个)。\n使用RateLimiter 有几个值得注意的地方:\n允许先消费，后付款，意思就是它可以来一个请求的时候一次性取走几个或者是剩下所有的令牌甚至多取，但是后面的请求就得为上一次请求买单，它需要等待桶中的令牌补齐之后才能继续获取令牌。\n总结针对于单个应用的限流 RateLimiter 够用了，如果是分布式环境可以借助 Redis 来完成。\n最近也怼了一个，可以参考。\n\n项目：https://github.com/crossoverJie/springboot-cloud\n\n\n博客：http://crossoverjie.top。\n\n","categories":["sbc"],"tags":["Java","SpringBoot","SpringCloud","RateLimiter"]},{"title":"sbc(五)Hystrix-服务容错与保护","url":"/2017/09/20/sbc5/","content":"\n前言看过 应用限流的朋友应该知道，限流的根本目的就是为了保障服务的高可用。\n本次再借助SpringCloud中的集成的Hystrix组件来谈谈服务容错。\n其实产生某项需求的原因都是为了解决某个需求。当我们将应用进行分布式模块部署之后,各个模块之间通过远程调用的方式进行交互(RPC)。拿我们平时最常见的下单买商品来说，点击下单按钮的一瞬间可能会向发送的请求包含：\n\n请求订单系统创建订单。\n请求库存系统扣除库存。\n请求用户系统更新用户交易记录。\n\n这其中的每一步都有可能因为网络、资源、服务器等原因造成延迟响应甚至是调用失败。当后面的请求源源不断的过来时延迟的资源也没有的到释放，这样的堆积很有可能把其中一个模块拖垮，其中的依赖关系又有可能把整个调用链中的应用Over最后导致整个系统不可能。这样就会产生一种现象:雪崩效应。\n之前讲到的限流也能起到一定的保护作用，但还远远不够。我们需要从各个方面来保障服务的高可用。\n比如：\n\n超时重试。\n断路器模式。\n服务降级。等各个方面来保障。\n\n\n\n使用HystrixSpringCloud中已经为我们集成了Netflix开源的Hystrix框架，使用该框架可以很好的帮我们做到服务容错。\nHystrix简介下面是一张官方的流程图:\n\n简单介绍下:\n\n在远程调用时，将请求封装到HystrixCommand进行同步或是异步调用，在调用过程中判断熔断器是否打开、线程池或是信号量是否饱和、执行过程中是否抛出异常，如果是的话就会进入回退逻辑。并且整个过程中都会收集运行状态来控制断路器的状态。\n\n不但如此该框架还拥有自我恢复功能，当断路器打开后，每次请求都会进入回退逻辑。当我们的应用恢复正常后也不能再进入回退逻辑吧。\n所以hystrix会在断路器打开后的一定时间将请求发送到服务提供者，如果正常响应就关闭断路器，反之则继续打开，这样就能很灵活的自我修复了。\nFeign整合Hystrix在之前的章节中已经使用Feign来进行声明式调用了，并且在实际开发中也是如此，所以这次我们就直接用Feign来整合Hystrix。\n使用了项目原有的sbc-user,sbc-order来进行演示，调用关系如下图:\n\nUser应用通过Order提供出来的order-client依赖调用了Order中的创建订单服务。\n其中主要修改的就是order-client，在之前的OrderServiceClient接口中增加了以下注解:\n@RequestMapping(value=&quot;/orderService&quot;)@FeignClient(name=&quot;sbc-order&quot;,        // fallbackFactory = OrderServiceFallbackFactory.class,        // FIXME: 2017/9/4 如果配置了fallback 那么fallbackFactory将会无效        fallback = OrderServiceFallBack.class,        configuration = OrderConfig.class)@RibbonClientpublic interface OrderServiceClient extends OrderService&#123;    @ApiOperation(&quot;获取订单号&quot;)    @RequestMapping(value = &quot;/getOrderNo&quot;, method = RequestMethod.POST)    BaseResponse&lt;OrderNoResVO&gt; getOrderNo(@RequestBody OrderNoReqVO orderNoReq) ;&#125;\n由于Feign已经默认整合了Hystrix所以不需要再额外加入依赖。\n服务降级对应的@FeignClient中的fallback属性则是服务容错中很关键的服务降级的具体实现，来看看OrderServiceFallBack类:\npublic class OrderServiceFallBack implements OrderServiceClient &#123;    @Override    public BaseResponse&lt;OrderNoResVO&gt; getOrderNo(@RequestBody OrderNoReqVO orderNoReq) &#123;        BaseResponse&lt;OrderNoResVO&gt; baseResponse = new BaseResponse&lt;&gt;() ;        OrderNoResVO vo = new OrderNoResVO() ;        vo.setOrderId(123456L);        baseResponse.setDataBody(vo);        baseResponse.setMessage(StatusEnum.FALLBACK.getMessage());        baseResponse.setCode(StatusEnum.FALLBACK.getCode());        return baseResponse;    &#125;&#125;\n该类实现了OrderServiceClient接口，可以很明显的看出其中的getOrderNo()方法就是服务降级时所触发的逻辑。\n光有实现还不够，我们需要将改类加入到Spring中管理起来。这样上文中@FeignClient的configuration属性就起到作用了，来看看对应的OrderConfig的代码:\n@Configurationpublic class OrderConfig &#123;    @Bean    public OrderServiceFallBack fallBack()&#123;        return new OrderServiceFallBack();    &#125;    @Bean    public OrderServiceFallbackFactory factory()&#123;        return new OrderServiceFallbackFactory();    &#125;&#125;\n其中new OrderServiceFallBack()并用了@Bean注解，等同于:\n&lt;bean id=&quot;orderServiceFallBack&quot; class=&quot;com.crossoverJie.order.feign.config.OrderServiceFallBack&quot;&gt;&lt;/bean&gt;\n\n这样每当请求失败就会执行回退逻辑，如下图:\n值得注意的是即便是执行了回退逻辑断路器也不一定打开了，我们可以通过应用的health端点来查看Hystrix的状态。\nps:想要查看该端点需要加入以下依赖:\n&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt;\n就拿刚才的例子来说，先关闭Order应用，在Swagger访问下面这个接口，肯定是会进入回退逻辑:\n@RestController@Api(&quot;用户服务API&quot;)@RequestMapping(value = &quot;/userService&quot;)@Validatedpublic interface UserService &#123;    @ApiOperation(&quot;hystrix容错调用&quot;)    @RequestMapping(value = &quot;/getUserByHystrix&quot;, method = RequestMethod.POST)    BaseResponse&lt;OrderNoResVO&gt; getUserByHystrix(@RequestBody UserReqVO userReqVO) ;&#125;\n\n查看health端点:\n发现Hystrix的状态依然是UP状态，表明当前断路器并没有打开。\n反复调用多次接口之后再次查看health端点:\n发现这个时候断路器已经打开了。\n\n这是因为断路器只有在达到了一定的失败阈值之后才会打开。\n\n输出异常进入回退逻辑之后还不算完，大部分场景我们都需要记录为什么回退，也就是具体的异常。这些信息对我们后续的系统监控，应用调优也有很大帮助。\n实现起来也很简单:上文中在@FeignClient注解中加入的fallbackFactory = OrderServiceFallbackFactory.class属性则是用于处理回退逻辑以及包含异常信息：\n/** * Function:查看fallback原因 * * @author crossoverJie *         Date: 2017/9/4 00:45 * @since JDK 1.8 */public class OrderServiceFallbackFactory implements FallbackFactory&lt;OrderServiceClient&gt;&#123;    private final static Logger LOGGER = LoggerFactory.getLogger(OrderServiceFallbackFactory.class);    @Override    public OrderServiceClient create(Throwable throwable) &#123;        return new OrderServiceClient() &#123;            @Override            public BaseResponse&lt;OrderNoResVO&gt; getOrderNo(@RequestBody OrderNoReqVO orderNoReq) &#123;                LOGGER.error(&quot;fallback:&quot; + throwable);                BaseResponse&lt;OrderNoResVO&gt; baseResponse = new BaseResponse&lt;&gt;() ;                OrderNoResVO vo = new OrderNoResVO() ;                vo.setOrderId(123456L);                baseResponse.setDataBody(vo);                baseResponse.setMessage(StatusEnum.FALLBACK.getMessage());                baseResponse.setCode(StatusEnum.FALLBACK.getCode());                return baseResponse;            &#125;        &#125;;    &#125;&#125;\n\n代码很简单，实现了FallbackFactory接口中的create()方法，该方法的入参就是异常信息，可以按照我们的需要自行处理，后面则是和之前一样的回退处理。\n2017-09-21 13:22:30.307 ERROR 27838 --- [rix-sbc-order-1] c.c.o.f.f.OrderServiceFallbackFactory    : fallback:java.lang.RuntimeException: com.netflix.client.ClientException: Load balancer does not have available server for client: sbc-order 。\nNote:\nfallbackFactory和fallback属性不可共用。\nHystrix监控Hystrix还自带了一套监控组件，只要依赖了spring-boot-starter-actuator即可通过/hystrix.stream端点来获得监控信息。\n冰冷的数据肯定没有实时的图表来的直观，所以Hystrix也自带Dashboard。\nHystrix与Turbine聚合监控为此我们新建了一个应用sbc-hystrix-turbine来显示hystrix-dashboard。目录结构和普通的springboot应用没有差异，看看主类:\n//开启EnableTurbine@EnableTurbine@SpringBootApplication@EnableHystrixDashboardpublic class SbcHystrixTurbineApplication &#123;\tpublic static void main(String[] args) &#123;\t\tSpringApplication.run(SbcHystrixTurbineApplication.class, args);\t&#125;&#125;\n\n\n其中使用@EnableHystrixDashboard开启Dashboard\n@EnableTurbine开启Turbine支持。\n\n以上这些注解需要以下这些依赖:\n&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-turbine&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-netflix-turbine&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt;\n\n\n实际项目中，我们的应用都是多节点部署以达到高可用的目的，单个监控显然不现实，所以需要使用Turbine来进行聚合监控。\n\n关键的application.properties配置文件:\n# 项目配置spring.application.name=sbc-hystrix-trubineserver.context-path=/server.port=8282# eureka地址eureka.client.serviceUrl.defaultZone=http://node1:8888/eureka/eureka.instance.prefer-ip-address=true# 需要加入的实例turbine.appConfig=sbc-user,sbc-orderturbine.cluster-name-expression=&quot;default&quot;\n其中turbine.appConfig配置我们需要监控的应用，这样当多节点部署的时候就非常方便了(同一个应用的多个节点spring.application.name值是相同的)。\n将该应用启动访问http://ip:port/hystrix.stream：\n\n由于我们的turbine和Dashboard是一个应用所以输入http://localhost:8282/turbine.stream即可。\n\n详细指标如官方描述:\n通过该面板我们就可以及时的了解到应用当前的各个状态，如果再加上一些报警措施就能帮我们及时的响应生产问题。\n总结服务容错的整个还是比较大的,博主也是摸着石头过河，关于本次的Hystrix只是一个入门版，后面会持续分析它的线程隔离、信号量隔离等原理。\n\n项目：https://github.com/crossoverJie/springboot-cloud\n\n\n博客：http://crossoverjie.top。\n\n","categories":["sbc"],"tags":["Java","SpringBoot","SpringCloud","Hystrix"]},{"title":"sbc(六) Zuul GateWay 网关应用","url":"/2017/11/28/sbc6/","content":"\n前言看过之前SBC系列的小伙伴应该都可以搭建一个高可用、分布式的微服务了。 目前的结构图应该如下所示:\n各个微服务之间都不存在单点，并且都注册于 Eureka ，基于此进行服务的注册于发现，再通过 Ribbon 进行服务调用，并具有客户端负载功能。\n一切看起来都比较美好，但这里却忘了一个重要的细节：\n\n当我们需要对外提供服务时怎么处理？\n\n这当然也能实现，无非就是将我们具体的微服务地址加端口暴露出去即可。\n那又如何来实现负载呢？\n简单！可以通过 Nginx F5 之类的工具进行负载。\n但是如果系统庞大，服务拆分的足够多那又有谁来维护这些路由关系呢？\n当然这是运维的活，不过这时候运维可能就要发飙了！\n并且还有一系列的问题:\n\n服务调用之间的一些鉴权、签名校验怎么做？\n由于服务端地址较多，客户端请求难以维护。\n\n针对于这一些问题 SpringCloud 全家桶自然也有对应的解决方案: Zuul。当我们系统整合 Zuul 网关之后架构图应该如下所示:\n\n\n\n我们在所有的请求进来之前抽出一层网关应用，将服务提供的所有细节都进行了包装，这样所有的客户端都是和网关进行交互，简化了客户端开发。\n同时具有如下功能:\n\nZuul 注册于 Eureka 并集成了 Ribbon 所以自然也是可以从注册中心获取到服务列表进行客户端负载。\n功能丰富的路由功能，解放运维。\n具有过滤器，所以鉴权、验签都可以集成。\n\n基于此我们来看看之前的架构中如何集成 Zuul 。\n集成 Zuul为此我新建了一个项目 sbc-gateway-zuul 就是一个基础的 SpringBoot 结构。其中加入了 Zuul 的依赖：\n&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt;&lt;/dependency&gt;\n\n由于需要将网关也注册到 Eureka 中，所以自然也需要:\n&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt;&lt;/dependency&gt;\n\n紧接着配置一些项目基本信息:\n# 项目配置spring.application.name=sbc-gateway-zuulserver.context-path=/server.port=8383# eureka地址eureka.client.serviceUrl.defaultZone=http://node1:8888/eureka/eureka.instance.prefer-ip-address=true\n\n在启动类中加入开启 Zuul 的注解，一个网关应用就算是搭好了。\n@SpringBootApplication//开启zuul代理@EnableZuulProxypublic class SbcGateWayZuulApplication &#123;&#125;\n\n启动 Eureka 和网关看到已经注册成功那就大功告成了:\n\n路由路由是网关的核心功能之一，可以使系统有一个统一的对外接口，下面来看看具体的应用。\n传统路由传统路由非常简单，和 Nginx 类似，由开发、运维人员来维护请求地址和对应服务的映射关系，类似于:\nzuul.routes.user-service.path=/user-service/**zuul.routes.user-sercice.url=http://localhost:8080/\n\n这样当我们访问 http://localhost:8383/user-service/getUserInfo/1 网关就会自动给我们路由到 http://localhost:8080/getUserInfo/1 上。\n可见只要我们维护好这个映射关系即可自由的配置路由信息(user-sercice 可自定义)，但是很明显这种方式不管是对运维还是开发都不友好。由于实际这种方式用的不多就再过多展开。\n服务路由对此 Zuul 提供了一种基于服务的路由方式。我们只需要维护请求地址与服务 ID 之间的映射关系即可，并且由于集成了 Ribbon , Zuul 还可以在路由的时候通过 Eureka 实现负载调用。\n具体配置：\nzuul.routes.sbc-user.path=/api/user/**zuul.routes.sbc-user.serviceId=sbc-user\n\n这样当输入 http://localhost:8383/api/user/getUserInfo/1 时就会路由到注册到 Eureka 中服务 ID 为 sbc-user 的服务节点，如果有多节点就会按照 Ribbon 的负载算法路由到其中一台上。\n以上配置还可以简写为:\n# 服务路由 简化配置zuul.routes.sbc-user=/api/user/**\n\n这样让我们访问 http://127.0.0.1:8383/api/user/userService/getUserByHystrix 时候就会根据负载算法帮我们路由到 sbc-user 应用上，如下图所示:\n启动了两个 sbc-user 服务。\n请求结果:\n一次路由就算完成了。\n在上面的配置中有看到 /api/user/** 这样的通配符配置，具体有以下三种配置需要了解:\n\n? 只能匹配任意的单个字符，如 /api/user/? 就只能匹配 /api/user/x  /api/user/y /api/user/z 这样的路径。\n* 只能匹配任意字符，如 /api/user/* 就只能匹配 /api/user/x /api/user/xy /api/user/xyz。\n** 可以匹配任意字符、任意层级。结合了以上两种通配符的特点，如 /api/user/** 则可以匹配 /api/user/x /api/user/x/y /api/user/x/y/zzz 这样的路径，最简单粗暴！\n\n谈到通配符匹配就不得不提到一个问题，如上面的 sbc-user 服务由于后期迭代更新，将 sbc-user 中的一部分逻辑抽成了另一个服务 sbc-user-pro。新应用的路由规则是 /api/user/pro/**,如果我们按照:\nzuul.routes.sbc-user=/api/user/**zuul.routes.sbc-user-pro=/api/user/pro/**\n\n进行配置的话，我们想通过 /api/user/pro/ 来访问 sbc-user-pro 应用，却由于满足第一个路由规则，所以会被 Zuul 路由到 sbc-user 这个应用上，这显然是不对的。该怎么解决这个问题呢？\n翻看路由源码 org.springframework.cloud.netflix.zuul.filters.SimpleRouteLocator 中的 locateRoutes() 方法:\n/** * Compute a map of path pattern to route. The default is just a static map from the * &#123;@link ZuulProperties&#125;, but subclasses can add dynamic calculations. */protected Map&lt;String, ZuulRoute&gt; locateRoutes() &#123;\tLinkedHashMap&lt;String, ZuulRoute&gt; routesMap = new LinkedHashMap&lt;String, ZuulRoute&gt;();\tfor (ZuulRoute route : this.properties.getRoutes().values()) &#123;\t\troutesMap.put(route.getPath(), route);\t&#125;\treturn routesMap;&#125;\n\n发现路由规则是遍历配置文件并放入 LinkedHashMap 中，由于 LinkedHashMap 是有序的，所以为了达到上文的效果，配置文件的加载顺序非常重要，因此我们只需要将优先匹配的路由规则放前即可解决。\n过滤器过滤器可以说是整个 Zuul 最核心的功能，包括上文提到路由功能也是由过滤器来实现的。\n摘抄官方的解释: Zuul 的核心就是一系列的过滤器，他能够在整个 HTTP 请求、响应过程中执行各样的操作。\n其实总结下来就是四个特征:\n\n过滤类型\n过滤顺序\n执行条件\n具体实现\n\n其实就是 ZuulFilter 接口中所定义的四个接口:\nString filterType();int filterOrder();boolean shouldFilter();Object run();\n\n官方流程图(生命周期):\n\n简单理解下就是:\n当一个请求进来时，首先是进入 pre 过滤器，可以做一些鉴权，记录调试日志等操作。之后进入 routing 过滤器进行路由转发，转发可以使用 Apache HttpClient 或者是 Ribbon 。post 过滤器呢则是处理服务响应之后的数据，可以进行一些包装来返回客户端。 error 则是在有异常发生时才会调用，相当于是全局异常拦截器。\n自定义过滤器接下来实现一个文初所提到的鉴权操作:\n新建一个 RequestFilter 类继承与 ZuulFilter 接口\n/** * Function: 请求拦截 * * @author crossoverJie *         Date: 2017/11/20 00:33 * @since JDK 1.8 */public class RequestFilter extends ZuulFilter &#123;    private Logger logger = LoggerFactory.getLogger(RequestFilter.class) ;    /**     * 请求路由之前被拦截 实现 pre 拦截器     * @return     */    @Override    public String filterType() &#123;        return &quot;pre&quot;;    &#125;    @Override    public int filterOrder() &#123;        return 0;    &#125;    @Override    public boolean shouldFilter() &#123;        return true;    &#125;    @Override    public Object run() &#123;        RequestContext currentContext = RequestContext.getCurrentContext();        HttpServletRequest request = currentContext.getRequest();        String token = request.getParameter(&quot;token&quot;);        if (StringUtil.isEmpty(token))&#123;            logger.warn(&quot;need token&quot;);            //过滤请求            currentContext.setSendZuulResponse(false);            currentContext.setResponseStatusCode(400);            return null ;        &#125;        logger.info(&quot;token =&#123;&#125;&quot;,token) ;        return null;    &#125;&#125;\n\n非常 easy，就简单校验下请求中是否包含 token，不包含就返回 401 code。\n不但如此，还需要将该类加入到 Spring 进行管理:\n新建了 FilterConf 类:\n@Configuration@Componentpublic class FilterConf &#123;    @Bean    public RequestFilter filter()&#123;        return  new RequestFilter() ;    &#125;&#125;\n\n这样重启之后就可以看到效果了:\n不传 token 时：\n\n传入 token 时：\n可见一些鉴权操作是可以放到这里来进行统一处理的。\n其余几个过滤器也是大同小异，可以根据实际场景来自定义。\nZuul 高可用Zuul 现在既然作为了对外的第一入口，那肯定不能是单节点，对于 Zuul 的高可用有以下两种方式实现。\nEureka 高可用第一种最容易想到和实现:我们可以部署多个 Zuul 节点，并且都注册于 Eureka ，如下图：\n\n这样虽然简单易维护，但是有一个严重的缺点：那就是客户端也得注册到 Eureka 上才能对 Zuul 的调用做到负载，这显然是不现实的。\n所以下面这种做法更为常见。\n基于 Nginx 高可用在调用 Zuul 之前使用 Nginx 之类的负载均衡工具进行负载，这样 Zuul 既能注册到 Eureka ，客户端也能实现对 Zuul 的负载，如下图：\n\n总结这样在原有的微服务架构的基础上加上网关之后另整个系统更加完善了，从网关的设计来看：大多数系统架构都有分层的概念，不能解决问题那就多分几层🤓。\n\n项目：https://github.com/crossoverJie/springboot-cloud\n\n\n博客：http://crossoverjie.top。\n\n","categories":["sbc"],"tags":["Java","SpringBoot","SpringCloud","Zuul"]},{"title":"【译】你可以用GitHub做的12件 Cool 事情","url":"/2017/11/05/translation1-12%20cool%20things%20you%20can%20do%20with%20GitHub/","content":"\n原文链接1 在 GitHub.com 编辑代码我将从我认为大家都知道的一件事情开始(尽管我是直到一周前才知道)。\n当你在 GitHub 查看文件时(任何文本文件，任何仓库中)，右上角会有一个小铅笔图标，点击它就可以编辑文件了。完成之后点击 Propose file change 按钮 GitHub 将会自动帮你 fork 该项目并且创建一个 pull request 。\n很厉害吧！他自动帮你 fork 了该 repo。\n不再需要 fork , pull ,本地编辑再 push 以及创建一个 PR 这样的流程了。\n这非常适合修复编写代码中出现的拼写错误和修正一个不太理想的想法。\n2 粘贴图片你不仅仅受限于输入文本和描述问题，你知道你可以直接从粘贴板中粘贴图片吗？当你粘贴时，你会看到图片已经被上传了(毫无疑问被上传到云端)之后会变成 Markdown 语法来显示图片。\n3 格式化代码如果你想写一段代码，你可以三个反引号开始 —— 就像你在研究MarkDown时所学到的 —— 之后 GitHub 会试着猜测你写的语言。\n但如果你写了一些类似于 Vue, Typescript, JSX 这样的语言，你可以明确指定得到正确的高亮。\n注意第一行中的\n```jsx\n\n\n\n\n这意味着代码段将会呈现出:\n\n(这个扩展于 gists 。顺便说一句，如果你使用 .jsx 后缀，就会得到JSX的语法高亮)\n这是一个所有受支持的语法列表。\n4 在 PR 中用关键词关闭 Issues假设你创建了一个用于修复 Issues #234 的 PR ,你可以在你 PR 的描述中填写 fixes #234 (或是在你 PR 任意评论中填写都是可以的)。之后合并这个 PR 时将会自动关闭填写的 Issues。怎么样,很 cool 吧。\n了解是更多相关的内容。\n5 链接到评论你是否有过想要链接到特殊 comment  的想法但却无法实现？那是因为你不知道怎么做。朋友那都是过去式了，现在我就告诉你，点击用户名旁边的日期&#x2F;时间即可链接到该 comment  。\n\n6 链接到代码我知道你想链接到具体的代码行上。\n尝试:查看文件时，点击代码旁边的行号。\n看到了吧，浏览器的 URL 已经被更新为行号了。如果你按住 shift,同时点击其他行号，URL 再次被更新，并且你也高亮显示页面中的一段代码。\n分享这个 URL ，访问时将会链接到该文件已经选中的那些代码段。\n但等一下，那指向的是当前的分支，如果文件发生了改变呢？也许一个在当前状态连接到文件的永久连接正是你想要的。\n我很懒，所以用一张截图展示以上的所有操作。\n\n谈到网址。。。\n7 像命令行一样使用 GitHub 链接使用 GitHub 自带的 UI 浏览也还不错，但有时直接在 URL 中输入是最快的方法。比如，我想跳转到我正在编辑的分支并和 master 进行对比，就可以在项目名称后面接上 /compare/branch-name 。\n与选中分支的对比页将会显示出来:\n以上就是和 master 分支的差异，如果想要合并分支的话，只需要输入 /compare/integration-branch...my-branch  即可。\n\n你还可以利用快捷键达到同样的效果，使用 ctrl + L 或者 cmd + L 可以将光标移动到 URL 上(至少在 Chrome 中可以)。 加上浏览器的自动补全 —— 你就可以在两个分支之间轻松切换了。\n8 在Issues创建列表你想在你的 issue 中看到复选框列表吗?\n\n你想在查看 issue 列表是它们以好看的 2 of 5 进度条呈现吗？\n\n太好了！你可以用以下语法来创建一个交互性的复选框:\n- [ ] Screen width (integer)- [x] Service worker support- [x] Fetch support- [ ] CSS flexbox support- [ ] Custom elements\n\n是由一个空格、中横线、空格、左括号、空格(或者是 X )、右括号、空格以及一些文本组成。\n你甚至可以真正的 选中&#x2F;取消 这些复选框！基于某些原因，对于我来说你看起来像是技术魔力。是真的能够选中这些复选框！甚至它还更新了底层源码。\n\nps：以下包括第九点 基于GitHub的项目面板 由于用的不多就没有翻译。\n\n10 GitHub wiki作为一个像维基百科那样的非结构化的页面集合， GitHub Wiki的供给(我把它称之为 Gwiki ) 是一个非常棒的功能。\n对于结构化的页面来说 —— 例如你的文档：不能说这个页面是其他页面的子页面，或则是有 “下一节”，“上一节” 这样的便捷按钮。并且 Hansel 和 Gretel 也没有，因为结构化页面并没有 breadcrumbs 这样的设计。\n我们继续，让 Gwiki 动起来，我从 NodeJS 的文档中复制了几页来作为 wiki 页面。然后创建了一个自定义侧边栏，帮助我更好地模拟一些实际的目录结构。尽管它不会突出显示你当前的页面位置，但侧边栏会一直存在。\n这些链接需要你手动维护，但总的来说，我认为它可以做得很好。 如果需要的话可以看看。\n\n虽然它与 GitBook ( Redux 文档所使用的)或者是定制网站相比仍有差距。但在你的 repo 中它有 80% 完全值得信赖的。\n我的建议是: 如果你已经有多个 README.md 文件，并且想要一些关于用户指南或更详细的文档的不同的页面，那么你应该选择 Gwiki。\n如果缺乏结构化&#x2F;导航开始让你不爽的话，那就试试其他的吧。\n11 GitHub Pages你可能已经知道使用 GitHub Pages 来托管一个静态网站。如果你不知道，现在就来学习，这一节是专门用于讨论使用 Jekyll 来构建一个站点的。\n最简单的就是： GitHub Pages + Jekyll 会通过一个漂亮的主题来渲染你的 README.md 文件。例如:通过 about-github  来查看的我的 README 页面。\n\n如果我在 GitHub 中点击了 settings选项，切换到 Github Pages 设置，然后选择一个 Jekyll theme。。。\n\n我就可以得到 Jekyll-themed 页面。\n\n从这点上我可以主要依据易编辑的 Markdown 文件来构建一个完整的静态站点。本质上是把 GitHub 变成了 CMS。\n虽然我没有实际使用过，但是 React Bootstrap 的网站都是使用它来构建的。所以它不会糟糕。\n注意:它要求 Ruby 运行本地环境( Windows 自行安装， macOS 自带)。\n12 把 GitHub 当做 CRM 使用假设你有一个存有一些文本内容的网站，你不想将文本内容存储于真正的 HTML 源码中。\n相反的，你想要将这些文本块存储于非开发人员能轻松的进行编辑的地方。可能是一个版本控制系统，甚至是一个审核流程。\n我的建议是:使用 GitHub 厂库中的 Markdown 文件来存储这些文本内容，然后使用前端组件来拉取这些文本块并展示在页面上。\n我是搞 React 的，所以这有一个 解析 Markdown 的组件例子，给定一些 Markdown 文件路径，它将会自动拉取并作为 HTML 显示出来。\nclass Markdown extends React.Component &#123;    constructor(props) &#123;      super(props);            // replace with your URL, obviously      this.baseUrl = &#x27;https://raw.githubusercontent.com/davidgilbertson/about-github/master/text-snippets&#x27;;            this.state = &#123;        markdown: &#x27;&#x27;,      &#125;;    &#125;    componentDidMount() &#123;      fetch(`$&#123;this.baseUrl&#125;/$&#123;this.props.url&#125;`)        .then(response =&gt; response.text())        .then((markdown) =&gt; &#123;          this.setState(&#123;markdown&#125;);        &#125;);    &#125;    render() &#123;      return (        &lt;div dangerouslySetInnerHTML=&#123;&#123;__html: marked(this.state.markdown)&#125;&#125; /&gt;      );    &#125;&#125;\n\n奖励环节 —— GitHub 工具我已经使用了 Octotree Chrome extension 有段时间了，现在我向大家推荐它！无论你是在查看哪个 repo 它都会在左侧给你一个树状面板。\n\n通过这个视频我了解到了 octobox，它是用于管理你的 GitHub Issues 收件箱，看起来相当不错！以上就是我针对于octobox的全部想法。\n其他就是这样了！我希望这里至少有三件事是你还不知道的。\n最后: hava a nice day！\n","categories":["翻译"],"tags":["GitHub"]},{"title":"你应该知道的 volatile 关键字","url":"/2018/03/09/volatile/","content":"\n前言不管是在面试还是实际开发中 volatile 都是一个应该掌握的技能。\n首先来看看为什么会出现这个关键字。\n内存可见性由于 Java 内存模型(JMM)规定，所有的变量都存放在主内存中，而每个线程都有着自己的工作内存(高速缓存)。\n线程在工作时，需要将主内存中的数据拷贝到工作内存中。这样对数据的任何操作都是基于工作内存(效率提高)，并且不能直接操作主内存以及其他线程工作内存中的数据，之后再将更新之后的数据刷新到主内存中。\n\n这里所提到的主内存可以简单认为是堆内存，而工作内存则可以认为是栈内存。\n\n如下图所示：\n\n所以在并发运行时可能会出现线程 B 所读取到的数据是线程 A 更新之前的数据。\n显然这肯定是会出问题的，因此 volatile 的作用出现了：\n\n当一个变量被 volatile 修饰时，任何线程对它的写操作都会立即刷新到主内存中，并且会强制让缓存了该变量的线程中的数据清空，必须从主内存重新读取最新数据。\n\n\n\nvolatile 修饰之后并不是让线程直接从主内存中获取数据，依然需要将变量拷贝到工作内存中。\n内存可见性的应用当我们需要在两个线程间依据主内存通信时，通信的那个变量就必须的用 volatile 来修饰：\npublic class Volatile implements Runnable&#123;    private static volatile boolean flag = true ;    @Override    public void run() &#123;        while (flag)&#123;        &#125;        System.out.println(Thread.currentThread().getName() +&quot;执行完毕&quot;);    &#125;    public static void main(String[] args) throws InterruptedException &#123;        Volatile aVolatile = new Volatile();        new Thread(aVolatile,&quot;thread A&quot;).start();        System.out.println(&quot;main 线程正在运行&quot;) ;        Scanner sc = new Scanner(System.in);        while(sc.hasNext())&#123;            String value = sc.next();            if(value.equals(&quot;1&quot;))&#123;                new Thread(new Runnable() &#123;                    @Override                    public void run() &#123;                        aVolatile.stopThread();                    &#125;                &#125;).start();                break ;            &#125;        &#125;        System.out.println(&quot;主线程退出了！&quot;);    &#125;    private void stopThread()&#123;        flag = false ;    &#125;&#125;\n\n主线程在修改了标志位使得线程 A 立即停止，如果没有用 volatile 修饰，就有可能出现延迟。\n但这里有个误区，这样的使用方式容易给人的感觉是：\n\n对 volatile 修饰的变量进行并发操作是线程安全的。\n\n这里要重点强调，volatile 并不能保证线程安全性！\n如下程序:\npublic class VolatileInc implements Runnable&#123;    private static volatile int count = 0 ; //使用 volatile 修饰基本数据内存不能保证原子性    //private static AtomicInteger count = new AtomicInteger() ;    @Override    public void run() &#123;        for (int i=0;i&lt;10000 ;i++)&#123;            count ++ ;            //count.incrementAndGet() ;        &#125;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        VolatileInc volatileInc = new VolatileInc() ;        Thread t1 = new Thread(volatileInc,&quot;t1&quot;) ;        Thread t2 = new Thread(volatileInc,&quot;t2&quot;) ;        t1.start();        //t1.join();        t2.start();        //t2.join();        for (int i=0;i&lt;10000 ;i++)&#123;            count ++ ;            //count.incrementAndGet();        &#125;        System.out.println(&quot;最终Count=&quot;+count);    &#125;&#125;\n\n当我们三个线程(t1,t2,main)同时对一个 int 进行累加时会发现最终的值都会小于 30000。\n\n这是因为虽然 volatile 保证了内存可见性，每个线程拿到的值都是最新值，但 count ++ 这个操作并不是原子的，这里面涉及到获取值、自增、赋值的操作并不能同时完成。\n\n\n所以想到达到线程安全可以使这三个线程串行执行(其实就是单线程，没有发挥多线程的优势)。\n\n也可以使用 synchronize 或者是锁的方式来保证原子性。\n\n还可以用 Atomic 包中 AtomicInteger 来替换 int，它利用了 CAS 算法来保证了原子性。\n\n\n指令重排内存可见性只是 volatile 的其中一个语义，它还可以防止 JVM 进行指令重排优化。\n举一个伪代码:\nint a=10 ;//1int b=20 ;//2int c= a+b ;//3\n\n一段特别简单的代码，理想情况下它的执行顺序是：1&gt;2&gt;3。但有可能经过 JVM 优化之后的执行顺序变为了 2&gt;1&gt;3。\n可以发现不管 JVM 怎么优化，前提都是保证单线程中最终结果不变的情况下进行的。\n可能这里还看不出有什么问题，那看下一段伪代码:\nprivate static Map&lt;String,String&gt; value ;private static volatile boolean flag = fasle ;//以下方法发生在线程 A 中 初始化 Mappublic void initMap()&#123;\t//耗时操作\tvalue = getMapValue() ;//1\tflag = true ;//2&#125;//发生在线程 B中 等到 Map 初始化成功进行其他操作public void doSomeThing()&#123;\twhile(!flag)&#123;\t\tsleep() ;\t&#125;\t//dosomething\tdoSomeThing(value);&#125;\n\n这里就能看出问题了，当 flag 没有被 volatile 修饰时，JVM 对 1 和 2 进行重排，导致 value 都还没有被初始化就有可能被线程 B 使用了。\n所以加上 volatile 之后可以防止这样的重排优化，保证业务的正确性。\n指令重排的的应用一个经典的使用场景就是双重懒加载的单例模式了:\npublic class Singleton &#123;    private static volatile Singleton singleton;    private Singleton() &#123;    &#125;    public static Singleton getInstance() &#123;        if (singleton == null) &#123;            synchronized (Singleton.class) &#123;                if (singleton == null) &#123;                    //防止指令重排                    singleton = new Singleton();                &#125;            &#125;        &#125;        return singleton;    &#125;&#125;\n\n这里的 volatile 关键字主要是为了防止指令重排。 \n如果不用 ，singleton = new Singleton();，这段代码其实是分为三步：\n\n分配内存空间。(1)\n初始化对象。(2)\n将 singleton 对象指向分配的内存地址。(3)\n\n加上 volatile 是为了让以上的三步操作顺序执行，反之有可能第二步在第三步之前被执行就有可能某个线程拿到的单例对象是还没有初始化的，以致于报错。\n总结volatile 在 Java 并发中用的很多，比如像 Atomic 包中的 value、以及 AbstractQueuedLongSynchronizer 中的 state 都是被定义为 volatile 来用于保证内存可见性。\n将这块理解透彻对我们编写并发程序时可以提供很大帮助。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["Java 进阶"],"tags":["Java","volatile","concurrent"]},{"title":"第一次总结","url":"/2016/05/07/%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%80%BB%E7%BB%93/","content":"前言\n昨天到今天一共花了差不多两天的时间终于把博客搭好了。还买了一个域名，现在就迫不及待的想把这段内容写下来。\n\n感谢\n首先非常感谢 嘟爷的帮忙，没有这些资料我可能还得自己研究好一段时间。# 过程我是前天无意间在微博上看到嘟爷的一篇博文，就仔细看了下，发现写的非常好，然后就将他所有的博文大致的浏览了一下。\n我很早以前就打算搭一个博客，但是百度了一下发现还是挺麻烦的，加上最近也比较忙所有一直也就没有做，直到看到这篇博文才顺利的搭起了这个博客。中途遇到不少问题也都顺利解决了，真是学到了不少的东西。\n\n熟练了Markdown语法。\n真正使用了编辑神器 Sublime。\n使用阿里云解析了github和coding里的Pages服务。\nhexo和常用的主题配置。\n\n我的配置\nHexo配置\n\n\nJackMan配置imglogo:\n  enable: true             ## display image logo true/false.\n  src: img/logo.gif        ## `.svg` and `.png` are recommended,please put image into the theme folder `/jacman/source/img`.\nfavicon: img/favicon.ico   ## size:32px*32px,`.ico` is recommended,please put image into the theme folder `/jacman/source/img`.     \napple_icon: img/jacman.jpg ## size:114px*114px,please put image into the theme folder `/jacman/source/img`.\nauthor_img: img/author.jpg ## size:220px*220px.display author avatar picture.if don&#39;t want to display,please don&#39;t set this.\nbanner_img: #img/banner.jpg ## size:1920px*200px+. Banner Picture\n### Theme Color \ntheme_color:\n    theme: &#39;#2ca6cb&#39;    ##the defaut theme color is blue\n\n# 代码高亮主题\n# available: default | night\nhighlight_theme: night\n\n#### index post is expanding or not \nindex:\n  expand: false           ## default is unexpanding,so you can only see the short description of each post.\n  excerpt_link: Read More  \n\nclose_aside: false  #close sidebar in post page if true\nmathjax: false      #enable mathjax if true\n\n### Creative Commons License Support, see http://creativecommons.org/ \n### you can choose: by , by-nc , by-nc-nd , by-nc-sa , by-nd , by-sa , zero\ncreative_commons: none\n\n结束语以上。。我做这个博客的初衷一是为了记录我的整个程序猿生涯的故事，二是希望能有大神能在过程中指出我的错误，能让我的水平更进一步。\n","categories":["blog"],"tags":["Markdown","总结"]},{"title":"分享几个 SpringBoot 实用的小技巧","url":"/2018/10/15/SpringBoot/SpringBoot-tips/","content":"\n前言最近分享的一些源码、框架设计的东西。我发现大家热情不是特别高，想想大多数应该还是正儿八经写代码的居多；这次就分享一点接地气的： SpringBoot 使用中的一些小技巧。\n算不上多高大上的东西，但都还挺有用。\n\n\n屏蔽外部依赖第一个是屏蔽外部依赖，什么意思呢？\n比如大家日常开发时候有没有这样的烦恼：\n项目是基于 SpringCloud 或者是 dubbo 这样的分布式服务，你需要依赖许多基础服务。\n\n比如说某个订单号的生成、获取用户信息等。\n\n由于服务拆分，这些功能都是在其他应用中以接口的形式提供，单测还好我还可以利用 Mock 把它屏蔽掉。\n但如果自己想把应用启动起来同时把自己相关的代码跑一遍呢？\n通常有几种做法：\n\n本地把所有的服务都启动起来。\n把注册中心换为开发环境，依赖开发环境的服务。\n直接把代码推送到开发环境自测。\n\n看起来三种都可以，以前我也是这么干的。但还是有几个小问题：\n\n本地启动有可能服务很多，全部起来电脑能不能撑住还两说，万一服务有问题就进行不下去了。\n依赖开发环境的前提是网络打通，还有一个问题就是开发环境代码很不稳定很大可能会影响你的测试。\n推送到开发环境应该是比较靠谱的方案，但如果想调试只有日志大法，没有本地 debug 的效率高效。\n\n那如何解决问题呢？既可以在本地调试也不用启动其他服务。\n其实也可以利用单测的做法，把其他外部依赖 Mock 掉就行了。\n大致的流程分为以下几步：\n\nSpringBoot 启动之后在 Spring 中找出你需要屏蔽的那个 API 的 bean（通常情况下这个接口都是交给 Spring 管理的）。\n手动从 bean 容器中删除该 bean。\n重新创建一个该 API 的对象，只不过是通过 Mock 出来的。\n再手动注册进 bean 容器中。\n\n以下面这段代码为例：\n@Overridepublic BaseResponse&lt;OrderNoResVO&gt; getUserByHystrix(@RequestBody UserReqVO userReqVO) &#123;    OrderNoReqVO vo = new OrderNoReqVO();    vo.setAppId(123L);    vo.setReqNo(userReqVO.getReqNo());    BaseResponse&lt;OrderNoResVO&gt; orderNo = orderServiceClient.getOrderNo(vo);    return orderNo;&#125;\n\n\n\n这是一个 SpringCloud 应用。\n\n它依赖于 orderServiceClient 获取一个订单号。\n其中的 orderServiceClient 就是一个外部 API，也是被 Spring 所管理。\n替换原有的 Bean下一步就是替换原有的 Bean。\n@Componentpublic class OrderMockServiceConfig implements CommandLineRunner &#123;    private final static Logger logger = LoggerFactory.getLogger(OrderMockServiceConfig.class);    @Autowired    private ApplicationContext applicationContext;    @Value(&quot;$&#123;excute.env&#125;&quot;)    private String env;    @Override    public void run(String... strings) throws Exception &#123;        // 非本地环境不做处理        if (&quot;dev&quot;.equals(env) || &quot;test&quot;.equals(env) || &quot;pro&quot;.equals(env)) &#123;            return;        &#125;        DefaultListableBeanFactory defaultListableBeanFactory = (DefaultListableBeanFactory) applicationContext.getAutowireCapableBeanFactory();        OrderServiceClient orderServiceClient = defaultListableBeanFactory.getBean(OrderServiceClient.class);        logger.info(&quot;======orderServiceClient &#123;&#125;=====&quot;, orderServiceClient.getClass());        defaultListableBeanFactory.removeBeanDefinition(OrderServiceClient.class.getCanonicalName());        OrderServiceClient mockOrderApi = PowerMockito.mock(OrderServiceClient.class,                invocationOnMock -&gt; BaseResponse.createSuccess(DateUtil.getLongTime() + &quot;&quot;, &quot;mock orderNo success&quot;));        defaultListableBeanFactory.registerSingleton(OrderServiceClient.class.getCanonicalName(), mockOrderApi);        logger.info(&quot;======mockOrderApi &#123;&#125;=====&quot;, mockOrderApi.getClass());    &#125;&#125;\n\n\n其中实现了 CommandLineRunner 接口，可以在 Spring 容器初始化完成之后调用 run() 方法。\n代码非常简单，简单来说首先判断下是什么环境，毕竟除开本地环境其余的都是需要真正调用远程服务的。\n之后就是获取 bean 然后手动删除掉。\n关键的一步：\nOrderServiceClient mockOrderApi = PowerMockito.mock(OrderServiceClient.class,                invocationOnMock -&gt; BaseResponse.createSuccess(DateUtil.getLongTime() + &quot;&quot;, &quot;mock orderNo success&quot;));defaultListableBeanFactory.registerSingleton(OrderServiceClient.class.getCanonicalName(), mockOrderApi);\n\n创建了一个新的 OrderServiceClient 对象并手动注册进了 Spring 容器中。\n第一段代码使用的是 PowerMockito.mock 的 API，他可以创建一个代理对象，让所有调用 OrderServiceClient 的方法都会做默认的返回。\nBaseResponse.createSuccess(DateUtil.getLongTime() + &quot;&quot;, &quot;mock orderNo success&quot;))\n\n测试一下，当我们没有替换时调用刚才那个接口并且本地也没有启动 OrderService：\n\n因为没有配置 fallback 所以会报错，表示找不到这个服务。\n替换掉 bean 时：\n\n再次请求没有报错，并且获得了我们默认的返回。\n\n通过日志也会发现 OrderServiceClient 最后已经被 Mock 代理了，并不会去调用真正的方法。\n配置加密下一个则是配置加密，这应该算是一个基本功能。\n比如我们配置文件中的一些账号和密码，都应该是密文保存的。\n因此这次使用了一个开源组件来实现加密与解密，并且对 SpringBoot 非常友好只需要几段代码即可完成。\n\n首先根据加密密码将需要加密的配置加密为密文。\n替换原本明文保存的配置。\n再使用时进行解密。\n\n使用该包也只需要引入一个依赖即可：\n&lt;dependency&gt;    &lt;groupId&gt;com.github.ulisesbocchio&lt;/groupId&gt;    &lt;artifactId&gt;jasypt-spring-boot-starter&lt;/artifactId&gt;    &lt;version&gt;1.14&lt;/version&gt;&lt;/dependency&gt;\n\n同时写一个单测根据密码生成密文，密码也可保存在配置文件中：\njasypt.encryptor.password=123456\n\n接着在单测中生成密文。\n@Autowiredprivate StringEncryptor encryptor;@Testpublic void getPass() &#123;    String name = encryptor.encrypt(&quot;userName&quot;);    String password = encryptor.encrypt(&quot;password&quot;);    System.out.println(name + &quot;----------------&quot;);    System.out.println(password + &quot;----------------&quot;);&#125;\n\n之后只需要使用密文就行。\n由于我这里是对数据库用户名和密码加密，所以还得有一个解密的过程。\n利用 Spring Bean 的一个增强接口即可实现：\n@Componentpublic class DataSourceProcess implements BeanPostProcessor &#123;    @Autowired    private StringEncryptor encryptor;    @Override    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123;        return bean;    &#125;    @Override    public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123;        if (bean instanceof DataSourceProperties)&#123;            DataSourceProperties dataSourceProperties = (DataSourceProperties) bean;            dataSourceProperties.setUsername(encryptor.decrypt(dataSourceProperties.getUsername())) ;            dataSourceProperties.setPassword(encryptor.decrypt(dataSourceProperties.getPassword()));            return dataSourceProperties ;        &#125;        return bean;    &#125;&#125;\n\n这样就可以在真正使用时还原为明文。\n同时也可以在启动命令中配置刚才的密码：\njava -Djasypt.encryptor.password=password -jar target/jasypt-spring-boot-demo-0.0.1-SNAPSHOT.jar\n\n总结这样两个小技巧就讲完了，大家有 SpringBoot 的更多使用技巧欢迎留言讨论。\n上文的一些实例代码可以在这里找到：\nhttps://github.com/crossoverJie/springboot-cloud\n欢迎关注公众号一起交流：\n","categories":["SpringBoot"],"tags":["Mock","加密"]},{"title":"动手实现一个 LRU cache","url":"/2018/04/07/algorithm/LRU-cache/","content":"\n前言LRU 是 Least Recently Used 的简写，字面意思则是最近最少使用。\n通常用于缓存的淘汰策略实现，由于缓存的内存非常宝贵，所以需要根据某种规则来剔除数据保证内存不被撑满。\n如常用的 Redis 就有以下几种策略：\n\n\n\n策略\n描述\n\n\n\nvolatile-lru\n从已设置过期时间的数据集中挑选最近最少使用的数据淘汰\n\n\nvolatile-ttl\n从已设置过期时间的数据集中挑选将要过期的数据淘汰\n\n\nvolatile-random\n从已设置过期时间的数据集中任意选择数据淘汰\n\n\nallkeys-lru\n从所有数据集中挑选最近最少使用的数据淘汰\n\n\nallkeys-random\n从所有数据集中任意选择数据进行淘汰\n\n\nno-envicition\n禁止驱逐数据\n\n\n\n摘抄自:https://github.com/CyC2018/Interview-Notebook/blob/master/notes/Redis.md#%E5%8D%81%E4%B8%89%E6%95%B0%E6%8D%AE%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5\n\n\n\n\n实现一之前也有接触过一道面试题，大概需求是：\n\n实现一个 LRU 缓存，当缓存数据达到 N 之后需要淘汰掉最近最少使用的数据。\nN 小时之内没有被访问的数据也需要淘汰掉。\n\n以下是我的实现：\npublic class LRUAbstractMap extends java.util.AbstractMap &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(LRUAbstractMap.class);    /**     * 检查是否超期线程     */    private ExecutorService checkTimePool ;    /**     * map 最大size     */    private final static int MAX_SIZE = 1024 ;    private final static ArrayBlockingQueue&lt;Node&gt; QUEUE = new ArrayBlockingQueue&lt;&gt;(MAX_SIZE) ;    /**     * 默认大小     */    private final static int DEFAULT_ARRAY_SIZE =1024 ;    /**     * 数组长度     */    private int arraySize ;    /**     * 数组     */    private Object[] arrays ;    /**     * 判断是否停止 flag     */    private volatile boolean flag = true ;    /**     * 超时时间     */    private final static Long EXPIRE_TIME = 60 * 60 * 1000L ;    /**     * 整个 Map 的大小     */    private volatile AtomicInteger size  ;    public LRUAbstractMap() &#123;        arraySize = DEFAULT_ARRAY_SIZE;        arrays = new Object[arraySize] ;        //开启一个线程检查最先放入队列的值是否超期        executeCheckTime();    &#125;    /**     * 开启一个线程检查最先放入队列的值是否超期 设置为守护线程     */    private void executeCheckTime() &#123;        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder()                .setNameFormat(&quot;check-thread-%d&quot;)                .setDaemon(true)                .build();        checkTimePool = new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS,                new ArrayBlockingQueue&lt;&gt;(1),namedThreadFactory,new ThreadPoolExecutor.AbortPolicy());        checkTimePool.execute(new CheckTimeThread()) ;    &#125;    @Override    public Set&lt;Entry&gt; entrySet() &#123;        return super.keySet();    &#125;    @Override    public Object put(Object key, Object value) &#123;        int hash = hash(key);        int index = hash % arraySize ;        Node currentNode = (Node) arrays[index] ;        if (currentNode == null)&#123;            arrays[index] = new Node(null,null, key, value);            //写入队列            QUEUE.offer((Node) arrays[index]) ;            sizeUp();        &#125;else &#123;            Node cNode = currentNode ;            Node nNode = cNode ;            //存在就覆盖            if (nNode.key == key)&#123;                cNode.val = value ;            &#125;            while (nNode.next != null)&#123;                //key 存在 就覆盖 简单判断                if (nNode.key == key)&#123;                    nNode.val = value ;                    break ;                &#125;else &#123;                    //不存在就新增链表                    sizeUp();                    Node node = new Node(nNode,null,key,value) ;                    //写入队列                    QUEUE.offer(currentNode) ;                    cNode.next = node ;                &#125;                nNode = nNode.next ;            &#125;        &#125;        return null ;    &#125;    @Override    public Object get(Object key) &#123;        int hash = hash(key) ;        int index = hash % arraySize ;        Node currentNode = (Node) arrays[index] ;        if (currentNode == null)&#123;            return null ;        &#125;        if (currentNode.next == null)&#123;            //更新时间            currentNode.setUpdateTime(System.currentTimeMillis());            //没有冲突            return currentNode ;        &#125;        Node nNode = currentNode ;        while (nNode.next != null)&#123;            if (nNode.key == key)&#123;                //更新时间                currentNode.setUpdateTime(System.currentTimeMillis());                return nNode ;            &#125;            nNode = nNode.next ;        &#125;        return super.get(key);    &#125;    @Override    public Object remove(Object key) &#123;        int hash = hash(key) ;        int index = hash % arraySize ;        Node currentNode = (Node) arrays[index] ;        if (currentNode == null)&#123;            return null ;        &#125;        if (currentNode.key == key)&#123;            sizeDown();            arrays[index] = null ;            //移除队列            QUEUE.poll();            return currentNode ;        &#125;        Node nNode = currentNode ;        while (nNode.next != null)&#123;            if (nNode.key == key)&#123;                sizeDown();                //在链表中找到了 把上一个节点的 next 指向当前节点的下一个节点                nNode.pre.next = nNode.next ;                nNode = null ;                //移除队列                QUEUE.poll();                return nNode;            &#125;            nNode = nNode.next ;        &#125;        return super.remove(key);    &#125;    /**     * 增加size     */    private void sizeUp()&#123;        //在put值时候认为里边已经有数据了        flag = true ;        if (size == null)&#123;            size = new AtomicInteger() ;        &#125;        int size = this.size.incrementAndGet();        if (size &gt;= MAX_SIZE) &#123;            //找到队列头的数据            Node node = QUEUE.poll() ;            if (node == null)&#123;                throw new RuntimeException(&quot;data error&quot;) ;            &#125;            //移除该 key            Object key = node.key ;            remove(key) ;            lruCallback() ;        &#125;    &#125;    /**     * 数量减小     */    private void sizeDown()&#123;        if (QUEUE.size() == 0)&#123;            flag = false ;        &#125;        this.size.decrementAndGet() ;    &#125;    @Override    public int size() &#123;        return size.get() ;    &#125;    /**     * 链表     */    private class Node&#123;        private Node next ;        private Node pre ;        private Object key ;        private Object val ;        private Long updateTime ;        public Node(Node pre,Node next, Object key, Object val) &#123;            this.pre = pre ;            this.next = next;            this.key = key;            this.val = val;            this.updateTime = System.currentTimeMillis() ;        &#125;        public void setUpdateTime(Long updateTime) &#123;            this.updateTime = updateTime;        &#125;        public Long getUpdateTime() &#123;            return updateTime;        &#125;        @Override        public String toString() &#123;            return &quot;Node&#123;&quot; +                    &quot;key=&quot; + key +                    &quot;, val=&quot; + val +                    &#x27;&#125;&#x27;;        &#125;    &#125;    /**     * copy HashMap 的 hash 实现     * @param key     * @return     */    public int hash(Object key) &#123;        int h;        return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);    &#125;    private void lruCallback()&#123;        LOGGER.debug(&quot;lruCallback&quot;);    &#125;    private class CheckTimeThread implements Runnable&#123;        @Override        public void run() &#123;            while (flag)&#123;                try &#123;                    Node node = QUEUE.poll();                    if (node == null)&#123;                        continue ;                    &#125;                    Long updateTime = node.getUpdateTime() ;                    if ((updateTime - System.currentTimeMillis()) &gt;= EXPIRE_TIME)&#123;                        remove(node.key) ;                    &#125;                &#125; catch (Exception e) &#123;                    LOGGER.error(&quot;InterruptedException&quot;);                &#125;            &#125;        &#125;    &#125;&#125;\n\n感兴趣的朋友可以直接从:\nhttps://github.com/crossoverJie/Java-Interview/blob/master/src/main/java/com/crossoverjie/actual/LRUAbstractMap.java\n下载代码本地运行。\n代码看着比较多，其实实现的思路还是比较简单：\n\n采用了与 HashMap 一样的保存数据方式，只是自己手动实现了一个简易版。\n内部采用了一个队列来保存每次写入的数据。\n写入的时候判断缓存是否大于了阈值 N，如果满足则根据队列的 FIFO 特性将队列头的数据删除。因为队列头的数据肯定是最先放进去的。\n再开启了一个守护线程用于判断最先放进去的数据是否超期（因为就算超期也是最先放进去的数据最有可能满足超期条件。）\n设置为守护线程可以更好的表明其目的（最坏的情况下，如果是一个用户线程最终有可能导致程序不能正常退出，因为该线程一直在运行，守护线程则不会有这个情况。）\n\n以上代码大体功能满足了，但是有一个致命问题。\n就是最近最少使用没有满足，删除的数据都是最先放入的数据。\n\n不过其中的 put get 流程算是一个简易的 HashMap 实现，可以对 HashMap 加深一些理解。\n\n实现二因此如何来实现一个完整的 LRU 缓存呢，这次不考虑过期时间的问题。\n其实从上一个实现也能想到一些思路：\n\n要记录最近最少使用，那至少需要一个有序的集合来保证写入的顺序。\n在使用了数据之后能够更新它的顺序。\n\n基于以上两点很容易想到一个常用的数据结构：链表。\n\n每次写入数据时将数据放入链表头结点。\n使用数据时候将数据移动到头结点。\n缓存数量超过阈值时移除链表尾部数据。\n\n因此有了以下实现：\npublic class LRUMap&lt;K, V&gt; &#123;    private final Map&lt;K, V&gt; cacheMap = new HashMap&lt;&gt;();    /**     * 最大缓存大小     */    private int cacheSize;    /**     * 节点大小     */    private int nodeCount;    /**     * 头结点     */    private Node&lt;K, V&gt; header;    /**     * 尾结点     */    private Node&lt;K, V&gt; tailer;    public LRUMap(int cacheSize) &#123;        this.cacheSize = cacheSize;        //头结点的下一个结点为空        header = new Node&lt;&gt;();        header.next = null;        //尾结点的上一个结点为空        tailer = new Node&lt;&gt;();        tailer.tail = null;        //双向链表 头结点的上结点指向尾结点        header.tail = tailer;        //尾结点的下结点指向头结点        tailer.next = header;    &#125;    public void put(K key, V value) &#123;        cacheMap.put(key, value);        //双向链表中添加结点        addNode(key, value);    &#125;    public V get(K key)&#123;        Node&lt;K, V&gt; node = getNode(key);        //移动到头结点        moveToHead(node) ;        return cacheMap.get(key);    &#125;    private void moveToHead(Node&lt;K,V&gt; node)&#123;        //如果是最后的一个节点        if (node.tail == null)&#123;            node.next.tail = null ;            tailer = node.next ;            nodeCount -- ;        &#125;        //如果是本来就是头节点 不作处理        if (node.next == null)&#123;            return ;        &#125;        //如果处于中间节点        if (node.tail != null &amp;&amp; node.next != null)&#123;            //它的上一节点指向它的下一节点 也就删除当前节点            node.tail.next = node.next ;            nodeCount -- ;        &#125;        //最后在头部增加当前节点        //注意这里需要重新 new 一个对象，不然原本的node 还有着下面的引用，会造成内存溢出。        node = new Node&lt;&gt;(node.getKey(),node.getValue()) ;        addHead(node) ;    &#125;    /**     * 链表查询 效率较低     * @param key     * @return     */    private Node&lt;K,V&gt; getNode(K key)&#123;        Node&lt;K,V&gt; node = tailer ;        while (node != null)&#123;            if (node.getKey().equals(key))&#123;                return node ;            &#125;            node = node.next ;        &#125;        return null ;    &#125;    /**     * 写入头结点     * @param key     * @param value     */    private void addNode(K key, V value) &#123;        Node&lt;K, V&gt; node = new Node&lt;&gt;(key, value);        //容量满了删除最后一个        if (cacheSize == nodeCount) &#123;            //删除尾结点            delTail();        &#125;        //写入头结点        addHead(node);    &#125;    /**     * 添加头结点     *     * @param node     */    private void addHead(Node&lt;K, V&gt; node) &#123;        //写入头结点        header.next = node;        node.tail = header;        header = node;        nodeCount++;        //如果写入的数据大于2个 就将初始化的头尾结点删除        if (nodeCount == 2) &#123;            tailer.next.next.tail = null;            tailer = tailer.next.next;        &#125;    &#125;        private void delTail() &#123;        //把尾结点从缓存中删除        cacheMap.remove(tailer.getKey());        //删除尾结点        tailer.next.tail = null;        tailer = tailer.next;        nodeCount--;    &#125;    private class Node&lt;K, V&gt; &#123;        private K key;        private V value;        Node&lt;K, V&gt; tail;        Node&lt;K, V&gt; next;        public Node(K key, V value) &#123;            this.key = key;            this.value = value;        &#125;        public Node() &#123;        &#125;        public K getKey() &#123;            return key;        &#125;        public void setKey(K key) &#123;            this.key = key;        &#125;        public V getValue() &#123;            return value;        &#125;        public void setValue(V value) &#123;            this.value = value;        &#125;    &#125;    @Override    public String toString() &#123;        StringBuilder sb = new StringBuilder() ;        Node&lt;K,V&gt; node = tailer ;        while (node != null)&#123;            sb.append(node.getKey()).append(&quot;:&quot;)                    .append(node.getValue())                    .append(&quot;--&gt;&quot;) ;            node = node.next ;        &#125;        return sb.toString();    &#125;&#125;\n\n源码：https://github.com/crossoverJie/Java-Interview/blob/master/src/main/java/com/crossoverjie/actual/LRUMap.java\n实际效果，写入时：\n    @Test    public void put() throws Exception &#123;        LRUMap&lt;String,Integer&gt; lruMap = new LRUMap(3) ;        lruMap.put(&quot;1&quot;,1) ;        lruMap.put(&quot;2&quot;,2) ;        lruMap.put(&quot;3&quot;,3) ;        System.out.println(lruMap.toString());        lruMap.put(&quot;4&quot;,4) ;        System.out.println(lruMap.toString());        lruMap.put(&quot;5&quot;,5) ;        System.out.println(lruMap.toString());    &#125;//输出：1:1--&gt;2:2--&gt;3:3--&gt;2:2--&gt;3:3--&gt;4:4--&gt;3:3--&gt;4:4--&gt;5:5--&gt;\n\n使用时：\n    @Test    public void get() throws Exception &#123;        LRUMap&lt;String,Integer&gt; lruMap = new LRUMap(3) ;        lruMap.put(&quot;1&quot;,1) ;        lruMap.put(&quot;2&quot;,2) ;        lruMap.put(&quot;3&quot;,3) ;        System.out.println(lruMap.toString());        System.out.println(&quot;==============&quot;);        Integer integer = lruMap.get(&quot;1&quot;);        System.out.println(integer);        System.out.println(&quot;==============&quot;);        System.out.println(lruMap.toString());    &#125;    //输出1:1--&gt;2:2--&gt;3:3--&gt;==============1==============2:2--&gt;3:3--&gt;1:1--&gt;\n\n实现思路和上文提到的一致，说下重点：\n\n数据是直接利用 HashMap 来存放的。\n内部使用了一个双向链表来存放数据，所以有一个头结点 header，以及尾结点 tailer。\n每次写入头结点，删除尾结点时都是依赖于 header tailer，如果看着比较懵建议自己实现一个链表熟悉下，或结合下文的对象关系图一起理解。\n使用数据移动到链表头时，第一步是需要在双向链表中找到该节点。这里就体现出链表的问题了。查找效率很低，最差需要 O(N)。之后依赖于当前节点进行移动。\n在写入头结点时有判断链表大小等于 2 时需要删除初始化的头尾结点。这是因为初始化时候生成了两个双向节点，没有数据只是为了形成一个数据结构。当真实数据进来之后需要删除以方便后续的操作（这点可以继续优化）。\n以上的所有操作都是线程不安全的，需要使用者自行控制。\n\n下面是对象关系图：\n初始化时\n写入数据时LRUMap&lt;String,Integer&gt; lruMap = new LRUMap(3) ;lruMap.put(&quot;1&quot;,1) ;\n\n\nlruMap.put(&quot;2&quot;,2) ;\n\nlruMap.put(&quot;3&quot;,3) ;\n\nlruMap.put(&quot;4&quot;,4) ;\n\n获取数据时数据和上文一样：\nInteger integer = lruMap.get(&quot;2&quot;);\n\n\n通过以上几张图应该是很好理解数据是如何存放的了。\n实现三其实如果对 Java 的集合比较熟悉的话，会发现上文的结构和 LinkedHashMap 非常类似。\n对此不太熟悉的朋友可以先了解下 LinkedHashMap 底层分析 。\n所以我们完全可以借助于它来实现：\npublic class LRULinkedMap&lt;K,V&gt; &#123;    /**     * 最大缓存大小     */    private int cacheSize;    private LinkedHashMap&lt;K,V&gt; cacheMap ;    public LRULinkedMap(int cacheSize) &#123;        this.cacheSize = cacheSize;        cacheMap = new LinkedHashMap(16,0.75F,true)&#123;            @Override            protected boolean removeEldestEntry(Map.Entry eldest) &#123;                if (cacheSize + 1 == cacheMap.size())&#123;                    return true ;                &#125;else &#123;                    return false ;                &#125;            &#125;        &#125;;    &#125;    public void put(K key,V value)&#123;        cacheMap.put(key,value) ;    &#125;    public V get(K key)&#123;        return cacheMap.get(key) ;    &#125;    public Collection&lt;Map.Entry&lt;K, V&gt;&gt; getAll() &#123;        return new ArrayList&lt;Map.Entry&lt;K, V&gt;&gt;(cacheMap.entrySet());    &#125;&#125;\n\n源码：https://github.com/crossoverJie/Java-Interview/blob/master/src/main/java/com/crossoverjie/actual/LRULinkedMap.java\n这次就比较简洁了，也就几行代码（具体的逻辑 LinkedHashMap 已经帮我们实现好了）\n实际效果:\n    @Test    public void put() throws Exception &#123;        LRULinkedMap&lt;String,Integer&gt; map = new LRULinkedMap(3) ;        map.put(&quot;1&quot;,1);        map.put(&quot;2&quot;,2);        map.put(&quot;3&quot;,3);        for (Map.Entry&lt;String, Integer&gt; e : map.getAll())&#123;            System.out.print(e.getKey() + &quot; : &quot; + e.getValue() + &quot;\\t&quot;);        &#125;        System.out.println(&quot;&quot;);        map.put(&quot;4&quot;,4);        for (Map.Entry&lt;String, Integer&gt; e : map.getAll())&#123;            System.out.print(e.getKey() + &quot; : &quot; + e.getValue() + &quot;\\t&quot;);        &#125;    &#125;    //输出1 : 1\t2 : 2\t3 : 3\t2 : 2\t3 : 3\t4 : 4\t    \n\n使用时：\n    @Test    public void get() throws Exception &#123;        LRULinkedMap&lt;String,Integer&gt; map = new LRULinkedMap(4) ;        map.put(&quot;1&quot;,1);        map.put(&quot;2&quot;,2);        map.put(&quot;3&quot;,3);        map.put(&quot;4&quot;,4);        for (Map.Entry&lt;String, Integer&gt; e : map.getAll())&#123;            System.out.print(e.getKey() + &quot; : &quot; + e.getValue() + &quot;\\t&quot;);        &#125;        System.out.println(&quot;&quot;);        map.get(&quot;1&quot;) ;        for (Map.Entry&lt;String, Integer&gt; e : map.getAll())&#123;            System.out.print(e.getKey() + &quot; : &quot; + e.getValue() + &quot;\\t&quot;);        &#125;    &#125;&#125;//输出1 : 1\t2 : 2\t3 : 3\t4 : 4\t2 : 2\t3 : 3\t4 : 4\t1 : 1\n\nLinkedHashMap 内部也有维护一个双向队列，在初始化时也会给定一个缓存大小的阈值。初始化时自定义是否需要删除最近不常使用的数据，如果是则会按照实现二中的方式管理数据。\n其实主要代码就是重写了 LinkedHashMap 的 removeEldestEntry 方法:\nprotected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123;    return false;&#125;\n\n它默认是返回 false，也就是不会管有没有超过阈值。\n所以我们自定义大于了阈值时返回 true，这样 LinkedHashMap 就会帮我们删除最近最少使用的数据。\n总结以上就是对 LRU 缓存的实现，了解了这些至少在平时使用时可以知其所以然。\n当然业界使用较多的还有 guava 的实现，并且它还支持多种过期策略。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["算法","LRU cache"]},{"title":"一致性 Hash 算法的实际应用","url":"/2019/03/01/algorithm/consistent-hash/","content":"\n前言记得一年前分享过一篇《一致性 Hash 算法分析》，当时只是分析了这个算法的实现原理、解决了什么问题等。\n但没有实际实现一个这样的算法，毕竟要加深印象还得自己撸一遍，于是本次就当前的一个路由需求来着手实现一次。\n背景看过《为自己搭建一个分布式 IM(即时通讯) 系统》的朋友应该对其中的登录逻辑有所印象。\n\n\n\n先给新来的朋友简单介绍下 cim 是干啥的：\n\n\n其中有一个场景是在客户端登录成功后需要从可用的服务端列表中选择一台服务节点返回给客户端使用。\n而这个选择的过程就是一个负载策略的过程；第一版本做的比较简单，默认只支持轮询的方式。\n虽然够用，但不够优雅😏。\n因此我的规划是内置多种路由策略供使用者根据自己的场景选择，同时提供简单的 API 供用户自定义自己的路由策略。\n先来看看一致性 Hash 算法的一些特点：\n\n构造一个 0 ~ 2^32-1 大小的环。\n服务节点经过 hash 之后将自身存放到环中的下标中。\n客户端根据自身的某些数据 hash 之后也定位到这个环中。\n通过顺时针找到离他最近的一个节点，也就是这次路由的服务节点。\n考虑到服务节点的个数以及 hash 算法的问题导致环中的数据分布不均匀时引入了虚拟节点。\n\n\n自定义有序 Map根据这些客观条件我们很容易想到通过自定义一个有序数组来模拟这个环。\n这样我们的流程如下：\n\n初始化一个长度为 N 的数组。\n将服务节点通过 hash 算法得到的正整数，同时将节点自身的数据（hashcode、ip、端口等）存放在这里。\n完成节点存放后将整个数组进行排序（排序算法有多种）。\n客户端获取路由节点时，将自身进行 hash 也得到一个正整数；\n遍历这个数组直到找到一个数据大于等于当前客户端的 hash 值，就将当前节点作为该客户端所路由的节点。\n如果没有发现比客户端大的数据就返回第一个节点（满足环的特性）。\n\n先不考虑排序所消耗的时间，单看这个路由的时间复杂度：\n\n最好是第一次就找到，时间复杂度为O(1)。\n最差为遍历完数组后才找到，时间复杂度为O(N)。\n\n理论讲完了来看看具体实践。\n我自定义了一个类：SortArrayMap\n他的使用方法及结果如下：\n\n\n可见最终会按照 key 的大小进行排序，同时传入 hashcode = 101 时会按照顺时针找到 hashcode = 1000 这个节点进行返回。\n\n下面来看看具体的实现。\n成员变量和构造函数如下：\n\n其中最核心的就是一个 Node 数组，用它来存放服务节点的 hashcode 以及 value 值。\n其中的内部类 Node 结构如下：\n\n\n写入数据的方法如下：\n\n相信看过 ArrayList 的源码应该有印象，这里的写入逻辑和它很像。\n\n写入之前判断是否需要扩容，如果需要则复制原来大小的 1.5 倍数组来存放数据。\n之后就写入数组，同时数组大小 +1。\n\n但是存放时是按照写入顺序存放的，遍历时自然不会有序；因此提供了一个 Sort 方法，可以把其中的数据按照 key 其实也就是 hashcode 进行排序。\n\n排序也比较简单，使用了 Arrays 这个数组工具进行排序，它其实是使用了一个 TimSort 的排序算法，效率还是比较高的。\n最后则需要按照一致性 Hash 的标准顺时针查找对应的节点：\n\n代码还是比较简单清晰的；遍历数组如果找到比当前 key 大的就返回，没有查到就取第一个。\n这样就基本实现了一致性 Hash 的要求。\n\nps:这里并不包含具体的 hash 方法以及虚拟节点等功能（具体实现请看下文），这个可以由使用者来定，SortArrayMap 可作为一个底层的数据结构，提供有序 Map 的能力，使用场景也不局限于一致性 Hash 算法中。\n\nTreeMap 实现SortArrayMap 虽说是实现了一致性 hash 的功能，但效率还不够高，主要体现在 sort 排序处。\n下图是目前主流排序算法的时间复杂度：\n\n最好的也就是 O(N) 了。\n这里完全可以换一个思路，不用对数据进行排序；而是在写入的时候就排好顺序，只是这样会降低写入的效率。\n比如二叉查找树，这样的数据结构 jdk 里有现成的实现；比如 TreeMap 就是使用红黑树来实现的，默认情况下它会对 key 进行自然排序。\n\n来看看使用 TreeMap 如何来达到同样的效果。运行结果：\n127.0.0.1000\n\n效果和上文使用 SortArrayMap 是一致的。\n只使用了 TreeMap 的一些 API：\n\n写入数据候，TreeMap 可以保证 key 的自然排序。\ntailMap 可以获取比当前 key 大的部分数据。\n当这个方法有数据返回时取第一个就是顺时针中的第一个节点了。\n如果没有返回那就直接取整个 Map 的第一个节点，同样也实现了环形结构。\n\n\nps:这里同样也没有 hash 方法以及虚拟节点（具体实现请看下文），因为 TreeMap 和 SortArrayMap 一样都是作为基础数据结构来使用的。\n\n性能对比为了方便大家选择哪一个数据结构，我用 TreeMap 和 SortArrayMap 分别写入了一百万条数据来对比。\n先是 SortArrayMap：\n\n耗时 2237 毫秒。\nTreeMap：\n\n耗时 1316毫秒。\n结果是快了将近一倍，所以还是推荐使用 TreeMap 来进行实现，毕竟它不需要额外的排序损耗。\ncim 中的实际应用下面来看看在 cim 这个应用中是如何具体使用的，其中也包括上文提到的虚拟节点以及 hash 算法。\n模板方法在应用的时候考虑到就算是一致性 hash 算法都有多种实现，为了方便其使用者扩展自己的一致性 hash 算法因此我定义了一个抽象类；其中定义了一些模板方法，这样大家只需要在子类中进行不同的实现即可完成自己的算法。\nAbstractConsistentHash，这个抽象类的主要方法如下：\n\n\nadd 方法自然是写入数据的。\nsort 方法用于排序，但子类也不一定需要重写，比如 TreeMap 这样自带排序的容器就不用。\ngetFirstNodeValue 获取节点。\nprocess 则是面向客户端的，最终只需要调用这个方法即可返回一个节点。\n\n下面我们来看看利用 SortArrayMap 以及 AbstractConsistentHash 是如何实现的。\n\n就是实现了几个抽象方法，逻辑和上文是一样的，只是抽取到了不同的方法中。\n只是在 add 方法中新增了几个虚拟节点，相信大家也看得明白。\n\n把虚拟节点的控制放到子类而没有放到抽象类中也是为了灵活性考虑，可能不同的实现对虚拟节点的数量要求也不一样，所以不如自定义的好。\n\n但是 hash 方法确是放到了抽象类中，子类不用重写；因为这是一个基本功能，只需要有一个公共算法可以保证他散列地足够均匀即可。\n因此在 AbstractConsistentHash 中定义了 hash 方法。\n\n\n这里的算法摘抄自 xxl_job，网上也有其他不同的实现，比如 FNV1_32_HASH 等；实现不同但是目的都一样。\n\n\n这样对于使用者来说就非常简单了：\n\n他只需要构建一个服务列表，然后把当前的客户端信息传入 process 方法中即可获得一个一致性 hash 算法的返回。\n\n同样的对于想通过 TreeMap 来实现也是一样的套路：\n\n他这里不需要重写 sort 方法，因为自身写入时已经排好序了。\n而在使用时对于客户端来说只需求修改一个实现类，其他的啥都不用改就可以了。\n\n运行的效果也是一样的。\n这样大家想自定义自己的算法时只需要继承 AbstractConsistentHash 重写相关方法即可，客户端代码无须改动。\n路由算法扩展性但其实对于 cim 来说真正的扩展性是对路由算法来说的，比如它需要支持轮询、hash、一致性hash、随机、LRU等。\n只是一致性 hash 也有多种实现，他们的关系就如下图：\n\n应用还需要满足对这一类路由策略的灵活支持，比如我也想自定义一个随机的策略。\n因此定义了一个接口：RouteHandle\npublic interface RouteHandle &#123;    /**     * 再一批服务器里进行路由     * @param values     * @param key     * @return     */    String routeServer(List&lt;String&gt; values,String key) ;&#125;\n\n其中只有一个方法，也就是路由方法；入参分别是服务列表以及客户端信息即可。\n而对于一致性 hash 算法来说也是只需要实现这个接口，同时在这个接口中选择使用 SortArrayMapConsistentHash 还是 TreeMapConsistentHash 即可。\n\n这里还有一个 setHash 的方法，入参是 AbstractConsistentHash；这就是用于客户端指定需要使用具体的那种数据结构。\n\n而对于之前就存在的轮询策略来说也是同样的实现 RouteHandle 接口。\n\n这里我只是把之前的代码搬过来了而已。\n接下来看看客户端到底是如何使用以及如何选择使用哪种算法。\n\n为了使客户端代码几乎不动，我将这个选择的过程放入了配置文件。\n\n\n\n如果想使用原有的轮询策略，就配置实现了 RouteHandle 接口的轮询策略的全限定名。\n如果想使用一致性 hash 的策略，也只需要配置实现了 RouteHandle 接口的一致性 hash 算法的全限定名。\n当然目前的一致性 hash 也有多种实现，所以一旦配置为一致性 hash 后就需要再加一个配置用于决定使用 SortArrayMapConsistentHash 还是 TreeMapConsistentHash 或是自定义的其他方案。\n同样的也是需要配置继承了 AbstractConsistentHash 的全限定名。\n\n不管这里的策略如何改变，在使用处依然保持不变。\n只需要注入 RouteHandle，调用它的 routeServer 方法。\n@Autowiredprivate RouteHandle routeHandle ;String server = routeHandle.routeServer(serverCache.getAll(),String.valueOf(loginReqVO.getUserId()));\n\n既然使用了注入，那其实这个策略切换的过程就在创建 RouteHandle bean 的时候完成的。\n\n也比较简单，需要读取之前的配置文件来动态生成具体的实现类，主要是利用反射完成的。\n这样处理之后就比较灵活了，比如想新建一个随机的路由策略也是同样的套路；到时候只需要修改配置即可。\n\n感兴趣的朋友也可提交 PR 来新增更多的路由策略。\n\n总结希望看到这里的朋友能对这个算法有所理解，同时对一些设计模式在实际的使用也能有所帮助。\n相信在金三银四的面试过程中还是能让面试官眼前一亮的，毕竟根据我这段时间的面试过程来看听过这个名词的都在少数😂（可能也是和候选人都在 1~3 年这个层级有关）。\n以上所有源码：\nhttps://github.com/crossoverJie/cim\n如果本文对你有所帮助还请不吝转发。\n","categories":["算法"]},{"title":"延时消息之时间轮","url":"/2019/09/27/algorithm/time%20wheel/","content":"\n前言近期在维护公司的调度平台，其中有个关键功能那就是定时任务；定时任务大家平时肯定接触的不少，比如 JDK 中的 Timer、ScheduledExecutorService、调度框架 Quartz 等。\n通常用于实现 XX 时间后的延时任务，或周期性任务；\n比如一个常见的业务场景：用户下单 N 分钟未能支付便自动取消订单。\n实现这类需求通常有两种方式：\n\n轮询定时任务：给定周期内扫描所有未支付的订单，查看时间是否到期。\n延时消息：订单创建的时候发送一条 N 分钟到期的信息，一旦消息消费后便可判断订单是否可以取消。\n\n\n\n先看第一种，这类方式实现较为简单，只需要启动一个定时任务即可；但缺点同样也很明显，这个间隔扫描的时间不好控制。\n给短了会造成很多无意义的扫描，增大数据库压力，给长了又会使得误差较大。\n当然最大的问题还是效率较低，随着订单增多耗时会呈线性增长，最差的情况甚至会出现上一波轮询还没有扫描完，下一波调度又来了。\n\n这时第二种方案就要显得靠谱多了，通过延时消息可以去掉不必要的订单扫描，实时性也比较高。\n延时消息这里我们不过多讨论这类需求如何实现；重点聊聊这个延时消息，看它是如何实现的，基于实现延时消息的数据结构还能实现定时任务。\n我在之前的开源 IM 项目中也加入了此类功能，可以很直观的发送一条延时消息，效果如下：\n\n使用 :delay hahah 2 发送了一条两秒钟的延时消息，另外一个客户端将会在两秒钟之后收到该消息。\n具体的实现步骤会在后文继续分析。\n时间轮要实现延时消息就不得不提到一种数据结构【时间轮】，时间轮听这名字可以很直观的抽象出它的数据结构。\n\n其实本质上它就是一个环形的数组，如图所示，假设我们创建了一个长度为 8 的时间轮。\n\ntask0 &#x3D; 当我们需要新建一个 5s 延时消息，则只需要将它放到下标为 5 的那个槽中。\ntask1 &#x3D; 而如果是一个 10s 的延时消息，则需要将它放到下标为 2 的槽中，但同时需要记录它所对应的圈数，不然就和 2 秒的延时消息重复了。\ntask2&#x3D; 当创建一个 21s 的延时消息时，它所在的位置就和 task0 相同了，都在下标为 5 的槽中，所以为了区别需要为他加上圈数为 2。\n\n通过这张图可以更直观的理解。\n当我们需要取出延时消息时，只需要每秒往下移动这个指针，然后取出该位置的所有任务即可。\n当然取出任务之前还得判断圈数是否为 0 ，不为 0 时说明该任务还得再轮几圈，同时需要将圈数 -1 。\n这样就可避免轮询所有的任务，不过如果时间轮的槽比较少，导致某一个槽上的任务非常多那效率也比较低，这就和 HashMap 的 hash 冲突是一样的。\n编码实现理论讲完后我们来看看实际的编码实现，为此我创建了一个 RingBufferWheel 类。\n它的主要功能如下：\n\n可以添加指定时间的延时任务，在这个任务中可以实现自己的业务逻辑。\n停止运行（包含强制停止和所有任务完成后停止）。\n查看待执行任务数量。\n\n首先直接看看这个类是如何使用的。\n\n我在这里创建了 65 个延时任务，每个任务都比前一个延后 1s 执行；同时自定义了一个 Job 类来实现自己的业务逻辑，最后调用 stop(false) 会在所有任务执行完毕后退出。\n\n构造函数\n先来看看其中的构造函数，这里一共有两个构造函数，用于接收一个线程池及时间轮的大小。\n线程池的作用会在后面讲到。\n这里的时间轮大小也是有讲究的，它的长度必须得是 2∧n，至于为什么有这个要求后面也会讲到。\n默认情况下会初始化一个长度为 64 的数组。\n添加任务\n下面来看看添加任务的逻辑，根据我们之前的那张抽象图其实很容易实现。\n\n\n首先我们要定义一个 Task 类，用于抽象任务；它本身也是一个线程，一旦延时到期便会执行其中的 run 函数，所以使用时便可继承该类，将业务逻辑写在 run() 中即可。\n它其中还有两个成员变量，也很好理解。\n\ncycleNum 用于记录该任务所在时间轮的圈数。\nkey 在这里其实就是延时时间。\n\n\n\n//通过 key 计算应该存放的位置private Set&lt;Task&gt; get(int key) &#123;    int index = mod(key, bufferSize);    return (Set&lt;Task&gt;) ringBuffer[index];&#125;private int mod(int target, int mod) &#123;    // equals target % mod    target = target + tick.get() ;    return target &amp; (mod - 1);&#125;\n\n\n首先是根据延时时间 (key) 计算出所在的位置，其实就和 HashMap 一样的取模运算，只不过这里使用了位运算替代了取模，同时效率会高上不少。\n\n这样也解释了为什么数组长度一定得是 2∧n。\n\n然后查看该位置上是否存在任务，不存在就新建一个；存在自然就是将任务写入这个集合并更新回去。\nprivate int cycleNum(int target, int mod) &#123;    //equals target/mod    return target &gt;&gt; Integer.bitCount(mod - 1);&#125;\n\n\n其中的 cycleNum() 自然是用于计算该任务所处的圈数，也是考虑到效率问题，使用位运算替代了除法。\n\nprivate void put(int key, Set&lt;Task&gt; tasks) &#123;    int index = mod(key, bufferSize);    ringBuffer[index] = tasks;&#125;\n\n而 put() 函数就非常简单了，就是将任务写入指定数组下标即可。\n启动时间轮任务写进去后下一步便是启动这个时间轮了，我这里定义了一个 start() 函数。\n\n其实本质上就是开启了一个后台线程来做这个事情：\n\n它会一直从时间轮中取出任务来运行，而运行这些任务的线程便是我们在初始化时传入的线程池；所以所有的延时任务都是由自定义的线程池调度完成的，这样可以避免时间轮的阻塞。\n这里调用的 remove(index) 很容易猜到是用于获取当前数组中的所有任务。\n\n逻辑很简单就不再赘述，不过其中的 size2Notify() 倒是值得说一下。\n\n他是用于在停止任务时，主线程等待所有延时任务执行完毕的唤醒条件。这类用法几乎是所有线程间通信的常规套路，值得收入技能包。\n停止时间轮刚才提到的唤醒主线程得配合这里的停止方法使用：\n\n如果是强制停止那便什么也不管，直接更新停止标志，同时关闭线程池即可。\n但如果是软停止（等待所有任务执行完毕）时，那就得通过上文提到的方式阻塞主线程，直到任务执行完毕后被唤醒。\nCIM 中的应用介绍了核心原理和基本 API 后，我们来看看实际业务场景如何结合使用（背景是一个即时通讯项目）。\n我这里所使用的场景在文初也提到了，就是真的发送一条延时消息；\n\n现有的消息都是实时消息，所以要实现一个延时消息便是在现有的发送客户端处将延时消息放入到这个时间轮中，在任务到期时再执行真正的消息发送逻辑。\n由于项目本身结合了 Spring，所以第一步自然是配置 bean。\n\nbean 配置好后其实就可以使用了。\n\n每当发送的是延时消息时，只需要将这个消息封装为一个 Job 放到时间轮中，然后在自己的业务类中完成业务即可。\n\n后续可以优化下 api，不用每次新增任务都要调用 start() 方法。\n\n这样一个延时消息的应用便完成了。\n总结时间轮这样的应用还非常多，比如 Netty 中的 HashedWheelTimer 工具原理也差不多，可以用于维护长连接心跳信息。\n甚至 Kafka 在这基础上还优化出了层级时间轮，这些都是后话了，大家感兴趣的话可以自行搜索资料或者抽时间我再完善一次。\n这篇文章从前期准备到撸码实现还是花了不少时间，如果对你有帮助的话还请点赞转发。\n本文的所有源码都可在此处查阅：\nhttps://github.com/crossoverJie/cim\n你的点赞与分享是对我最大的支持\n","categories":["算法","Netty"],"tags":["时间轮"]},{"title":"定时任务方案大百科","url":"/2019/10/14/algorithm/timer-detail/","content":"\n\n原文地址：https://crossoverjie.top\n\n前言节前有更新一篇定时任务的相关文章《延时消息之时间轮》，有朋友提出希望可以完整的介绍下常见的定时任务方案，于是便有了这篇文章。\n\n\nTimer本次会主要讨论大家使用较多的方案，首先第一个就是 Timer 定时器，它可以在指定时间后运行或周期性运行任务；使用方法也非常简单：\n\n这样便可创建两个简单的定时任务，分别在 3s/5s 之后运行。\n使用起来确实很简单，但也有不少毛病，想要搞清楚它所存在的问题首先就要理解其实现原理。\n实现原理定时任务要想做到按照我们给定的时间进行调度，那就得需要一个可以排序的容器来存放这些任务。\n在 Timer 中内置了一个 TaskQueue 队列，用于存放所有的定时任务。\n\n其实本质上是用数组来实现的一个最小堆，它可以让每次写入的定时任务都按照执行时间进行排序，保证在堆顶的任务执行时间是最小的。\n这样在需要执行任务时，每次只需要取出堆顶的任务运行即可，所以它取出任务的效率很高为。\n结合代码会比较容易理解：\n\n在写入任务的时候会将一些基本属性存放起来（任务的调度时间、周期、初始化任务状态等），最后就是要将任务写入这个内置队列中。\n\n在任务写入过程中最核心的方法便是这个 fixUp() ,它会将写入的任务从队列的中部通过执行时间与前一个任务做比对，一直不断的向前比较。\n如果这个时间是最早执行的，那最后将会被移动到堆顶。\n\n通过这个过程可以看出 Timer 新增一个任务的时间复杂度为。\n\n再来看看它执行任务的过程，其实在初始化 Timer 的时候它就会在后台启动一个线程用于从 TaskQueue 队列中获取任务进行调度。\n\n所以我们只需要看他的 run() 即可。\n\n从这段代码中很明显可以看出这个线程是一直不断的在调用\ntask = queue.getMin();\n来获取任务，最后使用 task.run() 来执行任务。\n\n从 getMin() 方法中可以看出和我们之前说的一致，每次都是取出堆顶的任务执行。\n一旦取出来的任务执行时间满足要求便可运行，同时需要将它从这个最小堆实现的队列中删除；也就是调用的 queue.removeMin() 方法。\n\n其实它的核心原理和写入任务类似，只不过是把堆尾的任务提到堆顶，然后再依次比较将任务往后移，直到到达合适的位置。\n\n从刚才的写入和删除任务的过程中其实也能看出，这个最小堆只是相对有序并不是绝对的有序。\n\n源码看完了，自然也能得出它所存在的问题了。\n\n后台调度任务的线程只有一个，所以导致任务是阻塞运行的，一旦其中一个任务执行周期过长将会影响到其他任务。\nTimer 本身没有捕获其他异常（只捕获了 InterruptedException），一旦任务出现异常（比如空指针）将导致后续任务不会被执行。\n\nScheduledExecutor既然 Timer 存在一些问题，于是在 JDK1.5 中的并发包中推出了 ScheduledThreadPoolExecutor 来替代 Timer，从它所在包路径也能看出它本身是支持任务并发执行的。\n先来看看它的类继承图：\n\n可以看到他本身也是一个线程池，继承了 ThreadPoolExecutor。\n\n从他的构造函数中也能看出，本质上也是创建了一个线程池，只是这个线程池中的阻塞队列是一个自定义的延迟队列 DelayedWorkQueue（与 Timer 中的 TaskQueue 作用一致）\n\n新建任务\n当我们写入一个定时任务时，首先会将任务写入到 DelayedWorkQueue 中，其实这个队列本质上也是使用数组实现的最小堆。\n\n新建任务时最终会调用到 offer() 方法，在这里也会使用 siftUp() 将写入的任务移动到堆顶。\n\n\n\n原理就和之前的 Timer 类似，只不过这里是通过自定义比较器来排序的，很明显它是通过任务的执行时间进行比较的。\n运行任务所以这样就能将任务按照执行时间的顺序排好放入到线程池中的阻塞队列中。\n这时就得需要回顾一下之前线程池的知识点了：\n\n在线程池中会利用初始化时候的后台线程从阻塞队列中获取任务，只不过在这里这个阻塞队列变为了 DelayedWorkQueue，所以每次取出来的一定是按照执行时间排序在前的任务。\n\n\n和 Timer 类似，要在任务取出后调用 finishPoll() 进行删除，也是将最后一个任务提到堆顶，然后挨个对比移动到合适的位置。\n而触发消费这个 DelayedWorkQueue 队列的地方则是在写入任务的时候。\n\n本质上是调用 ThreadPoolExecutor 的 addWorker() 来写入任务的，所以消费 DelayedWorkQueue 也是在其中触发的。\n这里更多的是关于线程池的知识点，不太清楚的可以先看看之前总结的线程池篇，这里就不再赘述。\n\n线程池没你想的那么简单\n线程池没你想的那么简单（续）\n\n原理看完了想必也知道和 Timer 的优势在哪儿了。\n\n\n\nTimer\nScheduledThreadPoolExecutor\n\n\n\n单线程阻塞\n多线程任务互不影响\n\n\n异常时任务停止\n依赖于线程池，单个任务出现异常不影响其他任务\n\n\n所以有定时任务的需求时很明显应当淘汰 Timer 了。\n时间轮最后一个是基于时间轮的定时任务，这个我在上一篇《延时消息之时间轮》有过详细介绍。\n通过源码分析我们也可以来做一个对比：\n\n\n\n\nScheduledThreadPoolExecutor\n基于时间轮\n\n\n\n写入效率\n基于最小堆，任务越多效率越低\n 与 HashMap 的写入类似，效率很高。\n\n\n执行效率\n 每次取出第一个，效率很高\n 每秒拨动一个指针取出任务\n\n\n所以当写入的任务较多时，推荐使用时间轮，它的写入效率更高。\n但任务很少时其实 ScheduledThreadPoolExecutor 也不错，毕竟它不会每秒都去拨动指针消耗 CPU ，而是一旦没有任务线程会阻塞直到有新的任务写入进来。\nRingBufferWheel 更新在之前的《延时消息之时间轮》中自定义了一个基于时间轮的定时任务工具 RingBufferWheel ，在网友的建议下这次顺便也做了一些调整，优化了 API 也新增了取消任务的 API。\n\n在之前的 API 中，每当新增一个任务都要调用一下 start()，感觉很怪异；这次直接将启动函数合并到 addTask 中，使用起来更加合理。\n同时任务的写入也支持并发了。\n\n不过这里需要注意的是 start() 在并发执行的时候只能执行一次，于是就利用了 CAS 来保证同时只有一个线程可以执行成功。\n同时在新增任务的时候会返回一个 taskId ，利用此 ID 便可实现取消任务的需求（虽然是比较少见），使用方法如下：\n\n感兴趣的朋友可以看下源码也很容易理解。\n分布式定时任务最后再扩展一下，上文我们所提到的所有方案都是单机版的，只能在单个进程中使用。\n一旦我们需要在分布式场景下实现定时任务的高可用、可维护之类的需求就得需要一个完善的分布式调度平台的支持。\n目前市面上流行的开源解决方案也不少：\n\nxxl_job\nelastic_job\nlight-task-scheduler\n\n我个人在工作中只使用过前面两者，都能很好的解决分布式调度的需求；比如高可用、统一管理、日志报警等。\n当然这些开源工具其实在定时调度这个功能上和上文中所提到的一些方案是分不开的，只是需要结合一些分布式相关的知识；比远程调用、统一协调、分布式锁、负载均衡之类的。\n感兴趣的朋友可以自行查看下他们的源码或官方文档。\n总结一个小小的定时器其实涉及到的知识点还不少，包括数据结构、多线程等，希望大家看完多少有些帮助，顺便帮忙点赞转发搞起🥳。\n本文所涉及到的所有源码：\nhttps://github.com/crossoverJie/cim\n你的点赞与分享是对我最大的支持\n","categories":["算法","Java","Netty"],"tags":["时间轮","Timer","Schedule"]},{"title":"2018 年度复盘","url":"/2018/12/30/annual-summary/2018/","content":"\n前言看着今年的进度表已经所剩无几，是时候来复盘一把了。\n\n\n\n从 16 年初写博客开始到现在我觉得写年终总结的习惯不错，毕竟每次看着去年的 flag 又可以复制粘贴了。\n今年我会从工作、技术、身体等方面回顾，这几块也是今年变化最大的几个点。\n工作先说工作吧，这个变化也贯穿了整年。\n从今年年初开始，我从上一个技术团队调到现在的部门；首先是组织结构上的变更，当然更主要的还是角色的变化。\n由一个开发人员转变为团队的技术负责人，说实话刚开始是措手不及的。\n以前我只需要对我写的代码负责，现在不行了。得对整个团队的产出负责；需要为每一个成员的质量、成长负责。\n这对于一个刚入门的菜鸟来说挑战无疑是巨大的。\n而且整个研发团队基本上是重头组建，我这入职一年多的都成了司龄最大的老员工了😢。\n随着人员的增加，对我的要求也越来越高。在请教了老司机后也逐渐的走上正轨了，虽然中间也踩了不少坑。\n总的来说：\n\n以前只关注我代码写的 6 不 6；现在重点是整个团队的研发进度、质量把控。这两点是评估我工作好坏的直接因素。\n\n要把这两项搞好我不得不提高一些通用技能：包括沟通协调、需求判断、排期风险、人员流动等。\n明年的人数还会持续增加，要学的东西还有很多。\n技术作为一个代码从业者，技术能力才是我的本职工作。\n随着今年业务性质的变化，我所接触的技术也略有不同。\n前几年打交道的主要是 web 相关的技术；大多数技术栈都是围绕着它来展开的。\n而今年不太一样的是在 web 的基础上，还需要涉及到网络。主要是现在业务和物联网相关，看平台的还好最直接的就是能支持了多少连接。\n这个就需要对物联网特有的一些协议有所了解、应用。\n所以今年恶补了 Netty 相关的知识，同时在平时的开发中进行了一些实践发现想要做好网络这种底层开发需要储备的知识太多了。\n什么操作系统、IO、TCP 都得掌握，正好也补习了这些短板。\n开源项目\nGitHub 官方的年度报告可以看出今年是开源大年。\n\n从我今年的贡献图可以看出也花了很多时间在这上面。\n从注册 GitHub 账号算起每年的提交量看起来今年确实是花了不少心思：\n\n最显著的体现就是 JCSprout 一年时间涨了 1W7 star。\n\n主要开源的有：\n\nJCSprout Java Core Sprout：处于萌芽阶段的 Java 核心知识库。\ncicada 基于 Netty 实现的快速、轻量级 HTTP 框架。\ndistributed-redis-tool 根据日常需求实现的一个分布式工具，包括分布式锁、分布式限流。\nnetty-action 看名字就知道，一个 netty 实战相关案例，现在也正在修改为可水平扩展的 IM 即使通讯系统；预计元旦后发布。\n\n要感谢每一位给我提 issue、PR 的朋友，希望来年能把挖的坑填完😭。\n技术博客\n从年初到现在一共撸了 49 篇博客，我还特意按照时间排序、阅读量做了一个柱状图（数据来源为个人博客：https://crossoverjie.top ）：\n\n统计了一下，全年这个博客的阅读总数为：22W，平均每篇差不多 4500 的阅读；虽说不能和一些大佬相比，但比去年可不知道高哪里去了。\n同时最高的有将近 2W 当然低的也有4 500的阅读数，不过从这个图中还是可以看出一些规律的。\n\n比如阅读量高的也是比较吸引眼球的，这不就是常说的“标题党”嘛。\n\n微信公众号再来谈谈公众号，现在做公众号的技术人也越来越多；不过在今年申请的账号已经没有留言功能了，还好我申请的早，至少和读者有一个交流的机会。\n今年也是把公众号从 0 做到了 1 ，也就是有了 1W+ 的关注数；不过说实话我确实没有画什么心思运营，里面的内容也都是同步于我的博客，除了几篇翻译之外可以说是 100% 原创。\n\n写过技术文的应该都知道产出一篇文章并不轻松，所以为了能正向激励我也会适当的接一些广告；这样不管是对读者还是我都有好处。\n\n中途也有一些朋友找我投稿，由于目前不是定位于做一个自媒体；我个人也不能完全对转载的文章理解透彻，还是希望做一个原创的技术号，所以抱歉都没有转载。\n\n身体从前几年的计划表中都提到了身体，但实话说直到现在记性没长只有体重长了。。。\n原本热爱的篮球也从每周一次调整为一个月一次，曾经潇洒的 crossover 也变为键盘里一个个的 Bug。\n从下面的视频中可以看得出来（需要 FQ 观看）：\n\n\n\n\n\n\n\n\n\n\n第一段是五年前的，后面两段为最近的。\n\n明年真得上心了，借着搬家到新小区内的篮球场看能否拯救我这多年的键盘手。\n总结回顾一下对于我个人的几个大事件吧：\n\n工作角色大变化，带来的挑战也很大。\n做了几个还算成功开源项目，并且带来了一些实质性的好处。\n公众号从 0 到 1 ，并且能补贴一些鸡腿钱。\n求婚成功，感谢高中班主任当年的不杀之恩（没有揭发我）。\n等了三年终于接房装修了。\n\n按照历史传统照例还是写个明年的 TODO-LIST 吧：\n\n别拖团队后腿，多和老司机学习下软技能。\n开源项目接着更新，这也是可持续发展道路之一。\n博客、公众号持续输出优质内容，只是更新周期可能会提高。\n要逼就往死里逼，看年底能否扣个篮！\n搞个事情，看能否把婚接了。\n\n对来年写总结的我诚恳的说一句：别在 Ctrl+C,Ctrl+V 了🙏。\n","categories":["annual-summary"]},{"title":"2019年度总结","url":"/2019/12/30/annual-summary/2019/","content":"\n前言消失两个多月后我胡汉三又回来了，比较遗憾的是这并不是一篇技术文，有兴趣的朋友就当做故事看吧。\n所以这其实是一份年终总结\n其实这段期间一直有朋友在问我咋不接着更新公众号了？甚至一点消息都没了。\n真不是不更，主要有以下几个原因：\n\n\n我对这个号的定位是【原创技术】博主，大家应该也知道，技术号是一种非常垂直的领域，能写的东西也就那么多；以我个人的工作阅历其实已经快被榨干了。\n平时大家应该也能看得出，不少公众号逐渐开始转载和原创混着来，甚至有些已经是全职的转载号了。\n当然我不是说这样不好，每个人的选择不同；只是我对我个人的要求是：要么就不写，不然就得是有意义的原创。\n乍一看还挺有骨气，然而现实却是两个多月没更新了。\n\n所以我现在也在不断学习，希望今年能再给大家带来一些有意思的内容。\n\n第二个原因今年下半年我换了份工作，面对全新的领域需要花上不少时间去学习；同时也需要完成几件人生大事，所以总得有侧重点。\n具体内容请接着往下看。\n回顾2019年对我来说真的是意义重大的一年，今年发生了许多大事；用我妈的话来说就是喜事连连。\n新家确实每一件事对我来说都不是小事；首先是终于搬进了新家。\n\n从去年接房到经历半年的装修终于是搬进了新家，告别了 3 年多的租房日子；但也不是一帆风顺，从装修中踩的坑，以及和家人意见不一致导致的分歧都让这个新房来之不易。\n期间为了让我的房间看起来更有极客范，满足所谓的”程序员“逼格；我还特地搞了一套智能家居，现在用了半年发现优点还是明显，但也偶尔会在智能和智障中来回切换。\n原本我打算拍一期与智能家居相关的视频来着，也是由于上面两个原因耽搁了，大家感兴趣的话也可以和大家分享。\n谈到视频，今年我还更新了 10 期 vlog。\n\n其实效果也不错，也有好多小伙伴反馈挺有意思，但也是自己懒就停更了，来年也得捡起来了，做视频我也很感兴趣，年纪大了后再看看当年的自己我想会很有意思。\n新车第二件大事自然就是提车了，这事其实完全没在我今年的计划清单里的。\n我从拿证到提车中间只间隔了一个周末。\n当然第一次事故来的也比较快，好在现在胆子也越来越大；刚提车那段时间，早上为了躲避早高峰我甚至六点过就开车去公司，同样的下班也是8、9点钟才最后一个走。让我一度怀疑我这车买来是遭罪的。\n\n还一度成为了公司笑柄，好在现在三个多月已经没那么怂了，当然我也不敢再立 flag 了，第一次剐蹭就是某天早上在公司炫耀不再早起后出事的。\n领证\n毫不犹豫这是今年办的最重要的一件事；从 2011 年开始的异地校园恋到现在我都不敢想象真的成了。前几天我填了一个调查问卷，文末让填写婚姻情况，当我习惯性的准备填写未婚时发现我居然也结婚了，当时的心情就像被陌生人归还丢失的钱包，不禁感慨还是好心人多啊。\n现在想想我的节奏是去年求婚、今年领证、明年婚礼；看似间隔了挺长但也循序渐进，希望各位读者朋友也早日脱单。\n工作今年还有一件大事就是我换了份工作，换工作的原因也不是公司不好，而是和我的职业发展规划有些许偏差，同时压力也大到爆炸。\n毫不夸张的说，最严重那段时间我甚至都不敢看到邮件、微信、短信等一系列与工作相关的通知，一度还有幻听，总感觉有微信 at 我的那个声音。\n当然除了这些我也真的非常感谢我之前的领导，非常的信任我，也是我迄今为止遇到的最 nice 的领导之一。\n好，马屁拍完后简单聊聊我现在的工作。现在这家公司是一家纯互联网的创业公司，也是少数可以盈利并活过三年的创业公司。\n公司氛围真的非常好，也是我工作以来氛围最 nice 的团队，注意这里没有”之一“。\n每周三我们有宅男快乐日（其实就是下班后一起打球），成为了每周大家最期待的日子。\n\n而我的工作内容也发生了变化，由上家公司偏团队管理转换为我熟悉的纯技术人员，可能有人说这不相当于降级了嘛？但其实我在加入这个团队时就明确表示不想做管理，我享受在我这个年龄阶段做纯研发工作的过程。\n也许再过几年我可能会后悔，但 who care 呢，现在舒服就够了。\n\n顺便插个广告，我们团队来年也要扩充人马，欢迎从外边回重庆及重庆本地的有志青年联系我😋。\n\n技术其实就技术上来说今年本身进步并不多，主要是下半年换工作后我的主力语言由 Java 切换为”人生苦短，我用 Python“了。期间大部分是在用 Python 来完成工程实践，也有部分工作是由我来维护 Java 相关的内容；随着我们业务的发展、人员的扩充不可避免的会由弱类型语言转换强类型，这是目前大部分技术团队发展的规律。\n所以就 Java 本身来说我也会继续学习，后面也会继续更新相关的技术文章，待我 Python 入化境后也会尝试写写相关内容。\n开源同样的下半年之后其实我也没怎么投入精力到开源项目中了，有好多 issues 都还没来得及处理。最近在写一个 orm 相关的框架，断断续续写了两个月，本来打算月初分享出来也被耽搁了。\n\n这其实最近再使用 Python ORM 时觉得还不错的一个 features 移植过来的。\n\n\n年初本来想在年底冲一冲争取访问量过百万，同样的还是因为没有更新现在还差一点🤣。\n公众号倒是自增长到了 2W 关注，这点我倒是真没想到，毕竟上次打开这个后台也是两个月前的事了🥶。\n\n总结现在来看看去年立下的 flag :\n\n好吧，只完成了一项；前几天看到有人说：不要在年初定目标，因为当前大脑容易兴奋不易看清自己。\n但目标不在年初立那还有啥意义？所以还是按照历史传统立个 flag：\n\n婚礼办了，去日本度个蜜月。\nPython 玩的更熟一些，同时能把之前的项目经验带到团队中。\n减肥、健身；这个计划从前年就开始了。\nvlog 得接着拍起来，再买台无人机换个拍摄视角。\n最后就是技术输出了，我预计应该是做不到一周一更了，但两周一更还是可以保证。\n\n","categories":["annual-summary"]},{"title":"可能是最晚的2020年终总结","url":"/2021/03/02/annual-summary/2020/","content":"\n前言首先还是祝大家新年快乐，虽然还有300来天。\n哈哈，圆规正传。\n\n近三年每年我都有写年终总结，每次看上一年的新年计划时心里就会骂一句：TMD 又白写了；当然今年也不例外。\n\n\n\n虽说计划对我来说意义不大，但回顾一年的重要事项用于茶余饭后的谈资还是很有用的。\n回顾依照惯例还是首先回归下 2020 这一整年对我来说有哪些重要事件。\n疫情首先自然是所有人都面临的疫情；我最直接的感受就是让我对二人世界有了新的认识。\n\n但也是第一次为老婆单独过生日，谁让她出生的日子挑在了正月初五呢，平常这个时候都还在走亲访友，能记得她生日的都是真爱了。\n为此我还特地为她准备了一个小惊喜；\n\n悄悄的订了一个蛋糕，运气不错，赶在小区彻底封闭前取到了货，然后在她玩游戏的时候突然袭击；具体见视频：\n\n \n\n\n蜜月旅行之后整个上半年对我来说都没有什么特别的记忆，因为我一直在期待下半年的婚假。\n我是在 19 年 12月中的时候领的证，按照规定我得在20年的12月中旬将婚假休了；由于上半年的疫情也不太乐观（头一年打算去日本的计划也不得不落空），加上工作的上的事情也没有怎么排开；所以我便准备在11月份出去旅行。\n说来也惭愧，这还是我们耍朋友8、9年来第一次正儿八经的出远门旅行。\n因为有足足 15 天的婚假，加上一个周末就是 17 天；所以我准备将整个旅行分为两部分。\n第一部分是在云南，主要地点是大理和丽江。\n第二部分则是作为内陆人一直想见的大海：三亚。\n为此我还特地做了一个行程表：\n\n\n云南之行我也做了一个视频回顾：\n \n\n\n\n在三亚的花费是整个旅程最多的部分，但我个人感觉并没有物超所值；\n一个是饮食不太习惯，当地的海鲜吃法对我这种吃惯了川菜的确实有点难以接受，即便尝过了麻辣香锅版的海鲜。\n最后三天还咬牙斥”巨资“入驻了三亚出名的亚特兰蒂斯酒店，原本想着这么贵的酒店总得图个清静吧；\n没想到办理入住的时候活活排了两个小时队，果然是贫穷限制了想象力。\n后面我们发现部分游客居然和我们一样，在市内采购了许多吃得拿到酒店里，因为基本房型只包含了早上的自助早餐。\n\n而这里”普普通通“餐厅大概是人均 2000 一位。\n住着 2 3千一晚的酒店，却在里边吃着 5 块的加大号方便面，可能这就是当代打工人和干饭人的基本素养吧。\n\n三亚部分的视频内容也在紧张制作中（文件夹建好了）感兴趣的不妨点个关注吧。\n\n婚礼再一个重要的事情就是婚礼了，本来我们两个是不太打算办婚礼的，但碍不过家人的要求最终就简简单单的走了个流程。\n在我们的观念中婚礼也就是走个形式，为了这个形式足够简单我们花了半天时间加上 700 块钱拍了”婚纱照“。\n\n甚至最终婚礼现场的婚纱也是网上花了 300 块买来的，后面还准备再二手卖掉回回血。\n婚礼前一周再和司仪沟通流程时，我还打算去掉所有的对话环节；想着几分钟就搞定不耽搁大家吃饭时间。\n 本以为我对待婚礼”无所谓“的态度加上我的二皮脸能让这个仪式快些结束，但真等到我上了那个T台，才发现时间是如此的漫长。\n耳边亲友们的欢呼仿佛和我没有丝毫关系，脑子里会不由自主的像放电影一样回顾我和老婆在一起的点滴；不得不庆幸我在”单身夜“后半段背了两个小时的演讲稿，不然当时可能我一句话都说不出来。\n当流程走到给双方父母敬茶时，司仪让我们低头看着父母两鬓的白发；虽说我深知这是惯用的煽情”套路“，但依然还是有些绷不住。\n果然是：男儿有泪不轻弹 只是未到伤心处。\n当时为了婚庆简洁不交”智商税“，婚礼整个过程的摄影都是我老弟帮忙拍摄的，所以回忆还得我自己来剪；同样的也是文件夹已建好。\n工作接下来聊聊工作，我记得我们公司算是到现场复工比较晚的一批；因为在疫情期间的远程工作我们发现效率还挺不错。\n后来我仔细一合计，可不就是因为远程工作，随时随地都可以写代码嘛，连出去玩的机会都没有。\n\n哈哈，开个玩笑；其实本质原因还是疫情前我们和产品的需求就一直是远程沟通，整个工作中最扯皮的事情以前我们都能”从容解决“，所以被迫远程时还不就张手就来。\n\n当五月份复工时，还有些一起工作三个多月却素未谋面的新同事，那真的是大型网友见面会。\n技术公众号老实说，20年我对自己的要求实在是有点低了；关注我的朋友应该知道整个 20 年我都没更新几篇技术文。\n\n最扯的是我在19年的总结时也说过类似的话，究其原因我自己总结了几点：\n\n个人”包袱“有点重，内心深处觉得毕竟是工作几年的老鸟，不能再像刚入行那样写一些浅显的入门文章。然而现实情况却是入门的看不上，有深度的一篇没有。\n第二点是我在影视飓风年度总结上学到的新词：”死亡三角状态“。\n\n\n大家是否觉得似曾相识；没错，这不就是 CAP 理论嘛？\n只是我在遇到这个问题时却是选择将三者都拉倒了底。。。\n发现这个问题后我在年初尝试做了一些转变，截止目前为止我转载了两篇优秀的原创博文，至少在数量和质量上能先提高上来。\n同时我也在逼迫自己可以做到周更，为什么觉得自己能做到？是因为我发现自己并不是缺时间，只是单纯的懒而已。\n毕竟两年前的我也能做到，是时候拿出那时的激情了。\n开源接下来聊聊开源，开源这事我觉得是我这几年做的最对的一个选择，但同样的也是在18年那段时间产出最多。\n20年一共磕磕盼盼写了三个项目：\n\nsqlalchemy-transfer 应该算是比较有技术含量的一个，主要是复习，相当于重学了一些编译原理的知识。\n用于解析 MySQL 的 DDL 语句生成 Python ORM 项目所需要的 model 文件。\n\n第二个 feign-plus，核心功能是可以不依赖于 SpringCloud 那一套包，从而可以直接在 SpringBoot 中实现 feign 的声明式调用。\n这个本身思路还不错，但由于公司技术栈现在转向 Golang 之后就没怎么维护了。\n\n最后一个 btb 其实是用 Go 重写了之前 Java 写的一个博客工具，可以备份、替换自己文章中的图片。\nbtb\n用 Go 重写之后便改为命令行交互了，使用起来更方便一些。\n遗憾的是原本是准备将 cim 也用 Go 重构一次，但终究是没有实施，不过今年一定会提上议程。\n总结还是照例看看去年的规划完成了多少：\n\n不出所料，大部分都没完成。\n但没关系，既然没完成那就顺延到今年吧：\n\n工作内容有数量+质量的产出。\n深入学习和理解 Go、Docker、云原生相关内容。\n博客周更。\n减肥。\n\n整个20年对我来说玩是玩好了，毕竟还出去玩了大半个月；但相对的个人进步确实太少。\n18 年时比现在忙的多，同时个人成长也是呈加速度走势；于是今年 21 年我也在公司主动承担更多的职责，往往不逼自己一把就不知道自己的上限在哪儿。\n最后放两段篮球的 mixtap ，希望今年大家身体健康。\n \n\n \n","categories":["annual-summary"]},{"title":"2021 年度报告","url":"/2022/01/27/annual-summary/2021/","content":"\n不知不觉年终总结就像每个人的 KPI 一样，年底不总结一下感觉今年就白过了似的。\n今年时间真的感觉过的特别快，经常感觉工资刚发不久结果没几天又到了发薪日；再也没有小时候一个暑假都能过一年的感觉。\n\n\n生活生活上来说最大的变化也许就是年龄+1了，今年也是我们结婚两年恋爱十年的时间；十年这样的跨度现在想想还是觉得不可思议。\n\n好在目前为止我们双方父母都没有催生，一切都顺其自然吧。\n\n虽然每天都是公司、家里两点一线的生活，但没想到的是今年居然喜欢上和我毫无关系的一项运动：足球。\n原本是公司每周组织的足球活动 14 缺 1，没事就去踢了一次，结果发现还挺好玩；\n虽然每次报名的都是原有的篮球队员，此消彼长自然篮球就没啥人报名了😂。\n贴几段足球小视频：\n \n\n \n\n \n\n\n\n今年最后一场\n\n工作总的来说今年工作上的变化是最大的，其实简单来说就是我们被收购了；之前总是在网上看别人公司的小道消息吃瓜，没想到这次吃到自己头上来了。\n幸运的是我个人受到的影响不大，毕竟也不是公司领导层，我们只需要做好自己的事情就行了。\n当然落实到我们日常工作最明显的变化可能就是开发流程的变化了，这点在后文会具体说明。\n今年因为主导了几个系统的重构以及部门内部技术的推动，相对去年来说成绩上还是有所提升的，所以年底也评了优，算是对我工作的肯定吧。\n但其实我个人不是特别满意，几个项目推动了一半，最终也没达到预期目标，只能寄托于来年了。\n技术由于公司的调整自然也带来了我们技术栈的变化；简单来说经历了几个阶段：\n\n其实每个阶段都和公司业务+组织架构+业务现状有着千丝万缕的关系；个人的喜好很难起决定性作用。\n所以绕来绕去，今年我又得开始写 Java 了；最近这三年时间从 Java 转到 Python，体验到了各种便捷的语法糖，又写了将近两年 Go 之后体验到了大道至简的优雅。\n三种语言都各有优势，但从内心深处来讲我还是更愿意写 Go；可能是不想再去卷很难用到的八股文、配置繁琐的 maven 等。\n由于团队内部有些同事没有接触过 Java ，所以让我以新手角度带大家一起学习；新版本的 JDK 语法糖+ lombok + mapstruct 这类工具，配合上最新的 IDEA 开发起来也是非常舒服的。\n用单纯的 SpringBoot 结合 k8s 后，之前的 https://github.com/crossoverJie/feign-plus 就有用武之处了。\n博客\n今年的技术原创博客产量也不高，满打满算将近 20 篇；其中大部分都与 Go 有关，近期确实大部分时间都是在写 Go，但也只是用了点皮毛；不出意外的话来年会 Java 和 Go 的内容都会写一点。\n开源今年的开源项目上我最喜欢的应该就是 https://github.com/crossoverJie/ptg\n\n这是命令行的接口压测工具，同时也是一个 gRPC 的客户端 app。\n\n\nUI 确实是我的极限了，我自己还有部分小伙伴使用了一段时间还是挺好用的。\n\n最近正在加 stream 调用相关的功能。\n\n\n其实在公司内部也有用过 Go 重构过调度中心，就是大家用的挺多的 xxl-job。\n由于我们的项目都是 gRPC 协议，同时运维体系之类的原因就用 Go 重写了一版。\n\n最终在每日百万次数的调度下成功率≈99.9%，已经可以满足业务使用了；\n但后期如果业务上不再使用 Go 的话难免会有些可惜，所以我也在想和公司沟通下，可以把这个调度中心开源出来，同时以前也说过我们内部也做了一个 Go 的业务框架，现有的调度业务代码接入调度中心也是通过该业务框架实现的。\n所以也准备都开源出来，但时间上还不好说，总之希望来年还能有机会多写写  Go，能参与开源是最好的。\n\n最后希望来年疫情能彻底结束了，至少能不限制的跨省旅旅游；也希望有机会能把前年办的健身卡利用起来，根据前几年的经验来看 flag 还是要少立。\n","categories":["annual-summary"]},{"title":"2022 年度总结","url":"/2023/01/18/annual-summary/2022/","content":"\n一转眼 2022 年又过去了，不多不少距离上次写年终总结过去了 365 天；今年的艰难情况想必大家都亲身经历过了；如果要举行卖惨大会的话今年也许我能排的上号。\n\n\n生活今年对大家影响最大的事应该都是疫情了，在年底的最后几天家里的老人还是没顶住疫情的冲击离开了，原以为成年后我已经看淡了生老病死，直到我现在敲下这几行字时才发现这么难过。\n悲伤的事暂且不提，还是聊聊今年生活上的好事吧。\n健身首先是健身这个我念叨了几年的运动今年终于被我提上了议程。\n\n本质原因是请了私教，果然是花自己的钱才会心疼。\n\n\n体重也由巅峰的 75kg 降到了66kg 左右。\n \n\n一段视频便能看出差距。\n游戏今年不记得被哪个视频安利了微软的 XGP 服务，冲动下单了 xbox，顺道集齐了御三家的全家桶。\n不得不说 XGP 服务是真的香，游戏也很多，我玩的最多的就是光环、地平线5、奥日这几款游戏；原以为 xbox 后续会成为我的主力机，直到几个月后我在 tb 奸商那儿购买的 XGP 服务被微软退款后我就没怎么碰了。\n后面老头环上线，也是我唯一一款花钱购买的 xbox 游戏，在被老头环揉拧了几周后手残党也被劝退，一直到现在估计三个月没开过机了。\n不过最近倒还喜欢上玩 Steam 上的一些独立小游戏，特别是肉鸽类型的，比如这个“吸血鬼幸存者”玩着真的非常上头。\n\n当然今年最期待依然是那个带我入主机坑的“塞尔达传说”，恨不得现在马上快进到5月12号发售日。\n世界杯今年还有件大事那就是世界杯的召开，真没想到我还会对足球这么感兴趣；因为当时是封控在家远程工作，所以我几乎把凌晨三点场的都看完了。\n那段时间因为离职心情还比较 EMO，感谢世界杯带给了我一个月的快乐时间。\n当然不出意外的在世界杯期间发生了意外。\n\n大半夜睡得好好的，眼睛被我老婆的手指甲刨到了，连夜赶往医院，最后就成了“带土”的 cos 低配版。\n工作经常都有大佬说三年是在一家公司的敏感时间，如果感觉不到提升那就需要适当的跳出舒适圈，其实我压根没这个打算，但生活总在你没准备好的时候推你一把。\n\n由于不可抗力因素，我还是离开了这家我有生以来呆的最开心的一家公司；虽然有许多不舍，但江湖总会相见。\n这不我微信里最活跃的依然还是那个群。\n\n后面我花了一个月的时间把重庆大大小小的公司几乎都看了一遍，甚至还差点去成都工作了；最后阴差阳错的来到了现在的公司做我之前非常向往的基础架构+中间件研发，目前也比较满意。\n\n找工作那段时间也碰到许多有意思的和狗血的事情，年后单独分享。\n\n技能GScript\n今年个人最满意的就是恶补了编译原理的知识，顺带还做了一个脚本语言；现在已经可以拿来编写网站了；也算是一个小目标达成吧。\n回想起开发 gscript 的那段时间，真的是没日没夜的干，每完成一个功能就开心的飞起。\n云原生除此之外在来到现在这家公司后接触了大量 k8s 相关的知识点，也算是把之前学到的理论实践上了；这不昨晚上才在生产环境升级了 Pulsar，这个技能树终于点亮了一些叶子节点。\n博客\n今年的技术博客产出居然 23 篇，其中大部分都是和编译原理相关的，也是我一步步学习编译原理到实现脚本语言的过程。\n\n同时今年也养成了每日看一篇英文博客的习惯，坚持了几个月效果还是很明显的；比如以前我非常排斥看一些英文资料，要么靠一键翻译，要么就直接只看中文内容。\n现在几乎没有这种排斥的感觉了，大部分英文内容也会耐心的阅读完，这点在我订阅了 Pulsar 的开发组邮件后越发明显，明显的能够知道他们在讲些什么，这点与我多年前订阅 Dubbo 社区邮件的感觉完全不同。\n目标以上就是整年的流水账式的回顾，又到了经典的保留立 flag 环节。\n\n首先是健身保持，都说健身是按年算的，希望到今年四月份为期一年的时候能看到健身的痕迹。\n今年好歹的出去玩一玩，比如港澳地区或是日本，念叨几年了。\n工作技术上能够再提交几个 Pulsar 的 PR，最好是能融入社区；混个脸熟。\n云原生和编译原理相关的继续学起来，下半年把 GScript 实现为编写型语言。\n\n","categories":["annual-summary"]},{"title":"我的 2023","url":"/2024/02/17/annual-summary/2023/","content":"今天是春节的最后一天，因为工作上临时有点事，很不情愿的打开电脑看着也就 10 天没看代码觉得非常陌生。\n\n\n之后便准备将迟迟未写的 2023 总结补完，这个传统从16年至今已经坚持将近 7 年时间了，今年当然也不能意外。\n\n健身今年要说最让我印象深刻的事就是健身了，为此我投入了大量的时间。\n我记得是在 22 年四月份当时是因为确实长胖太明显了，下定决心找个教练进行训练，效果确实也有。\n去年也分享过，最后从 75kg 减到 66kg；但大部分时间都是被动的进行训练，所以到了 23 年初的时候其实就反弹不少了。\n而今年最大的不同是我由原先的被动健身改为主动了，甚至到后面一天不练还浑身不舒服。\n所以今年我大部分时间都是自己锻炼，因为我是个 I 人，比较喜欢一个人，所以夏天的时候是每天早上 7 点多去健身房然后再去公司。 \n到了冬天早上确实是起不来，就改为了中午去训练。\n就这样不知不觉就坚持了大半年，直到现在。\n训练日志见文末。\n\n甚至现在偶尔找教练训练时，他说我比他练的都勤🤣。\n\n最终达到的效果就是生活作息更加规律，同时身体素质也是肉眼可见的提升。\n\n自重引体从一个不能做-&gt;反握-&gt;对握-&gt;正握-&gt;正握做组\n俯卧撑从 5 个-&gt;10个&#x2F;组-&gt;15个&#x2F;组-&gt;25个&#x2F;组-&gt;一次最多做 40 个-&gt;钻石俯卧撑 15&#x2F;组\n\n其余的就是胸肌有些轮廓、肩部也比以往更圆润一些，腹肌在某些特定角度也可以若隐若现（当然这个得体脂足够低才行），今年的主要目标是上半年认真刷刷脂。\n工作工作今年大体上没有什么变化，但经济不景气应该每个人都能感受到；目前我能苟着的同时还能学一些自己感兴趣的东西就非常满足了。\n\n\n到现在依然很怀恋在上家公司的日子。\n\n今年在公司主要还是维护 Pulsar，同时也给社区贡献了一些代码，算是这么些年来最认真参与开源的一年。\n感兴趣的可以看看之前写的文章：\n\n如何给开源项目发起提案\n新手如何快速参与开源项目\n\n相比我以前的工作来说，现在的岗位是基础架构，所以接触的几乎都是一些开源产品，这也是我个人感兴趣的方向。\n所以虽然同事之间的交流没有之前的公司那么频繁（我们部门和业务团队在不同的城市），但因为由兴趣驱动，所以也没那么枯燥。\n英语对了，年中的时候还头脑发热去报了一个英语线下培训班，上了两月后发现除非是连续每天上 8 小时突击几个月，不然别想一下子速成。\n平时没有使用英语的环境，那就只能自己创造了，我现在会坚持每天看一个油管的科技视频，目前看生肉有字幕的情况下勉强可以理解。\n同时又因为今年长期都在水开源社区，导致我现在看英文文档、邮件之类的不借助翻译也没那么吃力，算是开了一个好头。\n今年争取再多听听英文播客，虽然暂时无法通过英语找到远程工作，但利用英文确实可以打开新世界。\n播客今年算是播客的重度用户，其实听播客的习惯前几年就有了，但那时候大部分是再开车的时候听，今年因为每天有1~2小时的健身时间，所以健身的时候几乎都是听播客过来的。\n个人觉得播客是非常好的内容输入源，比很多视频内容的质量还高；这里推荐几个我常听的频道：\n\n枫言枫语\n硬地骇客\n爱否科技\n开源面对面\n捕蛇者说\n皮蛋漫游记等\n\n副业在年底的时候无意间利用 Pulsar 完成了我人生的第一笔咨询服务，当时还发了个朋友圈。没想到之后又有个朋友来咨询了一些关于职场的问题，完事后客户满意度还挺高。\n于是我今年也准备好好筹备下，说不定真能做成一个副业。\n\n打个广告，感兴趣的也可以私聊。\n\n\n年底还好运获得了掘金的签约资格：我算是掘金最早一批用户了，记得是 16 年就开始在上面发布文章，这也是长期坚持获得的肯定。\n而且掘金由于被字节收购后资金明显比前几年宽裕，参与过几次征文活动还是收获了一些现金奖励。\n现在和掘金签约后还能获得更多的现金和流量奖励，对作者和平台来说都是双赢，只是今后的文章需要先在掘金发布三个月后才可以同步到其他平台。\n所以掘金还没关注的我的朋友赶紧关注一波吧：https://juejin.cn/user/835284565229597\n技能技能上除了刚才在工作中提到的 Pulsar 外还额外学习了：\n\nVictoriaMetrics 入门到安装\nVictoriaLog 一个新的日志存储数据库，之前也写过一篇介绍使用文章。\n还给 VictoriaLog 做过一点贡献。\n\n\nGrafana 更熟练了\nkubernetes 的一些知识点也数量了，写过几个小工具：\n优雅重启 Pod\n批量替换应用镜像\n\n\nIstio 的应用，包含网关和服务调用等\n在公司内部做过两次分享（关于 Pulsar 和开源的内容）\n年底的时候还写过一个 OTel 的 extension，熟悉了 OTel 的一些概念和实践。\n\n从今年长期使用的 tag 来看，果然还是 Pulsar 和 kubernetes 使用的最多。\n博客今年的博客数据产量算是比较多的了，确实也是有我工作的关系，平时接触到的大部分都是些技术问题，所以能写的东西也就比较多了。\n同时也再尝试每周发布技术周刊：\n目前发了十几期，效果不错，大部分都是一些英文文章，自己也能学到一些东西。\n开源之前也提到了今年算是我比较深入的参与开源项目，以往大部分都是发布一些个人作品，当然也有给一些个人或者小项目提过 PR，现在看来多少有点”小打小闹“了。\n因为在公司主要维护 Pulsar，所以不可避免的就需要和社区沟通，不管是反馈 Bug 还是修复问题流程都比以往正规，毕竟这也是一个 Apache 顶级项目。\n\n主要活跃的是 Pulsar 主仓库，合并了 14 个 PR。\n\n其次是 pulsar-client-go 也就是 Pulsar 的 Go 客户端，合并了 6 个 PR。\n\n\n\n然后是 VictoriaMetrics，其实主要就是给他们新发布的 VictoriaLogs 修了个 Bug，也是第一次被单独提及的贡献。\n\n最后就是年底的时候在一个做可观测性大佬的公众号下看到的项目：cprobe\n主要是贡献了一个 helm 安装仓库以及几个插件，这是一个对新手很友好的项目，对开源感兴趣的都可以来参与下。\n当然贡献数量不能作为评判参与开源的唯一标准，但确实比较好量化的指标，今年加油继续贡献。\n目标又到了给往年打分的环节了：\n去年算是完成了 60%，今年的定一些容易实现的目标：\n\n卧推 80kg\n体脂保持在 13% 左右（误差不能多于 2）\n去海外玩估计有点难度，那就先定国内吧，哪里都行.\n年底开源社区争取提名一个 Committer\n英语可以达到生肉油管的程度\n做一个副业试试\n\n往年记录\n2022\n2021\n2020\n2019\n2018\n2016\n\n\n长图预警\n\n\n","categories":["annual-summary"]},{"title":"我的 2024","url":"/2025/03/03/annual-summary/2024/","content":"这些年我一直都是按照农历新年来写年终总结的，都说不出正月都是年，前些年一直都比较规律，今年确实是时间超了一些。\n主要原因还是年末接了个活，需要在年初上线，导致这段时间都没太多时间写内容。\n最近事情终于告一段落后才开始码字。\n\n\n\n本来打算用 AI 来写的，想想还是算了，现在 AI 大热的时代，越是手工打造的内容越是珍贵🐶\n\n健身\n\n回想起来 2024 年投入最多的还是健身，手上的老茧都换了几轮了；\n以前还不信真有人一天没事就往健身房跑吗？现在回想起来在健身房的那 1～2 小时是一天最放松的时间，带个耳机听着播客，感受肌肉的发力（听着是有点油腻）完全进入心流的状态。\n不过因为我大部分的时间都是自己练，所以对自己也不够狠，全是自己能接受的强度，加上也没啥天赋（从小体育就是我的弱项）所以肌肉线条也不是很明显。\n\n\n以上都是凹了半天造型才拍出来的，和健身大佬完全没法比；去年 11 月份从乐刻换到了一个有自由卧推和深蹲的健身房，动作基本上都是从零开始，现在卧推 70kg、深蹲 80kg 已经比较满意了。\n我的要求不高，保证在不受伤的前提下卧推能到 80kg 做组就满意了。\n\n就像我朋友说的，看你也练了一年多了咋还这么菜？我的回复是：这一年多如果不练，那岂不是更菜，现在还在打基础的阶段🙂\n\n工作今年的工作依然是按部就班的进行，在公司依然是负责基础架构，主要还是维护内部的消息队列 Pulsar、可观测性工具 OpenTelemetry、服务网格、StarRocks 等。\n当然 24 年还是完成了一个小目标：成为了两个 Apache 项目的 Committer。\n\n为此我还写了一篇文章：我是如何从零到成为 Apache 顶级项目的 Committer，感兴趣的朋友可以看看详细过程。\n副业23 年的时候第一次接到了咨询相关的付费业务，也就是从那时候开始尝试做一些副业，目前咨询服务了几个客户，反馈都还不错：\n今年准备加大力度再宣传一下：\n同时 24 年在咨询的基础上开了知识星球，也没有认真宣传，目前有大约 90 个用户，非常感谢他们的支持：\n有需要的朋友也可以扫码关注下，等这段时间忙过之后会重点运营。\n掘金签约作者\n去年也和掘金签约了半年时间：也就是这半年期间写的文章需要首发在掘金平台，同时还能拿到相应的佣金；对创作者和平台来说确实是双赢的结果。\n平台收获了优质的文章，作者也能获得一定的激励；希望国内越来越多的平台可以效仿，而不是往自家平台里倒垃圾（DDDD）。\n技能博客\n去年 24 年因为签约了掘金，所以写的还是比较积极，一共写了 53 篇博客，平均每个月写 4 篇+。\n今年写的内容主要包含了：\n这里我问了下 AI，其实几乎都是我工作中接触到的技术问题，比如 Pulsar、OpenTelemetry、StarRocks 等。\n开源\n\n24年投入开源的时间还是蛮多的，毕竟我的工作中的有部分时间也是和开源相关的。\nApache PulsarPulsar 的贡献主要是分为主仓库和 pulsar-client-go 两个。\n主仓库这边我的改动不是很多，比较大的就是重构了 cli，其他的都是些边角料。\n\n其余在 pulsar-client-go 中主要是同步了一些 java-client 的 feature 过来，以及修复一些 bug。\n\nApache HertzBeat\n除此之外在空余时间我还参与了 Apache HertzBeat 项目（一个开源的实时监控项目），当时项目刚进入 Apache 孵化器不久，我主要是帮助完善了一些单元测试、优化了 CI 流程、代码 checkstyle 等工作。后来由于时间限制参与的不多了，但也在一直有在关注着。\nOpenTelemetry\n去年年中的时候由于公司可观测的技术栈全面迁移到 OpenTelemetry，所以也花了一些时间学习并使用它，并结合我们的场景给社区提了一些 PR.\n\nInstrumentation\nPulsar 相关的 metrics 埋点： https://github.com/open-telemetry/opentelemetry-java-instrumentation/pull/11591\n支持 PowerJob: https://github.com/open-telemetry/opentelemetry-java-instrumentation/pull/12086\n\n\nOperator\nOperator 支持部署 java extensions: https://github.com/open-telemetry/opentelemetry-operator/pull/2761\n\n\nConvention:\n完善了 RPC 的一些语义: https://github.com/open-telemetry/semantic-conventions/pull/1281\n补全了 Pulsar 的一些语义： https://github.com/open-telemetry/semantic-conventions/pull/1099\n\n\n\n本来准备下半年再接再厉多贡献一些，争取成为 Member，结果因为把重心切到 Starrocks 之后这边暂时就没在跟进了，今年也许会重启更新。\nStarRocks这也是去年第一次接触到的技术栈，和大部分业务开发一样，以前顶多接触过关系型数据库，对这类大数据产品接触很少。\n第一次参与是领导让我看看能否给它的物化视图加一个参数，好在我要修改的部分（FrontEnd)都是 Java 写的（BackEnd 是 cpp 写的），至少代码看起来无压力。\n\n所以就花了一些时间来从头研究，到目前为止也给社区提交了一些 feature 和修复了 bug，主要都是和物化视图相关的内容。\n\n这里也额外提一下：即便是对毫不熟悉的项目，哪怕看起来是数据库这种比较复杂的技术栈，只要能看懂代码、复现问题，那就都可以解决，首先心理上就不要害怕。\n\n目标\n最后再看看去年的目标，完成率不说 100% 吧，80% 还是有的，今年目标看来要再定高一些了：\n\n卧推 PR 85kg\n体脂达到一次 13%\n国内游一次\n再完成一个开源社区的 Member\n\n好了，流水账记完了，今年也要抓紧开始搬砖了，咱们明年再见。\n往年记录\n2023\n\n2022\n\n2021\n\n2020\n\n2019\n\n2018\n\n2016\n\n\n","categories":["OB"]},{"title":"GoodBye 2016,Welcome 2017 | 码农砌墙记","url":"/2016/12/31/annual-summary/GoodBye%202016,Welcome%202017%20%7C%20%E7%A0%81%E5%86%9C%E7%A0%8C%E5%A2%99%E8%AE%B0/","content":"前言\n早在这个月初的时候我就很想写一篇年终总结了，因为这一年相对于去年确实是经历的太多了。结果一直等到31号，在家里和媳妇吃完晚饭就马上打开电脑开码。\n五月二十三-第一次跳槽\n根据整年的时间线开始第一件大事自然就是换公司了。\n\n先来点前景提要:我是14年11月份参加工作的。当时其实还没有毕业就在一家给大型企业做定制软件开发的公司实习。刚开始工作的时候什么事情都觉得非常新奇，一个在学校学的东西能运用到实际开发中并能给用户带来便利让我觉得做码农真是一件非常正确的选择啊(ps当时真是太年轻)。\n后来真是造化弄人，当时负责我参与的这个项目的负责人跳槽了，我自然就成了整个公司最熟悉此项目的人了。现在不得不佩服公司老板真是心大啊，居然让一个实习生来负责这个项目。就这样我成了整个项目的负责人，从之后的开发到测试到上线到后面的维护几乎都是我一个人在负责。来一张当时上线的截图：\n由于这次项目的顺利验收，公司也对我越来越信任。之后也就理所当然的又负责了几个项目。\n\n\n\n虽然离开了但真的非常感谢公司当时对一个什么都不懂的新人给予信任。\n\n之后随着技术的提升我接触了github、v站这样的技术论坛，逐渐的发现天外有天，我这点雕虫小技真的完全不算什么，真正机遇与挑战并存的地方是互联网。\n但是此时我已经在这家公司做了一年多了，突然离开这个舒适圈来到一个陌生的环境是需要很大勇气的，或者说需要一个刺激点。\n正好@嘟爷成了这个导火索。那个时候我正在搭我的个人博客正好看到了他的文章，觉得写得非常好。而且正好他也正准备转向互联网，于是我给他写了一封很长的邮件说了我心中的一些疑惑与顾虑让他给点建议。\n在他的建议之下我才开始投递简历准备换一家互联网公司，感谢嘟爷给了我一个这么正确的建议。\n之后我顺利的进入了一个创业公司，开始了狭义的互联网开发道路，为什么是狭义请接着往后看。\n搭建个人博客\n搭建博客这事也是必须的拿出来说一说的。\n\n上面说到我看了嘟爷的博客才开始搭建自己的博客，到现在为止由于我的拖延症(加上是真的懒)一共写了20篇。不能说写的有多好，但确实是我在工作和学习中的一些总结。\n让我意外的是我博客的访问量，下图是我cnzz的统计截图：\n\n六月二十一-开源项目关于开源项目，之前我在github上面看很多优秀的开源项目，也很佩服那些作者，于是就想着自己能不能也搞一个，但是一来就造个轮子对我来说确实有点不现实。\n于是我换了一个思路，由于现在我勉强也不算是新入门的菜鸟了，但我是从菜鸟过来的，深知刚开始的时候找资料的痛苦。不是资料太老就是没有体系，讲一点是一点的那种。\n于是就有了现在这个项目:会不定期更新一些在实际开发中使用的技巧(ps:目前不是很忙基本上一周一更)。 没有复杂的业务流程，更不是XXXX系统，只有一些技术的分享。\n从六月二十一号到现在还是有100多颗星了：\n\n九月二十三-第二次跳槽看到这里是不是觉得我有病啊，怎么又是跳槽。。。\n其实我也不想，我在上面说到开始了我的狭义互联网开发，为什么是狭义呢？\n因为做了一段时间才发现这个项目除了是部署在云服务器上和有一个微信端之外和我之前所做的项目貌似没有本质上的区别，还是一个管理系统。\n这里我不评价公司的业务，但是公司的技术总监在修改问题的时候是直接在云服务器上登陆数据库删除数据，会不会觉得很奇葩。最奇葩的是删除的时候忘了写where条件导致把整张表的数据都删了，这个时候如果是你你会不会怀疑那啥。。\n除此之外技术总监本人还是挺好的，不过我更觉得他适合做销售总监。\n加上后来公司的业务没有发展起来，所做的系统又老是出问题(联想上文)，加上还在流传我们技术部要裁人。那我还不如自己走(现在V站逛多了突然觉得好亏)。\n于是我开始了我的第二次跳槽，前后时间才间隔4个月，不得不感慨命运弄人啊。\n之后我来到现在这家员工5000余人的真正的互联网公司，开始了真正意义的互联网开发。这里必须得感谢我的面试官也是我现在这个项目的leader，给了我这个互联网菜鸟机会。\n不过命运总是如此的相识，明年也就是下周他就换部门了，意味着现在这个项目我又成负责人了。希望一切顺利吧。\n技术相关前面说到我是九月份的时候才进入这家正真意义的互联网公司的，所以体术提升最明显也是在这段时间。\n这段时间所学的起码是我在前面两家公司一年都学不到的，这里我大致列了一下：\n\n熟悉了一个互联网产品的生命周期(关于开发、测试、预发布、灰度以及上线)\n熟悉了一些关于并发、主从、缓存、调度、容器这些主流的技术。\n最重要的一点，学会了不加班不舒服斯基。\n\n身体相关不知是错觉还是什么，感觉今年看到IT行业猝死或者是出事的新闻越来越多，加上我这个今年才22岁的青年有时候也会腰疼脖子酸，导致我对于身体也是越来越担忧。\n其实我从初中的时候就开始打篮球，在工作之前也是对篮球完全是痴迷的状态，每天不打球就浑身难受。刚工作的那段时间还能坚持每周末去打球，但是今年能做到一个月打一次都非常难得了。。\n再此，我立个flag，明天下午出去打球，明年坚持至少每两周打一次球。\n2017小目标到这里也基本上总结的差不多了，还有半个小时就是17年了。\n还是定一个17年的小目标吧：\n\n\n博客坚持写，至少保持两周一更。\n开源项目坚持维护，争取造一个轮子出来。\n坚持锻炼，我还得养家糊口。\n最后希望家人朋友都平平安安。\n\n\n","categories":["annual-summary"]},{"title":"Go 去找个对象吧","url":"/2021/02/24/basic/Go-OOP/","content":"\n前言我的读者中应该大部分都是 Java 从业者，不知道写 Java 这些年是否真的有找到对象？\n没找到也没关系，总不能在一棵树上吊死，我们也可以来 Go 这边看看，说不定会有新发现。\n开个玩笑，本文会以一个 Javaer 的角度来聊聊 Go 语言中的面向对象。\n\n\nOOP面向对象这一词来源于Object Oriented Programming，也就是大家常说的 OOP。\n对于 Go 是否为面向对象的编程语言，这点也是讨论已久；不过我们可以先看看官方的说法:\n\n其他的我们暂且不看，Yes and No. 这个回答就比较微妙了，为了这篇文章还能写下去我们先认为 Go 是面向对象的。\n\n面向对象有着三个重要特征：\n\n封装\n继承\n多态\n\n封装Go 并没有 Class 的概念，却可以使用 struct 来达到类似的效果，比如我们可以对汽车声明如下：\ntype Car struct &#123;\tName string\tPrice float32&#125;\n\n与 Java 不同的是，struct 中只存储数据，不能定义行为，也就是方法。\n当然也能为 Car 定义方法，只是写法略有不同：\nfunc (car *Car) Info()  &#123;\tfmt.Printf(&quot;%v price: [%v]&quot;, car.Name, car.Price)&#125;func main() &#123;\tcar := Car&#123;\t\tName: &quot;BMW&quot;,\t\tPrice: 100.0,\t&#125;\tcar.Info()&#125;\n\n在方法名称前加上 (car *Car) 便能将该方法指定给 Car ，其中的 car 参数可以理解为 Java 中的 this 以及 Python 中的 self，就语义来说我觉得 go 更加简单一些。\n毕竟我见过不少刚学习 Java 的萌新非常不理解 this 的含义与用法。\n匿名结构体既然谈到结构体了那就不得不聊聊 Go 支持的匿名结构体（虽然和面向对象没有太大关系）\nfunc upload(path string) &#123;\tbody, err := ioutil.ReadAll(res.Body)\tsmsRes := struct &#123;\t\tSuccess bool   `json:&quot;success&quot;`\t\tCode    string `json:&quot;code&quot;`\t\tMessage string `json:&quot;message&quot;`\t\tData    struct &#123;\t\t\tURL string `json:&quot;url&quot;`\t\t&#125; `json:&quot;data&quot;`\t\tRequestID string `json:&quot;RequestId&quot;`\t&#125;&#123;&#125;\terr = json.Unmarshal(body, &amp;smsRes)\tfmt.Printf(smsRes.Message)&#125;\n\nGo 允许我们在方法内部创建一个匿名的结构体，后续还能直接使用该结构体来获取数据。\n这点在我们调用外部接口解析响应数据时非常有用，创建一个临时的结构体也不用额为维护；同时还能用面向对象的方式获取数据。\n相比于将数据存放在 map 中用字段名获取要优雅许多。\n继承Go 语言中并没有 Java、C++ 这样的继承概念，类之间的关系更加扁平简洁。\n各位 Javaer 应该都看过这类图：\n\n相信大部分新手看到这图时就已经懵逼，更别说研究各个类之间的关系了。\n不过这样好处也明显：如果我们抽象合理，整个系统结构会很好维护和扩展；但前提是我们能抽象合理。\n在 Go 语言中更推荐使用组合的方式来复用数据：\ntype ElectricCar struct &#123;\tCar\tBattery int32&#125;func main() &#123;\txp := ElectricCar&#123;\t\tCar&#123;Name: &quot;xp&quot;, Price: 200&#125;,\t\t70,\t&#125;\tfmt.Println(xp.Name)&#125;\n\n这样我们便可以将公共部分的数据组合到新的 struct 中，并能够直接使用。\n接口(多态)面向接口编程的好处这里就不在赘述了，我们来看看 Go 是如何实现的：\ntype ElectricCar struct &#123;\tCar\tBattery int32&#125;type PetrolCar struct &#123;\tCar\tGasoline int32&#125;//定义一个接口type RunService interface &#123;\tRun()&#125;// 实现1func (car *PetrolCar) Run() &#123;\tfmt.Printf(&quot;%s PetrolCar run \\n&quot;, car.Name)&#125;// 实现2func (car *ElectricCar)Run() &#123;\tfmt.Printf(&quot;%s ElectricCar run \\n&quot;, car.Name)&#125;func Do(run RunService) &#123;\trun.Run()&#125;func main() &#123;\txp := ElectricCar&#123;\t\tCar&#123;Name: &quot;xp&quot;, Price: 200&#125;,\t\t70,\t&#125;\tpetrolCar := PetrolCar&#123;\t\tCar&#123;Name: &quot;BMW&quot;, Price: 300&#125;,\t\t50,\t&#125;\tDo(&amp;xp)\tDo(&amp;petrolCar)&#125;\n\n首先定义了一个接口 RunService；ElectricCar 与 PetrolCar 都实现了该接口。\n可以看到 Go 实现一个接口的方式并不是 implement，而是用结构体声明一个相同签名的方法。\n这种实现模式被称为”鸭子类型“，Python 中的接口也是类似的鸭子类型。\n\n详细介绍可以参考这篇：Python 中的面向接口编程\n接口当然也是可以扩展的，类似于 struct 中的嵌套：\ntype DiService interface &#123;\tDi()&#125;//定义一个接口type RunService interface &#123;\tDiService\tRun()&#125;\n\n\n得益于 Go 的强类型，刚才的 struct 也得实现 DiService 这个接口才能编译通过。\n总结到这里应该是能理解官方所说的 Yes and No. 的含义了；Go 对面向对象的语法不像 Java 那么严苛，甚至整个语言中都找不到 object(对象) 这个关键词；但是利用 Go 里的其他特性也是能实现 OOP 的。\n是否为面向对象我觉得并不重要，主要目的是我们能写出易扩展好维护的代码。\n例如官方标准库中就有许多利用接口编程的例子：\n\n由于公司技术栈现在主要由 Go 为主，后续也会继续更新 Go 相关的实战经验；如果你也对学习 Go 感兴趣那不妨点个关注吧。\n","categories":["基础原理"],"tags":["Java","Go","OOP","鸭子类型"]},{"title":"一文搞懂参数传递原理","url":"/2021/01/11/basic/parameter-trans/","content":"\n前言最近一年多的时间陆续接触了一些对我来说陌生的语言，主要就是 Python 和 Go，期间为了快速实现需求只是依葫芦画瓢的撸代码；并没有深究一些细节与原理。\n就拿参数传递一事来说各个语言的实现细节各不相同，但又有类似之处；在许多新手入门时容易搞不清楚，导致犯一些低级错误。\n\n\nJava基本类型传递先拿我最熟悉的 Java 来说，我相信应该没人会写这样的代码：\n@Test  public void testBasic() &#123;      int a = 10;      modifyBasic(a);      System.out.println(String.format(&quot;最终结果 main a==%s&quot;, a));  &#125;  private void modifyBasic(int aa) &#123;      System.out.println(String.format(&quot;修改之前 aa==%s&quot;, aa));      aa = 20;      System.out.println(String.format(&quot;修改之后 aa==%s&quot;, aa));  &#125;\n\n输出结果:\n修改之前 aa==10修改之后 aa==20最终结果 main a==10\n\n不过从这段代码的目的来看应该是想要修改 a 的值，从直觉上来说如果修改成功也是能理解的。\n至于结果与预期不符合的根本原因是理解错了参数的值传递与引用传递。\n\n在这之前还是先明确下值传递与引用传递的区别：\n\n这里咱们先抛出结论，Java 采用的是值传递；这样也能解释为什么上文的例子没有成功修改原始数据。\n参考下图更好理解：\n\n当发生函数调用的时候 a 将自己传入到 modifyBasic 方法中，同时将自己的值复制了一份并赋值给了一个新变量 aa 从图中可以看出这是 a 和 aa 两个变量没有一毛钱关系，所以对 aa 的修改并不会影响到 a。\n有点类似于我把苹果给了老婆，她把苹果削好了；但我手里这颗并没有变化，因为她只是从餐盘里拿了一颗一模一样的苹果削好了。\n如果我想要她那颗，只能让她把削好的苹果给我；也就类似于使用方法的返回值。\na = modifyBasic(a);\n\n引用类型传递下面来看看引用类型的传递：\n\tprivate class Car&#123;      private String name;      public Car(String name) &#123;          this.name = name;      &#125;      @Override      public String toString() &#123;          return &quot;Car&#123;&quot; +                  &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; +                  &#x27;&#125;&#x27;;      &#125;  &#125;@Test  public void test01()&#123;      Car car1 = new Car(&quot;benz&quot;);      modifyCar1(car1);      System.out.println(String.format(&quot;最终结果 main car1==%s&quot;, car1));  &#125;  private void modifyCar1(Car car)&#123;      System.out.println(String.format(&quot;修改之前 car==%s&quot;, car));      car.name = &quot;bwm&quot;;      System.out.println(String.format(&quot;修改之后 car==%s&quot;, car));  &#125;\n\n在这个例子里先创建了一个 benz 的 car1，通过一个方法修改为 bmw 那最开始的  car1 会受到影响嘛？\n修改之前 car==Car&#123;name=&#x27;benz&#x27;&#125;修改之后 car==Car&#123;name=&#x27;bwm&#x27;&#125;最终结果 main car1==Car&#123;name=&#x27;bwm&#x27;&#125;\n\n结果可能会与部分人预期相反，这样的修改却是可以影响到原有数据的？这岂不是和值传递不符，看样子这是引用传递吧？\n别急，通过下图分析后大家就能明白：\n\n在 test01 方法中我们创建了一个 car1 的对象，该对象存放于堆内存中，假设内存地址为 0x1102 ，于是 car1 这个变量便应用了这块内存地址。\n当我们调用 modifyCar1 这个方法的时候会在该方法栈中创建一个变量 car ,接下来重点到了：\n这个 car 变量是由原本的入参 car1 复制而来，所以它所对应的堆内存依然是 0x1102；\n所以当我们通过 car 这个变量修改了数据后，本质上修改的是同一块堆内存中的数据。从而原本引用了这块内存地址的 car1 也能查看到对应的变化。\n这里理解起来可能会比较绕，但我们记住一点就行：\n传递引用类型的数据时，传递的并不是引用本身，依然是值；只是这个值 是内存地址罢了。\n因为把相同的内存地址传过去了，所以对数据的操作依然会影响到外部。\n所以同理，类似于这样的代码也会影响到外部原始数据：\n@Test   public void testList()&#123;       List&lt;Integer&gt; list = new ArrayList&lt;&gt;();       list.add(1);       addList(list);       System.out.println(list);   &#125;   private void addList(List&lt;Integer&gt; list) &#123;       list.add(2);   &#125;   [1, 2]\n\n\n那如果是这样的代码：\n@Test   public void test02()&#123;       Car car1 = new Car(&quot;benz&quot;);       modifyCar(car1);       System.out.println(String.format(&quot;最终结果 main car1==%s&quot;, car1));   &#125;   private void modifyCar(Car car2) &#123;       System.out.println(String.format(&quot;修改之前 car2==%s&quot;, car2));       car2 = new Car(&quot;bmw&quot;);       System.out.println(String.format(&quot;修改之后 car2==%s&quot;, car2));   &#125;\n\n假设 Java 是引用传递那最终的结果应该是打印 bmw 才对。\n修改之前 car2==Car&#123;name=&#x27;benz&#x27;&#125;修改之后 car2==Car&#123;name=&#x27;bmw&#x27;&#125;最终结果 main car1==Car&#123;name=&#x27;benz&#x27;&#125;\n\n从结果又能佐证这里依然是值传递。\n\n如果是引用传递，原本的 0x1102 应该是被直接替换为新创建的 0x1103 才对；而实际情况如上图所示，car2 直接重新引用了一个对象，两个对象之间互不干扰。\nGo相对于 Java 来说  Go 的用法又有所不同，不过我们也可以先得出结论：\nGo语言的参数也是值传递。\n在 Go 语言中数据类型主要有以下两种：\n\n值类型与引用类型；\n值类型先以值类型举例：\nfunc main() &#123;\ta :=10\tmodifyValue(a)\tfmt.Printf(&quot;最终 a=%v&quot;, a)&#125;func modifyValue(a int) &#123;\ta = 20&#125;输出：最终 a=10\n\n\n函数调用过程与之前的 Java 类似，本质上传递到函数中的值也是  a  的拷贝，所以对其的修改不会影响到原始数据。\n当我们把代码稍加修改：\nfunc main() &#123;\ta :=10\tfmt.Printf(&quot;传递之前a的内存地址%p \\n&quot;, &amp;a)\tmodifyValue(&amp;a)\tfmt.Printf(&quot;最终 a=%v&quot;, a)&#125;\tfunc modifyValue(a *int) &#123;\tfmt.Printf(&quot;传递之后a的内存地址%p \\n&quot;, &amp;a)\t*a = 20&#125;传递之前a的内存地址0xc0000b4040 传递之后a的内存地址0xc0000ae020最终 a=20\n\n从结果来看最终 a 的值是被方法修改了，这点便是 Go 与 Java 很大的不同点：\n在 Go 中存在着指针的概念，我们可以将变量通过指针的方式传递到不同的方法中，在方法里便可通过这个指针访问甚至修改原始数据。\n那这么一看不就是引用传递嘛？\n其实不然，我们仔细看看刚才的输出会发现参数传递前后的内存地址并不相同。\n传递之前a的内存地址0xc0000b4040 传递之后a的内存地址0xc0000ae020\n\n这也恰好论证了值传递，因为这里实际传递的是指针的拷贝。\n也就是说 modifyValue 方法中的参数与入参的&amp;a都是同一块内存的指针，但指针本身也是需要内存来存放的，所以在方法调用过程中新建了一个指针 a ，从而导致他们的内存地址不同。\n虽然内存地址不同，但指向的数据都是同一块，所以方法内修改后原始数据也受到了影响。\n引用类型对于 map slice channel 这类引用类型又略有不同：\nfunc main() &#123;\tvar personList = []string&#123;&quot;张三&quot;,&quot;李四&quot;&#125;\tmodifySlice(personList)\tfmt.Printf(&quot;slice=%v \\n&quot;, personList)&#125;func modifySlice(personList []string) &#123;\tpersonList[1] = &quot;王五&quot;&#125;slice=[张三 王五]\n\n最终我们会发现原始数据也被修改了，但我们并没有传递指针；同样的特性也适用于 map 。\n但其实我们查看 slice 的源码会发现存放数据的 array 就是指针类型：\ntype slice struct &#123;\tarray unsafe.Pointer\tlen   int\tcap   int&#125;\n\n所以我们可以直接对数据进行修改，相当于间接的带了指针。\n\n使用建议那我们在什么时候使用指针呢？有以下几点建议：\n\n如果参数是基本的值类型，比如 int,float 建议直接传值。\n如果需要修改基本的值类型，那只能是指针；但考虑到代码可读性还是建议将修改后的值返回用于重新赋值。\n数据量较大时建议使用指针，减少不必要的值拷贝。（具体多大可以自行判断）\n\nPython在 Python 中变量是否可变是影响参数传递的重要因素：\n\n如上图所示，bool int float 这些不可变类型在参数传递过程中是不能修改原始数据的。\nif __name__ == &#x27;__main__&#x27;:\t\tx = 1    modify(x)    print(&#x27;最终 x=&#123;&#125;&#x27;.format(x))\tdef modify(val):    val = 2最终 x=1\n\n原理与 Java Go中类似，是基于值传递的，这里就不再复述。\n这里重点看看可变数据类型在参数传递中的过程：\nif __name__ == &#x27;__main__&#x27;:\t\tx = [1]    modify(x)    print(&#x27;最终 x=&#123;&#125;&#x27;.format(x))\tdef modify(val):    val.append(2)最终 x=[1, 2]\n\n最终数据受到了影响，那么就表明这是引用传递嘛？再看个例子试试：\nif __name__ == &#x27;__main__&#x27;:\t\tx = [1]    modify(x)    print(&#x27;最终 x=&#123;&#125;&#x27;.format(x))\tdef modify(val):    val = [1, 2, 3]最终 x=[1]\n\n显而易见这并不是引用传递，如果是引用传递最终 x 应当等于 [1, 2 ,3] 。\n从结果来看这个传递过程非常类似 Go 中的指针传递，val 拿到的也是 x 这个参数内存地址的拷贝；他们都指向了同一块内存地址。\n所以对这块数据的修改本质上改的是同一份数据，但一旦重新赋值就会创建一块新的内存从而不会影响到原始数据。\n\n与 Java 中的上图类似。\n所以总结下：\n\n对于不可变数据：在参数传递时传递的是值，对参数的修改不会影响到原有数据。\n对于可变数据：传递的是内存地址的拷贝，对参数的操作会影响到原始数据。\n\n\n这么说来这三种都是值传递了，那有没有引用传递的语言呢？\n当然，C++是支持引用传递的：\n#include &lt;iostream&gt;using namespace std; class Box&#123;   public:      double len;&#125;;void modify(Box&amp; b); int main ()&#123;\tBox b1;\tb1.len=100;\tcout &lt;&lt; &quot;调用前，b1 的值：&quot; &lt;&lt; b1.len &lt;&lt; endl;\tmodify(b1);\tcout &lt;&lt; &quot;调用后，b1 的值：&quot; &lt;&lt; b1.len &lt;&lt; endl;\treturn 0;&#125; void modify(Box&amp; b)&#123;\tb.len=10.0;\tBox b2;\tb2.len = 999;\tb = b2;  \treturn;&#125;调用前，b1 的值：100调用后，b1 的值：999\n\n可以看到把新对象 b2 赋值给入参 b 后是会影响到原有数据的。\n总结其实这几种语言看下来会发现他们中也有许多相似之处，所以通常我们在掌握一门语言后也能快速学习其他语言。\n但往往是这些基础中的基础最让人忽略，希望大家在日常编码时能够考虑到这些基础知识多想想一定会写出更漂亮的代码(bug)。\n","categories":["基础原理"],"tags":["Java","Python","Golang"]},{"title":"Python 中的面向接口编程","url":"/2021/01/14/basic/python-oop/","content":"\n前言”面向接口编程“写 Java 的朋友耳朵已经可以听出干茧了吧，当然这个思想在 Java 中非常重要，甚至几乎所有的编程语言都需要，毕竟程序具有良好的扩展性、维护性谁都不能拒绝。\n\n\n最近无意间看到了我刚开始写 Python 时的部分代码，当时实现的需求有个很明显的特点：\n\n不同对象具有公共的行为能力，但具体每个对象的实现方式又各不相同。\n\n说人话就是商户需要接入平台，接入的步骤相同，但具体实现不同。\n作为一个”资深“ Javaer，需求还没看完我就洋洋洒洒的把各个实现类写好了：\n\n当然最终也顺利实现需求，甚至把组里一个没写过 Java 的大哥唬的一愣一愣的，直呼牛逼。\n\n不过事后也给我吐槽：\n\n你这设计是不错，但是感觉好复杂，跟代码时要找到真正的业务逻辑（实现类）得绕几圈。\n\n截止目前 Python 写多了，我总算是能总结他的感受：就是不够 Pythonic。\n虽说 Python 没有类似 Java 这样的 Interface 特性，但作为面向对象的高级语言也是支持继承的；\n在这里我们也可以利用继承的特性来实现面向接口编程：\nclass Car:    def run(self):        passclass Benz(Car):    def run(self):        print(&quot;benz run&quot;)class BMW(Car):    def run(self):        print(&quot;bwm run&quot;)def run(car):    car.run()if __name__ == &quot;__main__&quot;:    benz = Benz()    bmw = BMW()    run(benz)    run(bmw)\n\n代码非常简单，在 Python 中也没有类似于 Java 中的 extends 关键字，只需要在类声明末尾用括号包含基类即可。\n这样在每个子类中就能单独实现业务逻辑，方便扩展和维护。\n类型检查由于 Python 作为一个动态类型语言，无法做到 Java 那样在编译期间校验一个类是否完全实现了某个接口的所有方法。\n为此 Python 提供了解决办法，那就是 abc(Abstract Base Classes) ，当我们将基类用 abc 声明时就能近似做到：\nimport abcclass Car(abc.ABC):    @abc.abstractmethod    def run(self):        passclass Benz(Car):    def run(self):        print(&quot;benz run&quot;)class BMW(Car):    passdef run(car):    car.run()if __name__ == &quot;__main__&quot;:    benz = Benz()    bmw = BMW()    run(benz)    run(bmw)\n\n一旦有类没有实现方法时，运行期间便会抛出异常：\nbmw = BMW()TypeError: Can&#x27;t instantiate abstract class BMW with abstract methods run\n\n虽然无法做到在运行之前（毕竟不需要编译）进行校验，但有总比没有好。\n鸭子类型以上两种方式看似已经毕竟优雅的实现面向接口编程了，但实际上也不够 Pythonic。\n在继续之前我们先聊聊接口的本质到底是什么？\n在 Java 这类静态语言中面向接口编程是比较麻烦的，也就是我们常说的子类向父类转型，因此需要编写额外的代码。\n带来的好处也是显而易见，只需要父类便可运行。\n但我们也不必过于执着于接口，它本身只是一个协议、规范，并不特指 Java 中的 Interface，甚至有些语言压根没有这个关键字。\n动态语言的特性也不需要强制校验是否实现了方法。\n在 Python 中我们可以利用鸭子类型来优雅的实现面向接口编程。\n在这之前先了解下鸭子类型，借用维基百科的说法：\n\n“当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。”\n\n我用大白话翻译下就是：\n即便两个完全不想干的类，如果他们都实现了相同的方法，那就可以把他们当做同一类型的类来使用。\n举个简单例子：\nclass Order:    def create(self):        passclass User:    def create(self):        passdef create(obj):    obj.create()if __name__ == &quot;__main__&quot;:    order = Order()    user = User()    create(order)    create(user)\n\n这里的 order 和 user 本身完全没有关系，只是他们都有相同方法，又得益于动态语言没法校验类型的特点，所以完全可以在运行的时候认为他们是同一种类型。\n因此基于鸭子类型，之前的代码我们可以稍作简化：\nclass Car:    def run(self):        passclass Benz:    def run(self):        print(&quot;benz run&quot;)class BMW:    def run(self):        print(&quot;bwm run&quot;)def run(car):    car.run()if __name__ == &quot;__main__&quot;:    benz = Benz()    bmw = BMW()    run(benz)    run(bmw)\n\n因为在鸭子类型中我们在意的是它的行为，而不是他们的类型；所以完全可以不用继承便可以实现面向接口编程。\n总结我觉得平时没有接触过动态类型语言的朋友，在了解完这些之后会发现新大陆，就像是 Python 老手第一次使用 Java 时；虽然觉得语法啰嗦，但也会羡慕它的类型检查、参数验证这类特点。\n动静语言之争这里不做讨论了，各有各的好，鞋好不好穿只有自己知道。\n随便提一下其实不止动态语言具备鸭子类型，有些静态语言也能玩这个骚操作，感兴趣下次再介绍。\n","categories":["基础原理"],"tags":["OOP","鸭子类型","Python"]},{"title":"词法分析器的应用","url":"/2020/03/23/compilation/Lexer/","content":"\n前言最近大部分时间都在撸 Python，其中也会涉及到将数据库表转换为 Python 中 ORM 框架的 Model，但我们并没有找到一个合适的工具来做这个意义不大的”体力活“，所以每次新建表后大家都是根据自己的表结构手写一遍 Model。\n一两张表还好，一旦 10 几张表都要写一遍时那痛苦只有自己知道；这时程序员的 slogan 再次印证：一切毫无意义的体力劳动终将被计算机取代。\n\n\nintellij plugin既然没有现成的工具那就自己写一个吧，演示效果如下：\n考虑到我们主要是用 PyCharm 开发，正好 jetbrains 也提供了 SDK 用于开发插件，所以 UI 层面可以不用额外考虑了。\n使用流程很简单，只需要导入 DDL 语句就可以生成 Python 所需要的 Model 代码。\n例如导入以下 DDL：\nCREATE TABLE `user` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `userName` varchar(20) DEFAULT NULL COMMENT &#x27;用户名&#x27;,  `password` varchar(100) DEFAULT NULL COMMENT &#x27;密码&#x27;,  `roleId` int(11) DEFAULT NULL COMMENT &#x27;角色ID&#x27;,  PRIMARY KEY (`id`),  ) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8\n\n便会生成对应的 Python 代码：\nclass User(db.Model):    __tablename__ = &#x27;user&#x27;    id = db.Column(db.Integer, primary_key=True, autoincrement=True)    userName = db.Column(db.String)  # 用户名    password = db.Column(db.String)  # 密码    roleId = db.Column(db.Integer)  # 角色ID\n\n\n词法解析仔细对比源文件及目标代码会很容易找出规律，无非就是解析出表名、字段、及字段的属性（是否为主键、类型、长度），最后再转换为 Python 所需要的模板即可。\n在我动手之前我认为是非常简单的，无非就是解析字符串，但实际上手后发现不是那么回事；主要是有以下几个问题：\n\n如何识别出表名称？\n同样的如何识别出字段名称，同时还得关联上该字段的类型、长度、注释。\n如何识别出主键？\n\n总结一句话，如何通过一系列规则识别出一段字符串中的关键信息，这同样也是 MySQL Server 所做的事情。\n在开始真正解析 DDL 之前，先来看下一段简单的脚本如何解析：\nx = 20\n\n按照我们平时开发的经验，这条语句分为以下几部分：\n\nx 表示变量\n= 表示赋值符号\n20 表示赋值结果\n\n所以我们对这段脚本的解析结果应当为：\nVAR \t xGE \t    =VAL \t 100\n\n这个解析过程在编译原理中称为”词法解析“，可能大家听到编译原理这几个字就头大（我也是）；对于刚才那段脚本我们可以编写一个非常简单的词法解析器生成这样的结果。\n状态迁移再开始之前先捋一下思路，可以看到上文的结果中通过 VAR 表示变量、GE 表示赋值符号 ”&#x3D;“、VAL 表示赋值结果，现在需要重点记住这三个状态。\n在依次读取字符解析时，程序就是在这几个状态中来回切换，如下图：\n\n默认为初始状态。\n当字符为字母时进入 VAR 状态。\n当字符为 ”&#x3D;“ 符号时进入 GE 状态。\n\n\n同理，当不满足这几个状态时候又会回到初始从而再次确认新的状态。\n光看图有点抽象，直接来看核心代码：\npublic class Result&#123;    public TokenType tokenType ;    public StringBuilder text = new StringBuilder();&#125;\n\n首先定义了一个结果类，收集最终的解析结果；其中的 TokenType 就对应了图中的三种状态，简单的用枚举值来表示。\npublic enum TokenType &#123;    INIT,    VAR,    GE,    VAL&#125;\n\n首先对应到第一张图：初始化状态。\n需要对当前解析的字符定义一个 TokenType：\n和图中描述的流程一致，判断当前字符给定一个状态即可。\n接着对应到第二张图：状态之间的转换。\n\n会根据不同的状态进入不同的 case，在不同的 case 中判断是否应当跳转到其他状态（进入 INIT 状态后会重新生成状态）。\n举个例子： x = 20:\n首选会进入 VAR 状态，接着下一个字符为空格，自然在 38 行中重新进入初始状态，导致再次确定下一个字符 = 进入 GE 状态。\n当脚本为 ab = 30:第一个字符为 a 也是进入 VAR 状态，第二个字符为 b，依然为字母，所以进入 36 行，状态不会改变，同时将 b 这个字符追加进来；后续步骤就和上一个例子一致了。\n多说无益，建议大家自己跑一下单测就会明白：https://github.com/crossoverJie/sqlalchemy-transfer/blob/master/src/test/java/top/crossoverjie/plugin/core/lab/TestLexerTest.java\nDDL 解析简单的解析完成后来看看 DDL 这样的脚本应当如何解析：\nCREATE TABLE `user` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `userName` varchar(20) DEFAULT NULL COMMENT &#x27;用户名&#x27;,  `password` varchar(100) DEFAULT NULL COMMENT &#x27;密码&#x27;,  `roleId` int(11) DEFAULT NULL COMMENT &#x27;角色ID&#x27;,  PRIMARY KEY (`id`),  ) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8\n\n原理类似，首先还是要看出规律（也就是语法）：\n\n表名是第一行语句，同时以 CREATE TABLE 开头。\n每一个字段的信息（名称、类型、长度、备注）都是以 “&#96;” 符号开头 “,” 结尾。 \n主键是以 PRIMART 字符串开头的字段，以 ) 结尾。\n\n根据我们需要解析的数据种类，我这里定义了这个枚举：\n然后在初始化类型时进行判断赋值：\n由于需要解析的数据不少，所以这里的判断条件自然也就多了。\n递归解析针对于 DDL 的语法规则，我们这里还有需要有特殊处理的地方；比如解析具体字段信息时如何关联起来？\n举个例子：\n`userName` varchar(20) DEFAULT NULL COMMENT &#x27;用户名&#x27;,`password` varchar(100) DEFAULT NULL COMMENT &#x27;密码&#x27;,\n\n这里我们解析出来的数据得有一个映射关系：\n\n所以我们只能一个字段的全部信息解析完成并且关联好之后才能解析下一个字段。\n于是这里我采用了递归的方式进行解析（不一定是最好的，欢迎大家提出更优的方案）。\n&#125; else if (value == &#x27;`&#x27; &amp;&amp; pStatus == Status.BASE_INIT) &#123;    result.tokenType = DDLTokenType.FI;    result.text.append(value);&#125; \n\n当当前字符为 ”&#96;“ 符号时，将状态置为 “FI”(FieldInfo)，同时当解析到为 “,” 符号时便进入递归处理。\n\n可以理解为将这一段字符串单独提取出来处理：\n`userName` varchar(20) DEFAULT NULL COMMENT &#x27;用户名&#x27;,\n\n接着再将这段字符递归调用当前方法再次进行解析，这时便按照字段名称、类型、长度、注释的规则解析即可。\n同时既然存在递归，还需要将子递归的数据关联起来，所以我在返回结果中新增了一个 pid 的字段，这个也容易理解。\n默认值为 0，一旦递归后便自增 +1，保证每次递归的数据都是唯一的。\n用同样的方法在解析主键时也是先将整个字符串提取出来:\nPRIMARY KEY (`id`)\n\n只不过是 “P” 打头 “)” 结尾。\n&#125; else if (value == &#x27;P&#x27; &amp;&amp; pStatus == Status.BASE_INIT) &#123;    result.tokenType = DDLTokenType.P_K;    result.text.append(value);&#125; \n\n\n也是将整段字符串递归解析，再递归的过程中进行状态切换 P_K ---&gt; P_K_V 最终获取到主键。\n\n\n所以通过对刚才那段 DDL 解析得到的结果如下：\n\n这样每个字段也通过了 pid 进行了区分关联。\n所以现在只需要对这个词法解析器进行封装，便可以提供一个简单的 API 来获取表中的数据了。\n\n总结到此整个词法解析器的全部内容都已经完成了，虽然实现的是一个小功能，但我自己花的时间可不少，其中光复习编译原理就让人头疼。\n但这还只是整个编译语言知识点的冰山一角，后续还有语法、语义、中间、目标代码等一系列内容，都是一个比一个难啃。\n其实我相信大多数人和我想法一样，这个东西太底层而且枯燥，真正从事这方面工作的也都是凤毛麟角，所以花这时间干啥呢？\n所以我也决定这个弄完后就弃坑啦。\n\n\n哈哈，开个玩笑，或许有生之年自己也能实现一门编程语言，当老了和儿子吹牛时也能有点资本。\n本文所有源码及插件地址：\nhttps://github.com/crossoverJie/sqlalchemy-transfer\n大家看完记得点赞分享一键三连哦\n","categories":["编译原理"],"tags":["Java","递归","DDL"]},{"title":"『并发包入坑指北』之阻塞队列","url":"/2019/04/09/concurrent/ArrayBlockingQueue/","content":"\n前言较长一段时间以来我都发现不少开发者对 jdk 中的 J.U.C（java.util.concurrent）也就是 Java 并发包的使用甚少，更别谈对它的理解了；但这却也是我们进阶的必备关卡。\n之前或多或少也分享过相关内容，但都不成体系；于是便想整理一套与并发包相关的系列文章。\n其中的内容主要包含以下几个部分：\n\n根据定义自己实现一个并发工具。\nJDK 的标准实现。\n实践案例。\n\n\n\n基于这三点我相信大家对这部分内容不至于一问三不知。\n既然开了一个新坑，就不想做的太差；所以我打算将这个列表下的大部分类都讲到。\n\n所以本次重点讨论 ArrayBlockingQueue。\n自己实现在自己实现之前先搞清楚阻塞队列的几个特点：\n\n基本队列特性：先进先出。\n写入队列空间不可用时会阻塞。\n获取队列数据时当队列为空时将阻塞。\n\n实现队列的方式多种，总的来说就是数组和链表；其实我们只需要搞清楚其中一个即可，不同的特性主要表现为数组和链表的区别。\n这里的 ArrayBlockingQueue 看名字很明显是由数组实现。\n我们先根据它这三个特性尝试自己实现试试。\n初始化队列我这里自定义了一个类：ArrayQueue，它的构造函数如下：\npublic ArrayQueue(int size) &#123;    items = new Object[size];&#125;\n\n很明显这里的 items 就是存放数据的数组；在初始化时需要根据大小创建数组。\n\n写入队列写入队列比较简单，只需要依次把数据存放到这个数组中即可，如下图：\n\n但还是有几个需要注意的点：\n\n队列满的时候，写入的线程需要被阻塞。\n写入过队列的数量大于队列大小时需要从第一个下标开始写。\n\n先看第一个队列满的时候，写入的线程需要被阻塞，先来考虑下如何才能使一个线程被阻塞，看起来的表象线程卡住啥事也做不了。\n有几种方案可以实现这个效果:\n\nThread.sleep(timeout)线程休眠。\nobject.wait() 让线程进入 waiting 状态。\n\n\n当然还有一些 join、LockSupport.part 等不在本次的讨论范围。\n\n阻塞队列还有一个非常重要的特性是：当队列空间可用时（取出队列），写入线程需要被唤醒让数据可以写入进去。\n所以很明显Thread.sleep(timeout)不合适，它在到达超时时间之后便会继续运行；达不到空间可用时才唤醒继续运行这个特点。\n其实这样的一个特点很容易让我们想到 Java 的等待通知机制来实现线程间通信；更多线程见通信的方案可以参考这里：深入理解线程通信\n所以我这里的做法是，一旦队列满时就将写入线程调用 object.wait() 进入 waiting 状态，直到空间可用时再进行唤醒。\n/** * 队列满时的阻塞锁 */private Object full = new Object();/** * 队列空时的阻塞锁 */private Object empty = new Object();\n\n\n所以这里声明了两个对象用于队列满、空情况下的互相通知作用。\n在写入数据成功后需要使用 empty.notify()，这样的目的是当获取队列为空时，一旦写入数据成功就可以把消费队列的线程唤醒。\n\n这里的 wait 和 notify 操作都需要对各自的对象使用 synchronized 方法块，这是因为 wait 和 notify 都需要获取到各自的锁。\n\n消费队列上文也提到了：当队列为空时，获取队列的线程需要被阻塞，直到队列中有数据时才被唤醒。\n\n代码和写入的非常类似，也很好理解；只是这里的等待、唤醒恰好是相反的，通过下面这张图可以很好理解：\n\n总的来说就是：\n\n写入队列满时会阻塞直到获取线程消费了队列数据后唤醒写入线程。\n消费队列空时会阻塞直到写入线程写入了队列数据后唤醒消费线程。\n\n测试先来一个基本的测试：单线程的写入和消费。\n\n3123123412345\n\n通过结果来看没什么问题。\n\n当写入的数据超过队列的大小时，就只能消费之后才能接着写入。\n\n2019-04-09 16:24:41.040 [Thread-0] INFO  c.c.concurrent.ArrayQueueTest - [Thread-0]1232019-04-09 16:24:41.040 [main] INFO  c.c.concurrent.ArrayQueueTest - size=32019-04-09 16:24:41.047 [main] INFO  c.c.concurrent.ArrayQueueTest - 12342019-04-09 16:24:41.048 [main] INFO  c.c.concurrent.ArrayQueueTest - 123452019-04-09 16:24:41.048 [main] INFO  c.c.concurrent.ArrayQueueTest - 123456\n\n从运行结果也能看出只有当消费数据后才能接着往队列里写入数据。\n\n\n\n而当没有消费时，再往队列里写数据则会导致写入线程被阻塞。\n并发测试\n三个线程并发写入300条数据，其中一个线程消费一条。\n=====0299\n\n最终的队列大小为 299，可见线程也是安全的。\n\n由于不管是写入还是获取方法里的操作都需要获取锁才能操作，所以整个队列是线程安全的。\n\nArrayBlockingQueue下面来看看 JDK 标准的 ArrayBlockingQueue 的实现，有了上面的基础会更好理解。\n初始化队列\n看似要复杂些，但其实逐步拆分后也很好理解：\n第一步其实和我们自己写的一样，初始化一个队列大小的数组。\n第二步初始化了一个重入锁，这里其实就和我们之前使用的 synchronized 作用一致的；\n只是这里在初始化重入锁的时候默认是非公平锁，当然也可以指定为 true 使用公平锁；这样就会按照队列的顺序进行写入和消费。\n\n更多关于 ReentrantLock 的使用和原理请参考这里：ReentrantLock 实现原理\n\n三四两步则是创建了 notEmpty notFull 这两个条件，他的作用于用法和之前使用的 object.wait/notify 类似。\n这就是整个初始化的内容，其实和我们自己实现的非常类似。\n写入队列\n其实会发现阻塞写入的原理都是差不多的，只是这里使用的是 Lock 来显式获取和释放锁。\n同时其中的 notFull.await();notEmpty.signal(); 和我们之前使用的 object.wait/notify 的用法和作用也是一样的。\n当然它还是实现了超时阻塞的 API。\n\n也是比较简单，使用了一个具有超时时间的等待方法。 \n消费队列再看消费队列：\n\n也是差不多的，一看就懂。\n而其中的超时 API 也是使用了 notEmpty.awaitNanos(nanos) 来实现超时返回的，就不具体说了。\n实际案例说了这么多，来看一个队列的实际案例吧。\n背景是这样的：\n\n有一个定时任务会按照一定的间隔时间从数据库中读取一批数据，需要对这些数据做校验同时调用一个远程接口。\n\n简单的做法就是由这个定时任务的线程去完成读取数据、消息校验、调用接口等整个全流程；但这样会有一个问题：\n假设调用外部接口出现了异常、网络不稳导致耗时增加就会造成整个任务的效率降低，因为他都是串行会互相影响。\n所以我们改进了方案：\n\n其实就是一个典型的生产者消费者模型：\n\n生产线程从数据库中读取消息丢到队列里。\n消费线程从队列里获取数据做业务逻辑。\n\n这样两个线程就可以通过这个队列来进行解耦，互相不影响，同时这个队列也能起到缓冲的作用。\n但在使用过程中也有一些小细节值得注意。\n因为这个外部接口是支持批量执行的，所以在消费线程取出数据后会在内存中做一个累加，一旦达到阈值或者是累计了一个时间段便将这批累计的数据处理掉。\n但由于开发者的大意，在消费的时候使用的是 queue.take() 这个阻塞的 API；正常运行没啥问题。\n可一旦原始的数据源，也就是 DB 中没数据了，导致队列里的数据也被消费完后这个消费线程便会被阻塞。\n这样上一轮积累在内存中的数据便一直没机会使用，直到数据源又有数据了，一旦中间间隔较长时便可能会导致严重的业务异常。\n所以我们最好是使用 queue.poll(timeout) 这样带超时时间的 api，除非业务上有明确的要求需要阻塞。\n这个习惯同样适用于其他场景，比如调用 http、rpc 接口等都需要设置合理的超时时间。\n总结关于 ArrayBlockingQueue 的相关分享便到此结束，接着会继续更新其他并发容器及并发工具。\n对本文有任何相关问题都可以留言讨论。\n本文涉及到的所有源码：\nhttps://github.com/crossoverJie/JCSprout/blob/master/src/main/java/com/crossoverjie/concurrent/ArrayQueue.java\n你的点赞与分享是对我最大的支持\n","categories":["并发"],"tags":["concurrent","ArrayBlockingQueue"]},{"title":"『并发包入坑指北』之向大佬汇报任务","url":"/2019/04/28/concurrent/CountDownLatch/","content":"\n前言在面试过程中聊到并发相关的内容时，不少面试官都喜欢问这类问题：\n\n当 N 个线程同时完成某项任务时，如何知道他们都已经执行完毕了。\n\n这也是本次讨论的话题之一，所以本篇为『并发包入坑指北』的第二篇；来聊聊常见的并发工具。\n\n\n自己实现其实这类问题的核心论点都是：如何在一个线程中得知其他线程是否执行完毕。\n假设现在有 3 个线程在运行，需要在主线程中得知他们的运行结果；可以分为以下几步：\n\n定义一个计数器为 3。\n每个线程完成任务后计数减一。\n一旦计数器减为 0 则通知等待的线程。\n\n所以也很容易想到可以利用等待通知机制来实现，和上文的『并发包入坑指北』之阻塞队列的类似。\n按照这个思路自定义了一个 MultipleThreadCountDownKit 工具，构造函数如下：\n\n考虑到并发的前提，这个计数器自然需要保证线程安全，所以采用了 AtomicInteger。\n所以在初始化时需要根据线程数量来构建对象。\n计数器减一当其中一个业务线程完成后需要将这个计数器减一，直到减为0为止。\n/** * 线程完成后计数 -1 */public void countDown()&#123;    if (counter.get() &lt;= 0)&#123;        return;    &#125;    int count = this.counter.decrementAndGet();    if (count &lt; 0)&#123;        throw new RuntimeException(&quot;concurrent error&quot;) ;    &#125;    if (count == 0)&#123;        synchronized (notify)&#123;            notify.notify();        &#125;    &#125;&#125;\n\n利用 counter.decrementAndGet() 来保证多线程的原子性，当减为 0 时则利用等待通知机制来 notify 其他线程。\n等待所有线程完成而需要知道业务线程执行完毕的其他线程则需要在未完成之前一直处于等待状态，直到上文提到的在计数器变为 0 时得到通知。\n/** * 等待所有的线程完成 * @throws InterruptedException */public void await() throws InterruptedException &#123;    synchronized (notify)&#123;        while (counter.get() &gt; 0)&#123;            notify.wait();        &#125;        if (notifyListen != null)&#123;            notifyListen.notifyListen();        &#125;    &#125;&#125;\n\n原理也很简单，一旦计数器还存在时则会利用 notify 对象进行等待，直到被业务线程唤醒。\n同时这里新增了一个通知接口可以自定义实现唤醒后的一些业务逻辑，后文会做演示。\n并发测试主要就是这两个函数，下面来做一个演示。\n\n\n初始化了三个计数器的并发工具 MultipleThreadCountDownKit\n创建了三个线程分别执行业务逻辑，完毕后执行 countDown()。\n线程 3 休眠了 2s 用于模拟业务耗时。\n主线程执行 await() 等待他们三个线程执行完毕。\n\n\n通过执行结果可以看出主线程会等待最后一个线程完成后才会退出；从而达到了主线程等待其余线程的效果。\nMultipleThreadCountDownKit multipleThreadKit = new MultipleThreadCountDownKit(3);multipleThreadKit.setNotify(() -&gt; LOGGER.info(&quot;三个线程完成了任务&quot;));\n\n也可以在初始化的时候指定一个回调接口，用于接收业务线程执行完毕后的通知。\n\n当然和在主线程中执行这段逻辑效果是一样的（和执行 await() 方法处于同一个线程）。\nCountDownLatch当然我们自己实现的代码没有经过大量生产环境的验证，所以主要的目的还是尝试窥探官方的实现原理。\n所以我们现在来看看 juc 下的 CountDownLatch 是如何实现的。\n\n通过构造函数会发现有一个 内部类 Sync，他是继承于 AbstractQueuedSynchronizer ；这是 Java 并发包中的基础框架，都可以单独拿来讲了，所以这次重点不是它，今后我们再着重介绍。\n\n这里就可以把他简单理解为提供了和上文类似的一个计数器及线程通知工具就行了。\n\ncountDown其实他的核心逻辑和我们自己实现的区别不大。\npublic void countDown() &#123;    sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123;    if (tryReleaseShared(arg)) &#123;        doReleaseShared();        return true;    &#125;    return false;&#125;\n\n利用这个内部类的 releaseShared 方法，我们可以理解为他想要将计数器减一。\n\n看到这里有没有似曾相识的感觉。\n\n没错，在 JDK1.7 中的 AtomicInteger 自减就是这样实现的（利用 CAS 保证了线程安全）。\n只是一旦计数器减为 0 时则会执行 doReleaseShared 唤醒其他的线程。\n\n这里我们只需要关心红框部分（其他的暂时不用关心，这里涉及到了 AQS 中的队列相关），最终会调用 LockSupport.unpark 来唤醒线程；就相当于上文调用 object.notify()。\n所以其实本质上还是相同的。\nawait其中的 await() 也是借用 Sync 对象的方法实现的。\npublic void await() throws InterruptedException &#123;    sync.acquireSharedInterruptibly(1);&#125;public final void acquireSharedInterruptibly(int arg)        throws InterruptedException &#123;    if (Thread.interrupted())        throw new InterruptedException();    //判断计数器是否还未完成        if (tryAcquireShared(arg) &lt; 0)        doAcquireSharedInterruptibly(arg);&#125;protected int tryAcquireShared(int acquires) &#123;    return (getState() == 0) ? 1 : -1;&#125;\n\n一旦还存在未完成的线程时，则会调用 doAcquireSharedInterruptibly 进入阻塞状态。\n\nprivate final boolean parkAndCheckInterrupt() &#123;    LockSupport.park(this);    return Thread.interrupted();&#125;\n\n同样的由于这也是 AQS 中的方法，我们只需要关心红框部分；其实最终就是调用了 LockSupport.park 方法，也就相当于执行了 object.wait() 。\n\n所有的业务线程执行完毕后会在计数器减为 0 时调用 LockSupport.unpark 来唤醒线程。\n等待线程一旦计数器 &gt; 0 时则会利用 LockSupport.park 来等待唤醒。\n\n这样整个流程也就串起来了，它的使用方法也和上文的类似。\n\n就不做过多介绍了。\n实际案例同样的来看一个实际案例。\n在上一篇《一次分表踩坑实践的探讨》提到了对于全表扫描的情况下，需要利用多线程来提高查询效率。\n比如我们这里分为了 64 张表，计划利用 8 个线程来分别处理这些表的数据，伪代码如下：\nCountDownLatch count = new CountDownLatch(64);ConcurrentHashMap total = new ConcurrentHashMap();for(Integer i=0;i&lt;=63;i++)&#123;\texecutor.execute(new Runnable()&#123;\t\t@Override\t\tpublic void run()&#123;\t\t\tList value = queryTable(i);\t\t\ttotal.put(value,NULL);\t\t\tcount.countDown();\t\t&#125;\t&#125;) ;\t&#125;count.await();System.out.println(&quot;查询完毕&quot;);\n\n这样就可以实现所有数据都查询完毕后再做统一汇总；代码挺简单，也好理解（当然也可以使用线程池的 API）。\n总结CountDownLatch 算是 juc 中一个高频使用的工具，学会和理解他的使用会帮助我们更容易编写并发应用。\n文中涉及到的源码：\nhttps://github.com/crossoverJie/JCSprout/blob/master/src/main/java/com/crossoverjie/concurrent/communication/MultipleThreadCountDownKit.java\n你的点赞与分享是对我最大的支持\n","categories":["并发"],"tags":["concurrent","CountDownLatch"]},{"title":"线程池没你想的那么简单","url":"/2019/05/20/concurrent/threadpool-01/","content":"\n前言原以为线程池还挺简单的（平时常用，也分析过原理），这次是想自己动手写一个线程池来更加深入的了解它；但在动手写的过程中落地到细节时发现并没想的那么容易。结合源码对比后确实不得不佩服 Doug Lea 。\n我觉得大部分人直接去看 java.util.concurrent.ThreadPoolExecutor 的源码时都是看一个大概，因为其中涉及到了许多细节处理，还有部分 AQS 的内容，所以想要理清楚具体细节并不是那么容易。\n\n\n与其挨个分析源码不如自己实现一个简版，当然简版并不意味着功能缺失，需要保证核心逻辑一致。\n所以也是本篇文章的目的：\n\n自己动手写一个五脏俱全的线程池，同时会了解到线程池的工作原理，以及如何在工作中合理的利用线程池。\n\n再开始之前建议对线程池不是很熟悉的朋友看看这几篇：\n这里我截取了部分内容，也许可以埋个伏笔（坑）。\n\n\n\n具体请看这两个链接。\n\n如何优雅的使用和理解线程池\n线程池中你不容错过的一些细节\n\n由于篇幅限制，本次可能会分为上下两篇。\n创建线程池现在进入正题，新建了一个 CustomThreadPool 类,它的工作原理如下：\n\n简单来说就是往线程池里边丢任务，丢的任务会缓冲到队列里；线程池里存储的其实就是一个个的 Thread ，他们会一直不停的从刚才缓冲的队列里获取任务执行。\n流程还是挺简单。\n先来看看我们这个自创的线程池的效果如何吧：\n初始化了一个核心为3、最大线程数为5、队列大小为 4 的线程池。\n先往其中丢了 10 个任务，由于阻塞队列的大小为 4 ，最大线程数为 5 ，所以由于队列里缓冲不了最终会创建 5 个线程（上限）。\n过段时间没有任务提交后（sleep）则会自动缩容到三个线程（保证不会小于核心线程数）。\n构造函数来看看具体是如何实现的。\n下面则是这个线程池的构造函数：\n\n会有以下几个核心参数：\n\nminiSize 最小线程数，等效于 ThreadPool 中的核心线程数。\nmaxSize 最大线程数。\nkeepAliveTime 线程保活时间。\nworkQueue 阻塞队列。\nnotify 通知接口。\n\n大致上都和 ThreadPool 中的参数相同，并且作用也是类似的。\n需要注意的是其中初始化了一个 workers 成员变量：\n/** * 存放线程池 */private volatile Set&lt;Worker&gt; workers;public CustomThreadPool(int miniSize, int maxSize, long keepAliveTime,                        TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, Notify notify) &#123;       workers = new ConcurrentHashSet&lt;&gt;();&#125;\n\nworkers 是最终存放线程池中运行的线程，在 j.u.c 源码中是一个 HashSet 所以对他所有的操作都是需要加锁。\n我这里为了简便起见就自己定义了一个线程安全的 Set 称为 ConcurrentHashSet。\n\n其实原理也非常简单，和 HashSet 类似也是借助于 HashMap 来存放数据，利用其 key 不可重复的特性来实现 set ，只是这里的 HashMap 是用并发安全的 ConcurrentHashMap 来实现的。\n这样就能保证对它的写入、删除都是线程安全的。\n不过由于 ConcurrentHashMap 的 size() 函数并不准确，所以我这里单独利用了一个 AtomicInteger 来统计容器大小。\n创建核心线程往线程池中丢一个任务的时候其实要做的事情还蛮多的，最重要的事情莫过于创建线程存放到线程池中了。\n当然我们不能无限制的创建线程，不然拿线程池来就没任何意义了。于是 miniSize maxSize 这两个参数就有了它的意义。\n但这两个参数再哪一步的时候才起到作用呢？这就是首先需要明确的。\n\n从这个流程图可以看出第一步是需要判断是否大于核心线程数，如果没有则创建。\n\n结合代码可以发现在执行任务的时候会判断是否大于核心线程数，从而创建线程。\n\nworker.startTask() 执行任务部分放到后面分析。\n\n\n这里的 miniSize 由于会在多线程场景下使用，所以也用 volatile 关键字来保证可见性。\n队列缓冲\n结合上面的流程图，第二步自然是要判断队列是否可以存放任务（是否已满）。\n\n优先会往队列里存放。\n上至封顶\n一旦写入失败则会判断当前线程池的大小是否大于最大线程数，如果没有则继续创建线程执行。\n不然则执行会尝试阻塞写入队列（j.u.c 会在这里执行拒绝策略）\n以上的步骤和刚才那张流程图是一样的，这样大家是否有看出什么坑嘛？\n时刻小心\n从上面流程图的这两步可以看出会直接创建新的线程。\n这个过程相对于中间直接写入阻塞队列的开销是非常大的，主要有以下两个原因：\n\n创建线程会加锁，虽说最终用的是 ConcurrentHashMap 的写入函数，但依然存在加锁的可能。\n会创建新的线程，创建线程还需要调用操作系统的 API 开销较大。\n\n\n所以理想情况下我们应该避免这两步，尽量让丢入线程池中的任务进入阻塞队列中。\n\n执行任务任务是添加进来了，那是如何执行的？\n在创建任务的时候提到过 worker.startTask() 函数：\n/** * 添加任务，需要加锁 * @param runnable 任务 */private void addWorker(Runnable runnable) &#123;    Worker worker = new Worker(runnable, true);    worker.startTask();    workers.add(worker);&#125;\n\n也就是在创建线程执行任务的时候会创建 Worker 对象，利用它的 startTask() 方法来执行任务。\n所以先来看看 Worker 对象是长啥样的：\n\n其实他本身也是一个线程，将接收到需要执行的任务存放到成员变量 task 处。\n而其中最为关键的则是执行任务 worker.startTask() 这一步骤。\npublic void startTask() &#123;    thread.start();&#125;\n\n其实就是运行了 worker 线程自己，下面来看 run 方法。\n\n\n第一步是将创建线程时传过来的任务执行（task.run）,接着会一直不停的从队列里获取任务执行，直到获取不到新任务了。\n任务执行完毕后将内置的计数器 -1 ，方便后面任务全部执行完毕进行通知。\nworker 线程获取不到任务后退出，需要将自己从线程池中释放掉（workers.remove(this)）。\n\n从队列里获取任务其实 getTask 也是非常关键的一个方法，它封装了从队列中获取任务，同时对不需要保活的线程进行回收。\n\n很明显，核心作用就是从队列里获取任务；但有两个地方需要注意：\n\n当线程数超过核心线程数时，在获取任务的时候需要通过保活时间从队列里获取任务；一旦获取不到任务则队列肯定是空的，这样返回 null 之后在上文的 run() 中就会退出这个线程；从而达到了回收线程的目的，也就是我们之前演示的效果\n这里需要加锁，加锁的原因是这里肯定会出现并发情况，不加锁会导致 workers.size() &gt; miniSize 条件多次执行，从而导致线程被全部回收完毕。\n\n关闭线程池最后来谈谈线程关闭的事；\n\n还是以刚才那段测试代码为例,如果提交任务后我们没有关闭线程，会发现即便是任务执行完毕后程序也不会退出。\n从刚才的源码里其实也很容易看出来，不退出的原因是 Worker 线程一定还会一直阻塞在 task = workQueue.take(); 处，即便是线程缩容了也不会小于核心线程数。\n通过堆栈也能证明：\n\n恰好剩下三个线程阻塞于此处。\n而关闭线程通常又有以下两种：\n\n立即关闭：执行关闭方法后不管现在线程池的运行状况，直接一刀切全部停掉，这样会导致任务丢失。\n不接受新的任务，同时等待现有任务执行完毕后退出线程池。\n\n立即关闭我们先来看第一种立即关闭：\n/** * 立即关闭线程池，会造成任务丢失 */public void shutDownNow() &#123;    isShutDown.set(true);    tryClose(false);&#125;/** * 关闭线程池 * * @param isTry true 尝试关闭      --&gt; 会等待所有任务执行完毕 *              false 立即关闭线程池--&gt; 任务有丢失的可能 */private void tryClose(boolean isTry) &#123;    if (!isTry) &#123;        closeAllTask();    &#125; else &#123;        if (isShutDown.get() &amp;&amp; totalTask.get() == 0) &#123;            closeAllTask();        &#125;    &#125;&#125;/** * 关闭所有任务 */private void closeAllTask() &#123;    for (Worker worker : workers) &#123;        //LOGGER.info(&quot;开始关闭&quot;);        worker.close();    &#125;&#125;public void close() &#123;    thread.interrupt();&#125;\n\n很容易看出，最终就是遍历线程池里所有的 worker 线程挨个执行他们的中断函数。\n我们来测试一下：\n\n可以发现后面丢进去的三个任务其实是没有被执行的。\n完事后关闭而正常关闭则不一样：\n/** * 任务执行完毕后关闭线程池 */public void shutdown() &#123;    isShutDown.set(true);    tryClose(true);&#125;\n\n\n他会在这里多了一个判断，需要所有任务都执行完毕之后才会去中断线程。\n同时在线程需要回收时都会尝试关闭线程：\n\n来看看实际效果：\n\n回收线程上文或多或少提到了线程回收的事情，其实总结就是以下两点：\n\n一旦执行了 shutdown/shutdownNow 方法都会将线程池的状态置为关闭状态，这样只要 worker 线程尝试从队列里获取任务时就会直接返回空，导致 worker 线程被回收。\n一旦线程池大小超过了核心线程数就会使用保活时间来从队列里获取任务，所以一旦获取不到返回 null 时就会触发回收。\n\n但如果我们的队列足够大，导致线程数都不会超过核心线程数，这样是不会触发回收的。\n\n比如这里我将队列大小调为 10 ，这样任务就会累计在队列里，不会创建五个 worker 线程。\n所以一直都是 Thread-1~3 这三个线程在反复调度任务。\n总结本次实现了线程池里大部分核心功能，我相信只要看完并动手敲一遍一定会对线程池有不一样的理解。\n结合目前的内容来总结下：\n\n线程池、队列大小要设计的合理，尽量的让任务从队列中获取执行。\n慎用 shutdownNow() 方法关闭线程池，会导致任务丢失（除非业务允许）。\n如果任务多，线程执行时间短可以调大 keepalive 值，使得线程尽量不被回收从而可以复用线程。\n\n同时下次会分享一些线程池的新特性，如：\n\n执行带有返回值的线程。\n异常处理怎么办？\n所有任务执行完怎么通知我？\n\n本文所有源码：\nhttps://github.com/crossoverJie/JCSprout/blob/master/src/main/java/com/crossoverjie/concurrent/CustomThreadPool.java\n你的点赞与分享是对我最大的支持\n","categories":["并发"],"tags":["concurrent","ThreadPool"]},{"title":"Python 中的 os.popen 函数 与 Pipe 管道的坑","url":"/2021/05/12/cs/Linux%20Pipe/","content":"\n前言最近用 Python 写了几个简单的脚本来处理一些数据，因为只是简单功能所以我就直接使用 print 来打印日志。\n任务运行时偶尔会出现一些异常：\n\n\n\n因为我在不同地方都有打印日志，导致每次报错的地方都不太一样，从而导致程序运行结果非常诡异；有时候是这段代码没有运行，下一次就可能是另外一段代码没有触发。\n虽说当时有注意到 Broken pipe 这个关键异常，但没有特别在意，因为代码中也有一些发送 http 请求的地方，一直以为是网络 IO 出现了问题，压根没往 print 这个最基本的打印函数上思考🤔。\n直到这个问题反复出现我才认真看了这个异常，定睛一看 print 不也是 IO 操作嘛，难道真的是自带的  print 函数都出问题了？\n\n但在本地、测试环境我运行无数次也没能发现异常；于是我找运维拿到了线上的运行方式。\n原来为了方便维护大家提交上来的脚本任务，运维自己有维护一个统一的脚本，在这个脚本中使用：\ncmd = &#x27;python /xxx/test.py&#x27;os.popen(cmd)\n来触发任务，这也是与我在本地、开发环境的唯一区别。\npopen 原理为此我在开发环境模拟出了异常：\ntest.py:\nimport timeif __name__ == &#x27;__main__&#x27;:    time.sleep(20)    print &#x27;1000&#x27;*1024\n\ntask.py:\nimport osimport timeif __name__ == &#x27;__main__&#x27;:    start = int(time.time())    cmd = &#x27;python test.py&#x27;    os.popen(cmd)    end = int(time.time())    print &#x27;end****&#123;&#125;s&#x27;.format(end-start)\n\n运行:\npython task.py\n等待 20s 必然会复现这个异常：\nTraceback (most recent call last):  File &quot;test.py&quot;, line 4, in &lt;module&gt;    print &#x27;1000&#x27;*1024IOError: [Errno 32] Broken pipe\n\n为什么会出现这个异常呢？\n首先得了解 os.popen(command[, mode[, bufsize]]) 这个函数的运行原理。\n根据官方文档的解释，该函数会执行 fork 一个子进程执行 command 这个命令，同时将子进程的标准输出通过管道连接到父进程；\n也就该方法返回的文件描述符。\n这里画个图能更好地理解其中的原理：\n在这里的使用场景中并没有获取 popen() 的返回值，所以 command 的执行本质上是异步的；\n也就是说当 task.py 执行完毕后会自动关闭读取端的管道。如图所示，关闭之后子进程会向 pipe 中输出  print &#39;1000&#39;*1024，由于这里输出的内容较多会一下子填满管道的缓冲区；\n于是写入端会收到 SIGPIPE 信号，从而导致 Broken pipe 的异常。\n从维基百科中我们也可以看出这个异常产生的一些条件：\n其中也提到了 SIGPIPE 信号。\n解决办法既然知道了问题原因，那解决起来就比较简单了，主要有以下几个方案：\n\n使用 read() 函数读取管道中的数据，全部读取之后再关闭。\n如果不需要子进程中的输出时，也可以将 command 的标准输出重定向到 /dev/null。\n也可以使用 Python3 的 subprocess.Popen 模块来运行。\n\n这里使用第一种方案进行演示：\nimport osimport timeif __name__ == &#x27;__main__&#x27;:    start = int(time.time())    cmd = &#x27;python test.py&#x27;    with os.popen(cmd) as p:        print p.read()    end = int(time.time())    print &#x27;end****&#123;&#125;s&#x27;.format(end-start)\n\n\n运行 task.py 之后不会再抛异常，同时也将 command 的输出打印出来。\n线上修复时我没有采用这个方案，为了方便查看日志，还是使用标准的日志框架将日志输出到了 es 中，方便统一在 kibana 中进行查看。\n由于日志框架并没有使用到管道，所以自然也不会有这个问题。\n更多内容问题虽然是解决了，其中还是涉及到了一些咱们平时不太注意的知识点，这次我们就来一起回顾一下。\n首先是父子进程的内容，这个在 c/c++/python 中比较常见，在 Java/golang 中直接使用多线程、协程会更多一些。\n比如这次提到的 Python 中的 os.popen() 就是创建了一个子进程，既然是子进程那肯定是需要和父进程进行通信才能达到协同工作的目的。\n很容易想到，父子进程之间可以通过上文提到的管道（匿名管道）来进行通信。\n还是以刚才的 Python 程序为例，当运行 task.py 后会生成两个进程：\n分别进入这两个程序的 /proc/pid/fd 目录可以看到这两个进程所打开的文件描述符。\n父进程：\n\n子进程：\n\n可以看到子进程的标准输出与父进程关联，也就是 popen() 所返回的那个文件描述符。\n\n这里的 0 1 2 分别对应一个进程的stdin(标准输入)&#x2F;stdout(标准输出)&#x2F;stderr(标准错误)。\n\n还有一点需要注意的是，当我们在父进程中打开的文件描述符，子进程也会继承过去；\n比如在 task.py 中新增一段代码：\nx = open(&quot;1.txt&quot;, &quot;w&quot;)\n\n之后查看文件描述符时会发现父子进程都会有这个文件：\n但相反的，子进程中打开的文件父进程是不会有的，这个应该很容易理解。\n总结一些基础知识在排查一些诡异问题时显得尤为重要，比如本次涉及到的父子进程的管道通信，最后来总结一下：\n\nos.popen() 函数是异步执行的，如果需要拿到子进程的输出，需要自行调用 read() 函数。\n父子进程是通过匿名管道进行通信的，当读取端关闭时，写入端输出到达管道最大缓存时会收到 SIGPIPE 信号，从而抛出 Broken pipe 异常。\n子进程会继承父进程的文件描述符。\n\n你的点赞与分享是对我最大的支持\n","categories":["cs"],"tags":["Python","pipe","fd"]},{"title":"线程池没你想的那么简单（续）","url":"/2019/06/06/concurrent/threadpool-02/","content":"\n前言前段时间写过一篇《线程池没你想的那么简单》，和大家一起撸了一个基本的线程池，具备：\n\n线程池基本调度功能。\n线程池自动扩容缩容。\n队列缓存线程。\n关闭线程池。\n\n\n\n这些功能，最后也留下了三个待实现的 features 。\n\n执行带有返回值的线程。\n异常处理怎么办？\n所有任务执行完怎么通知我？\n\n这次就实现这三个特性来看看 j.u.c 中的线程池是如何实现这些需求的。\n\n再看本文之前，强烈建议先查看上文《线程池没你想的那么简单》\n\n任务完成后的通知大家在用线程池的时候或多或少都会有这样的需求：\n线程池中的任务执行完毕后再通知主线程做其他事情，比如一批任务都执行完毕后再执行下一波任务等等。\n\n以我们之前的代码为例：\n\n总共往线程池中提交了 13 个任务，直到他们都执行完毕后再打印 “任务执行完毕” 这个日志。\n\n执行结果如下：\n\n为了简单的达到这个效果，我们可以在初始化线程池的时候传入一个接口的实现，这个接口就是用于任务完成之后的回调。\n\npublic interface Notify &#123;    /**     * 回调     */    void notifyListen() ;&#125;\n\n以上就是线程池的构造函数以及接口的定义。\n所以想要实现这个功能的关键是在何时回调这个接口？\n仔细想想其实也简单：只要我们记录提交到线程池中的任务及完成的数量，他们两者的差为 0 时就认为线程池中的任务已执行完毕；这时便可回调这个接口。\n所以在往线程池中写入任务时我们需要记录任务数量：\n\n为了并发安全的考虑，这里的计数器采用了原子的 AtomicInteger 。\n\n\n而在任务执行完毕后就将计数器 -1 ，一旦为 0 时则任务任务全部执行完毕；这时便可回调我们自定义的接口完成通知。\n\nJDK 的实现这样的需求在 jdk 中的 ThreadPoolExecutor 中也有相关的 API ，只是用法不太一样，但本质原理都大同小异。\n我们使用 ThreadPoolExecutor 的常规关闭流程如下：\nexecutorService.shutdown();while (!executorService.awaitTermination(100, TimeUnit.MILLISECONDS)) &#123;    logger.info(&quot;thread running&quot;);&#125;\n\n线程提交完毕后执行 shutdown() 关闭线程池，接着循环调用 awaitTermination() 方法，一旦任务全部执行完毕后则会返回 true 从而退出循环。\n这两个方法的目的和原理如下：\n\n执行 shutdown() 后会将线程池的状态置为关闭状态，这时将会停止接收新的任务同时会等待队列中的任务全部执行完毕后才真正关闭线程池。\nawaitTermination 会阻塞直到线程池所有任务执行完毕或者超时时间已到。\n\n为什么要两个 api 结合一起使用呢？\n主要还在最终的目的是：所有线程执行完毕后再做某件事情，也就是在线程执行完毕之前其实主线程是需要被阻塞的。\nshutdown() 执行后并不会阻塞，会立即返回，所有才需要后续用循环不停的调用 awaitTermination()，因为这个 api 才会阻塞线程。\n其实我们查看源码会发现，ThreadPoolExecutor 中的阻塞依然也是等待通知机制的运用，只不过用的是 LockSupport 的 API 而已。\n带有返回值的线程接下来是带有返回值的线程，这个需求也非常常见；比如需要线程异步计算某些数据然后得到结果最终汇总使用。\n先来看看如何使用（和 jdk 的类似）：\n首先任务是不能实现 Runnable 接口了，毕竟他的 run() 函数是没有返回值的；所以我们改实现一个 Callable 的接口：\n\n这个接口有一个返回值。\n同时在提交任务时也稍作改动：\n\n首先是执行任务的函数由 execute() 换为了 submit()，同时他会返回一个返回值 Future，通过它便可拿到线程执行的结果。\n最后通过第二步将所有执行结果打印出来：\n\n实现原理再看具体实现之前先来思考下这样的功能如何实现？\n\n首先受限于 jdk 的线程 api 的规范，要执行一个线程不管是实现接口还是继承类，最终都是执行的 run() 函数。\n所以我们想要一个线程有返回值无非只能是在执行 run() 函数时去调用一个有返回值的方法，再将这个返回值存放起来用于后续使用。\n\n比如我们这里新建了一个 Callable&lt;T&gt; 的接口：\npublic interface Callable&lt;T&gt; &#123;    /**     * 执行任务     * @return 执行结果     */    T call() ;&#125;\n\n它的 call 函数就是刚才提到的有返回值的方法，所以我们应当在线程的 run() 函数中去调用它。\n接着还会有一个 Future 的接口，他的主要作用是获取线程的返回值，也就是 再将这个返回值存放起来用于后续使用 这里提到的后续使用。\n既然有了接口那自然就得有它的实现 FutureTask，它实现了 Future 接口用于后续获取返回值。\n同时实现了 Runnable 接口会把自己变为一个线程。\n\n所以在它的 run() 函数中会调用刚才提到的具有返回值的 call() 函数。\n\n再次结合 submit() 提交任务和 get() 获取返回值的源码来看会更加理解这其中的门道。\n/** * 有返回值 * * @param callable * @param &lt;T&gt; * @return */public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; callable) &#123;    FutureTask&lt;T&gt; future = new FutureTask(callable);    execute(future);    return future;&#125;\n\nsubmit() 非常简单，将我们丢进来的 Callable 对象转换为一个 FutureTask 对象，然后再调用之前的 execute() 来丢进线程池（后续的流程就和一个普通的线程进入线程池的流程一样）。\n\nFutureTask 本身也是线程，所以可以直接使用 execute() 函数。\n\n\n而 future.get() 函数中 future 对象由于在 submit() 中返回的真正对象是 FutureTask，所以我们直接看其中的源码就好。\n\n由于 get() 在线程没有返回之前是一个阻塞函数，最终也是通过 notify.wait() 使线程进入阻塞状态来实现的。\n而使其从 wait() 中返回的条件必然是在线程执行完毕拿到返回值的时候才进行唤醒。\n也就是图中的第二部分；一旦线程执行完毕（callable.call()）就会唤醒 notify 对象，这样 get 方法也就能返回了。\n\n同样的道理，ThreadPoolExecutor 中的原理也是类似，只不过它考虑的细节更多所以看起来很复杂，但精简代码后核心也就是这些。\n甚至最终使用的 api 看起来都是类似的：\n\n异常处理最后一个是一些新手使用线程池很容易踩坑的一个地方：那就是异常处理。\n比如类似于这样的场景：\n\n创建了只有一个线程的线程池，这个线程只做一件事，就是一直不停的 while 循环。\n但是循环的过程中不小心抛出了一个异常，巧的是这个异常又没有被捕获。你觉得后续会发生什么事情呢？\n是线程继续运行？还是线程池会退出？\n\n通过现象来看其实哪种都不是，线程既没有继续运行同时线程池也没有退出，会一直卡在这里。\n当我们 dump 线程快照会发现：\n\n这时线程池中还有一个线程在运行，通过线程名称会发现这是新创建的一个线程（之前是Thread-0，现在是 Thread-1）。\n它的线程状态为 WAITING ，通过堆栈发现是卡在了 CustomThreadPool.java:272 处。\n\n就是卡在了从队列里获取任务的地方，由于此时的任务队列是空的，所以他会一直阻塞在这里。\n看到这里，之前关注的朋友有没有似曾相识的感觉。\n没错，我之前写过两篇：\n\n一个线程罢工的诡异事件\n线程池中你不容错过的一些细节\n\n线程池相关的问题，当时的讨论也非常“激烈”，其实最终的原因和这里是一模一样的。\n所以就这次简版的代码来看看其中的问题：\n\n现在又简化了一版代码我觉得之前还有疑问的朋友这次应该会更加明白。\n其实在线程池内部会对线程的运行捕获异常，但它并不会处理，只是用于标记是否执行成功；\n一旦执行失败则会回收掉当前异常的线程，然后重新创建一个新的 Worker 线程继续从队列里取任务然后执行。\n所以最终才会卡在从队列中取任务处。\n其实 ThreadPoolExecutor 的异常处理也是类似的，具体的源码就不多分析了，在上面两篇文章中已经说过几次。\n所以我们在使用线程池时，其中的任务一定要做好异常处理。\n总结这一波下来我觉得线程池搞清楚没啥问题了，总的来看它内部运用了非常多的多线程解决方案，比如：\n\nReentrantLock 重入锁来保证线程写入的并发安全。\n利用等待通知机制来实现线程间通信（线程执行结果、等待线程池执行完毕等）。\n\n最后也学会了：\n\n标准的线程池关闭流程。\n如何使用有返回值的线程。\n线程异常捕获的重要性。\n\n最后本文所有源码（结合其中的测试代码使用）：\nhttps://github.com/crossoverJie/JCSprout/blob/master/src/main/java/com/crossoverjie/concurrent/CustomThreadPool.java\n你的点赞与分享是对我最大的支持\n","categories":["并发"],"tags":["concurrent","ThreadPool"]},{"title":"不要小看小小的 emoji 表情😂","url":"/2019/09/10/cs/not%20easy%20emoji/","content":"\n前言好久没更新了，最近事比较多，或许下个月就会恢复到正常的发文频次。\n这篇文章得从一个 emoji 表情开始，我之前开源的一个 IM 项目中有朋友提到希望可以支持 emoji 表情传输。\nhttps://github.com/crossoverJie/cim/issues/12\n\n正好那段时间有空，加上这功能看着也比较简单准备把它实现了。\n\n\n但在真正实现时却发现没那么简单。\n\n\n我首先尝试将一个 emoji 表情存入数据库看看：\n\n果不其然的出错了，导致这个异常的原因是目前数据库所支持的编码中并不能存放 emoji，那 emoji 表情到底是个什么东西呢。\n本质上来说计算机所存储的信息都是二进制 01，emoji 也不例外，只要存储和读取（编解码）的方式一致那就可以准确的展示这个信息。\n\n更多编解码的内容后文再介绍，这里先想想如何快速解决问题。\n\n存储 emoji虽说想要在 MySQL 中存储 emoji 的方式也有好几种，比如可以升级存储字符集到可以存放 emoji ，但这种需要 MySQL 的版本支持。\n所以更保险的方式还是在应用层解决，比如我们是否可以将 emoji 当做字符串存储，只是显示的时候要格式化为一个 emoji 表情，这样对于所有的数据库版本都可兼容。\n于是我们这里的需求是一个 emoji 表情转换为字符串，同时还得将这个字符串转换为 emoji。\n为此我在 GitHub 上找到了一个库，它可以方便的将一个 emoji 转换为字符串的别名，同时也支持将这个别名转换为 emoji。\nhttps://github.com/vdurmont/emoji-java\n@Testpublic void emoji() throws Exception&#123;    String str = &quot;An :grinning:awesome :smiley:string &amp;#128516;with a few :wink:emojis!&quot;;    String result = EmojiParser.parseToUnicode(str);    System.out.println(result);    result = EmojiParser.parseToAliases(str);    System.out.println(result);&#125;\n\n\n所以基于这个基础库最终实现了表情功能。\n\n其实它本质上是自己维护了一个 emoji 的别名及它的 Unicode 编码(本质上是 UTF-16)的映射关系，再每次格式化数据的时候都会从这个表中进行翻译。\n\n编码知识回顾自此需求是完成了，但还有几个问题待解决。\n\nJava 中是如何存储 emoji 的？\nemoji 是如何进行编码的？\n\nASCII在谈 emoji 之前非常有必要了解下计算机编码鼻祖的 ASCII 码。\n大家现在都知道在计算机内部存储数据本质上都是二进制的 0&#x2F;1，对于一个字节来说有 8 位；每一位可以表示两种状态，也就是 0 或 1，这样排列组合下来，一个字节就可以表示 256(2∧8) 种不同的状态。\n\n\n对于美国来说他们日常使用的英语只需要 26 个英文字母，再加上一些标点符号就足够用计算机来进行信息交流。\n于是上个世纪 60年代定义了一套二进制与英文字符的映射关系，可以表明 128 个不同的英文字符，也就是现在的 ASCII 码。\n这样我们就可以使用一个字节来表示现代英文，看起来非常不错。\nUnicode随着计算机的发展，逐渐在欧洲、亚洲地区流行；再利用这套 ASCII 码进行信息交流显然是不行的，很多地区压根就不使用英文，而且也远超了 128 位字符（中文就更不用说了）。\n虽说一个字节在 ASCII 码中只用了 128 位，但剩下(258-128)的依然不足用用于描述其他语言。\n这时如果能有一种包含了世界上所有的文字的字符集，每一个地区的文字都在这个字符集中有唯一的二进制表示，这样便不会出现乱码问题了。\nUnicode 就是来做这个的，截止目前 Unicode 已经收录了 10W+ 的字符，你所能使用的字符都包含进去了。\nUTF-8Unicode 虽说包含了几乎所有的文字，但在我们日常使用好像很少看到他的身影，我们用的更多的还是 UTF-8 这样的编码规则。\n这也有几方面的原因，比如说除开英文，其他大部分的文字都需要用 2 个甚至更多的字节来表示；如果统一都用 Unicode 来表示，那必然需要以占用字节最多的字符长度为标准。\n比如汉字需要 2 个字节来表示，而英文只需要一个字节；这时就得规定 2 个字节表示一个字符，不然汉字就没法表示了。\n但这样也会带来一个问题：用两个字节表示英文会使得第一个字节完全是浪费的，如果一段信息全是英文那对内存的浪费是巨大的。\n\n这时大家应该都能想到，我们需要一个可变的长度的字符编码规则，当是英文时我们就用一个字节表示，甚至可以完全兼容 ASCII 码。\nUTF-8 便是实现这个需求的，它利用两种规则可以表示一个字节以及多字节的字符。\n\n大致规则如下：\n\n当第一个字节的第一位为 0 时便表示为单字节字符，此时和 ASCII 码一致，完全兼容。\n当第一个字节为 1 时，有几个 1 便代表是几个字节 Unicode 字符。\n\n这样便可根据字符的长度最大程度的节省存储空间。\n当然还有其他的编码规则，比如 UTF-16、UTF-32，平时用的不多，但本质上都和 UTF-8 一样，都是 Unicode 的不同实现，也是用于表示世界上大部分文字的字符集。\nJava 中的 emoji现在来回到本次的主题，emoji。\n刚才说到 Unicode 包含了世界上大部分的字符，emoji 自然也不例外。\n\nhttps://apps.timwhitlock.info/emoji/tables/unicode\n这个表格中包含了所有的 emoji 以及它所对应的 Unicode 编码，同时也有对应的 UTF-8 编码的实现。\n从图中也可以看出 emoji 表情用 UTF-8 表示时会占用 4 个字节，那在 Java 中它会是怎么存储的呢？\n很简单，debug 一下就知道了。\n\n在 Java 中也是通过 char 来存储 emoji 的，char 作为基本数据类型会占用 2 个字节；从刚才的图中可以看出，emoji 使用 UTF-8 会占用四个字节，这样很明显 char 是没法存储的，所以在这里其实是使用 UTF-16 编码进行存储。\n基于这个原理，我们也可以自己实现将一个 emoji 表情转换为字符串，同时也可通过字符串转换为 emoji。\n\n总结从这次研究 emoji 可以看出，任何一门基础知识都是应用的根基，在计算机行业尤为突出，希望大家看完这篇能回忆起大学课堂被老师支配的恐惧😂。\n随便提一下，相关源码可在这里查看：\nhttps://github.com/crossoverJie/cim\n你的点赞与分享是对我最大的支持\n","categories":["cs"],"tags":["emoji","unicode","utf-8","ascii"]},{"title":"模板方法实践","url":"/2022/12/27/desigin-patterns/template-method/","content":"\n前言最近不出意外的阳了，加上刚入职新公司不久，所以也没怎么更新；这两天好些后分享一篇前段时间的一个案例：\n最近在设计一个对某个中间件的测试方案，这个测试方案需要包含不同的测试逻辑，但相同的是需要对各个环节进行记录；比如统计耗时、调用通知 API 等相同的逻辑。\n如果每个测试都单独写这些逻辑那无疑是做了许多重复工作了。\n\n\n基于以上的特征很容易能想到模板方法这个设计模式。\n这是一种有上层定义框架，下层提供不同实现的设计模式。\n比如装修房子的时候业主可以按照自己的喜好对不同的房间进行装修，但是整体的户型图不能做修改，比如承重墙是肯定不能打的。\n而这些固定好的条条框框就是上层框架给的约束，下层不同的实现就有业主自己决定；所以对于整栋楼来说框架都是固定好的，让业主在有限的范围内自由发挥也方便物业的管理。\n具体实现以我这个案例的背景为例，首先需要定义出上层框架：\nJavaEvent 接口：\npublic interface Event &#123;    /**     * 新增一个任务     */    void addJob();    /**     * 单个任务执行完毕     *     * @param jobName    任务名称     * @param finishCost 任务完成耗时     */    void finishOne(String jobName, String finishCost);    /**单个任务执行异常     * @param jobDefine 任务     * @param e 异常     */    void oneException(AbstractJobDefine jobDefine, Exception e);    /**     * 所有任务执行完毕     */    void finishAll();&#125;\n\npublic void start() &#123;    event.addJob();    try &#123;        CompletableFuture.runAsync(() -&gt; &#123;            StopWatch watch = new StopWatch();            try &#123;                watch.start(jobName);                // 不同的子业务实现                run(client);            &#125; catch (Exception e) &#123;                event.oneException(this, e);            &#125; finally &#123;                watch.stop();                event.finishOne(jobName, StrUtil.format(&quot;cost: &#123;&#125;s&quot;, watch.getTotalTimeSeconds()));            &#125;        &#125;, TestCase.EXECUTOR).get(timeout, TimeUnit.SECONDS);    &#125; catch (Exception e) &#123;        event.oneException(this, e);    &#125;&#125;/** Run busy code * @param client * @throws Exception e */public abstract void run(Client client) throws Exception;    \n\n其中最核心的就是 run 函数，它是一个抽象函数，具体实现交由子类完成；这样不同的测试用例之间也互不干扰，同时整体的流程完全相同：\n\n记录任务数量\n统计耗时\n异常记录\n\n等流程。\n\n接下来看看如何使用：\nAbstractJobDefine job1 = new Test1(event, &quot;测试1&quot;, client, 10);CompletableFuture&lt;Void&gt; c1 = CompletableFuture.runAsync(job1::start, EXECUTOR);AbstractJobDefine job2 = new Test2(event, &quot;测试2&quot;, client, 10);CompletableFuture&lt;Void&gt; c2 = CompletableFuture.runAsync(job2::start, EXECUTOR);AbstractJobDefine job3 = new Test3(event, &quot;测试3&quot;, client, 20);CompletableFuture&lt;Void&gt; c3 = CompletableFuture.runAsync(job3::start, EXECUTOR);CompletableFuture&lt;Void&gt; all = CompletableFuture.allOf(c1, c2, c3);all.whenComplete((___, __) -&gt; &#123;    event.finishAll();    client.close();&#125;).get();\n\n显而易见 Test1~3 都继承了 AbstractJobDefine 同时实现了其中的 run 函数，使用的时候只需要创建不同的实例等待他们都执行完成即可。\n以前在 Java 中也有不同的应用：\nhttps://crossoverjie.top/2019/03/01/algorithm/consistent-hash/?highlight=%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95#%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95\nGo同样的示例用 Go 自然也可以实现：\n\nfunc TestJobDefine_start(t *testing.T) &#123;\tevent := NewEvent()\tj1 := &amp;JobDefine&#123;\t\tEvent:   event,\t\tRun:     &amp;run1&#123;&#125;,\t\tJobName: &quot;job1&quot;,\t\tParam1:  &quot;p1&quot;,\t\tParam2:  &quot;p2&quot;,\t&#125;\tj2 := &amp;JobDefine&#123;\t\tEvent:   event,\t\tRun:     &amp;run2&#123;&#125;,\t\tJobName: &quot;job2&quot;,\t\tParam1:  &quot;p11&quot;,\t\tParam2:  &quot;p22&quot;,\t&#125;\tj1.Start()\tj2.Start()\tfor _, ch := range event.GetChan() &#123;\t\t&lt;-ch\t&#125;\tlog.Println(&quot;finish all&quot;)&#125;func (r *run2) Run(param1, param2 string) error &#123;\tlog.Printf(&quot;run3 param1:%s, param2:%s&quot;, param1, param2)\ttime.Sleep(time.Second * 3)\treturn errors.New(&quot;test err&quot;)&#125;func (r *run1) Run(param1, param2 string) error &#123;\tlog.Printf(&quot;run1 param1:%s, param2:%s&quot;, param1, param2)\treturn nil&#125;\n\n使用起来也与 Java 类似，创建不同的实例；最后等待所有的任务执行完毕。\n总结设计模式往往是对某些共性能力的抽象，但也没有一个设计模式可以适用于所有的场景；需要对不同的需求选择不同的设计模式。\n至于在工作中如何进行正确的选择，那就需要自己日常的积累了；比如多去了解不同的设计模式对于的场景，或者多去阅读优秀的代码，Java 中的 InputStream/Reader/Writer 这类 IO 相关的类都有具体的应用。\n","categories":["设计模式"],"tags":["Java","Go"]},{"title":"分布式(一) 搞定服务注册与发现","url":"/2018/08/27/distributed/distributed-discovery-zk/","content":"\n背景最近在做分布式相关的工作，由于人手不够只能我一个人来怼；看着这段时间的加班表想想就是够惨的。\n不过其中也有遇到的不少有意思的事情今后再拿来分享，今天重点来讨论服务的注册与发现。\n分布式带来的问题我的业务比较简单，只是需要知道现在有哪些服务实例可供使用就可以了（并不是做远程调用，只需要拿到信息即可）。\n要实现这一功能最简单的方式可以在应用中配置所有的服务节点，这样每次在使用时只需要通过某种算法从配置列表中选择一个就可以了。\n但这样会有一个非常严重的问题：\n由于应用需要根据应用负载情况来灵活的调整服务节点的数量，这样我的配置就不能写死。\n不然就会出现要么新增的节点没有访问或者是已经 down 掉的节点却有请求，这样肯定是不行的。\n往往要解决这类分布式问题都需要一个公共的区域来保存这些信息，比如是否可以利用 Redis？\n每个节点启动之后都向 Redis 注册信息，关闭时也删除数据。\n其实就是存放节点的 ip + port，然后在需要知道服务节点信息时候只需要去 Redis 中获取即可。\n\n\n如下图所示：\n\n但这样会导致每次使用时都需要频繁的去查询 Redis，为了避免这个问题我们可以在每次查询之后在本地缓存一份最新的数据。这样优先从本地获取确实可以提高效率。\n但同样又会出现新的问题，如果服务提供者的节点新增或者删除消费者这边根本就不知道情况。\n要解决这个问题最先想到的应该就是利用定时任务定期去更新服务列表。\n以上的方案肯定不完美，并且不优雅。主要有以下几点：\n\n基于定时任务会导致很多无效的更新。\n定时任务存在周期性，没法做到实时，这样就可能存在请求异常。\n如果服务被强行 kill，没法及时清除 Redis，这样这个看似可用的服务将永远不可用！\n\n所以我们需要一个更加靠谱的解决方案，这样的场景其实和 Dubbo 非常类似。\n用过的同学肯定对这张图不陌生。\n\n\n引用自 Dubbo 官网\n\n其中有一块非常核心的内容（红框出）就是服务的注册与发现。\n通常来说消费者是需要知道服务提供者的网络地址(ip + port)才能发起远程调用，这块内容和我上面的需求其实非常类似。\n而 Dubbo 则是利用 Zookeeper 来解决问题。\nZookeeper 能做什么在具体讨论怎么实现之前先看看 Zookeeper 的几个重要特性。\nZookeeper 实现了一个类似于文件系统的树状结构：\n\n这些节点被称为 znode(名字叫什么不重要)，其中每个节点都可以存放一定的数据。\n最主要的是 znode 有四种类型：\n\n永久节点（除非手动删除，节点永远存在）\n永久有序节点（按照创建顺序会为每个节点末尾带上一个序号如：root-1）\n瞬时节点（创建客户端与 Zookeeper 保持连接时节点存在，断开时则删除并会有相应的通知）\n瞬时有序节点（在瞬时节点的基础上加上了顺序）\n\n考虑下上文使用 Redis 最大的一个问题是什么？\n其实就是不能实时的更新服务提供者的信息。\n那利用 Zookeeper 是怎么实现的？\n主要看第三个特性：瞬时节点\nZookeeper 是一个典型的观察者模式。\n\n由于瞬时节点的特点，我们的消费者可以订阅瞬时节点的父节点。\n当新增、删除节点时所有的瞬时节点也会自动更新。\n更新时会给订阅者发起通知告诉最新的节点信息。\n\n这样我们就可以实时获取服务节点的信息，同时也只需要在第一次获取列表时缓存到本地；也不需要频繁和 Zookeeper 产生交互，只用等待通知更新即可。\n并且不管应用什么原因节点 down 掉后也会在 Zookeeper 中删除该信息。\n效果演示这样实现方式就变为这样。\n\n为此我新建了一个应用来进行演示：\nhttps://github.com/crossoverJie/netty-action/tree/master/netty-action-zk\n就是一个简单的 SpringBoot 应用，只是做了几件事情。\n\n应用启动时新开一个线程用于向 Zookeeper 注册服务。\n同时监听一个节点用于更新本地服务列表。\n提供一个接口用于返回一个可有的服务节点。\n\n我在本地启动了两个应用分别是：127.0.0.1:8083,127.0.0.1:8084。来看看效果图。\n两个应用启动完成：\n\n\n\n当前 Zookeeper 的可视化树状结构：\n\n\n当想知道所有的服务节点信息时：\n\n\n想要获取一个可用的服务节点时：\n\n这里只是采取了简单的轮询。\n\n当 down 掉一个节点时：应用会收到通知更新本地缓存。同时 Zookeeper 中的节点会自动删除。\n\n\n\n再次获取最新节点时：\n\n\n当节点恢复时自然也能获取到最新信息。本地缓存也会及时更新。\n\n\n编码实现实现起来倒也比较简单，主要就是 ZKClient 的 api 使用。\n贴几段比较核心的吧。\n注册\n启动注册 Zookeeper。\n\n\n主要逻辑都在这个线程中。\n\n首先创建父节点。如上图的 Zookeeper 节点所示；需要先创建 /route 根节点，创建的时候会判断是否已经存在。\n接着需要判断是否需要将自己注册到 Zookeeper 中，因为有些节点只是用于服务发现，他自身是不需要承担业务功能（是我自己项目的需求）。\n将当前应用的所在 ip 以及端口注册上去，同时需要监听根节点 /route ，这样才能在其他服务上下线时候获得通知。\n\n根据本地缓存\n监听到服务变化\n\npublic void subscribeEvent(String path) &#123;    zkClient.subscribeChildChanges(path, new IZkChildListener() &#123;        @Override        public void handleChildChange(String parentPath, List&lt;String&gt; currentChilds) throws Exception &#123;            logger.info(&quot;清除/更新本地缓存 parentPath=【&#123;&#125;】,currentChilds=【&#123;&#125;】&quot;, parentPath,currentChilds.toString());            //更新所有缓存/先删除 再新增            serverCache.updateCache(currentChilds) ;        &#125;    &#125;);&#125;\n\n可以看到这里是更新了本地缓存，该缓存采用了 Guava 提供的 Cache，感兴趣的可以查看之前的源码分析。\n/** * 更新所有缓存/先删除 再新增 * * @param currentChilds */public void updateCache(List&lt;String&gt; currentChilds) &#123;    cache.invalidateAll();    for (String currentChild : currentChilds) &#123;        String key = currentChild.split(&quot;-&quot;)[1];        addCache(key);    &#125;&#125;\n\n客户端负载\n同时在客户端提供了一个负载算法。\n\n其实就是一个轮询的实现：\n/** * 选取服务器 * * @return */public String selectServer() &#123;    List&lt;String&gt; all = getAll();    if (all.size() == 0) &#123;        throw new RuntimeException(&quot;路由列表为空&quot;);    &#125;    Long position = index.incrementAndGet() % all.size();    if (position &lt; 0) &#123;        position = 0L;    &#125;    return all.get(position.intValue());&#125;\n\n当然这里可以扩展出更多的如权重、随机、LRU 等算法。\nZookeeper 其他优势及问题Zookeeper 自然是一个很棒的分布式协调工具，利用它的特性还可以有其他作用。\n\n数据变更发送通知这一特性可以实现统一配置中心，再也不需要在每个服务中单独维护配置。\n利用瞬时有序节点还可以实现分布式锁。\n\n在实现注册、发现这一需求时，Zookeeper 其实并不是最优选。\n由于 Zookeeper 在 CAP 理论中选择了 CP（一致性、分区容错性），当 Zookeeper 集群有半数节点不可用时是不能获取到任何数据的。\n对于一致性来说自然没啥问题，但在注册、发现的场景下更加推荐 Eureka，已经在 SpringCloud 中得到验证。具体就不在本文讨论了。\n但鉴于我的使用场景来说 Zookeeper 已经能够胜任。\n总结本文所有完整代码都托管在 GitHub。\nhttps://github.com/crossoverJie/netty-action。\n一个看似简单的注册、发现功能实现了，但分布式应用远远不止这些。\n由于网络隔离之后带来的一系列问题还需要我们用其他方式一一完善；后续会继续更新分布式相关内容，感兴趣的朋友不妨持续关注。\n你的点赞与转发是最大的支持。\n","categories":["Distributed"],"tags":["SpringBoot","Zookeeper"]},{"title":"分布式工具的一次小升级⏫","url":"/2018/06/07/distributed-lock/distributed-lock-redis-update/","content":"\n前言之前在做 秒杀架构实践 时有提到对 distributed-redis-tool 的一次小升级，但是没有细说。\n其实主要原因是：\n\n秒杀时我做压测：由于集成了这个限流组件，并发又比较大，所以导致连接、断开 Redis 非常频繁。最终导致获取不了 Redis connection 的异常。\n\n池化技术这就是一个典型的对稀缺资源使用不善导致的。\n何为稀缺资源？常见的有：\n\n线程\n数据库连接\n网络连接等\n\n这些资源都有共同的特点：创建销毁成本较高。\n\n\n这里涉及到的 Redis 连接也属于该类资源。\n我们希望将这些稀有资源管理起来放到一个池子里，当需要时就从中获取，用完就放回去，不够用时就等待（或返回）。\n这样我们只需要初始化并维护好这个池子，就能避免频繁的创建、销毁这些资源（也有资源长期未使用需要缩容的情况）。\n通常我们称这项姿势为池化技术，如常见的：\n\n线程池\n各种资源的连接池等。\n\n为此我将使用到 Redis 的 分布式锁、分布式限流 都升级为利用连接池来获取 Redis 的连接。\n这里以分布式锁为例：\n将使用的 api 修改为：\n原有：\n@Configurationpublic class RedisLockConfig &#123;    @Bean    public RedisLock build()&#123;        //Need to get Redis connection         RedisLock redisLock = new RedisLock() ;        HostAndPort hostAndPort = new HostAndPort(&quot;127.0.0.1&quot;,7000) ;        JedisCluster jedisCluster = new JedisCluster(hostAndPort) ;        RedisLock redisLock = new RedisLock.Builder(jedisCluster)                .lockPrefix(&quot;lock_test&quot;)                .sleepTime(100)                .build();                        return redisLock ;    &#125;&#125;\n\n现在：\n@Configurationpublic class RedisLockConfig &#123;    private Logger logger = LoggerFactory.getLogger(RedisLockConfig.class);            @Autowired    private JedisConnectionFactory jedisConnectionFactory;        @Bean    public RedisLock build() &#123;        RedisLock redisLock = new RedisLock.Builder(jedisConnectionFactory,RedisToolsConstant.SINGLE)                .lockPrefix(&quot;lock_&quot;)                .sleepTime(100)                .build();        return redisLock;    &#125;&#125;\n\n将以前的 Jedis 修改为 JedisConnectionFactory，后续的 Redis 连接就可通过这个对象获取。\n并且显示的传入使用 RedisCluster 还是单机的 Redis。\n所以在真正操作 Redis 时需要修改：\npublic boolean tryLock(String key, String request) &#123;    //get connection    Object connection = getConnection();    String result ;    if (connection instanceof Jedis)&#123;        result =  ((Jedis) connection).set(lockPrefix + key, request, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, 10 * TIME);        ((Jedis) connection).close();    &#125;else &#123;        result = ((JedisCluster) connection).set(lockPrefix + key, request, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, 10 * TIME);        try &#123;            ((JedisCluster) connection).close();        &#125; catch (IOException e) &#123;            logger.error(&quot;IOException&quot;,e);        &#125;    &#125;    if (LOCK_MSG.equals(result)) &#123;        return true;    &#125; else &#123;        return false;    &#125;&#125;//获取连接private Object getConnection() &#123;    Object connection ;    if (type == RedisToolsConstant.SINGLE)&#123;        RedisConnection redisConnection = jedisConnectionFactory.getConnection();        connection = redisConnection.getNativeConnection();    &#125;else &#123;        RedisClusterConnection clusterConnection = jedisConnectionFactory.getClusterConnection();        connection = clusterConnection.getNativeConnection() ;    &#125;    return connection;&#125;    \n\n最大的改变就是将原有操作 Redis 的对象（T extends JedisCommands）改为从连接池中获取。\n由于使用了 org.springframework.data.redis.connection.jedis.JedisConnectionFactory 作为 Redis 连接池。\n所以需要再使用时构件好这个对象：\nJedisPoolConfig config = new JedisPoolConfig();config.setMaxIdle(10);config.setMaxTotal(300);config.setMaxWaitMillis(10000);config.setTestOnBorrow(true);config.setTestOnReturn(true);RedisClusterConfiguration redisClusterConfiguration = new RedisClusterConfiguration();redisClusterConfiguration.addClusterNode(new RedisNode(&quot;10.19.13.51&quot;, 7000));//单机JedisConnectionFactory jedisConnectionFactory = new JedisConnectionFactory(config);//集群//JedisConnectionFactory jedisConnectionFactory = new JedisConnectionFactory(redisClusterConfiguration) ;jedisConnectionFactory.setHostName(&quot;47.98.194.60&quot;);jedisConnectionFactory.setPort(6379);jedisConnectionFactory.setPassword(&quot;&quot;);jedisConnectionFactory.setTimeout(100000);jedisConnectionFactory.afterPropertiesSet();//jedisConnectionFactory.setShardInfo(new JedisShardInfo(&quot;47.98.194.60&quot;, 6379));//JedisCluster jedisCluster = new JedisCluster(hostAndPort);HostAndPort hostAndPort = new HostAndPort(&quot;10.19.13.51&quot;, 7000);JedisCluster jedisCluster = new JedisCluster(hostAndPort);redisLock = new RedisLock.Builder(jedisConnectionFactory, RedisToolsConstant.SINGLE)        .lockPrefix(&quot;lock_&quot;)        .sleepTime(100)        .build();\n\n看起比较麻烦，需要构建对象的较多。\n但整合 Spring 使用时就要清晰许多。\n配合 SpringSpring 很大的一个作用就是帮我们管理对象，所以像上文那些看似很复杂的对象都可以交由它来管理：\n&lt;!-- jedis 配置 --&gt; &lt;bean id=&quot;JedispoolConfig&quot; class=&quot;redis.clients.jedis.JedisPoolConfig&quot;&gt;     &lt;property name=&quot;maxIdle&quot; value=&quot;$&#123;redis.maxIdle&#125;&quot;/&gt;     &lt;property name=&quot;maxTotal&quot; value=&quot;$&#123;redis.maxTotal&#125;&quot;/&gt;     &lt;property name=&quot;maxWaitMillis&quot; value=&quot;$&#123;redis.maxWait&#125;&quot;/&gt;     &lt;property name=&quot;testOnBorrow&quot; value=&quot;$&#123;redis.testOnBorrow&#125;&quot;/&gt;     &lt;property name=&quot;testOnReturn&quot; value=&quot;$&#123;redis.testOnBorrow&#125;&quot;/&gt; &lt;/bean&gt; &lt;!-- redis服务器中心 --&gt; &lt;bean id=&quot;connectionFactory&quot; class=&quot;org.springframework.data.redis.connection.jedis.JedisConnectionFactory&quot;&gt;     &lt;property name=&quot;poolConfig&quot; ref=&quot;JedispoolConfig&quot;/&gt;     &lt;property name=&quot;port&quot; value=&quot;$&#123;redis.port&#125;&quot;/&gt;     &lt;property name=&quot;hostName&quot; value=&quot;$&#123;redis.host&#125;&quot;/&gt;     &lt;property name=&quot;password&quot; value=&quot;$&#123;redis.password&#125;&quot;/&gt;     &lt;property name=&quot;timeout&quot; value=&quot;$&#123;redis.timeout&#125;&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;redisTemplate&quot; class=&quot;org.springframework.data.redis.core.RedisTemplate&quot;&gt;     &lt;property name=&quot;connectionFactory&quot; ref=&quot;connectionFactory&quot;/&gt;     &lt;property name=&quot;keySerializer&quot;&gt;         &lt;bean class=&quot;org.springframework.data.redis.serializer.StringRedisSerializer&quot;/&gt;     &lt;/property&gt;     &lt;property name=&quot;valueSerializer&quot;&gt;         &lt;bean class=&quot;org.springframework.data.redis.serializer.StringRedisSerializer&quot;/&gt;     &lt;/property&gt; &lt;/bean&gt;\n\n这个其实没多少好说的，就算是换成 SpringBoot 也是创建 JedispoolConfig,connectionFactory,redisTemplate 这些 bean 即可。\n总结换为连接池之后再进行压测自然没有出现获取不了 Redis 连接的异常（并发达到一定的量也会出错）说明更新是很有必要的。\n推荐有用到该组件的朋友都升级下，也欢迎提出 Issues 和 PR。\n项目地址：\nhttps://github.com/crossoverJie/distributed-redis-tool\n","categories":["Distributed Tools"],"tags":["Distributed Lock","Distributed Limited"]},{"title":"基于 Redis 的分布式锁","url":"/2018/03/29/distributed-lock/distributed-lock-redis/","content":"\n前言分布式锁在分布式应用中应用广泛，想要搞懂一个新事物首先得了解它的由来，这样才能更加的理解甚至可以举一反三。\n首先谈到分布式锁自然也就联想到分布式应用。\n在我们将应用拆分为分布式应用之前的单机系统中，对一些并发场景读取公共资源时如扣库存，卖车票之类的需求可以简单的使用同步或者是加锁就可以实现。\n但是应用分布式了之后系统由以前的单进程多线程的程序变为了多进程多线程，这时使用以上的解决方案明显就不够了。\n因此业界常用的解决方案通常是借助于一个第三方组件并利用它自身的排他性来达到多进程的互斥。如：\n\n基于 DB 的唯一索引。\n基于 ZK 的临时有序节点。\n基于 Redis 的 NX EX 参数。\n\n这里主要基于 Redis 进行讨论。\n\n\n实现既然是选用了 Redis，那么它就得具有排他性才行。同时它最好也有锁的一些基本特性：\n\n高性能(加、解锁时高性能)\n可以使用阻塞锁与非阻塞锁。\n不能出现死锁。\n可用性(不能出现节点 down 掉后加锁失败)。\n\n这里利用 Redis set key 时的一个 NX 参数可以保证在这个 key 不存在的情况下写入成功。并且再加上 EX 参数可以让该 key 在超时之后自动删除。\n所以利用以上两个特性可以保证在同一时刻只会有一个进程获得锁，并且不会出现死锁(最坏的情况就是超时自动删除 key)。\n加锁实现代码如下：\nprivate static final String SET_IF_NOT_EXIST = &quot;NX&quot;;private static final String SET_WITH_EXPIRE_TIME = &quot;PX&quot;;public  boolean tryLock(String key, String request) &#123;    String result = this.jedis.set(LOCK_PREFIX + key, request, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, 10 * TIME);    if (LOCK_MSG.equals(result))&#123;        return true ;    &#125;else &#123;        return false ;    &#125;&#125;\n\n注意这里使用的 jedis 的\nString set(String key, String value, String nxxx, String expx, long time);\n\napi。\n该命令可以保证 NX EX 的原子性。\n一定不要把两个命令(NX EX)分开执行，如果在 NX 之后程序出现问题就有可能产生死锁。\n阻塞锁同时也可以实现一个阻塞锁：\n//一直阻塞public void lock(String key, String request) throws InterruptedException &#123;    for (;;)&#123;        String result = this.jedis.set(LOCK_PREFIX + key, request, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, 10 * TIME);        if (LOCK_MSG.equals(result))&#123;            break ;        &#125; //防止一直消耗 CPU \t        Thread.sleep(DEFAULT_SLEEP_TIME) ;    &#125;&#125; //自定义阻塞时间 public boolean lock(String key, String request,int blockTime) throws InterruptedException &#123;    while (blockTime &gt;= 0)&#123;        String result = this.jedis.set(LOCK_PREFIX + key, request, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, 10 * TIME);        if (LOCK_MSG.equals(result))&#123;            return true ;        &#125;        blockTime -= DEFAULT_SLEEP_TIME ;        Thread.sleep(DEFAULT_SLEEP_TIME) ;    &#125;    return false ;&#125;\n\n解锁解锁也很简单，其实就是把这个 key 删掉就万事大吉了，比如使用 del key 命令。\n但现实往往没有那么 easy。\n如果进程 A 获取了锁设置了超时时间，但是由于执行周期较长导致到了超时时间之后锁就自动释放了。这时进程 B 获取了该锁执行很快就释放锁。这样就会出现进程 B 将进程 A 的锁释放了。\n所以最好的方式是在每次解锁时都需要判断锁是否是自己的。\n这时就需要结合加锁机制一起实现了。\n加锁时需要传递一个参数，将该参数作为这个 key 的 value，这样每次解锁时判断 value 是否相等即可。\n所以解锁代码就不能是简单的 del了。\npublic  boolean unlock(String key,String request)&#123;    //lua script    String script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;;    Object result = null ;    if (jedis instanceof Jedis)&#123;        result = ((Jedis)this.jedis).eval(script, Collections.singletonList(LOCK_PREFIX + key), Collections.singletonList(request));    &#125;else if (jedis instanceof JedisCluster)&#123;        result = ((JedisCluster)this.jedis).eval(script, Collections.singletonList(LOCK_PREFIX + key), Collections.singletonList(request));    &#125;else &#123;        //throw new RuntimeException(&quot;instance is error&quot;) ;        return false ;    &#125;    if (UNLOCK_MSG.equals(result))&#123;        return true ;    &#125;else &#123;        return false ;    &#125;&#125;\n\n这里使用了一个 lua 脚本来判断 value 是否相等，相等才执行 del 命令。\n使用 lua 也可以保证这里两个操作的原子性。\n因此上文提到的四个基本特性也能满足了：\n\n使用 Redis 可以保证性能。\n阻塞锁与非阻塞锁见上文。\n利用超时机制解决了死锁。\nRedis 支持集群部署提高了可用性。\n\n使用我自己有撸了一个完整的实现，并且已经用于了生产，有兴趣的朋友可以开箱使用:\nmaven 依赖：\n&lt;dependency&gt;    &lt;groupId&gt;top.crossoverjie.opensource&lt;/groupId&gt;    &lt;artifactId&gt;distributed-redis-lock&lt;/artifactId&gt;    &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt;\n\n配置 bean :\n@Configurationpublic class RedisLockConfig &#123;    @Bean    public RedisLock build()&#123;        RedisLock redisLock = new RedisLock() ;        HostAndPort hostAndPort = new HostAndPort(&quot;127.0.0.1&quot;,7000) ;        JedisCluster jedisCluster = new JedisCluster(hostAndPort) ;        // Jedis 或 JedisCluster 都可以        redisLock.setJedisCluster(jedisCluster) ;        return redisLock ;    &#125;&#125;\n\n使用：\n@Autowiredprivate RedisLock redisLock ;public void use() &#123;    String key = &quot;key&quot;;    String request = UUID.randomUUID().toString();    try &#123;        boolean locktest = redisLock.tryLock(key, request);        if (!locktest) &#123;            System.out.println(&quot;locked error&quot;);            return;        &#125;        //do something    &#125; finally &#123;        redisLock.unlock(key,request) ;    &#125;&#125;\n\n使用很简单。这里主要是想利用 Spring 来帮我们管理 RedisLock 这个单例的 bean，所以在释放锁的时候需要手动(因为整个上下文只有一个 RedisLock 实例)的传入 key 以及 request(api 看起来不是特别优雅)。\n也可以在每次使用锁的时候 new 一个 RedisLock 传入 key 以及 request，这样倒是在解锁时很方便。但是需要自行管理 RedisLock 的实例。各有优劣吧。\n项目源码在：\nhttps://github.com/crossoverJie/distributed-lock-redis\n欢迎讨论。\n单测在做这个项目的时候让我不得不想提一下单测。\n因为这个应用是强依赖于第三方组件的(Redis)，但是在单测中我们需要排除掉这种依赖。比如其他伙伴 fork 了该项目想在本地跑一遍单测，结果运行不起来：\n\n有可能是 Redis 的 ip、端口和单测里的不一致。\nRedis 自身可能也有问题。\n也有可能是该同学的环境中并没有 Redis。\n\n所以最好是要把这些外部不稳定的因素排除掉，单测只测我们写好的代码。\n于是就可以引入单测利器 Mock 了。\n它的想法很简答，就是要把你所依赖的外部资源统统屏蔽掉。如：数据库、外部接口、外部文件等等。\n使用方式也挺简单，可以参考该项目的单测：\n@Testpublic void tryLock() throws Exception &#123;    String key = &quot;test&quot;;    String request = UUID.randomUUID().toString();    Mockito.when(jedisCluster.set(Mockito.anyString(), Mockito.anyString(), Mockito.anyString(),            Mockito.anyString(), Mockito.anyLong())).thenReturn(&quot;OK&quot;);    boolean locktest = redisLock.tryLock(key, request);    System.out.println(&quot;locktest=&quot; + locktest);    Assert.assertTrue(locktest);    //check    Mockito.verify(jedisCluster).set(Mockito.anyString(), Mockito.anyString(), Mockito.anyString(),            Mockito.anyString(), Mockito.anyLong());&#125;\n\n这里只是简单演示下，可以的话下次仔细分析分析。\n它的原理其实也挺简单，debug 的话可以很直接的看出来：\n\n这里我们所依赖的 JedisCluster 其实是一个 cglib 代理对象。所以也不难想到它是如何工作的。\n比如这里我们需要用到 JedisCluster 的 set 函数并需要它的返回值。\nMock 就将该对象代理了，并在实际执行 set 方法后给你返回了一个你自定义的值。\n这样我们就可以随心所欲的测试了，完全把外部依赖所屏蔽了。\n总结至此一个基于 Redis 的分布式锁完成，但是依然有些问题。\n\n如在 key 超时之后业务并没有执行完毕但却自动释放锁了，这样就会导致并发问题。\n就算 Redis 是集群部署的，如果每个节点都只是 master 没有 slave，那么 master 宕机时该节点上的所有 key 在那一时刻都相当于是释放锁了，这样也会出现并发问题。就算是有 slave 节点，但如果在数据同步到 salve 之前 master 宕机也是会出现上面的问题。\n\n感兴趣的朋友还可以参考 Redisson 的实现。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["Distributed Tools"],"tags":["Distributed Lock"]},{"title":"ElasticSearch 索引 VS MySQL 索引","url":"/2020/08/24/elasticsearch/ElasticSearch%20VS%20MySQL/","content":"\n前言这段时间在维护产品的搜索功能，每次在管理台看到 elasticsearch 这么高效的查询效率我都很好奇他是如何做到的。\n\n这甚至比在我本地使用 MySQL 通过主键的查询速度还快。\n\n\n\n为此我搜索了相关资料：\n\n这类问题网上很多答案，大概意思呢如下：\n\nES 是基于 Lucene 的全文检索引擎，它会对数据进行分词后保存索引，擅长管理大量的索引数据，相对于 MySQL 来说不擅长经常更新数据及关联查询。\n\n说的不是很透彻，没有解析相关的原理；不过既然反复提到了索引，那我们就从索引的角度来对比下两者的差异。\nMySQL 索引先从 MySQL 说起，索引这个词想必大家也是烂熟于心，通常存在于一些查询的场景，是典型的空间换时间的案例。\n以下内容以 Innodb 引擎为例。\n\n常见的数据结构假设由我们自己来设计 MySQL 的索引，大概会有哪些选择呢？\n散列表首先我们应当想到的是散列表，这是一个非常常见且高效的查询、写入的数据结构，对应到 Java 中就是 HashMap\n\n这个数据结构应该不需要过多介绍了，它的写入效率很高O(1),比如我们要查询 id=3 的数据时，需要将 3 进行哈希运算，然后再这个数组中找到对应的位置即可。\n但如果我们想查询 1≤id≤6 这样的区间数据时，散列表就不能很好的满足了，由于它是无序的，所以得将所有数据遍历一遍才能知道哪些数据属于这个区间。\n有序数组\n有序数组的查询效率也很高，当我们要查询 id=4 的数据时，只需要通过二分查找也能高效定位到数据O(logn)。\n同时由于数据也是有序的，所以自然也能支持区间查询；这么看来有序数组适合用做索引咯?\n自然是不行，它有另一个重大问题；假设我们插入了 id=2.5 的数据，就得同时将后续的所有数据都移动一位，这个写入效率就会变得非常低。\n平衡二叉树既然有序数组的写入效率不高，那我们就来看看写入效率高的，很容易就能想到二叉树；这里我们以平衡二叉树为例：\n\n由于平衡二叉树的特性：\n\n左节点小于父节点、右节点大于父节点。\n\n所以假设我们要查询 id=11 的数据，只需要查询 10—&gt;12—&gt;11 便能最终找到数据，时间复杂度为O(logn)，同理写入数据时也为O(logn)。\n但依然不能很好的支持区间范围查找，假设我们要查询5≤id≤20 的数据时，需要先查询10节点的左子树再查询10节点的右子树最终才能查询到所有数据。\n导致这样的查询效率并不高。\n跳表跳表可能不像上边提到的散列表、有序数组、二叉树那样日常见的比较多，但其实 Redis 中的 sort set 就采用了跳表实现。\n这里我们简单介绍下跳表实现的数据结构有何优势。\n我们都知道即便是对一个有序链表进行查询效率也不高，由于它不能使用数组下标进行二分查找，所以时间复杂度是o(n)\n但我们也可以巧妙的优化链表来变相的实现二分查找，如下图：\n\n我们可以为最底层的数据提取出一级索引、二级索引，根据数据量的不同，我们可以提取出 N 级索引。\n当我们查询时便可以利用这里的索引变相的实现了二分查找。\n假设现在要查询 id=13 的数据，只需要遍历 1—&gt;7—&gt;10—&gt;13 四个节点便可以查询到数据，当数越多时，效率提升会更明显。\n同时区间查询也是支持，和刚才的查询单个节点类似，只需要查询到起始节点，然后依次往后遍历（链表有序）到目标节点便能将整个范围的数据查询出来。\n同时由于我们在索引上不会存储真正的数据，只是存放一个指针，相对于最底层存放数据的链表来说占用的空间便可以忽略不计了。\n平衡二叉树的优化但其实 MySQL 中的 Innodb 并没有采用跳表，而是使用的一个叫做 B+ 树的数据结构。\n这个数据结构不像是二叉树那样大学老师当做基础数据结构经常讲到，由于这类数据结构都是在实际工程中根据需求场景在基础数据结构中演化而来。\n比如这里的 B+ 树就可以认为是由平衡二叉树演化而来。\n刚才我们提到二叉树的区间查询效率不高，针对这一点便可进行优化：\n\n在原有二叉树的基础上优化后：所有的非叶子都不存放数据，只是作为叶子节点的索引，数据全部都存放在叶子节点。\n这样所有叶子节点的数据都是有序存放的，便能很好的支持区间查询。\n只需要先通过查询到起始节点的位置，然后在叶子节点中依次往后遍历即可。\n当数据量巨大时，很明显索引文件是不能存放于内存中，虽然速度很快但消耗的资源也不小；所以 MySQL 会将索引文件直接存放于磁盘中。\n这点和后文提到 elasticsearch 的索引略有不同。\n由于索引存放于磁盘中，所以我们要尽可能的减少与磁盘的 IO（磁盘 IO 的效率与内存不在一个数量级）\n通过上图可以看出，我们要查询一条数据至少得进行 4 次IO，很明显这个 IO 次数是与树的高度密切相关的，树的高度越低 IO 次数就会越少，同时性能也会越好。\n那怎样才能降低树的高度呢？\n\t\n我们可以尝试把二叉树变为三叉树，这样树的高度就会下降很多，这样查询数据时的 IO 次数自然也会降低，同时查询效率也会提高许多。\n\n这其实就是 B+ 树的由来。\n\n使用索引的一些建议其实通过上图对 B+树的理解，也能优化日常工作的一些小细节；比如为什么需要主键是有序递增的？\n假设我们写入的主键数据是无序的，那么有可能后写入数据的 id 小于之前写入的，这样在维护 B+树 索引时便有可能需要移动已经写好数据。\n如果是按照递增写入数据时则不会有这个考虑，每次只需要依次写入即可。\n\n所以我们才会要求数据库主键尽量是趋势递增的，不考虑分表的情况时最合理的就是自增主键。\n\n整体来看思路和跳表类似，只是针对使用场景做了相关的调整（比如数据全部存储于叶子节点）。\nES 索引MySQL 聊完了，现在来看看 Elasticsearch 是如何来使用索引的。\n正排索引在 ES 中采用的是一种名叫倒排索引的数据结构；在正式讲倒排索引之前先来聊聊和他相反的正排索引。\n\n以上图为例，我们可以通过 doc_id 查询到具体对象的方式称为使用正排索引，其实也能理解为一种散列表。\n\n本质是通过 key 来查找 value。\n\n比如通过 doc_id=4 便能很快查询到 name=jetty wang,age=20 这条数据。\n倒排索引那如果反过来我想查询 name 中包含了 li 的数据有哪些？这样如何高效查询呢？\n仅仅通过上文提到的正排索引显然起不到什么作用，只能依次将所有数据遍历后判断名称中是否包含 li ；这样效率十分低下。\n但如果我们重新构建一个索引结构：\n\n当要查询 name 中包含 li 的数据时，只需要通过这个索引结构查询到 Posting List 中所包含的数据，再通过映射的方式查询到最终的数据。\n这个索引结构其实就是倒排索引。\nTerm Dictionary但如何高效的在这个索引结构中查询到 li 呢，结合我们之前的经验，只要我们将 Term 有序排列，便可以使用二叉树搜索树的数据结构在o(logn) 下查询到数据。\n将一个文本拆分成一个一个独立Term 的过程其实就是我们常说的分词。\n而将所有 Term 合并在一起就是一个 Term Dictionary，也可以叫做单词词典。\n\n英文的分词相对简单，只需要通过空格、标点符号将文本分隔便能拆词，中文则相对复杂，但也有许多开源工具做支持（由于不是本文重点，对分词感兴趣的可以自行搜索）。\n\n当我们的文本量巨大时，分词后的 Term 也会很多，这样一个倒排索引的数据结构如果存放于内存那肯定是不够存的，但如果像 MySQL 那样存放于磁盘，效率也没那么高。\nTerm Index所以我们可以选择一个折中的方法，既然无法将整个 Term Dictionary 放入内存中，那我们可以为Term Dictionary 创建一个索引然后放入内存中。\n这样便可以高效的查询Term Dictionary ，最后再通过Term Dictionary 查询到 Posting List。\n相对于 MySQL 中的 B+树来说也会减少了几次磁盘IO。\n\n这个 Term Index 我们可以使用这样的 Trie树 也就是我们常说的字典树 来存放。\n更多关于字典树的内容请查看这里。\n\n如果我们是以 j 开头的 Term 进行搜索，首先第一步就是通过在内存中的 Term Index 查询出以 j 打头的 Term 在 Term Dictionary 字典文件中的哪个位置（这个位置可以是一个文件指针，可能是一个区间范围）。\n紧接着在将这个位置区间中的所有 Term 取出，由于已经排好序，便可通过二分查找快速定位到具体位置；这样便可查询出 Posting List。\n最终通过 Posting List 中的位置信息便可在原始文件中将目标数据检索出来。\n更多优化当然 ElasticSearch 还做了许多针对性的优化，当我们对两个字段进行检索时，就可以利用 bitmap 进行优化。\n比如现在需要查询 name=li and age=18 的数据，这时我们需要通过这两个字段将各自的结果 Posting List 取出。\n\n最简单的方法是分别遍历两个集合，取出重复的数据，但这个明显效率低下。\n这时我们便可使用 bitmap 的方式进行存储（还节省存储空间），同时利用先天的位与 *计算便可得出结果。*\n[1, 3, 5]       ⇒ 10101\n[1, 2, 4, 5] ⇒ 11011\n这样两个二进制数组求与便可得出结果：\n10001 ⇒ [1, 5]\n最终反解出 Posting List 为[1, 5],这样的效率自然是要高上许多。\n同样的查询需求在 MySQL 中并没有特殊优化，只是先将数据量小的数据筛选出来之后再筛选第二个字段，效率自然也就没有 ES 高。\n当然在最新版的 ES 中也会对 Posting List 进行压缩，具体压缩规则可以查看官方文档，这里就不具体介绍了。\n总结最后我们来总结一下：\n\n通过以上内容可以看出再复杂的产品最终都是基础数据结构组成，只是会对不同应用场景针对性的优化，所以打好数据结构与算法的基础后再看某个新的技术或中间件时才能快速上手，甚至自己就能知道优化方向。\n最后画个饼，后续我会尝试按照 ES 倒排索引的思路做一个单机版的搜索引擎，只有自己写一遍才能加深理解。\n\n更好的阅读体验请访问此处：https://www.notion.so/ElasticSearch-VS-MySQL-54bddcc092c64c26b2127f1fb9772a23\n\n你的点赞与分享是对我最大的支持œ\n","categories":["数据结构"],"tags":["MySQL","Elasticsearch","倒排索引","B+ 树"]},{"title":"记于 2018 年高考！","url":"/2018/06/06/exam/2018-06-07-The-university-entrance-exam/","content":"2012&#x2F;02&#x2F;28\n\n2012年二月二十八日。\n\n这天学校举行了高考 100 天誓师大会，当时完全不知道意味着什么，只感觉现场热血沸腾、激情高涨，心里告诉自己就算只剩下 100 天我也能考上清华其次也是北大。\n2012&#x2F;06&#x2F;03\n2012年六月三日。\n\n\n晚自习拿出前段时间刚拍的毕业合照，恨死摄影师，没有抓拍到我最帅的角度😡。\n\n\n2012&#x2F;06&#x2F;04\n2012年六月四日。\n\n\n离校前的最后一晚，我们像往常每周的音乐晚自习一样，由音乐委员（@猪娅）带着大家唱可米小子的青春纪念册。\n小红姐（班主任）特别的没来查岗。\n心里想着，这就是青春嘛？也不过如此。\n大家拿着热和的手机（才发的，平时会收）肆意的拍着照片：\n\n那时没有美颜、没有修图，一切都是那么和谐。\n2012&#x2F;06&#x2F;05\n2012年六月五日。\n\n是进津（江津）赶考，走时特地在六食堂买了一个包子，没想到是在学校最后一顿早餐。\n车上大家有说有笑，嗯，就像是资深导游带的一个低价旅游团，每人心里充满了惊喜却不知即将面临什么。\n\n2012&#x2F;06&#x2F;07\n2012年六月七日。\n\n大家在各自的考场奋笔疾书，用两天四场考试来为高中三年画上句号。\n有的梦想进入理想的大学、和心仪的 TA 长相厮守，当然也有回家继承百万家产😂。\n而我当时只想快速的结束这一切，高中三年，特别是高三这年真的是够了。每天做不完的卷子，背不完的诗词，还得想着为陈家老祖宗出一位正儿八经的大学生。\n所以考试完全采用人卷合一的心态（能做就做，不会就过）快速的过完了这两天。\n\n这些作文题目还看得懂嘛。。\n2018&#x2F;06&#x2F;07\n2018年六月七日。\n\n高中学过许多关于时光飞逝的成语、古诗，但都没有亲身体会那么深刻！\n六年时间，红了樱桃，绿了芭蕉。\n有的步入职场、升职加薪、求婚成功、穿上婚纱、组建家庭、初为人母。\n每人都过着各自的生活，但一旦相见就有数不尽的话题（@江源），逃课打球、翻墙上网、暗恋女神、天天向上、作业卷子。\n这句话送给高2012级10班的所有同学：\n愿你出走半生，归来仍是少年\n一大波图片即将袭来：\n摆拍虽好，不要抽烟哦：\n\n小红姐生日快乐，永远十八：\n\n集体生日，年年十八：\n\n\n状元书摊，不是第一不卖：\n\n","categories":["小情绪"]},{"title":"动态代理与RPC","url":"/2020/04/28/framework-design/dynamic-rpc/","content":"\n前言随着最近关注 cim 项目的人越发增多，导致提的问题以及 Bug 也在增加，在修复问题的过程中难免代码洁癖又上来了。\n看着一两年前写的东西总是怀疑这真的是出自自己手里嘛？有些地方实在忍不住了便开始了漫漫重构之路。\n\n\n前后对比在开始之前先简单介绍一下 cim 这个项目，下面是它的架构图：\n简单来说就是一个 IM 即时通讯系统，主要有以下部分组成：\n\nIM-server 自然就是服务端了，用于和客户端保持长连接。\nIM-client 客户端，可以简单认为是类似于的 QQ 这样的客户端工具；当然功能肯定没那么丰富，只提供了一些简单消息发送、接收的功能。\nRoute 路由服务，主要用于客户端鉴权、消息的转发等；提供一些 http 接口，可以用于查看系统状态、在线人数等功能。\n\n当然服务端、路由都可以水平扩展。\n\n\n这是一个消息发送的流程图，假设现在部署了两个服务端 A、B 和一个路由服务；其中 ClientA 和 ClientB 分别和服务端 A、B 保持了长连接。\n当 ClientA 向 ClientB 发送一个 hello world 时，整个的消息流转如图所示：\n\n先通过 http 将消息发送到 Route 服务。\n路由服务得知 ClientB 是连接在 ServerB 上；于是再通过 http 将消息发送给 ServerB。\n最终 ServerB 将消息通过与 ClientB 的长连接通道 push 下去，至此消息发送成功。\n\n这里我截取了 ClientA 向 Route 发起请求的代码：可以看到这就是利用 okhttp 发起了一个 http 请求，这样虽然能实现功能，但其实并不优雅。\n举个例子：假设我们需要对接支付宝的接口，这里发送一个 http 请求自然是没问题；但对于支付宝内部各部门直接互相调用接口时那就不应该再使用原始的 http 请求了。\n应该是由服务提供方提供一个 api 包，服务消费者只需要依赖这个包就可以实现接口调用。\n\n当然最终使用的是 http、还是自定义私有协议都可以。\n\n也类似于我们在使用 Dubbo 或者是 SpringCloud 时，通常是直接依赖一个 api 包，便可以像调用一个本地方法一样调用远程服务了，并且完全屏蔽了底层细节，不管是使用的 http 还是 其他私有协议都没关系，对于调用者来说完全不关心。\n这么一说是不是有内味了，这不就是 RPC 的官方解释嘛。\n对应到这里也是同样的道理，Client 、Route、Server 本质上都是一个系统，他们互相的接口调用也应当是走 RPC 才合理。\n所以我重构之后的变成这样了：\n\n是不是代码也简洁了许多，就和调用本地方法一样了，而且这样也有几个好处：\n\n完全屏蔽了底层细节，可以更好的实现业务及维护代码。\n即便是服务提供方修改了参数，在编译期间就能很快发现，而像之前那样调用是完全不知情的，所以也增加了风险。\n\n绕不开的动态代理下面来聊聊具体是如何实现的。\n其实在上文《动态代理的实际应用》 中也有讲到，原理是类似的。\n要想做到对调用者无感知，就得创建一个接口的代理对象；在这个代理对象中实现编码、调用、解码的过程。\n\n对应到此处其实就是创建一个 routeApi 的代理对象，关键就是这段代码：\nRouteApi routeApi = new ProxyManager&lt;&gt;(RouteApi.class, routeUrl, okHttpClient).getInstance();\n\n完整源码如下：\n其中的 getInstance() 函数就是返回了需要被代理的接口对象；而其中的 ProxyInvocation 则是一个实现了 InvocationHandler 接口的类，这套代码就是利用 JDK 实现动态代理的三板斧。\n\n查看 ProxyInvocation 的源码会发现当我们调用被代理接口的任意一个方法时，都会执行这里的 invoke() 方法。\n而 invoke() 方法自然就实现了上图中提到的：编码、远程调用、解码的过程；相信大家很容易看明白，由于不是本次探讨的重点就不过多介绍了。\n总结其实理解这些就也就很容易看懂 Dubbo 这类 RPC 框架的核心源码了，总体的思路也是类似的，只不过使用的私有协议，所以在编解码时会有所不同。\n所以大家要是想自己动手实现一个 RPC 框架，不妨参考这个思路试试，当用自己写的代码跑通一个 RPC 的 helloworld 时的感觉是和自己整合了一个 Dubbo、SpringCloud 这样的第三方框架的感觉是完全不同的。\n本文的所有源码：\nhttps://github.com/crossoverJie/cim\n你的点赞与分享是对我最大的支持\n","categories":["cim","rpc","动态代理"],"tags":["Java","Netty"]},{"title":"5分钟学会 gRPC","url":"/2022/03/08/framework-design/grpc/","content":"\n介绍我猜测大部分长期使用 Java 的开发者应该较少会接触 gRPC，毕竟在 Java 圈子里大部分使用的还是 Dubbo/SpringClound 这两类服务框架。\n我也是近段时间有机会从零开始重构业务才接触到 gRPC 的，当时选择 gRPC 时也有几个原因：\n\n\n基于云原生的思路开发部署项目，而在云原生中 gRPC 几乎已经是标准的通讯协议了。\n开发语言选择了 Go，在 Go 圈子中 gRPC 显然是更好的选择。\n公司内部有部分业务使用的是 Python 开发，在多语言兼容性上 gRPC 支持的非常好。\n\n\n\n经过线上一年多的平稳运行，可以看出 gRPC 还是非常稳定高效的；rpc 框架中最核心的几个要点：\n\n序列化\n通信协议\nIDL（接口描述语言）\n\n这些在 gRPC 中分别对应的是：\n\n基于 Protocol Buffer 序列化协议，性能高效。\n基于 HTTP/2 标准协议开发，自带 stream、多路复用等特性；同时由于是标准协议，第三方工具的兼容性会更好（比如负载均衡、监控等）\n编写一份 .proto 接口文件，便可生成常用语言代码。\n\nHTTP&#x2F;2学习 gRPC 之前首先得知道它是通过什么协议通信的，我们日常不管是开发还是应用基本上接触到最多的还是 HTTP/1.1 协议。\n\n由于 HTTP/1.1 是一个文本协议，对人类非常友好，相反的对机器性能就比较低。\n需要反复对文本进行解析，效率自然就低了；要对机器更友好就得采用二进制，HTTP/2 自然做到了。\n除此之外还有其他优点：\n\n多路复用：可以并行的收发消息，互不影响\nHPACK 节省 header 空间，避免 HTTP1.1 对相同的 header 反复发送。\n\nProtocolgRPC 采用的是 Protocol 序列化，发布时间比 gRPC 早一些，所以也不仅只用于 gRPC，任何需要序列化 IO 操作的场景都可以使用它。\n它会更加的省空间、高性能；之前在开发 https://github.com/crossoverJie/cim 时就使用它来做数据交互。\npackage order.v1;service OrderService&#123;  rpc Create(OrderApiCreate) returns (Order) &#123;&#125;  rpc Close(CloseApiCreate) returns (Order) &#123;&#125;  // 服务端推送  rpc ServerStream(OrderApiCreate) returns (stream Order) &#123;&#125;  // 客户端推送  rpc ClientStream(stream OrderApiCreate) returns (Order) &#123;&#125;    // 双向推送  rpc BdStream(stream OrderApiCreate) returns (stream Order) &#123;&#125;&#125;message OrderApiCreate&#123;  int64 order_id = 1;  repeated int64 user_id = 2;  string remark = 3;  repeated int32 reason_id = 4;&#125;\n\n使用起来也是非常简单的，只需要定义自己的 .proto 文件，便可用命令行工具生成对应语言的 SDK。\n具体可以参考官方文档：https://grpc.io/docs/languages/go/generated-code/\n调用protoc --go_out=. --go_opt=paths=source_relative \\   --go-grpc_out=. --go-grpc_opt=paths=source_relative \\   test.proto\n生成代码之后编写服务端就非常简单了，只需要实现生成的接口即可。\nfunc (o *Order) Create(ctx context.Context, in *v1.OrderApiCreate) (*v1.Order, error) &#123;\t// 获取 metadata\tmd, ok := metadata.FromIncomingContext(ctx)\tif !ok &#123;\t\treturn nil, status.Errorf(codes.DataLoss, &quot;failed to get metadata&quot;)\t&#125;\tfmt.Println(md)\tfmt.Println(in.OrderId)\treturn &amp;v1.Order&#123;\t\tOrderId: in.OrderId,\t\tReason:  nil,\t&#125;, nil&#125;\n\n\n客户端也非常简单，只需要依赖服务端代码，创建一个 connection 然后就和调用本地方法一样了。\n这是经典的 unary(一元)调用，类似于 http 的请求响应模式，一个请求对应一次响应。\n\nServer streamgRPC 除了常规的 unary 调用之外还支持服务端推送，在一些特定场景下还是很有用的。\n \nfunc (o *Order) ServerStream(in *v1.OrderApiCreate, rs v1.OrderService_ServerStreamServer) error &#123;\tfor i := 0; i &lt; 5; i++ &#123;\t\trs.Send(&amp;v1.Order&#123;\t\t\tOrderId: in.OrderId,\t\t\tReason:  nil,\t\t&#125;)\t&#125;\treturn nil&#125;\n服务端的推送如上所示，调用 Send 函数便可向客户端推送。\nfor &#123;\tmsg, err := rpc.RecvMsg()\tif err == io.EOF &#123;\t\tmarshalIndent, _ := json.MarshalIndent(msgs, &quot;&quot;, &quot;\\t&quot;)\t\tfmt.Println(msg)\t\treturn\t&#125;&#125;\n\n客户端则通过一个循环判断当前接收到的数据包是否已经截止来获取服务端消息。\n为了能更直观的展示这个过程，优化了之前开发的一个 gRPC 客户端，可以直观的调试 stream 调用。\n\n\n上图便是一个服务端推送示例。\n\nClient Stream\n除了支持服务端推送之外，客户端也支持。\n\n客户端在同一个连接中一直向服务端发送数据，服务端可以并行处理消息。\n\n// 服务端代码func (o *Order) ClientStream(rs v1.OrderService_ClientStreamServer) error &#123;\tvar value []int64\tfor &#123;\t\trecv, err := rs.Recv()\t\tif err == io.EOF &#123;\t\t\trs.SendAndClose(&amp;v1.Order&#123;\t\t\t\tOrderId: 100,\t\t\t\tReason:  nil,\t\t\t&#125;)\t\t\tlog.Println(value)\t\t\treturn nil\t\t&#125;\t\tvalue = append(value, recv.OrderId)\t\tlog.Printf(&quot;ClientStream receiv msg %v&quot;, recv.OrderId)\t&#125;\tlog.Println(&quot;ClientStream finish&quot;)\treturn nil&#125;\t// 客户端代码\tfor i := 0; i &lt; 5; i++ &#123;\t\tmessages, _ := GetMsg(data)\t\trpc.SendMsg(messages[0])\t&#125;\treceive, err := rpc.CloseAndReceive()\n\n代码与服务端推送类似，只是角色互换了。\n\nBidirectional Stream\n同理，当客户端、服务端同时都在发送消息也是支持的。\n// 服务端func (o *Order) BdStream(rs v1.OrderService_BdStreamServer) error &#123;\tvar value []int64\tfor &#123;\t\trecv, err := rs.Recv()\t\tif err == io.EOF &#123;\t\t\tlog.Println(value)\t\t\treturn nil\t\t&#125;\t\tif err != nil &#123;\t\t\tpanic(err)\t\t&#125;\t\tvalue = append(value, recv.OrderId)\t\tlog.Printf(&quot;BdStream receiv msg %v&quot;, recv.OrderId)\t\trs.SendMsg(&amp;v1.Order&#123;\t\t\tOrderId: recv.OrderId,\t\t\tReason:  nil,\t\t&#125;)\t&#125;\treturn nil&#125;// 客户端\tfor i := 0; i &lt; 5; i++ &#123;\t\tmessages, _ := GetMsg(data)\t\t// 发送消息\t\trpc.SendMsg(messages[0])\t\t// 接收消息\t\treceive, _ := rpc.RecvMsg()\t\tmarshalIndent, _ := json.MarshalIndent(receive, &quot;&quot;, &quot;\\t&quot;)\t\tfmt.Println(string(marshalIndent))\t&#125;\trpc.CloseSend()\n\n其实就是将上诉两则合二为一。\n\n通过调用示例很容易理解。\n元数据gRPC 也支持元数据传输，类似于 HTTP 中的 header。\n// 客户端写入metaStr := `&#123;&quot;lang&quot;:&quot;zh&quot;&#125;`var m map[string]stringerr := json.Unmarshal([]byte(metaStr), &amp;m)md := metadata.New(m)// 调用时将 ctx 传入即可ctx := metadata.NewOutgoingContext(context.Background(), md)// 服务端接收md, ok := metadata.FromIncomingContext(ctx)if !ok &#123;\treturn nil, status.Errorf(codes.DataLoss, &quot;failed to get metadata&quot;)&#125;fmt.Println(md)\t\n\ngRPC gatewaygRPC 虽然功能强大使用也很简单，但对于浏览器、APP的支持还是不如 REST 应用广泛（浏览器也支持，但应用非常少）。\n为此社区便创建了 https://github.com/grpc-ecosystem/grpc-gateway 项目，可以将 gRPC 服务暴露为 RESTFUL API。\n\n\n为了让测试可以习惯用 postman 进行接口测试，我们也将 gRPC 服务代理出去，更方便的进行测试。\n\n反射调用作为一个 rpc 框架，泛化调用也是必须支持的，可以方便开发配套工具；gRPC 是通过反射支持的，通过拿到服务名称、pb 文件进行反射调用。\nhttps://github.com/jhump/protoreflect 这个库封装了常见的反射操作。\n上图中看到的可视化 stream 调用也是通过这个库实现的。\n负载均衡由于 gRPC 是基于 HTTP/2 实现的，客户端和服务端会保持长连接；这时做负载均衡就不像是 HTTP 那样简单了。\n而我们使用 gRPC 想达到效果和 HTTP 是一样的，需要对请求进行负载均衡而不是连接。\n通常有两种做法：\n\n客户端负载均衡\n服务端负载均衡\n\n客户端负载均衡在 rpc 调用中应用广泛，比如 Dubbo 就是使用的客户端负载均衡。\ngRPC 中也提供有相关接口，具体可以参考官方demo。\nhttps://github.com/grpc/grpc-go/blob/87eb5b7502/examples/features/load_balancing&#x2F;README.md\n客户端负载均衡相对来说对开发者更灵活（可以自定义适合自己的策略），但相对的也需要自己维护这块逻辑，如果有多种语言那就得维护多份。\n所以在云原生这个大基调下，更推荐使用服务端负载均衡。\n可选方案有：\n\nistio\nenvoy\napix\n\n这块我们也在研究，大概率会使用 envoy/istio。\n总结gRPC 内容还是非常多的，本文只是作为一份入门资料希望能让不了解 gRPC 的能有一个基本认识；这在云原生时代确实是一门必备技能。\n\n对文中的 gRPC 客户端感兴趣的朋友，可以参考这里的源码：https://github.com/crossoverJie/ptg\n\n","categories":["framework"],"tags":["Go","gRPC"]},{"title":"分表后需要注意的二三事","url":"/2019/06/13/framework-design/sharding-db-02/","content":"\n前言本篇是上一篇《一次分表踩坑实践的探讨》，所以还没看过的朋友建议先看上文。\n还是先来简单回顾下上次提到了哪些内容：\n\n分表策略：哈希、时间归档等。\n分表字段的选择。\n数据迁移方案。\n\n\n\n而本篇文章的背景是在我们上线这段时间遇到的一些问题并尝试解决的方案。\n问题产生之前提到在分表应用上线前我们需要将原有表的数据迁移到新表中，这样才能保证业务不受影响。\n\n所以我们单独写了一个迁移应用，它负责将大表中的数据迁移到 64 张分表，而再迁移过程中产生的数据毕竟是少数，最后在上线当晚再次迁移过去即可。\n一切想的很美好，当这个应用上线后却发现没这么简单。\n数据库负载升高首先第一个问题是数据库自己就顶不住了，在我们上这个迁移程序之前数据库的压力本身就比较大，这个应用一上去就成了最后一根稻草。\n最后导致的结果是：所有连接了数据库的程序大部分的操作都出现超时，获取不到数据库连接等一系列的异常。\n最后没办法我们只能把这个应用放到凌晨执行，但其实后面观察发现依然不行。\n虽说凌晨的业务量下降，但依然有少部分的请求过来，也会出现各种数据库异常。\n再一个是迁移程序的效率也非常低下，按照这样是速度，我们预估了一下迁移时间，大约需要 10 几天才能把三张最大的表（3、4亿的数据）迁移到分表中。\n于是我们换了一个方案，将这个迁移程序在从库中运行，最后再用运维的方法将分表直接导入进主库。\n因为从库的压力要比主库小很多，对业务的影响很小，同时迁移的效率也要快很多。\n即便是这样也花了一晚上+一个白天的时间才将一张 1亿的数据迁移完成，但是业务上的压力越来越大，数据量再不断新增，这个效率依然不够。\n兼容方案最终没办法只有想一个不迁移数据的方案，但是新产生的数据还是往分表里写，至少保证大表的数据不再新增。\n但这样对于以前的数据咋办呢？总不能不让看了吧。\n其实对于数据的操作无非就分为增删改查，就这四种操作来看看如何兼容。\n新增\n新增最简单，所有的数据根据分表规则直接写入新表，这样可以保证老表的数据不再新增。\n删除删除就要比新增稍微复杂一些，比如用户想要删除他个人产生的一条信息（比如说是订单数据），有可能这个数据在新表也可能在老表。\n\n所以删除时优先删除新表（毕竟新产生的数据访问的频次越高），如果删除失败再从老表删除一次。\n修改\n而修改同理，同样的会不确定数据存在于哪里，所以先要修改新表，失败后再次修改老表。\n查询查询相对就要复杂一些了，因为这些大表的数据大部分都是存放一个用户产生的多条记录（比如一个用户的订单信息）。\n这时在页面上通常都会有分页，并且按照时间进行排序。\n麻烦的地方就出在这里：既然是要分页那就有可能出现要查询一部分分表数据和原来的大表数据做组合。\n所以这里的查询其实分为三种情况。\n\n\n首先查询的时候要计算这个用户所在分表中的数据可以分为几页。\n第一步首先判断当前页是否可以在分表中全部获取，如果可以则直接从分表中取出数据返回（假设分页中总共可以查询 2 页数据，当前为第 1 页，那就全部取分表数据）。\n如果不可以就要判断当前页数在分表中是否取不到任何一条数据，如果是则直接取老表数据（比如现在要取第 5 页的数据，分表中一共才只有 2 页数据，所以第 5 页数据只能全部从老表中获取）。\n但如果分表和老表都存在一部分数据时，则需要同时取两张表然后做一个汇总再返回。\n\n这种逻辑只适用于根据分表字段进行查询分页的前提下\n\n我想肯定会有朋友提出这样是否会有性能问题？\n同时如果在计算分表分页数量时出现并发写入的情况，导致分页数量不准从而对后续的查询出现影响该怎么处理？\n首先第一个性能问题：\n其实这个要看怎么取舍，为了这样的兼容目的其实会比常规查询多出几个步骤：\n\n判断当前页是否可以在分表中查询。\n当新老表中都有数据时候需要额外多查询一张大表。\n\n第一个判断逻辑其实是在内存中计算，这个损耗我觉得完全可以忽略不计。\n至于第二步确实会有损耗，毕竟多查了一张表。\n但在分表之前所有的数据都是从老表中获取的，当时的业务也没有出现问题；现在多的只是查询分表而已，但分表的数据量肯定要比大表小的多，而且有索引，所以这个效率也不会慢多少。\n而且根据局部性原理及用户的使用习惯来看，老表中的数据很少会去查询，随着时间的推移所有的数据肯定都会从分表中获取，逐渐老表就会成为历史表。\n而第二个并发带来的问题我觉得影响也不大，一定要这个分页准的前提肯定得是加锁了，但为了这样一个不痒的小问题却带来性能的下降，我觉得是不划算的。\n而且后续我们也可以慢慢的将老表的数据迁移到新表，这样就可以完全去掉这个兼容逻辑了，所有的数据都从分表中获取。\n总结还是之前那句话，这里的各种操作、方法不适合所有人，毕竟脱离场景都是耍牛氓。\n比如分表搞的早，业务上允许一定的时间将数据迁移到分表那就不会有这次的兼容处理。\n甚至一开始业务规划合理、团队架构师看的长远，一来就将关键数据分表存储那根本就不会有数据迁移这个流程（大厂有经验的团队可能，小公司小作坊都得靠自己摸索）。\n这段期间也被数据库折腾惨了，数据库是最后一根稻草果然也不是瞎说的。\n你的点赞与分享是对我最大的支持\n","categories":["架构"],"tags":["db"]},{"title":"一次难得的分库分表实践","url":"/2019/07/24/framework-design/sharding-db-03/","content":"\n背景前不久发过两篇关于分表的文章：\n\n一次分表踩坑实践的探讨\n分表后需要注意的二三事\n\n从标题可以看得出来，当时我们只做了分表；还是由于业务发展，截止到现在也做了分库，目前看来都还比较顺利，所以借着脑子还记得清楚来一次复盘。\n\n\n先来回顾下整个分库分表的流程如下：\n\n整个过程也很好理解，基本符合大部分公司的一个发展方向。\n很少会有业务一开始就会设计为分库分表，虽说这样会减少后续的坑，但部分公司刚开始都是以业务为主。\n直到业务发展到单表无法支撑时，自然而然会考虑分表甚至分库的事情。\n于是本篇会作一次总结，之前提过的内容可能会再重复一次。\n分表首先讨论下什么样的情况下适合分表？\n根据我的经验来看，当某张表的数据量已经达到千万甚至上亿，同时日增数据量在 2% 以上。\n当然这些数字并不是绝对的，最重要的还是对这张表的写入和查询都已经影响到正常业务执行，比如查询速度明显下降，数据库整体 IO 居高不下等。\n而谈到分表时我们着重讨论的还是水平分表；\n\n也就是将一张大表数据通过某种路由算法将数据尽可能的均匀分配到 N 张小表中。\nRange而分表策略也有好几种，分别适用不同的场景。\n首先第一种是按照范围划分，比如我们可以将某张表的创建时间按照日期划分存为月表；也可以将某张表的主键按照范围划分，比如 【110000】在一张表，【1000120000】在一张表，以此类推。\n\n这样的分表适合需要对数据做归档处理，比如系统默认只提供近三个月历史数据的查询功能，这样也方便操作；只需要把三月之前的数据单独移走备份保存即可）。\n这个方案有好处也有弊端：\n\n好处是自带水平扩展，不需要过多干预。\n缺点是可能会出现数据不均匀的情况（比如某个月请求暴增）。\n\nHash按照日期这样的范围分表固然简单，但适用范围还是比较窄；毕竟我们大部分的数据查询都不想带上时间。\n比如某个用户想查询他产生的所有订单信息，这是很常见的需求。\n于是我们分表的维度就得改改，分表算法可以采用主流的 hash+mod 的组合。\n这是一个经典的算法，大名鼎鼎的 HashMap 也是这样来存储数据。\n假设我们这里将原有的一张大表订单信息分为 64 张分表：\n\n这里的 hash 便是将我们需要分表的字段进行一次散列运算，使得经过散列的数据尽可能的均匀并且不重复。\n当然如果本身这个字段就是一个整形并且不重复也可以省略这个步骤，直接进行 Mod 得到分表下标即可。\n分表数量选择至于这里的分表数量（64）也是有讲究的，具体设为多少这个没有标准值，需要根据自身业务发展，数据增量进行预估。\n根据我个人的经验来看，至少需要保证分好之后的小表在业务发展的几年之内都不会出现单表数据量过大（比如达到千万级）。\n我更倾向于在数据库可接受的范围内尽可能的增大这个分表数，毕竟如果后续小表也达到瓶颈需要再进行一次分表扩容，那是非常痛苦的。\n\n目前笔者还没经历这一步，所以本文没有相关介绍。\n\n但是这个数量又不是瞎选的，和 HashMap 一样，也建议得是 2^n，这样可以方便在扩容的时尽可能的少迁移数据。\nRange + Hash当然还有一种思路，Range 和 Hash 是否可以混用。\n比如我们一开始采用的是 Hash 分表，但是数据增长巨大，导致每张分表数据很快达到瓶颈，这样就不得不再做扩容，比如由 64 张表扩容到 256 张。\n但扩容时想要做到不停机迁移数据非常困难，即便是停机，那停多久呢？也不好说。\n所以我们是否可以在 Mod 分表的基础上再分为月表，借助于 Range 自身的扩展性就不用考虑后续数据迁移的事情了。\n\n这种方式理论可行，但我没有实际用过，给大家的思路做个参考吧。\n烦人的数据迁移分表规则弄好后其实只是完成了分表的第一步，真正麻烦的是数据迁移，或者说是如何做到对业务影响最小的数据迁移。\n除非是一开始就做了分表，所以数据迁移这一步骤肯定是跑不掉的。\n下面整理下目前我们的做法供大家参考：\n\n一旦分表上线后所有的数据写入、查询都是针对于分表的，所以原有大表内的数据必须得迁移到分表里，不然对业务的影响极大。\n我们估算了对一张 2 亿左右的表进行迁移，自己写的迁移程序，大概需要花 4~5 天的时间才能完成迁移。\n意味着这段时间内，以前的数据对用户是不可见的，显然这样业务不能接受。\n于是我们做了一个兼容处理：分表改造上线后，所有新产生的数据写入分表，但对历史数据的操作还走老表，这样就少了数据迁移这一步骤。\n只是需要在操作数据之前做一次路由判断，当新数据产生的足够多时（我们是两个月时间），几乎所有的操作都是针对于分表，再从库启动数据迁移，数据迁移完毕后将原有的路由判断去掉。\n最后所有的数据都从分表产生和写入。\n\n至此整个分表操作完成。\n\n\n业务兼容同时分表之后还需要兼容其他业务；比如原有的报表业务、分页查询等，现在来看看我们是如何处理的。\n报表首先是报表，没分表之前之间查询一张表就搞定了，现在不同，由一张表变为 N 张表。\n所以原有的查询要改为遍历所有的分表，考虑到性能可以利用多线程并发查询分表数据然后汇总。\n不过只依靠 Java 来对这么大量的数据做统计分析还是不现实，刚开始可以应付过去，后续还得用上大数据平台来处理。\n查询再一个是查询，原有的分页查询肯定是不能用了，毕竟对上亿的数据分页其实没什么意义。\n只能提供通过分表字段的查询，比如是按照订单 ID 分表，那查询条件就得带上这个字段，不然就会涉及到遍历所有表。\n这也是所有分表之后都会遇到的一个问题，除非不用 MySQL 这类关系型数据库。\n分库分表完成后可以解决单表的压力，但数据库本身的压力却没有下降。\n我们在完成分表之后的一个月内又由于数据库里“其他表”的写入导致整个数据库 IO 增加，而且这些“其他表”还和业务关系不大。\n也就是说一些可有可无的数据导致了整体业务受影响，这是非常不划算的事情。\n于是我们便把这几张表单独移到一个新的数据库中，完全和现有的业务隔离开来。\n这样就会涉及到几个改造：\n\n应用自身对这些数据的查询、写入都要改为调用一个独立的 Dubbo 服务，由这个服务对迁移的表进行操作。\n暂时不做数据迁移，所以查询时也得按照分表那样做一个兼容，如果查询老数据就要在当前库查询，新数据就要调用 Dubbo 接口进行查询。\n对这些表的一些关联查询也得改造为查询 Dubbo 接口，在内存中进行拼接即可。\n如果数据量确实很大，也可将同步的 Dubbo 接口换为写入消息队列来提高吞吐量。\n\n目前我们将这类数据量巨大但对业务不太影响的表单独迁到一个库后，数据库的整体 IO 下降明显，业务也恢复正常。\n总结最后我们还需要做一步历史数据归档的操作，将 N 个月之前的数据要定期迁移到 HBASE 之类存储，保证 MySQL 中的数据一直保持在一个可接受的范围。\n而归档数据的查询便依赖于大数据提供服务。\n本次分库分表是一次非常难得的实践操作，网上大部分的资料都是在汽车出厂前就换好了轮胎。\n而我们大部分碰到的场景都是要对高速路上跑着的车子换胎，一不小心就“车毁人亡”。\n有更好的方式方法欢迎大家评论区留言讨论。\n你的点赞与分享是对我最大的支持\n","categories":["架构"],"tags":["db"]},{"title":"一次分表踩坑实践的探讨","url":"/2019/04/16/framework-design/sharding-db/","content":"\n前言之前不少人问我“能否分享一些分库分表相关的实践”，其实不是我不分享，而是真的经验不多🤣；和大部分人一样都是停留在理论阶段。\n不过这次多少有些可以说道了。\n先谈谈背景，我们生产数据库随着业务发展量也逐渐起来；好几张单表已经突破亿级数据，并且保持每天 200+W 的数据量增加。\n而我们有些业务需要进行关联查询、或者是报表统计；在这样的背景下大表的问题更加突出（比如一个查询功能需要跑好几分钟）。\n\n\n\n可能很多人会说：为啥单表都过亿了才想方案解决？其实不是不想，而是由于历史原因加上错误预估了数据增长才导致这个局面。总之原因比较复杂，也不是本次讨论的重点。\n\n临时方案由于需求紧、人手缺的情况下，整个处理的过程分为几个阶段。\n第一阶段应该是去年底，当时运维反应 MySQL 所在的主机内存占用很高，整体负载也居高不下，导致整个 MySQL 的吞吐量明显降低（写入、查询数据都明显减慢）。\n为此我们找出了数据量最大的几张表，发现大部分数据量在7&#x2F;8000W 左右，少数的已经突破一亿。\n通过业务层面进行分析发现，这些数据多数都是用户产生的一些日志型数据，而且这些数据在业务上并不是强相关的，甚至两三个月前的数据其实已经不需要实时查询了。\n因为接近年底，尽可能的不想去动应用，考虑是否可以在运维层面缓解压力；主要的目的就是把单表的数据量降低。\n原本是想把两个月之前的数据直接迁移出来放到备份表中，但在准备实施的过程中发现一个大坑。\n\n表中没有一个可以排序的索引，导致我们无法快速的筛选出一部分数据！这真是一个深坑，为后面的一些优化埋了个地雷；即便是加索引也需要花几个小时（具体多久没敢在生产测试）。\n\n如果我们强行按照时间进行筛选，可能查询出 4000W 的数据就得花上好几个小时；这显然是行不通的。\n于是我们便想到了一个大胆的想法：这部分数据是否可以直接不要了？\n这可能是最有效及最快的方式了，和产品沟通后得知这部分数据真的只是日志型的数据，即便是报表出不来今后补上也是可以的。\n于是我们就简单粗暴的做了以下事情：\n\n修改原有表的表名，比如加上(_190416bak)。\n再新建一张和原有表名称相同的表。\n\n这样新的数据就写到了新表，同时业务上也是使用的这个数据量较小的新表。\n虽说过程不太优雅，但至少是解决了问题同时也给我们做技术改造预留了时间。\n分表方案之前的方案虽说可以缓解压力，但不能根本解决问题。\n有些业务必须得查询之前的数据，导致之前那招行不通了，所以正好我们就借助这个机会把表分了。\n我相信大部分人虽说没有做过实际做过分表，但也见过猪跑；网上一搜各种方案层出不穷。\n我认为最重要的一点是要结合实际业务找出需要 sharding 的字段，同时还有上线阶段的数据迁移也非常重要。\n时间可能大家都会说用 hash 的方式分配得最均匀，但我认为这还是需要使用历史数据的场景才用哈希分表。\n而对于不需要历史数据的场景，比如业务上只查询近三个月的数据。\n这类需求完成可以采取时间分表，按照月份进行划分，这样改动简单，同时对历史数据也比较好迁移。\n于是我们首先将这类需求的表筛选出来，按照月份进行拆分，只是在查询的时候拼接好表名即可；也比较好理解。\n哈希刚才也提到了：需要根据业务需求进行分表策略。\n而一旦所有的数据都有可能查询时，按照时间分表也就行不通了。（也能做，只是如果不是按照时间进行查询时需要遍历所有的表）\n因此我们计划采用 hash 的方式分表，这算是业界比较主流的方式就不再赘述。\n采用哈希时需要将 sharding 字段选好，由于我们的业务比较单纯；是一个物联网应用，所有的数据都包含有物联网设备的唯一标识（IMEI），并且这个字段天然的就保持了唯一性；大多数的业务也都是根据这个字段来的，所以它非常适合来做这个 sharding 字段。\n在做分表之前也调研过 MyCAT 及 sharding-jdbc(现已升级为 shardingsphere)，最终考虑到对开发的友好性及不增加运维复杂度还是决定在 jdbc 层 sharding 的方式。\n但由于历史原因我们并不太好集成 sharding-jdbc，但基于 sharding 的特点自己实现了一个分表策略。\n这个简单也好理解：\nint index = hash(sharding字段) % 分表数量 ;select xx from &#x27;busy_&#x27;+index where sharding字段 = xxx;\n\n其实就是算出了表名，然后路由过去查询即可。\n只是我们实现的非常简单：修改了所有的底层查询方法，每个方法都里都做了这样的一个判断。\n并没有像 sharding-jdbc 一样，代理了数据库的查询方法；其中还要做 SQL解析--&gt;SQL路由--&gt;执行SQL--&gt;合并结果 这一系列的流程。\n如果自己再做一遍无异于重新造了一个轮子，并且并不专业，只是在现有的技术条件下选择了一个快速实现达成效果的方法。\n不过这个过程中我们节省了将 sharding 字段哈希的过程，因为每一个 IMEI 号其实都是一个唯一的整型，直接用它做 mod 运算即可。\n还有一个是需要一个统一的主键生成规则，分表后不能再依赖于单表的字段自增了；方法还是挺多的：\n\n比如时间戳+随机数可满足大部分业务。\nUUID，生成简单，但没法做排序。\n雪花算法统一生成主键ID。\n\n大家可以根据自己的实际情况做选择。\n业务调整因为我们并没有使用第三方的 sharding-jdbc 组件，所以没有办法做到对代码的低侵入性；每个涉及到分表的业务代码都需要做底层方法的改造（也就是路由到正确的表）。\n考虑到后续业务的发展，我们决定将拆分的表分为 64 张；加上后续引入大数据平台足以应对几年的数据增长。\n\n这里还有个小细节需要注意：分表的数量需要为 2∧N 次方，因为在取模的这种分表方式下，即便是今后再需要分表影响的数据也会尽量的小。\n\n再修改时只能将表名称进行全局搜索，然后加以修改，同时根据修改的方法倒推到表现的业务并记录下来，方便后续回归测试。\n\n当然无法避免查询时利用非 sharding 字段导致的全表扫描，这是所有分片后都会遇到的问题。\n因此我们在修改分表方法的底层查询时同时也会查看是否有走分片字段，如果不是，那是否可以调整业务。\n比如对于一个上亿的数据是否还有必要存在按照分页查询、日期查询？这样的业务是否真的具有意义？\n我们尽可能的引导产品按照这样的方式来设计产品或者做出调整。\n但对于报表这类的需求确实也没办法，比如统计表中某种类型的数据；这种我们也可以利用多线程的方式去并行查询然后汇总统计来提高查询效率。\n有时也有一些另类场景：\n\n比如一个千万表中有某一特殊类型的数据只占了很小一部分，比如说几千上万条。\n\n这时页面上需要对它进行分页查询是比较正常的（比如某种投诉消息，客户需要一条一条的单独处理），但如果我们按照 IMEI 号或者是主键进行分片后再分页查询那就比较蛋疼了。\n所以这类型的数据建议单独新建一张表来维护，不要和其他数据混合在一起，这样不管是做分页还是 like 都比较简单和独立。\n验证代码改完，开发也单测完成后怎么来验证分表的业务是否正常也比较麻烦。\n一个是测试麻烦，再一个是万一哪里改漏了还是查询的原表，但这样在测试环境并不会有异常，一旦上线产生了生产数据到新的 64 张表后想要再修复就比较麻烦了。\n所以我们取了个巧，直接将原表的表名修改，比如加一个后缀；这样在测试过程中观察前后台有无报错就比较容易提前发现这个问题。\n上线流程测试验收通过后只是分表这个需求的80%，剩下如何上线也是比较头疼。\n一旦应用上线后所有的查询、写入、删除都会先走路由然后到达新表；而老数据在原表里是不会发生改变的。\n数据迁移所以我们上线前的第一步自然是需要将原有的数据进行迁移，迁移的目的是要分片到新的 64 张表中，这样才会对原有的业务无影响。\n因此我们需要额外准备一个程序，它需要将老表里的数据按照分片规则复制到新表中；\n在我们这个场景下，生产数据有些已经上亿了，这个迁移过程我们在测试环境模拟发现耗时是非常久的。而且我们老表中对于 create_time 这样用于筛选数据的字段没有索引（以前的技术债），所以查询起来就更加慢了。\n最后没办法，我们只能和产品协商告知用户对于之前产生的数据短期可能会查询不到，这个时间最坏可能会持续几天（我们只能在凌晨迁移，白天会影响到数据库负载）。\n总结这便是我们这次的分表实践，虽说不少过程都不优雅，但受限于条件也只能折中处理。\n但我们后续的计划是，修改我们底层的数据连接（目前是自己封装的一个 jar 包，导致集成 sharding-jdbc 比较麻烦）最终逐渐迁移到 sharding-jdbc .\n最后得出了几个结论：\n\n一个好的产品规划非常有必要，可以在合理的时间对数据处理（不管是分表还是切入归档）。\n每张表都需要一个可以用于排序查询的字段（自增ID、创建时间），整个过程由于没有这个字段导致耽搁了很长时间。\n分表字段需要谨慎，要全盘的考虑业务情况，尽量避免出现查询扫表的情况。\n\n最后欢迎留言讨论。\n你的点赞与分享是对我最大的支持\n","categories":["架构"],"tags":["db"]},{"title":"Git cherry-pick 使用小技巧","url":"/2025/09/11/git/git-tips-cherry-pick/","content":"背景前段时间我在实现 StarRocks 的一个关于资源限制的特性，由于该功能需要基于最新的 3.5 tag 进行开发，所以我需要需要从 3.5 的 tag 里拉出一个分支（3.5-feature）开发完成后再向上游的 main 分支提交 PR。\n\n\n解决方案如果直接使用 3.5-feature 分支向 main 提交 PR 会有很多之前的 commit 导致文件修改很多，所以得换一种方式提交这些变更。\nstash解决这个问题的办法也蛮多的，第一种是使用 git stash。\n这个可以先将修改临时保存，然后再需要的时候再将 stash 里的数据应用到当前分支。\n但这样有几个问题：\n\n开发的过程中没法提交代码了，提交之后就没法保存到 stash 里，没有提交代码总会有丢数据的风险。\n后续需要多次提交时，stash 也不支持，毕竟 stash 之后当前分支的代码就没有了。\n\n手动复制第二种办法那就是先把代码提交到 3.5-feature，然后重新基于 main 分支重新拉取一个分支，再手动对比 3.5-feature 的提交记录进行修改。\n这样的好处是提交记录比较干净，但坏处是改动较多的话容易遗漏代码，并且每次在 3.5-feature 的改动都需要人工同步过来。\ncherry-pick我第一次看到 cherry-pick 的用法还是在 Pulsar 社区里，经常看到有 PR 将某个在 main 分支的特性 cherry-pick 到其他分支。\n\n或者是一些重要的安全更新也需要在一些维护版本的分支进行修复。\ncherry-pick 的主要作用是将其他分支的特定提交精确合并到当前分支，以便在不合并整个分支的情况下同步修复、hot-fix 或者是复用代码；\n使用流程如下：\ngit checkout main git pull origin main git checkout -b main-feature\n\n# 只pick你真正需要的功能相关提交# 你的核心功能提交1 git cherry-pick abc123 # 你的核心功能提交2git cherry-pick def456 ...\n\n\n\n这里的 abc123 和 def456 都是提交记录的 hash 值，可以通过 git log 命令获取；也可以在 github 的提交记录页面复制。\n\n如果碰到冲突先解决，然后执行：\ngit cherry-pick --continuegit push origin main-feature\n\n假设存在两个分支：\na - b - c - d   main     \\       e - f - g main-feature\n\ngit cherry-pick f\n现在需要将 main-feature 分支里的 f 提交到 main 分支，cherry-pick 之后会变成这样：\na - b - c - d - f   main     \\       e - f - g main-feature\nf 这个提交就会被追加到头部。\n适用场景cherry-pick 主要应用与一些小的修复，安全更新、紧急 bug 的处理，如果一个分支上的大部分 commit 都要被 cherry-pick 到另一个分支，不如考虑直接 使用 merge 或者 rebase。\n就像这个功能的名称一样：摘樱桃。选择合适的果子进行摘取，而不是把所有的提交都合并过去。\n#Blog #Github #Git\n","categories":["git"],"tags":["cherry-pick"]},{"title":"几百行代码实现一个 JSON 解析器","url":"/2022/06/28/gjson/gjson01/","content":"\n前言之前在写 gscript时我就在想有没有利用编译原理实现一个更实际工具？毕竟真写一个语言的难度不低，并且也很难真的应用起来。\n一次无意间看到有人提起 JSON 解析器，这类工具充斥着我们的日常开发，运用非常广泛。\n以前我也有思考过它是如何实现的，过程中一旦和编译原理扯上关系就不由自主的劝退了；但经过这段时间的实践我发现实现一个 JSON 解析器似乎也不困难，只是运用到了编译原理前端的部分知识就完全足够了。\n得益于 JSON 的轻量级，同时语法也很简单，所以核心代码大概只用了 800 行便实现了一个语法完善的 JSON 解析器。\n\n\n首先还是来看看效果：\nimport &quot;github.com/crossoverJie/xjson&quot;func TestJson(t *testing.T) &#123;\tstr := `&#123;   &quot;glossary&quot;: &#123;       &quot;title&quot;: &quot;example glossary&quot;,\t\t&quot;age&quot;:1,\t\t&quot;long&quot;:99.99,\t\t&quot;GlossDiv&quot;: &#123;           &quot;title&quot;: &quot;S&quot;,\t\t\t&quot;GlossList&quot;: &#123;               &quot;GlossEntry&quot;: &#123;                   &quot;ID&quot;: &quot;SGML&quot;,\t\t\t\t\t&quot;SortAs&quot;: &quot;SGML&quot;,\t\t\t\t\t&quot;GlossTerm&quot;: &quot;Standard Generalized Markup Language&quot;,\t\t\t\t\t&quot;Acronym&quot;: &quot;SGML&quot;,\t\t\t\t\t&quot;Abbrev&quot;: &quot;ISO 8879:1986&quot;,\t\t\t\t\t&quot;GlossDef&quot;: &#123;                       &quot;para&quot;: &quot;A meta-markup language, used to create markup languages such as DocBook.&quot;,\t\t\t\t\t\t&quot;GlossSeeAlso&quot;: [&quot;GML&quot;, &quot;XML&quot;, true, null]                   &#125;,\t\t\t\t\t&quot;GlossSee&quot;: &quot;markup&quot;               &#125;           &#125;       &#125;   &#125;&#125;`\tdecode, err := xjson.Decode(str)\tassert.Nil(t, err)\tfmt.Println(decode)\tv := decode.(map[string]interface&#123;&#125;)\tglossary := v[&quot;glossary&quot;].(map[string]interface&#123;&#125;)\tassert.Equal(t, glossary[&quot;title&quot;], &quot;example glossary&quot;)\tassert.Equal(t, glossary[&quot;age&quot;], 1)\tassert.Equal(t, glossary[&quot;long&quot;], 99.99)\tglossDiv := glossary[&quot;GlossDiv&quot;].(map[string]interface&#123;&#125;)\tassert.Equal(t, glossDiv[&quot;title&quot;], &quot;S&quot;)\tglossList := glossDiv[&quot;GlossList&quot;].(map[string]interface&#123;&#125;)\tglossEntry := glossList[&quot;GlossEntry&quot;].(map[string]interface&#123;&#125;)\tassert.Equal(t, glossEntry[&quot;ID&quot;], &quot;SGML&quot;)\tassert.Equal(t, glossEntry[&quot;SortAs&quot;], &quot;SGML&quot;)\tassert.Equal(t, glossEntry[&quot;GlossTerm&quot;], &quot;Standard Generalized Markup Language&quot;)\tassert.Equal(t, glossEntry[&quot;Acronym&quot;], &quot;SGML&quot;)\tassert.Equal(t, glossEntry[&quot;Abbrev&quot;], &quot;ISO 8879:1986&quot;)\tglossDef := glossEntry[&quot;GlossDef&quot;].(map[string]interface&#123;&#125;)\tassert.Equal(t, glossDef[&quot;para&quot;], &quot;A meta-markup language, used to create markup languages such as DocBook.&quot;)\tglossSeeAlso := glossDef[&quot;GlossSeeAlso&quot;].(*[]interface&#123;&#125;)\tassert.Equal(t, (*glossSeeAlso)[0], &quot;GML&quot;)\tassert.Equal(t, (*glossSeeAlso)[1], &quot;XML&quot;)\tassert.Equal(t, (*glossSeeAlso)[2], true)\tassert.Equal(t, (*glossSeeAlso)[3], &quot;&quot;)\tassert.Equal(t, glossEntry[&quot;GlossSee&quot;], &quot;markup&quot;)&#125;\n\n从这个用例中可以看到支持字符串、布尔值、浮点、整形、数组以及各种嵌套关系。                                                                                      \n实现原理\n这里简要说明一下实现原理，本质上就是两步：\n\n词法解析：根据原始输入的 JSON 字符串解析出 token，也就是类似于 &quot;&#123;&quot; &quot;obj&quot; &quot;age&quot; &quot;1&quot; &quot;[&quot; &quot;]&quot; 这样的标识符，只是要给这类标识符分类。\n根据生成的一组 token 集合，以流的方式进行读取，最终可以生成图中的树状结构，也就是一个 JSONObject 。\n\n下面来重点看看这两个步骤具体做了哪些事情。\n词法分析BeginObject  &#123;String  &quot;name&quot;SepColon  :String  &quot;cj&quot;SepComma  ,String  &quot;object&quot;SepColon  :BeginObject  &#123;String  &quot;age&quot;SepColon  :Number  10SepComma  ,String  &quot;sex&quot;SepColon  :String  &quot;girl&quot;EndObject  &#125;SepComma  ,String  &quot;list&quot;SepColon  :BeginArray  [\n\n其实词法解析就是构建一个有限自动机的过程（DFA)，目的是可以生成这样的集合（token）,只是我们需要将这些 token进行分类以便后续做语法分析的时候进行处理。\n比如 &quot;&#123;&quot; 这样的左花括号就是一个 BeginObject 代表一个对象声明的开始，而 &quot;&#125;&quot; 则是 EndObject 代表一个对象的结束。\n其中 &quot;name&quot; 这样的就被认为是 String 字符串，以此类推 &quot;[&quot; 代表 BeginArray\n这里我一共定义以下几种 token 类型：\ntype Token stringconst (\tInit        Token = &quot;Init&quot;\tBeginObject       = &quot;BeginObject&quot;\tEndObject         = &quot;EndObject&quot;\tBeginArray        = &quot;BeginArray&quot;\tEndArray          = &quot;EndArray&quot;\tNull              = &quot;Null&quot;\tNull1             = &quot;Null1&quot;\tNull2             = &quot;Null2&quot;\tNull3             = &quot;Null3&quot;\tNumber            = &quot;Number&quot;\tFloat             = &quot;Float&quot;\tBeginString       = &quot;BeginString&quot;\tEndString         = &quot;EndString&quot;\tString            = &quot;String&quot;\tTrue              = &quot;True&quot;\tTrue1             = &quot;True1&quot;\tTrue2             = &quot;True2&quot;\tTrue3             = &quot;True3&quot;\tFalse             = &quot;False&quot;\tFalse1            = &quot;False1&quot;\tFalse2            = &quot;False2&quot;\tFalse3            = &quot;False3&quot;\tFalse4            = &quot;False4&quot;\t// SepColon :\tSepColon = &quot;SepColon&quot;\t// SepComma ,\tSepComma = &quot;SepComma&quot;\tEndJson  = &quot;EndJson&quot;)\n\n\n其中可以看到  true&#x2F;false&#x2F;null 会有多个类型，这点先忽略，后续会解释。\n\n以这段 JSON 为例：&#123;&quot;age&quot;:1&#125;，它的状态扭转如下图：\n总的来说就是依次遍历字符串，然后更新一个全局状态，根据该状态的值进行不同的操作。\n部分代码如下：\n感兴趣的朋友可以跑跑单例 debug 一下就很容易理解：\nhttps://github.com/crossoverJie/xjson/blob/main/token_test.go\n以这段 JSON 为例：\nfunc TestInitStatus(t *testing.T) &#123;\tstr := `&#123;&quot;name&quot;:&quot;cj&quot;, &quot;age&quot;:10&#125;`\ttokenize, err := Tokenize(str)\tassert.Nil(t, err)\tfor _, tokenType := range tokenize &#123;\t\tfmt.Printf(&quot;%s  %s\\n&quot;, tokenType.T, tokenType.Value)\t&#125;&#125;\n\n最终生成的 token 集合如下：\nBeginObject  &#123;String  &quot;name&quot;SepColon  :String  &quot;cj&quot;SepComma  ,String  &quot;age&quot;SepColon  :Number  10EndObject  &#125;\n\n\n提前检查由于 JSON 的语法简单，一些规则甚至在词法规则中就能校验。\n举个例子：JSON 中允许 null 值，当我们字符串中存在 nu  nul 这类不匹配 null 的值时，就可以提前抛出异常。比如当检测到第一个字符串为 n 时，那后续的必须为 u-&gt;l-&gt;l 不然就抛出异常。\n浮点数同理，当一个数值中存在多个 . 点时，依然需要抛出异常。\n这也是前文提到 true/false/null 这些类型需要有多个中间状态的原因。\n生成 JSONObject 树在讨论生成 JSONObject 树之前我们先来看这么一个问题，给定一个括号集合，判断是否合法。\n\n[&lt;()&gt;] 这样是合法的。\n[&lt;()&gt;) 而这样是不合法的。\n\n如何实现呢？其实也很简单，只需要利用栈就能完成，如下图所示：利用栈的特性，依次遍历数据，遇到是左边的符号就入栈，当遇到是右符号时就与栈顶数据匹配，能匹配上就出栈。\n当匹配不上时则说明格式错误，数据遍历完毕后如果栈为空时说明数据合法。\n其实仔细观察 JSON 的语法也是类似的：\n&#123;    &quot;name&quot;: &quot;cj&quot;,    &quot;object&quot;: &#123;        &quot;age&quot;: 10,        &quot;sex&quot;: &quot;girl&quot;    &#125;,    &quot;list&quot;: [        &#123;            &quot;1&quot;: &quot;a&quot;        &#125;,        &#123;            &quot;2&quot;: &quot;b&quot;        &#125;    ]&#125;\n\nBeginObject:&#123; 与 EndObject:&#125; 一定是成对出现的，中间如论怎么嵌套也是成对的。而对于 &quot;age&quot;:10 这样的数据，: 冒号后也得有数据进行匹配，不然就是非法格式。\n所以基于刚才的括号匹配原理，我们也能用类似的方法来解析 token 集合。\n我们也需要创建一个栈，当遇到 BeginObject 时就入栈一个 Map，当遇到一个 String 键时也将该值入栈。\n当遇到 value 时，就将出栈一个 key,同时将数据写入当前栈顶的 map 中。\n当然在遍历 token 的过程中也需要一个全局状态，所以这里也是一个有限状态机。\n\n举个例子：当我们遍历到 Token 类型为 String，值为 &quot;name&quot; 时，预期下一个 token 应当是 :冒号；\n所以我们得将当前的 status 记录为 StatusColon，一旦后续解析到 token 为 SepColon 时，就需要判断当前的 status 是否为 StatusColon ，如果不是则说明语法错误，就可以抛出异常。\n同时值得注意的是这里的 status 其实是一个集合，因为下一个状态可能是多种情况。\n&#123;&quot;e&quot;:[1,[2,3],&#123;&quot;d&quot;:&#123;&quot;f&quot;:&quot;f&quot;&#125;&#125;]&#125;比如当我们解析到一个 SepColon 冒号时，后续的状态可能是 value 或 BeginObject &#123; 或 BeginArray [\n因此这里就得把这三种情况都考虑到，其他的以此类推。\n具体解析过程可以参考源码：https://github.com/crossoverJie/xjson/blob/main/parse.go\n\n虽然是借助一个栈结构就能将 JSON 解析完毕，不知道大家发现一个问题没有：这样非常容易遗漏规则，比如刚才提到的一个冒号后面就有三种情况，而一个 BeginArray 后甚至有四种情况（StatusArrayValue, StatusBeginArray, StatusBeginObject, StatusEndArray）\n这样的代码读起来也不是很直观，同时容易遗漏语法，只能出现问题再进行修复。\n既然提到了问题那自然也有相应的解决方案，其实就是语法分析中常见的递归下降算法。\n我们只需要根据 JSON 的文法定义，递归的写出算法即可，这样代码阅读起来非常清晰，同时也不会遗漏规则。\n完整的 JSON 语法查看这里：https://github.com/antlr/grammars-v4/blob/master/json/JSON.g4\n我也预计将下个版本改为递归下降算法来实现。\n总结当目前为止其实只是实现了一个非常基础的 JSON 解析，也没有做性能优化，和官方的 JSON 包对比性能差的不是一星半点。\ncpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHzBenchmarkJsonDecode-12            372298             15506 ns/op             512 B/op         12 allocs/opBenchmarkDecode-12                141482             43516 ns/op           30589 B/op        962 allocs/opPASS\n\n同时还有一些基础功能没有实现，比如将解析后的 JSONObject 可以反射生成自定义的 Struct，以及我最终想实现的支持 JSON 的四则运算：\nxjson.Get(&quot;glossary.age+long*(a.b+a.c)&quot;)\n\n目前我貌似没有发现有类似的库实现了这个功能，后面真的完成后应该会很有意思，感兴趣的朋友请持续关注。\n源码：https://github.com/crossoverJie/xjson\n","categories":["xjson","compiler"],"tags":["go"]},{"title":"用面向对象的方式操作 JSON 甚至还能做四则运算 JSON 库","url":"/2022/07/04/gjson/gjson02/","content":"\n前言在之前实现的 JSON 解析器中当时只实现了将一个 JSON 字符串转换为一个 JSONObject，并没有将其映射为一个具体的 struct；如果想要获取值就需要先做断言将其转换为 map 或者是切片再来获，会比较麻烦。\ndecode, err := xjson.Decode(`&#123;&quot;glossary&quot;:&#123;&quot;title&quot;:&quot;example glossary&quot;,&quot;age&quot;:1&#125;&#125;`)assert.Nil(t, err)glossary := v[&quot;glossary&quot;].(map[string]interface&#123;&#125;)assert.Equal(t, glossary[&quot;title&quot;], &quot;example glossary&quot;)assert.Equal(t, glossary[&quot;age&quot;], 1)\n\n但其实转念一想，部分场景我们甚至我们只需要拿到 JSON 中的某个字段的值，这样还需要先声明一个 struct 会略显麻烦。\n经过查询发现已经有了一个类似的库来解决该问题，https://github.com/tidwall/xjson 并且 star 数还很多（甚至名字都是一样的😂），说明这样的需求大家还是很强烈的。\n于是我也打算增加类似的功能，使用方式如下：\n\n\n\n最后还加上了一个四则运算的功能。\n\n面向对象的方式操作 JSON因为功能类似，所以我参考了 tidwall 的 API 但去掉一些我觉得暂时用不上的特性，并调整了一点语法。\n当前这个版本只能通过确定的 key 加上 . 点符号访问数据，如果是数组则用 [index] 的方式访问下标。[] 符号访问数组我觉得要更符合直觉一些。\n以下是一个包含多重嵌套 JSON 的访问示例：\nstr := `&#123;&quot;name&quot;: &quot;bob&quot;,&quot;age&quot;: 20,&quot;skill&quot;: &#123;    &quot;lang&quot;: [        &#123;            &quot;go&quot;: &#123;                &quot;feature&quot;: [                    &quot;goroutine&quot;,                    &quot;channel&quot;,                    &quot;simple&quot;,                    true                ]            &#125;        &#125;    ]&#125;&#125;`name := xjson.Get(str, &quot;name&quot;)assert.Equal(t, name.String(), &quot;bob&quot;)age := xjson.Get(str, &quot;age&quot;)assert.Equal(t, age.Int(), 20)assert.Equal(t, xjson.Get(str,&quot;skill.lang[0].go.feature[0]&quot;).String(), &quot;goroutine&quot;)assert.Equal(t, xjson.Get(str,&quot;skill.lang[0].go.feature[1]&quot;).String(), &quot;channel&quot;)assert.Equal(t, xjson.Get(str,&quot;skill.lang[0].go.feature[2]&quot;).String(), &quot;simple&quot;)assert.Equal(t, xjson.Get(str,&quot;skill.lang[0].go.feature[3]&quot;).Bool(), true)\n这样的语法使用个人觉得还是满符合直觉的，相信对使用者来说也比较简单。\n返回值参考了 tidwall 使用了一个 Result 对象，它提供了多种方法可以方便的获取各种类型的数据\nfunc (r Result) String() stringfunc (r Result) Bool() boolfunc (r Result) Int() intfunc (r Result) Float() float64func (r Result) Map() map[string]interface&#123;&#125;func (r Result) Array() *[]interface&#123;&#125;func (r Result) Exists() bool\n比如使用 Map()/Array() 这两个函数可以将 JSON 数据映射到 map 和切片中，当然前提是传入的语法返回的是一个合法 JSONObject 或数组。\n实现原理在实现之前需要先定义一个基本语法，主要支持以下四种用法：\n\n单个 key 的查询：Get(json,&quot;name&quot;)\n嵌套查询： Get(json,&quot;obj1.obj2.obj3.name&quot;)\n数组查询：Get(json,&quot;obj.array[0]&quot;)\n数组嵌套查询：Get(json,&quot;obj.array[0].obj2.obj3[1].name&quot;)\n\n语法很简单，符合我们日常接触到语法规则，这样便可以访问到 JSON 数据中的任何一个值。\n其实实现过程也不复杂，我们已经在上一文中实现将 JSON 字符串转换为一个 JSONObject 了。\n这次只是额外再解析刚才定义的语法为 token，然后解析该 token 的同时再从生成好的 JSONObject 中获取数据。\n最后在解析完 token 时拿到的 JSONObject 数据返回即可。\n\n\n我们以这段查询代码为例：\n首先第一步是对查询语法做词法分析，最终得到下图的 token。\n\n在词法分析过程中也可以做简单的语法校验；比如如果包含数组查询，并不是以 ] 符号结尾时就抛出语法错误。\n\n接着我们遍历语法的 token。如下图所示：\n\n每当遍历到 token 类型为 Key 时便从当前的 JSONObject 对象中获取数据，并用获取到的值替覆盖为当前的 JSONObject。\n其中每当遇到 . [ ] 这样的 token 时便消耗掉，直到我们将 token 遍历完毕，这时将当前 JSONObject 返回即可。\n在遍历过程中当遇到非法格式时，比如 obj_list[1.] 便会返回一个空的 JSONObject。\n语法校验这点其实也很容易办到，因为根据我们的语法规则，Array 中的 index 后一定紧接的是一个 EndArray，只要不是一个 EndArray 便能知道语法不合法了。\n有兴趣的可以看下解析过程的源码：\nhttps://github.com/crossoverJie/xjson/blob/cfbca51cc9bc0c77e6cb9c9ad3f964b2054b3826/json.go#L46\n对 JSON 做四则运算str := `&#123;&quot;name&quot;:&quot;bob&quot;, &quot;age&quot;:10,&quot;magic&quot;:10.1, &quot;score&quot;:&#123;&quot;math&quot;:[1,2]&#125;&#125;`result := GetWithArithmetic(str, &quot;(age+age)*age+magic&quot;)assert.Equal(t, result.Float(), 210.1)result = GetWithArithmetic(str, &quot;(age+age)*age&quot;)assert.Equal(t, result.Int(), 200)result = GetWithArithmetic(str, &quot;(age+age) * age + score.math[0]&quot;)assert.Equal(t, result.Int(), 201)result = GetWithArithmetic(str, &quot;(age+age) * age - score.math[0]&quot;)assert.Equal(t, result.Int(), 199)result = GetWithArithmetic(str, &quot;score.math[1] / score.math[0]&quot;)assert.Equal(t, result.Int(), 2)\n\n最后我还扩展了一下语法，可以支持对 JSON 数据中的整形（int、float）做四则运算，虽然这是一个小众需求，但做完我觉得还挺有意思的，目前在市面上我还没发现有类似功能的库，可能和小众需求有关🤣。\n其中核心的四则运算逻辑是由之前写的脚本解释器提供的:\nhttps://github.com/crossoverJie/gscript\n单独提供了一个函数，传入一个四则运算表达式返回计算结果。\n\n由于上一版本还不支持 float，所以这次专门适配了一下。\n\n限于篇幅，更多关于这个四则运算的实现逻辑会在后面继续分享。\n总结至此算是我第一次利用编译原理的知识解决了一点特定领域问题，在大学以及工作这些年一直觉得编译原理比较高深，所以内心一直是抗拒的，但经过这段时间的学习和实践慢慢的也掌握到了一点门道。\n不过目前也只是冰山一角，后面的编译原理后端更是要涉及到计算机底层知识，所以依然任重而道远。\n已上都是题外话，针对于这个库我也会长期维护；为了能达到生产的使用要求，尽量提高了单测覆盖率，目前是98%。\n也欢迎大家使用，提 bug🐛。\n后面会继续优化，比如支持转义字符、提高性能等。\n感兴趣的朋友请持续关注：https://github.com/crossoverJie/xjson\n","categories":["xjson","compiler"],"tags":["go"]},{"title":"XJSON 是如何实现四则运算的？","url":"/2022/07/12/gjson/xjson03/","content":"\n前言在上一篇中介绍了 xjson 的功能特性以及使用查询语法快速方便的获取 JSON 中的值。\n\n同时这次也更新了一个版本，主要是两个升级：\n\n对转义字符的支持。\n性能优化，大约提升了30%⬆️。\n\n\n转义字符先说第一个转义字符，不管是原始 JSON 字符串中存在转义字符，还是查询语法中存在转义字符都已经支持，具体用法如下：\nstr = `&#123;&quot;1a.b.[]&quot;:&quot;b&quot;&#125;`get = Get(str, &quot;1a\\\\.b\\\\.\\\\[\\\\]&quot;)assert.Equal(t, get.String(), &quot;b&quot;)str = `&#123;&quot;.&quot;:&quot;b&quot;&#125;`get = Get(str, &quot;\\\\.&quot;)assert.Equal(t, get.String(), &quot;b&quot;)str = `&#123;&quot;a&quot;:&quot;&#123;\\&quot;a\\&quot;:\\&quot;123\\&quot;&#125;&quot;&#125;`get = Get(str, &quot;a&quot;)fmt.Println(get)assert.Equal(t, get.String(), &quot;&#123;\\&quot;a\\&quot;:\\&quot;123\\&quot;&#125;&quot;)assert.Equal(t, Get(get.String(), &quot;a&quot;).String(), &quot;123&quot;)str = `&#123;&quot;a&quot;:&quot;&#123;\\&quot;a\\&quot;:[1,2]&#125;&quot;&#125;`get = Get(str, &quot;a&quot;)fmt.Println(get)assert.Equal(t, get.String(), &quot;&#123;\\&quot;a\\&quot;:[1,2]&#125;&quot;)assert.Equal(t, Get(get.String(), &quot;a[0]&quot;).Int(), 1)\n\n性能优化性能也有部分优化，大约比上一版本提升了 30%。\npkg: github.com/crossoverJie/xjson/benckmarkcpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHzBenchmarkDecode-12        \t   14968\t     77130 ns/op\t   44959 B/op\t    1546 allocs/opPASS------------------------------------pkg: github.com/crossoverJie/xjson/benckmarkcpu: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHzBenchmarkDecode-12        \t   19136\t     62960 ns/op\t   41593 B/op\t    1407 allocs/opPASS\n\n但总体来说还有不少优化空间，主要是上限毕竟低，和官方库比还是有不小的差距。\n实现四则运算接下来聊聊四则运算是如何实现的，这本身算是一个比较有意思的 feature，虽然用的场景不多🙂。\n先来看看是如何使用的：\njson :=`&#123;&quot;alice&quot;:&#123;&quot;age&quot;:10&#125;,&quot;bob&quot;:&#123;&quot;age&quot;:20&#125;,&quot;tom&quot;:&#123;&quot;age&quot;:20&#125;&#125;`query := &quot;(alice.age+bob.age) * tom.age&quot;arithmetic := GetWithArithmetic(json, query)assert.Equal(t, arithmetic.Int(), 600)\n输入一个 JSON 字符串以及计算公式然后得到计算结果。\n其实实现原理也比较简单，总共分为是三步:\n\n对 json 进行词法分析，得到一个四则运算的第一步 token。\n基于该 token 流，生产出最终的四则运算表达式，比如 (3+2)*5\n调用四则运算处理器，拿到最终结果。\n\n\n先看第一步，根据 (alice.age+bob.age) * tom.age 解析出 token：\n第二步，解析该 token，碰到 Identifier 类型时，将其解析为具体的数据。而其他类型的 token 直接拼接字符串即可，最终生成表达式：(10+20)*20\n\n这一步的核心功能是由 xjson.Get(json, query) 函数提供的。\n\n关键代码如下图所示：\n最终的目的就是能够生成一个表达式，只要拿到这个四则运算表达式便能得到最终计算结果。\n而最终的计算逻辑其实也挺简单，构建一个 AST 树，然后深度遍历递归求解即可，如下图所示：\n\n这一步的核心功能是有之前实现的脚本解释器 gscipt 提供的。\n\n感兴趣的朋友可以查看源码。\n总结一个 JSON 库的功能其实并不多，欢迎大家分享平时用 JSON 库的常用功能；也欢迎大家体验下这个库。\n源码地址：https://github.com/crossoverJie/xjson\n","categories":["xjson","compiler"],"tags":["go"]},{"title":"用位运算为你的程序加速","url":"/2022/08/01/gjson/xjson04-bitwisee-operation/","content":"\n前言最近在持续优化之前编写的 JSON 解析库 xjson，主要是两个方面的优化。\n第一个是支持将一个 JSONObject 对象输出为 JSON 字符串。\n这点在上个版本中只是利用自带的 Print 函数打印数据：\nfunc TestJson4(t *testing.T)  &#123;\tstr := `&#123;&quot;people&quot;:&#123;&quot;name&quot;:&#123;&quot;first&quot;:&quot;bob&quot;&#125;&#125;&#125;`\tfirst := xjson.Get(str, &quot;people.name.first&quot;)\tassert.Equal(t, first.String(), &quot;bob&quot;)\tget := xjson.Get(str, &quot;people&quot;)\tfmt.Println(get.String())\t//assert.Equal(t, get.String(),`&#123;&quot;name&quot;:&#123;&quot;first&quot;:&quot;bob&quot;&#125;&#125;`)&#125;\n\nOutput:\nmap[name:map[first:bob]]\n\n\n\n本次优化之后便能直接输出 JSON 字符串了：\n\n实现过程也很简单，只需要递归遍历 object 中的数据，然后拼接字符串即可，核心代码如下：\nfunc (r Result) String() string &#123;\tswitch r.Token &#123;\tcase String:\t\treturn fmt.Sprint(r.object)\tcase Bool:\t\treturn fmt.Sprint(r.object)\tcase Number:\t\ti, _ := strconv.Atoi(fmt.Sprint(r.object))\t\treturn fmt.Sprintf(&quot;%d&quot;, i)\tcase Float:\t\ti, _ := strconv.ParseFloat(fmt.Sprint(r.object), 64)\t\treturn fmt.Sprintf(&quot;%f&quot;, i)\tcase JSONObject:\t\treturn object2JSONString(r.object)\tcase ArrayObject:\t\treturn object2JSONString(r.Array())\tdefault:\t\treturn &quot;&quot;\t&#125;&#125;\n\n\n用位运算优化第二个优化主要是提高了性能，查询一个复杂 JSON 数据的时候性能提高了大约 ⏫16%.\n# 优化前BenchmarkDecode-12         90013             66905 ns/op           42512 B/op       1446 allocs/op# 优化后BenchmarkDecode-12        104746             59766 ns/op           37749 B/op       1141 allocs/op\n\n\n这里截取了一些重点改动的部分：\n\n在 JSON 解析过程中会有一个有限状态机状态迁移的过程，而迁移的时候可能会出现多个状态。\n比如当前解析到的 token 值为 &#123;，那它接下来的 token 可能会为 ObjectKey:&quot;name&quot;,也可能会是 BeginObject:&#123;,当然也可能会是 EndObject:&#125;，所以在优化之前我是将状态全部存放在一个集合中的，在解析过程中如果发现状态不满足预期的列表时则会抛出语法异常的错误。\n\n所以优化之前是遍历这个集合来进行判断的，这样的时间复杂度为 O(N),但当我们换成位运算就不一样了，时间复杂度直接就变为O(1)了，同时还节省了一个切片的存储空间。\n我们简单来分析下这个位运算为什么会达到判断一个数据是否在一个集合中同样的效果。\n首先以这两个状态为例：\nStatusObjectKey   status = 0x0002StatusColon       status = 0x0004\n\n他们分别对应的二进制数据为：\nStatusObjectKey   status = 0x0002 //0010StatusColon       status = 0x0004 //0100\n\n当我们对这两个数据求 | 运算得到的数据是 0110：\nA:0010B:0100C:0110\n\n这时候如何我们如果用这两个原始数据与 C:0110 做 &amp; 运算时就会还原为刚才的两个数据。\n// input:A:0010C:0110// output:A:0010----------// input:B:0100C:0110// output:B:0100\n\n\n但我们换一个 D 与 C 求 &amp; 时：\nD: 1000 // 0x0008 对应的二进制为 1000C: 0110D&#x27;:0000\n将会得到一个 0 值，只要得出的数据大于 0 我们就能判断一个数据是否在给定的集合中了。\n\n当然这里有一个前提条件就是，我们输入的数据高位永远都是是 1 才行，也就是2的幂。\n\n同样的优化在解析查询语法时也有使用：\n\n其他奇淫巧技当然位运算还有一些其他技巧，比如判断奇偶数：\n// 偶数a &amp; 1 == 0// 奇数a &amp; 1 == 1\n\n乘法和除法，右移1一位是除以2，左移一位是乘以2.\nx := 2fmt.Println(x&gt;&gt;1) //1fmt.Println(x&lt;&lt;1) //4\n\n\n总结位运算在带来程序性能提升的同时也降低代码可读性，所以我们得按需选择是否使用；\n再一些底层库、框架代码对性能有极致追求的场景推荐使用，但在业务代码中对数据做加减乘除就没必要用位运算了，只会让后续的维护者一脸懵逼。\n相关代码：https://github.com/crossoverJie/xjson\n","categories":["xjson","compiler"],"tags":["go","Bitwise operation"]},{"title":"简单的 for 循环也会踩的坑","url":"/2021/12/28/go/for-mistake/","content":"\n前言最近实现某个业务时，需要读取数据然后再异步处理；在 Go 中实现起来自然就比较简单，伪代码如下：\nlist := []*Demo&#123;&#123;&quot;a&quot;&#125;, &#123;&quot;b&quot;&#125;&#125;for _, v := range list &#123;\tgo func() &#123;\t\tfmt.Println(&quot;name=&quot;+v.Name)\t&#125;()&#125;type Demo struct &#123;\tName string&#125;\n\n\n\n看似非常简单几行代码却和我们的预期不符，打印之后输出的是：\nname=bname=b\n\n并不是我们预期的：\nname=aname=b\n\n坑一由于写 go 的资历尚浅、道行更是浅薄，这 bug 我硬是找了个把小时；刚开始还以为是数据源的问题，经历了好几轮自我怀疑。总之过程先不表，先看看如何修复这个问题。\n首先第一种办法是使用临时变量：\nlist := []*Demo&#123;&#123;&quot;a&quot;&#125;, &#123;&quot;b&quot;&#125;&#125;for _, v := range list &#123;\ttemp:=v\tgo func() &#123;\t\tfmt.Println(&quot;name=&quot;+temp.Name)\t&#125;()&#125;\n\n这样便可正确输出，其实从这种写法中也能看出问题的端倪。\n在第一种没有使用临时变量时，主协程很快就运行完毕，这时候打印的子协程可能还没运行；当开始运行的时候，这里的 v 已经被最后一个赋值了。\n所以这里打印的一直都是最后一个变量。\n而使用临时变量会将当前遍历的值拷贝一份，自然就不会互相影响了。\n\n当然除了临时变量也可使用闭包解决。\nlist := []*Demo&#123;&#123;&quot;a&quot;&#125;, &#123;&quot;b&quot;&#125;&#125;for _, v := range list &#123;\tgo func(temp *Demo) &#123;\t\tfmt.Println(&quot;name=&quot;+temp.Name)\t&#125;(v)&#125;\n\n将参数通过闭包传递时，每个 goroutine 都会在自己的栈中存放一份参数的拷贝，这样也能区分了。\n坑二与之类似的还有第二个坑：\nlist2 := []Demo&#123;&#123;&quot;a&quot;&#125;, &#123;&quot;b&quot;&#125;&#125;var alist []*Demofor _, test := range list2 &#123;\talist = append(alist, &amp;test)&#125;fmt.Println(alist[0].Name, alist[1].Name)\n\n这段代码与我们预期不不符：\nb b\n\n但我们稍加修改就可以了：\nlist2 := []Demo&#123;&#123;&quot;a&quot;&#125;, &#123;&quot;b&quot;&#125;&#125;var alist []Demofor _, test := range list2 &#123;\tfmt.Printf(&quot;addr=%p\\n&quot;, &amp;test)\talist = append(alist, test)&#125;fmt.Println(alist[0].Name, alist[1].Name)\n\naddr=0xc000010240addr=0xc000010240a b\n\n顺便打印了内存地址，其实从结果中大概就能猜到原因；每次遍历打印的内存地址都是相同，所以如果我们存放的是指针，本质上存储的都是同一块内存地址的内容，所以值相同。\n而如果我们只存储值，不存指针自然也不会有这个问题。\n但如果想使用指针如何处理呢?\nlist2 := []Demo&#123;&#123;&quot;a&quot;&#125;, &#123;&quot;b&quot;&#125;&#125;var alist []*Demofor _, test := range list2 &#123;\ttemp := test\t//fmt.Printf(&quot;addr=%p\\n&quot;, &amp;test)\talist = append(alist, &amp;temp)&#125;fmt.Println(alist[0].Name, alist[1].Name)\n\n也简单，同样的使用临时变量即可。\n通过官方源码可以得知，for range 只是语法糖，本质上也是 for 循环；因为每次都是对同一个对象遍历赋值，所以便会出现这样的“乌龙”。\n\ndefer 的坑for 循环 + defer 也是组合坑（虽然不推荐这么用），还是先来看个例子：\n// demo1func main() &#123;\ta := []int&#123;1, 2, 3&#125;\tfor _, v := range a &#123;\t\tdefer fmt.Println(v)\t&#125;&#125;// demo2func main() &#123;\ta := []int&#123;1, 2, 3&#125;\tfor _, v := range a &#123;\t\tdefer func() &#123;\t\t\tfmt.Println(v)\t\t&#125;()\t&#125;&#125;\n\n分别输出：\n//demo1321//demo2333\n\ndemo1的结果很好理解，defer 可以理解为将执行语句放入到栈中，所以呈现的结果是先进后出。\n而demo2中，由于是闭包，闭包对变量 v 持有的是引用，所以在最终延迟执行时 v 已经被最后一个值赋值，所以打印出来都是相同的。\n解决方法与上文类似，传入参数即可解决：\nfor _, v := range a &#123;\tdefer func(v int) &#123;\t\tfmt.Println(v)\t&#125;(v)&#125;\n\n这类细节问题日常开发大概率是碰不上的，最有可能遇到的就是面试了，所以多了解了解也没坏处。\n总结类似于第一种情况在 for 循环中 goroutine 调用，我觉得  IDE 完全是可以做到提醒的；比如 IDEA 中就把大部分认为可能发的错误包含进去，期待后续 goland 的更新。\n但其实这几种错误官方博客已经提醒过了。\nhttps://github.com/golang/go/wiki/CommonMistakes#using-reference-to-loop-iterator-variable只是大部分人估计都没去看过，这事之后我也得花时间好好阅读下。\n","categories":["Go"],"tags":["for","goroutine"]},{"title":"利用 GitHub Action 自动发布 Docker","url":"/2021/03/26/go/github-actions/","content":"\n前言最近公司内部项目的发布流程接入了 GitHub Actions，整个体验过程还是比较美好的；本文主要目的是对于没有还接触过 GitHub Actions的新手，能够利用它快速构建自动测试及打包推送 Docker 镜像等自动化流程。\n\n\n创建项目本文主要以 Go 语言为例，当然其他语言也是类似的，与语言本身关系不大。\n这里我们首先在 GitHub 上创建一个项目，编写了几段简单的代码 main.go：\nvar version = &quot;0.0.1&quot;func GetVersion() string &#123;\treturn version&#125;func main() &#123;\tfmt.Println(GetVersion())&#125;\n\n内容非常简单，只是打印了了版本号；同时配套了一个单元测试 main_test.go：\nfunc TestGetVersion1(t *testing.T) &#123;\ttests := []struct &#123;\t\tname string\t\twant string\t&#125;&#123;\t\t&#123;name: &quot;test1&quot;, want: &quot;0.0.1&quot;&#125;,\t&#125;\tfor _, tt := range tests &#123;\t\tt.Run(tt.name, func(t *testing.T) &#123;\t\t\tif got := GetVersion(); got != tt.want &#123;\t\t\t\tt.Errorf(&quot;GetVersion() = %v, want %v&quot;, got, tt.want)\t\t\t&#125;\t\t&#125;)\t&#125;&#125;\n\n我们可以执行  go test 运行该单元测试。\n$ go test                          PASSok      github.com/crossoverJie/go-docker       1.729s\n\n自动测试当然以上流程完全可以利用 Actions 自动化搞定。\n首选我们需要在项目根路径创建一个 .github/workflows/*.yml 的配置文件，新增如下内容：\nname: go-dockeron: pushjobs:  test:    runs-on: ubuntu-latest    if: github.ref == &#x27;refs/heads/main&#x27; || startsWith(github.ref, &#x27;refs/tags&#x27;)    steps:      - uses: actions/checkout@v2      - name: Run Unit Tests        run: go test\n\n简单解释下：\n\nname 不必多说，是为当前工作流创建一个名词。\non 指在什么事件下触发，这里指代码发生 push 时触发，更多事件定义可以参考官方文档：\n\nEvents that trigger workflows\n\njobs 则是定义任务，这里只有一个名为 test 的任务。\n\n该任务是运行在 ubuntu-latest 的环境下，只有在 main 分支有推送或是有 tag 推送时运行。\n运行时会使用 actions/checkout@v2 这个由他人封装好的 Action，当然这里使用的是由官方提供的拉取代码 Action。\n\n基于这个逻辑，我们可以灵活的分享和使用他人的 Action 来简化流程，这点也是 GitHub Action扩展性非常强的地方。\n\n最后的 run 则是运行自己命令，这里自然就是触发单元测试了。\n\n如果是 Java 便可改为  mvn test.\n\n之后一旦我们在 main 分支上推送代码，或者有其他分支的代码合并过来时都会自动运行单元测试，非常方便。\n\n\n与我们本地运行效果一致。\n自动发布接下来考虑自动打包 Docker 镜像，同时上传到 Docker Hub；为此首先创建 Dockerfile ：\nFROM golang:1.15 AS builderARG VERSION=0.0.10WORKDIR /go/src/appCOPY main.go .RUN go build -o main -ldflags=&quot;-X &#x27;main.version=$&#123;VERSION&#125;&#x27;&quot; main.goFROM debian:stable-slimCOPY --from=builder /go/src/app/main /go/bin/mainENV PATH=&quot;/go/bin:$&#123;PATH&#125;&quot;CMD [&quot;main&quot;]\n\n这里利用 ldflags 可在编译期间将一些参数传递进打包程序中，比如打包时间、go 版本、git 版本等。\n这里只是将 VERSION 传入了  main.version 变量中，这样在运行时就便能取到了。\ndocker build -t go-docker:last .docker run --rm go-docker:0.0.100.0.10\n\n接着继续编写 docker.yml 新增自动打包 Docker 以及推送到 docker hub 中。\ndeploy:    runs-on: ubuntu-latest    needs: test    if: startsWith(github.ref, &#x27;refs/tags&#x27;)    steps:      - name: Extract Version        id: version_step        run: |          echo &quot;##[set-output name=version;]VERSION=$&#123;GITHUB_REF#$&quot;refs/tags/v&quot;&#125;&quot;          echo &quot;##[set-output name=version_tag;]$GITHUB_REPOSITORY:$&#123;GITHUB_REF#$&quot;refs/tags/v&quot;&#125;&quot;          echo &quot;##[set-output name=latest_tag;]$GITHUB_REPOSITORY:latest&quot;      - name: Set up QEMU        uses: docker/setup-qemu-action@v1      - name: Set up Docker Buildx        uses: docker/setup-buildx-action@v1      - name: Login to DockerHub        uses: docker/login-action@v1        with:          username: $&#123;&#123; secrets.DOCKER_USER_NAME &#125;&#125;          password: $&#123;&#123; secrets.DOCKER_ACCESS_TOKEN &#125;&#125;      - name: PrepareReg Names        id: read-docker-image-identifiers        run: |          echo VERSION_TAG=$(echo $&#123;&#123; steps.version_step.outputs.version_tag &#125;&#125; | tr &#x27;[:upper:]&#x27; &#x27;[:lower:]&#x27;) &gt;&gt; $GITHUB_ENV          echo LASTEST_TAG=$(echo $&#123;&#123; steps.version_step.outputs.latest_tag  &#125;&#125; | tr &#x27;[:upper:]&#x27; &#x27;[:lower:]&#x27;) &gt;&gt; $GITHUB_ENV      - name: Build and push Docker images        id: docker_build        uses: docker/build-push-action@v2.3.0        with:          push: true          tags: |            $&#123;&#123;env.VERSION_TAG&#125;&#125;            $&#123;&#123;env.LASTEST_TAG&#125;&#125;          build-args: |            $&#123;&#123;steps.version_step.outputs.version&#125;&#125;\n\n新增了一个 deploy 的 job。\nneeds: testif: startsWith(github.ref, &#x27;refs/tags&#x27;)\n\n运行的条件是上一步的单测流程跑通，同时有新的 tag 生成时才会触发后续的 steps。\nname: Login to DockerHub\n在这一步中我们需要登录到 DockerHub，所以首先需要在 GitHub 项目中配置 hub 的 user_name 以及 access_token.\n\n\n配置好后便能在 action 中使用该变量了。\n\n这里使用的是由 docker 官方提供的登录 action(docker/login-action)。\n有一点要非常注意，我们需要将镜像名称改为小写，不然会上传失败，比如我的名称中 J 字母是大写的，直接上传时就会报错。\n\n所以在上传之前先要执行该步骤转换为小写。\n\n最后再用这两个变量上传到 Docker Hub。\n\n今后只要我们打上 tag 时，Action 就会自动执行单测、构建、上传的流程。\n总结GitHub Actions 非常灵活，你所需要的大部分功能都能在 marketplace 找到现成的直接使用，\n比如可以利用 ssh 登录自己的服务器，执行一些命令或脚本，这样想象空间就很大了。\n使用起来就像是搭积木一样，可以很灵活的完成自己的需求。\n参考链接：\nHow to Build a CI&#x2F;CD Pipeline with Go, GitHub Actions and Docker\n","categories":["CICD"],"tags":["Go","Docker","GitHub Actions"]},{"title":"Go channel VS Java BlockingQueue","url":"/2021/07/02/go/go%20channel%20vs%20java%20BlockingQueue/","content":"\n前言最近在实现两个需求，由于两者之间并没有依赖关系，所以想利用队列进行解耦；但在 Go 的标准库中并没有现成可用并且并发安全的数据结构；但 Go 提供了一个更加优雅的解决方案，那就是 channel。\nchannel 应用Go 与 Java 的一个很大的区别就是并发模型不同，Go 采用的是 CSP(Communicating sequential processes) 模型；用 Go 官方的说法：\n\nDo not communicate by sharing memory; instead, share memory by communicating.\n\n\n\n翻译过来就是：不用使用共享内存来通信，而是用通信来共享内存。\n而这里所提到的通信，在 Go 里就是指代的 channel。\n只讲概念并不能快速的理解与应用，所以接下来会结合几个实际案例更方便理解。\nfutrue taskGo 官方没有提供类似于 Java 的 FutureTask 支持：\n    public static void main(String[] args) throws InterruptedException, ExecutionException &#123;        ExecutorService executorService = Executors.newFixedThreadPool(2);        Task task = new Task();        FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(task);        executorService.submit(futureTask);        String s = futureTask.get();        System.out.println(s);        executorService.shutdown();    &#125;&#125;class Task implements Callable&lt;String&gt; &#123;    @Override    public String call() throws Exception &#123;        // 模拟http        System.out.println(&quot;http request&quot;);        Thread.sleep(1000);        return &quot;request success&quot;;    &#125;&#125;\n但我们可以使用 channel 配合 goroutine 实现类似的功能：\nfunc main() &#123;\tch := Request(&quot;https://github.com&quot;)\tselect &#123;\tcase r := &lt;-ch:\t\tfmt.Println(r)\t&#125;&#125;func Request(url string) &lt;-chan string &#123;\tch := make(chan string)\tgo func() &#123;\t\t// 模拟http请求\t\ttime.Sleep(time.Second)\t\tch &lt;- fmt.Sprintf(&quot;url=%s, res=%s&quot;, url, &quot;ok&quot;)\t&#125;()\treturn ch&#125;\n\ngoroutine 发起请求后直接将这个 channel 返回，调用方会在请求响应之前一直阻塞，直到 goroutine 拿到了响应结果。\ngoroutine 互相通信/**  * 偶数线程  */ public static class OuNum implements Runnable &#123;     private TwoThreadWaitNotifySimple number;     public OuNum(TwoThreadWaitNotifySimple number) &#123;         this.number = number;     &#125;     @Override     public void run() &#123;         for (int i = 0; i &lt; 11; i++) &#123;             synchronized (TwoThreadWaitNotifySimple.class) &#123;                 if (number.flag) &#123;                     if (i % 2 == 0) &#123;                         System.out.println(Thread.currentThread().getName() + &quot;+-+偶数&quot; + i);                         number.flag = false;                         TwoThreadWaitNotifySimple.class.notify();                     &#125;                 &#125; else &#123;                     try &#123;                         TwoThreadWaitNotifySimple.class.wait();                     &#125; catch (InterruptedException e) &#123;                         e.printStackTrace();                     &#125;                 &#125;             &#125;         &#125;     &#125; &#125; /**  * 奇数线程  */ public static class JiNum implements Runnable &#123;     private TwoThreadWaitNotifySimple number;     public JiNum(TwoThreadWaitNotifySimple number) &#123;         this.number = number;     &#125;     @Override     public void run() &#123;         for (int i = 0; i &lt; 11; i++) &#123;             synchronized (TwoThreadWaitNotifySimple.class) &#123;                 if (!number.flag) &#123;                     if (i % 2 == 1) &#123;                         System.out.println(Thread.currentThread().getName() + &quot;+-+奇数&quot; + i);                         number.flag = true;                         TwoThreadWaitNotifySimple.class.notify();                     &#125;                 &#125; else &#123;                     try &#123;                         TwoThreadWaitNotifySimple.class.wait();                     &#125; catch (InterruptedException e) &#123;                         e.printStackTrace();                     &#125;                 &#125;             &#125;         &#125;     &#125; &#125;\n\n这里截取了”两个线程交替打印奇偶数“的部分代码。\n\nJava 提供了 object.wait()/object.notify() 这样的等待通知机制，可以实现两个线程间通信。\ngo 通过 channel 也能实现相同效果：\nfunc main() &#123;\tch := make(chan struct&#123;&#125;)\tgo func() &#123;\t\tfor i := 1; i &lt; 11; i++ &#123;\t\t\tch &lt;- struct&#123;&#125;&#123;&#125;\t\t\t//奇数\t\t\tif i%2 == 1 &#123;\t\t\t\tfmt.Println(&quot;奇数:&quot;, i)\t\t\t&#125;\t\t&#125;\t&#125;()\tgo func() &#123;\t\tfor i := 1; i &lt; 11; i++ &#123;\t\t\t&lt;-ch\t\t\tif i%2 == 0 &#123;\t\t\t\tfmt.Println(&quot;偶数:&quot;, i)\t\t\t&#125;\t\t&#125;\t&#125;()\ttime.Sleep(10 * time.Second)&#125;\n\n\n本质上他们都是利用了线程(goroutine)阻塞然后唤醒的特性，只是 Java 是通过 wait&#x2F;notify 机制；\n而 go 提供的 channel 也有类似的特性：\n\n向 channel 发送数据时(ch&lt;-struct&#123;&#125;&#123;&#125;)会被阻塞，直到 channel 被消费(&lt;-ch)。\n\n\n以上针对于无缓冲 channel。\n\nchannel 本身是由 go 原生保证并发安全的，不用额外的同步措施，可以放心使用。\n广播通知不仅是两个 goroutine 之间通信，同样也能广播通知，类似于如下 Java 代码：\npublic static void main(String[] args) throws InterruptedException &#123;    for (int i = 0; i &lt; 10; i++) &#123;        new Thread(() -&gt; &#123;            try &#123;                synchronized (NotifyAll.class)&#123;                    NotifyAll.class.wait();                &#125;                System.out.println(Thread.currentThread().getName() + &quot;done....&quot;);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;).start();    &#125;    Thread.sleep(3000);    synchronized (NotifyAll.class)&#123;        NotifyAll.class.notifyAll();    &#125;&#125;\n\n主线程将所有等待的子线程全部唤醒，这个本质上也是通过 wait/notify 机制实现的，区别只是通知了所有等待的线程。\n换做是 go 的实现：\nfunc main() &#123;\tnotify := make(chan struct&#123;&#125;)\tfor i := 0; i &lt; 10; i++ &#123;\t\tgo func(i int) &#123;\t\t\tfor &#123;\t\t\t\tselect &#123;\t\t\t\tcase &lt;-notify:\t\t\t\t\tfmt.Println(&quot;done.......&quot;,i)\t\t\t\t\treturn\t\t\t\tcase &lt;-time.After(1 * time.Second):\t\t\t\t\tfmt.Println(&quot;wait notify&quot;,i)\t\t\t\t&#125;\t\t\t&#125;\t\t&#125;(i)\t&#125;\ttime.Sleep(1 * time.Second)\tclose(notify)\ttime.Sleep(3 * time.Second)&#125;\n\n当关闭一个 channel 后，会使得所有获取 channel 的 goroutine 直接返回，不会阻塞，正是利用这一特性实现了广播通知所有 goroutine 的目的。\n\n注意，同一个 channel 不能反复关闭，不然会出现panic。\n\nchannel 解耦以上例子都是基于无缓冲的 channel，通常用于 goroutine 之间的同步；同时 channel 也具备缓冲的特性：\nch :=make(chan T, 100)\n\n可以直接将其理解为队列，正是因为具有缓冲能力，所以我们可以将业务之间进行解耦，生产方只管往 channel 中丢数据，消费者只管将数据取出后做自己的业务。\n同时也具有阻塞队列的特性：\n\n当 channel 写满时生产者将会被阻塞。\n当 channel 为空时消费者也会阻塞。\n\n\n从上文的例子中可以看出，实现相同的功能 go 的写法会更加简单直接，相对的 Java 就会复杂许多（当然这也和这里使用的偏底层 api 有关）。\n\nJava 中的 BlockingQueue这些特性都与 Java 中的 BlockingQueue 非常类似，他们具有以下的相同点：\n\n可以通过两者来进行 goroutine/thread 通信。\n具备队列的特征，可以解耦业务。\n支持并发安全。\n\n同样的他们又有很大的区别，从表现上看：\n\nchannel 支持 select 语法，对 channel 的管理更加简洁直观。\nchannel 支持关闭，不能向已关闭的 channel 发送消息。\nchannel 支持定义方向，在编译器的帮助下可以在语义上对行为的描述更加准确。\n\n当然还有本质上的区别就是 channel 是 go 推荐的 CSP 模型的核心，具有编译器的支持，可以有很轻量的成本实现并发通信。\n而 BlockingQueue 对于 Java 来说只是一个实现了并发安全的数据结构，即便不使用它也有其他的通信方式；只是他们都具有阻塞队列的特征，所有在初步接触 channel 时容易产生混淆。\n\n\n\n相同点\nchannel 特有\n\n\n\n阻塞策略\n支持select\n\n\n设置大小\n支持关闭\n\n\n并发安全\n自定义方向\n\n\n普通数据结构\n编译器支持\n\n\n总结有过一门编程语言的使用经历在学习其他语言是确实是要方便许多，比如之前写过 Java 再看 Go 时就会发现许多类似之处，只是实现不同。\n拿这里的并发通信来说，本质上是因为并发模型上的不同；\nGo 更推荐使用通信来共享内存，而 Java 大部分场景都是使用共享内存来通信（这样就得加锁来同步）。\n带着疑问来学习确实会事半功倍。\n最近和网友讨论后再补充一下，其实 Go channel 的底层实现也是通过对共享内存的加锁来实现的，这点任何语言都不可避免。\n既然都是共享内存那和我们自己使用共享内存有什么区别呢？主要还是 channel 的抽象层级更高，我们使用这类高抽象层级的方式编写代码会更易理解和维护。\n但在一些特殊场景，需要追求极致的性能，降低加锁颗粒度时用共享内存会更加合适，所以 Go 官方也提供有 sync.Map/Mutex 这样的库；只是在并发场景下更推荐使用 channel 来解决问题。\n","categories":["Go"],"tags":["Java","Go","channel","BlockingQueue"]},{"title":"编写一个接口压测工具","url":"/2021/11/15/go/go-benchmark-test/","content":"\n前言前段时间有个项目即将上线，需要对其中的核心接口进行压测；由于我们的接口是 gRPC 协议，找了一圈发现压测工具并不像 HTTP 那么多。\n最终发现了 ghz 这个工具，功能也非常齐全。\n事后我在想为啥做 gRPC 压测的工具这么少，是有什么难点嘛？为了验证这个问题于是我准备自己写一个工具。\n\n\n特性前前后后大概花了个周末的时间完成了相关功能。\nhttps://github.com/crossoverJie/ptg/\n\n也是一个命令行工具，使用起来效果如上图；完整的命令如下：\nNAME:   ptg - Performance testing tool (Go)USAGE:   ptg [global options] command [command options] [arguments...]COMMANDS:   help, h  Shows a list of commands or help for one commandGLOBAL OPTIONS:   --thread value, -t value              -t 10 (default: 1 thread)   --Request value, --proto value        -proto http/grpc (default: http)   --protocol value, --pf value          -pf /file/order.proto   --fully-qualified value, --fqn value  -fqn package.Service.Method   --duration value, -d value            -d 10s (default: Duration of test in seconds, Default 10s)   --request value, -c value             -c 100 (default: 100)   --HTTP value, -M value                -m GET (default: GET)   --bodyPath value, --body value        -body bodyPath.json   --header value, -H value              HTTP header to add to request, e.g. &quot;-H Content-Type: application/json&quot;   --target value, --tg value            http://gobyexample.com/grpc:127.0.0.1:5000   --help, -h                            show help (default: false)\n\n考虑到受众，所以同时支持 HTTP 与 gRPC 接口的压测。\n做 gRPC 压测时所需的参数要多一些：\nscriptptg -t 10 -c 100 -proto grpc  -pf /xx/xx.proto -fqn hello.Hi.Say -body test.json  -tg &quot;127.0.0.1:5000&quot;\n\n比如需要提供 proto 文件的路径、具体的请求参数还有请求接口的全路径名称。\n\n目前只支持最常见的 unary call 调用，后续如果有需要的话也可以 stream。\n\n同时也支持压测时间、次数两种压测方式。\n安装想体验的朋友如果本地有 go 环境那直接运行：\ngo get github.com/crossoverJie/ptg\n\n没有环境也没关系，可以再 release 页面下载与自己环境对应的版本解压使用。\nhttps://github.com/crossoverJie/ptg/releases\n设计模式整个开发过程中还是有几个点想和大家分享，首先是设计模式。\n因为一开始设计时就考虑到需要支持不同的压测模式（次数、时间；后续也可以新增其他的模式）。\n所以我便根据压测的生命周期定义了一套接口：\ntype (\tModel interface &#123;\t\tInit()\t\tRun()\t\tFinish()\t\tPrintSate()\t\tShutdown()\t&#125;)\t\n\n从名字也能看出来，分别对应：\n\n压测初始化\n运行压测\n停止压测\n打印压测信息\n关闭程序、释放资源\n\n\n然后在两个不同的模式中进行实现。\n这其实就是一个典型的依赖倒置原则。\n\n程序员要依赖于抽象接口编程、不要依赖具体的实现。\n\n其实大白话就是咱们 Java 里常说的面向接口编程；这个编程技巧在开发框架、SDK或是多种实现的业务中常用。\n好处当然是显而易见：当接口定义好之后，不同的业务只需要根据接口实现自己的业务就好，完全不会互相影响；维护、扩展都很方便。\n支持 HTTP 和 gRPC 也是同理实现的：\ntype (\tClient interface &#123;\t\tRequest() (*Response, error)\t&#125;)\t\n\n当然前提得是前期的接口定义需要考虑周全、不能之后频繁修改接口定义，这样的接口就没有意义了。\ngoroutine另外一点则是不得不感叹 goroutine+select+channel 这套并发编程模型真的好用，并且也非常容易理解。\n很容易就能写出一套并发代码：\nfunc (c *CountModel) Init() &#123;\tc.wait.Add(c.count)\tc.workCh = make(chan *Job, c.count)\tfor i := 0; i &lt; c.count; i++ &#123;\t\tgo func() &#123;\t\t\tc.workCh &lt;- &amp;Job&#123;\t\t\t\tthread:   thread,\t\t\t\tduration: duration,\t\t\t\tcount:    c.count,\t\t\t\ttarget:   target,\t\t\t&#125;\t\t&#125;()\t&#125;&#125;\n\n比如这里需要初始化 N 个 goroutine 执行任务，只需要使用 go 关键字，然后利用 channel 将任务写入。\n当然在使用 goroutine+channel 配合使用时也得小心 goroutine 泄露的问题；简单来说就是在程序员退出时还有 goroutine 没有退出。\n比较常见的例子就是向一个无缓冲的 channel 中写数据，当没有其他 goroutine 来读取数时，写入的 goroutine 就会被一直阻塞，最终导致泄露。\n总结有 gRPC 接口压测需求的朋友欢迎试用，提出宝贵意见；当然 HTTP 接口也可以。\n源码地址：https://github.com/crossoverJie/ptg/\n最后如果有同样在学习 go 的朋友，特别是有 Java 开发经验的（这里大部分应该都写 Java）朋友，感兴趣的可以在公众号后台回复 “go群” 加入我创建的一个与 go 开发相关的技术群。\n","categories":["Go","设计模式"],"tags":["grpc","http","benchmark","performance","设计模式"]},{"title":"撸了一个可调试 gRPC 的 GUI 客户端","url":"/2021/11/28/go/go-grpc-client-gui/","content":"\n前言平时大家写完 gRPC 接口后是如何测试的？往往有以下几个方法：\n\n写单测代码，自己模拟客户端测试。\n\n可以搭一个 gRPC-Gateway 服务，这样就可以在 postman 中进行模拟。\n\n\n\n\n但这两种方法都不是特别优雅；第一种方法当请求结构体嵌套特别复杂时，在代码中维护起来就不是很直观；而且代码会特别长。\n第二种方法在 postman 中与请求 HTTP 接口一样，看起来非常直观；但需要额为维护一个 gRPC-Gateway 服务，同时接口定义发生变化时也得重新发布，使用起来稍显复杂。\n于是我经过一番搜索找到了两个看起来还不错的工具：\n\nBloomRPC\nhttps://github.com/fullstorydev/grpcui\n\n\n首先看 BloomRPC 页面美观，功能也很完善；但却有个非常难受的地方，那就是不支持 int64 数据的请求, 会有精度问题。\n\n\n这里我写了一个简单的接口，直接将请求的 int64 返回回来。\n\nfunc (o *Order) Create(ctx context.Context, in *v1.OrderApiCreate) (*v1.Order, error) &#123;\tfmt.Println(in.OrderId)\treturn &amp;v1.Order&#123;\t\tOrderId: in.OrderId,\t\tReason:  nil,\t&#125;, nil&#125;\n会发现服务端收到的数据精度已经丢失了。\n这个在我们大量使用 int64 的业务中非常难受，大部分接口都没法用了。\n\ngrpcui 是我在使用了 BloomRPC 一段时间之后才发现的工具，功能也比较完善; BloomRPC 中的精度问题也不存在。\n但由于我之前已经习惯了在 BloomRPC 中去调试接口，加上日常开发过程中我的浏览器几乎都是开了几十个 tap 页面，导致在其中找到 grpcui 不是那么方便。\n所以我就想着能不能有一个类似于  BloomRPC 的独立 APP，也支持 int64 的工具。\n\n准备找了一圈，貌似没有发现。恰好前段时间写了一个 gRPC 的压测工具，其实已经把该 APP 需要的核心功能也就是泛化调用实现了。\n由于核心能力是用 Go 实现的，所以这个 APP 最好也是用 Go 来写，这样复用代码会更方便一些；正好也想看看用 Go 来实现 GUI 应用效果如何。\n但可惜 Go 并没有提供原生的 GUI 库支持，最后翻来找去发现了一个库：fyne\n从 star 上看用的比较多，同时也支持跨平台打包；所以最终就决定使用该库在构建这个应用。\n核心功能整个 App 的交互流程我参考了  BloomRPC ，但作为一个不懂审美、设计的后端开发来说，整个过程中最难的就是布局了。\n\n这是我花了好几个晚上调试出来的第一版页面，虽然也能用但查看请求和响应数据非常不方便。\n于是又花了一个周末最终版如下（乍一看貌似没区别）：\n\n虽然页面上与 BloomRPC 还有一定差距，但也不影响使用；关键是 int64 的问题解决了；又可以愉快的撸码了。\n安装有类似需求也想体验的朋友可以在这里下载使用：https://github.com/crossoverJie/ptg/releases/download/0.0.2/ptg-mac-gui.tar\n由于我手上暂时没有 Windows 电脑，所以就没有打包 exe 程序；有相关需求的朋友可以自行下载源码编译：\ngit clone git@github.com:crossoverJie/ptg.gitcd ptgmake pkg-win\n\n后续计划当前版本的功能还比较简陋，只支持常用的 unary 调用；后续也会逐步加上 stream、metadata、工作空间的存储与还原等支持。\n对页面、交互有建议也欢迎提出。\n\n\n原本是准备上传到 brew 方便安装的，结果折腾了一晚上因为数据不够被拒了，所以对大家有帮助或者感兴趣的话帮忙点点关注（咋有种直播带货的感觉🐶） \n\n源码地址：https://github.com/crossoverJie/ptg\n","categories":["Go"],"tags":["grpc"]},{"title":"Go 日常开发常备第三方库和工具","url":"/2021/11/02/go/go-lib/","content":"\n不知不觉写 Go 已经快一年了，上线了大大小小好几个项目；心态也经历了几轮变化。\n因为我个人大概前五年时间写的是 Java ，中途写过一年多的 Python，所以刚接触到 Go 时的感觉如下图：\n既没有 Java 的生态，也没有 Python 这么多语法糖。\n\n\n写到现在的感觉就是：\n这里就不讨论这几门语言谁强谁弱了；重点和大家分享下我们日常开发中所使用到的一些第三方库与工具。\n这里我主要将这些库分为两类：\n\n业务开发\n基础工具开发\n\n业务开发首先是业务开发，主要包含了 web、数据库、Redis 等。\nGin ⭐️⭐️⭐️⭐️⭐️首先是 Gin，一款 HTTP 框架，使用简单、性能优秀、资料众多；你还在犹豫选择哪款框架时，那就选择它吧，基本没错。\n当然和它配套的 github.com&#x2F;swaggo&#x2F;gin-swagger swagger 工具也是刚需；利用它可以生成 swagger 文档。\nGORM ⭐️⭐️⭐️⭐️⭐️GORM 也没啥好说的，如果你喜欢 orm 的方式操作数据库，那就选它吧；同样的也是使用简单、资料较多。\n如果有读写分离需求，也可以使用 GORM 官方提供的插件 https://github.com/go-gorm/dbresolver ，配合 GORM 使用也是非常简单。\nerrors ⭐️⭐️⭐️⭐️⭐️Go 语言自身提供的错误处理比较简单，https://github.com/pkg/errors 提供了更强大的功能，比如：\n\n包装异常\n包装堆栈等。\n\n常用的有以下 API：\n// WithMessagef annotates err with the format specifier.func WithMessagef(err error, format string, args ...interface&#123;&#125;) error// WithStack annotates err with a stack trace at the point WithStack was called.func WithStack(err error) error\n\n\nzorolog ⭐️⭐️⭐️⭐️⭐️Go 里的日志打印库非常多，日志在日常开发中最好就是存在感低；也就是说性能强（不能影响到业务代码）、使用 API 简单。\n&quot;github.com/rs/zerolog/log&quot;log.Debug().Msgf(&quot;OrderID :%s&quot;, &quot;12121&quot;)\n\nexcelizehttps://github.com/qax-os/excelize是一个读写 Excel 的库，基本上你能遇到的 Excel 操作它都能实现。\nnow ⭐️⭐️⭐️⭐️https://github.com/jinzhu/now 是一个时间工具库：\n\n获取当前的年月日、时分秒。\n不同时区支持。\n最后一周、最后一个月等。\n\nimport &quot;github.com/jinzhu/now&quot;time.Now() // 2013-11-18 17:51:49.123456789 Monnow.BeginningOfMinute()        // 2013-11-18 17:51:00 Monnow.BeginningOfHour()          // 2013-11-18 17:00:00 Monnow.BeginningOfDay()           // 2013-11-18 00:00:00 Monnow.BeginningOfWeek()          // 2013-11-17 00:00:00 Sunnow.BeginningOfMonth()         // 2013-11-01 00:00:00 Frinow.BeginningOfQuarter()       // 2013-10-01 00:00:00 Tuenow.BeginningOfYear()          // 2013-01-01 00:00:00 Tuenow.EndOfMinute()              // 2013-11-18 17:51:59.999999999 Monnow.EndOfHour()                // 2013-11-18 17:59:59.999999999 Monnow.EndOfDay()                 // 2013-11-18 23:59:59.999999999 Monnow.EndOfWeek()                // 2013-11-23 23:59:59.999999999 Satnow.EndOfMonth()               // 2013-11-30 23:59:59.999999999 Satnow.EndOfQuarter()             // 2013-12-31 23:59:59.999999999 Tuenow.EndOfYear()                // 2013-12-31 23:59:59.999999999 Tuenow.WeekStartDay = time.Monday // Set Monday as first day, default is Sundaynow.EndOfWeek()                // 2013-11-24 23:59:59.999999999 Sun\n\nDecimal ⭐️⭐️⭐️⭐️当业务上需要精度计算时 https://github.com/shopspring/decimal 可以帮忙。\nimport (\t&quot;fmt&quot;\t&quot;github.com/shopspring/decimal&quot;)func main() &#123;\tprice, err := decimal.NewFromString(&quot;136.02&quot;)\tquantity := decimal.NewFromInt(3)\tfee, _ := decimal.NewFromString(&quot;.035&quot;)\ttaxRate, _ := decimal.NewFromString(&quot;.08875&quot;)\tsubtotal := price.Mul(quantity)\tpreTax := subtotal.Mul(fee.Add(decimal.NewFromFloat(1)))\ttotal := preTax.Mul(taxRate.Add(decimal.NewFromFloat(1)))\tfmt.Println(&quot;Subtotal:&quot;, subtotal)                      // Subtotal: 408.06\tfmt.Println(&quot;Pre-tax:&quot;, preTax)                         // Pre-tax: 422.3421\tfmt.Println(&quot;Taxes:&quot;, total.Sub(preTax))                // Taxes: 37.482861375\tfmt.Println(&quot;Total:&quot;, total)                            // Total: 459.824961375\tfmt.Println(&quot;Tax rate:&quot;, total.Sub(preTax).Div(preTax)) // Tax rate: 0.08875&#125;\n基本上你能想到的精度转换它都能做到；配合上 GORM 也可以将 model 字段声明为 decimal 的类型，数据库对应的也是 decimal ，这样使用起来时会更方便。\nAmount decimal.Decimal `gorm:&quot;column:amout;default:0.0000;NOT NULL&quot; json:&quot;amout&quot;` \n\nconfigor ⭐️⭐️⭐️⭐️https://github.com/jinzhu/configor 是一个配置文件读取库，支持 YAML/JSON/TOML 等格式。\ngo-cache ⭐️⭐️⭐️https://github.com/patrickmn/go-cache 是一个类似于 Java 中的 Guava cache，线程安全，使用简单；不需要分布式缓存的简单场景可以考虑。\nc := cache.New(5*time.Minute, 10*time.Minute)// Set the value of the key &quot;foo&quot; to &quot;bar&quot;, with the default expiration timec.Set(&quot;foo&quot;, &quot;bar&quot;, cache.DefaultExpiration)\n\ncopier ⭐️⭐️⭐️https://github.com/jinzhu/copier 看名字就知道这是一个数据复制的库，与 Java 中的 BeanUtils.copy() 类似；可以将两个字段相同但对象不同的  struct 进行数据复制，也支持深拷贝。\nfunc Copy(toValue interface&#123;&#125;, fromValue interface&#123;&#125;) (err error) \n\n在我们需要一个临时 struct 来存放数据时很有用，特别是一个 struct 中字段非常多时，一个个来回赋值确实有点费手指。\n但也要注意不要什么情况都使用，会带来一些弊端：\n\n当删除字段时，不能利用编译器提示。\n当一些字段需要额外人工处理时，代码不易阅读。\n反射赋值，有一定性能损耗。\n\n总之在业务开发时，还是建议人工编写，毕竟代码是给人看的。\nenv ⭐️⭐️⭐️https://github.com/caarlos0/env 这个库可以将我们的环境变量转换为一个 struct.\ntype config struct &#123;\tHome string `env:&quot;HOME&quot;`&#125;func main() &#123;\tcfg := config&#123;&#125;\tif err := env.Parse(&amp;cfg); err != nil &#123;\t\tfmt.Printf(&quot;%+v\\n&quot;, err)\t&#125;\tfmt.Printf(&quot;%+v\\n&quot;, cfg)&#125;\n\n这个在我们打包代码到不同的运行环境时非常有用，利用它可以方便的获取不同环境变量。\nuser_agent ⭐️⭐️⭐️https://github.com/mssola/user_agent 是一个格式化 user-agent 的小工具。\n当我们需要在服务端收集 user-agen 时可以更快的读取数据。\nfunc main() &#123;    ua := user_agent.New(&quot;Mozilla/5.0 (Linux; U; Android 2.3.7; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1&quot;)    fmt.Printf(&quot;%v\\n&quot;, ua.Mobile())   // =&gt; true    fmt.Printf(&quot;%v\\n&quot;, ua.Bot())      // =&gt; false    fmt.Printf(&quot;%v\\n&quot;, ua.Mozilla())  // =&gt; &quot;5.0&quot;    fmt.Printf(&quot;%v\\n&quot;, ua.Model())    // =&gt; &quot;Nexus One&quot;    fmt.Printf(&quot;%v\\n&quot;, ua.Platform()) // =&gt; &quot;Linux&quot;    fmt.Printf(&quot;%v\\n&quot;, ua.OS())     &#125;\n\nphonenumbers ⭐️⭐️⭐️https://github.com/nyaruka/phonenumbers 手机号码验证库，可以不用自己写正则表达式了。\n// parse our phone numbernum, err := phonenumbers.Parse(&quot;6502530000&quot;, &quot;US&quot;)\n\n\n\n\n基础工具接下来是一些基础工具库，包含一些主流的存储的客户端、中间件等。\ngomonkey ⭐️⭐️⭐️⭐️⭐️github.com&#x2F;agiledragon&#x2F;gomonkey 是一个 mock 打桩工具，当我们写单元测试时，需要对一些非接口函数进行 mock 会比较困难，这时就需要用到它了。\n由于它是修改了调用对应函数时机器跳转指令，而 CPU 架构的不同对应的指令也不同，所以在我们使用时还不兼容苹果的 M1 芯片，不过目前应该已经兼容了，大家可以试试。\ngoconvey ⭐️⭐️⭐️⭐️⭐️https://github.com/smartystreets/goconvey 也是配合单元测试的库，可以兼容 go test 命令。\n\n提供可视化 web UI。\n与 IDE 集成显示单元覆盖率。\n\ndig ⭐️⭐️⭐️⭐️⭐️https://github.com/uber-go/dig 这是一个依赖注入库，我们这里暂不讨论是否应该使用依赖注入，至少目前我们使用下来还是有几个好处：\n\n所有的对象都是单例。\n有一个统一的地方管理对象。\n使用时直接传递对象当做参数进来即可（容器会自动注入）。\n\n当然也有一些不太方便的地方：\n\n不熟悉时，一个对象是如何创建的不清楚。\n代码不是很好理解。\n\n我们内部有自己开发一个业务框架，其中所有的对象都交由 dig 进行管理，使用起来倒也是比较方便。\ncobra ⭐️⭐️⭐️⭐️https://github.com/spf13/cobra是一个功能强大的命令行工具库，我们用它来实现内部的命令行工具，同时也推荐使用 https://github.com/urfave/cli/ 我个人会更习惯用后者，要简洁一些。\nBloomRPC ⭐️⭐️⭐️⭐️https://github.com/uw-labs/bloomrpc 一个 gRPC 可视化工具，比起自己写 gRPC 客户端的代码那确实是要简单许多。\n但也有些小问题，比如精度。如果是 int64 超过了 2^56 服务端拿到的值会发生错误，这点目前还未解决。\nredis ⭐️⭐️⭐️⭐️https://github.com/go-redis/redis/ Redis 客户端，没有太多可说的；发展了许多年，该有的的功能都有了。\nelastic ⭐️⭐️⭐️⭐️https://github.com/olivere/elastic 这也是一个非常成熟的 elasticsearch 库。\nresty ⭐️⭐️⭐️⭐️https://github.com/go-resty/resty/  一个 http client, 使用起来非常简单：\n// Create a Resty Clientclient := resty.New()resp, err := client.R().    EnableTrace().    Get(&quot;https://httpbin.org/get&quot;)\n有点 Python requests 包那味了。\npulsar-client-go ⭐️⭐️⭐️Pulsar 官方出品的 go 语言客户端，相对于 Java 来说其他语言的客户端几乎都是后娘养的；功能会比较少，同时更新也没那么积极；但却没得选。\ngo-grpc-middleware ⭐️⭐️⭐️https://github.com/grpc-ecosystem/go-grpc-middleware 官方提供的 gRPC 中间件，可以自己实现内部的一些鉴权、元数据、日志等功能。\ngo-pilosa ⭐️⭐️⭐️https://github.com/pilosa/go-pilosa 是一个位图数据库的客户端，位图数据库的场景应用比较有限，通常是有标签需求时才会用到；比如求 N 个标签的交并补集；数据有一定规模后运营一定会提相关需求；可以备着以备不时之需。\npb ⭐️⭐️⭐️https://github.com/cheggaaa/pb 一个命令行工具进度条，编写命令行工具时使用它交互会更优雅。\n\n总结最后我汇总了一个表格，方便查看：\n\n\n\n名称\n类型\n功能\n星级\n\n\n\nGin\n业务开发\nHTTP 框架\n⭐️⭐️⭐️⭐️⭐️\n\n\nGORM\n业务开发\nORM 框架\n⭐️⭐️⭐️⭐️⭐️\n\n\nerrors\n业务开发\n异常处理库\n⭐️⭐️⭐️⭐️⭐️\n\n\nzorolog\n业务开发\n日志库\n⭐️⭐️⭐️⭐️⭐️\n\n\nexcelize\n业务开发\nExcel相关需求\n⭐️⭐️⭐️⭐️⭐️\n\n\nnow\n业务开发\n时间处理\n⭐️⭐️⭐️⭐️️\n\n\nDecimal\n业务开发\n精度处理\n⭐️⭐️⭐️⭐️️\n\n\nconfigor\n业务开发\n配置文件\n⭐️⭐️⭐️⭐️️\n\n\ngo-cache\n业务开发\n本地缓存\n⭐️⭐️⭐️\n\n\ncopier\n业务开发\n数据复制\n⭐️⭐️⭐️️️\n\n\nenv\n业务开发\n环境变量\n⭐️⭐️⭐️️️\n\n\nuser_agent\n业务开发\n读取 user-agent\n⭐️⭐️⭐️️️\n\n\nphonenumbers\n业务开发\n手机号码验证\n⭐️⭐️⭐️️️\n\n\ngomonkey\n基础工具\nmock工具\n⭐️⭐️⭐️⭐️⭐\n\n\ngoconvey\n基础工具\n单测覆盖率\n⭐️⭐️⭐️⭐️⭐\n\n\ndig\n基础工具\n依赖注入\n⭐️⭐️⭐️⭐️⭐\n\n\ncobra\n基础工具\n命令行工具\n⭐️⭐️⭐️⭐\n\n\ncli\n基础工具\n命令行工具\n⭐️⭐️⭐️⭐\n\n\nBloomRPC\n基础工具\ngRPC 调试客户端\n⭐️⭐️⭐️⭐\n\n\nredis\n基础工具\nRedis 客户端\n⭐️⭐️⭐️⭐\n\n\nelastic\n基础工具\nelasticsearch 客户端\n⭐️⭐️⭐️⭐\n\n\nresty\n基础工具\nhttp 客户端\n⭐️⭐️⭐️⭐\n\n\npulsar-client-go\n基础工具\nPulsar 客户端\n⭐️⭐️⭐️\n\n\ngo-grpc-middleware\n基础工具\ngRPC 中间件\n⭐️⭐️⭐\n\n\ngo-pilosa\n基础工具\npilosa 客户端\n⭐️⭐️⭐️\n\n\npb\n基础工具\n命令行工具进度条\n⭐️⭐️⭐️\n\n\n\n星级评分的规则主要是看实际使用的频次。\n\n最后夹带一点私货（其实也谈不上）文中提到了我们内部有基于以上库整合了一个业务开发框架；也基于该框架上线了大大小小10几个项目，改进空间依然不少，目前还是在快速迭代中。\n大概的用法，入口 main.go:最后截取我在内部的分享就概括了整体的思想--引用自公司一司姓同事。\n也许我们内部经过多次迭代，觉得有能力开放出来给社区带来一些帮助时也会尝试开源；现阶段就不嫌丑了。\n这些库都是我们日常开发最常用到的，也欢迎大家在评论区留下你们常用的库与工具。\n","categories":["Go"],"tags":["gomonkey","zerolog"]},{"title":"Go 里的超时控制","url":"/2021/10/28/go/go-timeout/","content":"\n前言日常开发中我们大概率会遇到超时控制的场景，比如一个批量耗时任务、网络请求等；一个良好的超时控制可以有效的避免一些问题（比如 goroutine 泄露、资源不释放等）。\n\n\nTimer在 go 中实现超时控制的方法非常简单，首先第一种方案是 Time.After(d Duration)：\nfunc main() &#123;\tfmt.Println(time.Now())\tx := &lt;-time.After(3 * time.Second)\tfmt.Println(x)&#125;\noutput:\n2021-10-27 23:06:04.304596 +0800 CST m=+0.0000856532021-10-27 23:06:07.306311 +0800 CST m=+3.001711390\n\ntime.After() 会返回一个 Channel，该 Channel 会在延时 d 段时间后写入数据。\n有了这个特性就可以实现一些异步控制超时的场景：\nfunc main() &#123;\tch := make(chan struct&#123;&#125;, 1)\tgo func() &#123;\t\tfmt.Println(&quot;do something...&quot;)\t\ttime.Sleep(4*time.Second)\t\tch&lt;- struct&#123;&#125;&#123;&#125;\t&#125;()\t\tselect &#123;\tcase &lt;-ch:\t\tfmt.Println(&quot;done&quot;)\tcase &lt;-time.After(3*time.Second):\t\tfmt.Println(&quot;timeout&quot;)\t&#125;&#125;\n\n这里假设有一个 goroutine 在跑一个耗时任务，利用 select 有一个 channel 获取到数据便退出的特性，当 goroutine 没有在有限时间内完成任务时，主 goroutine 便会退出，也就达到了超时的目的。\noutput:\ndo something...timeout\n\n\n\ntimer.After 取消，同时 Channel 发出消息，也可以关闭通道等通知方式。\n注意 Channel 最好是有大小，防止阻塞 goroutine ，导致泄露。\nContext第二种方案是利用 context，go 的 context 功能强大；利用 context.WithTimeout() 方法会返回一个具有超时功能的上下文。\nch := make(chan string)timeout, cancel := context.WithTimeout(context.Background(), 3*time.Second)defer cancel()go func() &#123;\ttime.Sleep(time.Second * 4)\tch &lt;- &quot;done&quot;&#125;()select &#123;case res := &lt;-ch:\tfmt.Println(res)case &lt;-timeout.Done():\tfmt.Println(&quot;timout&quot;, timeout.Err())&#125;\n\n同样的用法，context 的 Done() 函数会返回一个 channel，该 channel 会在当前工作完成或者是上下文取消生效。\ntimout context deadline exceeded\n\n通过 timeout.Err() 也能知道当前 context  关闭的原因。\ngoroutine 传递 context使用 context 还有一个好处是，可以利用其天然在多个 goroutine 中传递的特性，让所有传递了该 context 的 goroutine 同时接收到取消通知，这点在多 go 中应用非常广泛。\nfunc main() &#123;\ttotal := 12\tvar num int32\tlog.Println(&quot;begin&quot;)\tctx, cancelFunc := context.WithTimeout(context.Background(), 3*time.Second)\tfor i := 0; i &lt; total; i++ &#123;\t\tgo func() &#123;\t\t\t//time.Sleep(3 * time.Second)\t\t\tatomic.AddInt32(&amp;num, 1)\t\t\tif atomic.LoadInt32(&amp;num) == 10 &#123;\t\t\t\tcancelFunc()\t\t\t&#125;\t\t&#125;()\t&#125;\tfor i := 0; i &lt; 5; i++ &#123;\t\tgo func() &#123;\t\t\tselect &#123;\t\t\tcase &lt;-ctx.Done():\t\t\t\tlog.Println(&quot;ctx1 done&quot;, ctx.Err())\t\t\t&#125;\t\t\tfor i := 0; i &lt; 2; i++ &#123;\t\t\t\tgo func() &#123;\t\t\t\t\tselect &#123;\t\t\t\t\tcase &lt;-ctx.Done():\t\t\t\t\t\tlog.Println(&quot;ctx2 done&quot;, ctx.Err())\t\t\t\t\t&#125;\t\t\t\t&#125;()\t\t\t&#125;\t\t&#125;()\t&#125;\ttime.Sleep(time.Second*5)\tlog.Println(&quot;end&quot;, ctx.Err())\tfmt.Printf(&quot;执行完毕 %v&quot;, num)&#125;\n\n在以上例子中，无论 goroutine 嵌套了多少层，都是可以在 context 取消时获得消息（当然前提是 context 得传递走）\n某些特殊情况需要提前取消 context 时，也可以手动调用 cancelFunc() 函数。\nGin 中的案例Gin 提供的 Shutdown(ctx) 函数也充分使用了 context。\nctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)defer cancel()if err := srv.Shutdown(ctx); err != nil &#123;\tlog.Fatal(&quot;Server Shutdown:&quot;, err)&#125;log.Println(&quot;Server exiting&quot;)\n\n比如以上代码便是超时等待 10s 进行 Gin 的资源释放，实现的原理也和上文的例子相同。\n总结因为写 go 的时间不长，所以自己写了一个练手的项目：一个接口压力测试工具。\n\n其中一个很常见的需求就是压测 N 秒后退出，这里正好就应用到了相关知识点，同样是初学 go 的小伙伴可以参考。\nhttps://github.com/crossoverJie/ptg/blob/d0781fcb5551281cf6d90a86b70130149e1525a6/duration.go#L41\n","categories":["Go"],"tags":["timer"]},{"title":"写了一个 gorm 乐观锁插件","url":"/2021/03/15/go/gorm-optimistic/","content":"\n前言最近在用 Go 写业务的时碰到了并发更新数据的场景，由于该业务并发度不高，只是为了防止出现并发时数据异常。\n所以自然就想到了乐观锁的解决方案。\n\n\n实现乐观锁的实现比较简单，相信大部分有数据库使用经验的都能想到。\nUPDATE `table` SET `amount`=100,`version`=version+1 WHERE `version` = 1 AND `id` = 1\n\n需要在表中新增一个类似于 version 的字段，本质上我们只是执行这段 SQL，在更新时比较当前版本与数据库版本是否一致。\n\n如上图所示：版本一致则更新成功，并且将版本号+1；如果不一致则认为出现并发冲突，更新失败。\n这时可以直接返回失败，让业务重试；当然也可以再次获取最新数据进行更新尝试。\n\n我们使用的是 gorm 这个 orm 库，不过我查阅了官方文档却没有发现乐观锁相关的支持，看样子后续也不打算提供实现。\n\n不过借助 gorm 实现也很简单：\ntype Optimistic struct &#123;\tId      int64   `gorm:&quot;column:id;primary_key;AUTO_INCREMENT&quot; json:&quot;id&quot;`\tUserId  string  `gorm:&quot;column:user_id;default:0;NOT NULL&quot; json:&quot;user_id&quot;` // 用户ID\tAmount  float32 `gorm:&quot;column:amount;NOT NULL&quot; json:&quot;amount&quot;`             // 金额\tVersion int64   `gorm:&quot;column:version;default:0;NOT NULL&quot; json:&quot;version&quot;` // 版本&#125;func TestUpdate(t *testing.T) &#123;\tdsn := &quot;root:abc123@/test?charset=utf8&amp;parseTime=True&amp;loc=Local&quot;\tdb, err := gorm.Open(mysql.Open(dsn), &amp;gorm.Config&#123;&#125;)\tvar out Optimistic\tdb.First(&amp;out, Optimistic&#123;Id: 1&#125;)\tout.Amount = out.Amount + 10\tcolumn := db.Model(&amp;out).Where(&quot;id&quot;, out.Id).Where(&quot;version&quot;, out.Version).\t\tUpdateColumn(&quot;amount&quot;, out.Amount).\t\tUpdateColumn(&quot;version&quot;, gorm.Expr(&quot;version+1&quot;))\tfmt.Printf(&quot;#######update %v line \\n&quot;, column.RowsAffected)&#125;\n\n这里我们创建了一张 t_optimistic 表用于测试，生成的 SQL 也满足乐观锁的要求。\n不过考虑到这类业务的通用性，每次需要乐观锁更新时都需要这样硬编码并不太合适。对于业务来说其实 version 是多少压根不需要关心，只要能满足并发更新时的准确性即可。\n因此我做了一个封装，最终使用如下：\nvar out Optimisticdb.First(&amp;out, Optimistic&#123;Id: 1&#125;)out.Amount = out.Amount + 10if err = UpdateWithOptimistic(db, &amp;out, nil, 0, 0); err != nil &#123;\t\tfmt.Printf(&quot;%+v \\n&quot;, err)&#125;\n\n\n这里的使用场景是每次更新时将 amount 金额加上 10。\n\n这样只会更新一次，如果更新失败会返回一个异常。\n当然也支持更新失败时执行一个回调函数，在该函数中实现对应的业务逻辑，同时会使用该业务逻辑尝试更新 N 次。\nfunc BenchmarkUpdateWithOptimistic(b *testing.B) &#123;\tdsn := &quot;root:abc123@/test?charset=utf8&amp;parseTime=True&amp;loc=Local&quot;\tdb, err := gorm.Open(mysql.Open(dsn), &amp;gorm.Config&#123;&#125;)\tif err != nil &#123;\t\tfmt.Println(err)\t\treturn\t&#125;\tb.RunParallel(func(pb *testing.PB) &#123;\t\tvar out Optimistic\t\tdb.First(&amp;out, Optimistic&#123;Id: 1&#125;)\t\tout.Amount = out.Amount + 10\t\terr = UpdateWithOptimistic(db, &amp;out, func(model Lock) Lock &#123;\t\t\tbizModel := model.(*Optimistic)\t\t\tbizModel.Amount = bizModel.Amount + 10\t\t\treturn bizModel\t\t&#125;, 3, 0)\t\tif err != nil &#123;\t\t\tfmt.Printf(&quot;%+v \\n&quot;, err)\t\t&#125;\t&#125;)&#125;\n\n以上代码的目的是：\n将 amount 金额 +10，失败时再次依然将金额+10，尝试更新 3 次；经过上述的并行测试，最终查看数据库确认数据并没有发生错误。\n面向接口编程下面来看看具体是如何实现的；其实真正核心的代码也比较少：\nfunc UpdateWithOptimistic(db *gorm.DB, model Lock, callBack func(model Lock) Lock, retryCount, currentRetryCount int32) (err error) &#123;\tif currentRetryCount &gt; retryCount &#123;\t\treturn errors.WithStack(NewOptimisticError(&quot;Maximum number of retries exceeded:&quot; + strconv.Itoa(int(retryCount))))\t&#125;\tcurrentVersion := model.GetVersion()\tmodel.SetVersion(currentVersion + 1)\tcolumn := db.Model(model).Where(&quot;version&quot;, currentVersion).UpdateColumns(model)\taffected := column.RowsAffected\tif affected == 0 &#123;\t\tif callBack == nil &amp;&amp; retryCount == 0 &#123;\t\t\treturn errors.WithStack(NewOptimisticError(&quot;Concurrent optimistic update error&quot;))\t\t&#125;\t\ttime.Sleep(100 * time.Millisecond)\t\tdb.First(model)\t\tbizModel := callBack(model)\t\tcurrentRetryCount++\t\terr := UpdateWithOptimistic(db, bizModel, callBack, retryCount, currentRetryCount)\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t&#125;\treturn column.Error&#125;\n\n具体步骤如下：\n\n判断重试次数是否达到上限。\n获取当前更新对象的版本号，将当前版本号 +1。\n根据版本号条件执行更新语句。\n更新成功直接返回。\n更新失败 affected == 0  时，执行重试逻辑。\n重新查询该对象的最新数据，目的是获取最新版本号。\n执行回调函数。\n从回调函数中拿到最新的业务数据。\n递归调用自己执行更新，直到重试次数达到上限。\n\n\n\n这里有几个地方值得说一下；由于 Go 目前还不支持泛型，所以我们如果想要获取 struct 中的 version 字段只能通过反射。\n考虑到反射的性能损耗以及代码的可读性，有没有更”优雅“的实现方式呢？\n于是我定义了一个 interface:\ntype Lock interface &#123;\tSetVersion(version int64)\tGetVersion() int64&#125;\n\n其中只有两个方法，目的则是获取 struct 中的 version 字段；所以每个需要乐观锁的 struct 都得实现该接口，类似于这样：\nfunc (o *Optimistic) GetVersion() int64 &#123;\treturn o.Version&#125;func (o *Optimistic) SetVersion(version int64) &#123;\to.Version = version&#125;\n\n这样还带来了一个额外的好处：\n\n一旦该结构体没有实现接口，在乐观锁更新时编译器便会提前报错，如果使用反射只能是在运行期间才能进行校验。\n所以这里在接收数据库实体的便可以是 Lock 接口，同时获取和重新设置 version 字段也是非常的方便。\ncurrentVersion := model.GetVersion()model.SetVersion(currentVersion + 1)\n\n类型断言当并发更新失败时affected == 0，便会回调传入进来的回调函数，在回调函数中我们需要实现自己的业务逻辑。\nerr = UpdateWithOptimistic(db, &amp;out, func(model Lock) Lock &#123;\t\t\tbizModel := model.(*Optimistic)\t\t\tbizModel.Amount = bizModel.Amount + 10\t\t\treturn bizModel\t\t&#125;, 2, 0)\t\tif err != nil &#123;\t\t\tfmt.Printf(&quot;%+v \\n&quot;, err)\t\t&#125;\n\n但由于回调函数的入参只能知道是一个 Lock 接口，并不清楚具体是哪个 struct，所以在执行业务逻辑之前需要将这个接口转换为具体的 struct。\n这其实和 Java 中的父类向子类转型非常类似，必须得是强制类型转换，也就是说运行时可能会出问题。\n在 Go 语言中这样的行为被称为类型断言；虽然叫法不同，但目的类似。其语法如下：\nx.(T)x:表示 interface T:表示 向下转型的具体 struct\n\n所以在回调函数中得根据自己的需要将 interface 转换为自己的 struct，这里得确保是自己所使用的 struct ，因为是强制转换，编译器无法帮你做校验，具体能否转换成功得在运行时才知道。\n总结有需要的朋友可以在这里获取到源码及具体使用方式:\nhttps://github.com/crossoverJie/gorm-optimistic\n最近工作中使用了几种不同的编程语言，会发现除了语言自身的语法特性外大部分知识点都是相同的；\n比如面向对象、数据库、IO操作等；所以掌握了这些基本知识，学习其他语言自然就能触类旁通了。\n","categories":["数据库"],"tags":["Java","Go","OOP","乐观锁"]},{"title":"用 Go 实现一个 LRU cache","url":"/2021/12/20/go/lru-cache/","content":"\n前言早在几年前写过关于 LRU cache 的文章：https://crossoverjie.top/2018/04/07/algorithm/LRU-cache/\n当时是用 Java 实现的，最近我在完善 ptg 时正好需要一个最近最少使用的数据结构来存储历史记录。\n\nptg: Performance testing tool (Go), 用 Go 实现的 gRPC 客户端调试工具。\n\nGo 官方库中并没有相关的实现，考虑到程序的简洁就不打算依赖第三方库，自己写一个；本身复杂度也不高，没有几行代码。\n\n\n配合这个数据结构，我便在 ptg 中实现了请求历史记录的功能：\n\n将每次的请求记录存储到 lru cache 中，最近使用到的历史记录排在靠前，同时也能提供相关的搜索功能；具体可见下图。\n\n\n实现\n实现原理没什么好说的，和 Java 的一样：\n\n一个双向链表存储数据的顺序\n一个 map 存储最终的数据\n当数据达到上限时移除链表尾部数据\n将使用到的 Node 移动到链表的头结点\n\n虽然 Go 比较简洁，但好消息是基本的双向链表结构还是具备的。\n\n所以基于此便定义了一个 LruCache:\n\n根据之前的分析：\n\nsize 存储缓存大小。\n链表存储数据顺序。\nmap 存储数据。\nlock 用于控制并发安全。\n\n\n接下来重点是两个函数：写入、查询。\n写入时判断是否达到容量上限，达到后删除尾部数据；否则就想数据写入头部。\n而获取数据时，这会将查询到的结点移动到头结点。\n这些结点操作都由 List 封装好了的。\n所以使用起来也比较方便。\n最终就是通过这个 LruCache 实现了上图的效果，想要了解更多细节的可以参考源码:\nhttps://github.com/crossoverJie/ptg/blob/main/gui/lru.go\n","categories":["Go"],"tags":["LRU cache"]},{"title":"观察者模式的实际应用","url":"/2021/09/02/go/observer/","content":"\n前言设计模式不管是在面试还是工作中都会遇到，但我经常碰到小伙伴抱怨实际工作中自己应用设计模式的机会非常小。\n正好最近工作中遇到一个用观察者模式解决问题的场景，和大家一起分享。\n\n\n背景如下：\n在用户创建完订单的标准流程中需要做额外一些事情：\n同时这些业务也是不固定的，随时会根据业务发展增加、修改逻辑。\n如果直接将逻辑写在下单业务中，这一”坨“不是很核心的业务就会占据的越来越多，修改时还有可能影响到正常的下单流程。\n当然也有其他方案，比如可以启动几个定时任务，定期扫描扫描订单然后实现自己的业务逻辑；但这样会浪费许多不必要的请求。\n观察者模式因此观察者模式就应运而生，它是由事件发布者在自身状态发生变化时发出通知，由观察者获取消息实现业务逻辑。\n这样事件发布者和接收者就可以完全解耦，互不影响；本质上也是对开闭原则的一种实现。\n示例代码\n先大体看一下观察者模式所使用到的接口与关系：\n\n主体接口：定义了注册实现、循环通知接口。\n观察者接口：定义了接收主体通知的接口。\n主体、观察者接口都可以有多个实现。\n业务代码只需要使用 Subject.Nofity() 接口即可。\n\n\n接下来看看创建订单过程中的实现案例。\n\n代码采用 go 实现，其他语言也是类似。\n\n首先按照上图定义了两个接口：\ntype Subject interface &#123;\tRegister(Observer)\tNotify(data interface&#123;&#125;)&#125;type Observer interface &#123;\tUpdate(data interface&#123;&#125;)&#125;\n\n由于我们这是一个下单的事件，所以定义了 OrderCreateSubject 实现 Subject：\ntype OrderCreateSubject struct &#123;\tobserverList []Observer&#125;func NewOrderCreate() Subject &#123;\treturn &amp;OrderCreateSubject&#123;&#125;&#125;func (o *OrderCreateSubject) Register(observer Observer) &#123;\to.observerList = append(o.observerList, observer)&#125;func (o *OrderCreateSubject) Notify(data interface&#123;&#125;) &#123;\tfor _, observer := range o.observerList &#123;\t\tobserver.Update(data)\t&#125;&#125;\n\n其中的 observerList 切片是用于存放所有订阅了下单事件的观察者。\n接着便是编写观察者业务逻辑了，这里我实现了两个：\ntype B1CreateOrder struct &#123;&#125;func (b *B1CreateOrder) Update(data interface&#123;&#125;) &#123;\tfmt.Printf(&quot;b1.....data %v \\n&quot;, data)&#125;type B2CreateOrder struct &#123;&#125;func (b *B2CreateOrder) Update(data interface&#123;&#125;) &#123;\tfmt.Printf(&quot;b2.....data %v \\n&quot;, data)&#125;\n\n使用起来也非常简单：\nfunc TestObserver(t *testing.T) &#123;\tcreate := NewOrderCreate()\tcreate.Register(&amp;B1CreateOrder&#123;&#125;)\tcreate.Register(&amp;B2CreateOrder&#123;&#125;)\tcreate.Notify(&quot;abc123&quot;)&#125;\n\nOutput：\nb1.....data abc123 b2.....data abc123 \n\n\n创建一个创建订单的主体 subject。\n注册所有的订阅事件。\n在需要通知处调用 Notify 方法。\n\n这样一旦我们需要修改各个事件的实现时就不会互相影响，即便是要加入其他实现也是非常容易的：\n\n编写实现类。\n注册进实体。\n\n不会再修改核心流程。\n配合容器其实我们也可以省略掉注册事件的步骤，那就是使用容器；大致流程如下：\n\n自定义的事件全部注入进容器。\n再注册事件的地方从容器中取出所有的事件，挨个注册。\n\n\n这里所使用的容器是 https://github.com/uber-go/dig\n\n\n修改后的代码中，每当我们新增一个观察者（事件订阅）时，只需要使用容器所提供 Provide 函数注册进容器即可。\n同时为了让容器能够支持同一个对象存在多个实例也需要新增部分代码：\nObserver.go:\ntype Observer interface &#123;\tUpdate(data interface&#123;&#125;)&#125;type (\tInstance struct &#123;\t\tdig.Out\t\tInstance Observer `group:&quot;observers&quot;`\t&#125;\tInstanceParams struct &#123;\t\tdig.In\t\tInstances []Observer `group:&quot;observers&quot;`\t&#125;)\n\n在 observer 接口中需要新增两个结构体用于存放同一个接口的多个实例。\n\n group:&quot;observers&quot; 用于声明是同一个接口。\n\n创建具体观察者对象时返回 Instance 对象。\nfunc NewB1() Instance &#123;\treturn Instance&#123;\t\tInstance: &amp;B1CreateOrder&#123;&#125;,\t&#125;&#125;func NewB2() Instance &#123;\treturn Instance&#123;\t\tInstance: &amp;B2CreateOrder&#123;&#125;,\t&#125;&#125;\n\n\n其实就是用 Instance 包装了一次。\n\n这样在注册观察者时，便能从 InstanceParams.Instances 中取出所有的观察者对象了。\nerr = c.Invoke(func(subject Subject, params InstanceParams) &#123;\tfor _, instance := range params.Instances &#123;\t\tsubject.Register(instance)\t&#125;&#125;)\n\n\n\n这样在使用时直接从容器中获取主题对象，然后通知即可：\nerr = c.Invoke(func(subject Subject) &#123;\tsubject.Notify(&quot;abc123&quot;)&#125;)\n\n更多关于 dig 的用法可以参考官方文档：\nhttps://pkg.go.dev/go.uber.org/dig#hdr-Value_Groups\n总结有经验的开发者会发现和发布订阅模式非常类似，当然他们的思路是类似的；我们不用纠结与两者的差异（面试时除外）；学会其中的思路更加重要。\n","categories":["Go","设计模式"],"tags":["设计模式","observer"]},{"title":"一个小时学会用 Go 编写命令行工具","url":"/2020/12/08/go/one-hour-write-cli-app/","content":"\n前言最近因为项目需要写了一段时间的 Go ，相对于 Java 来说语法简单同时又有着一些 Python 之类的语法糖，让人大呼”真香“。\n\n\n\n但现阶段相对来说还是 Python 写的多一些，偶尔还得回炉写点 Java ；自然对 Go 也谈不上多熟悉。\n于是便利用周末时间自己做个小项目来加深一些使用经验。于是我便想到了之前利用 Java 写的一个博客小工具。\n那段时间正值微博图床大量图片禁止外链，导致许多个人博客中的图片都不能查看。这个工具可以将文章中的图片备份到本地，还能将图片直接替换到其他图床。\n\n我个人现在是一直在使用，通常是在码字的时候利用 iPic 之类的工具将图片上传到微博图床（主要是方便+免费）。写完之后再通过这个工具一键切换到 [SM.MS](http://sm.MS) 这类付费图床，同时也会将图片备份到本地磁盘。\n改为用 Go 重写为 cli 工具后使用效果如下：\n\n需要掌握哪些技能之所以选择这个工具用 Go 来重写；一个是功能比较简单，但也正好可以利用到 Go 的一些特点，比如网络 IO、协程同步之类。\n同时修改为命令行工具后是不是感觉更极客了呢。\n再开始之前还是先为不熟悉 Go 的 Javaer 介绍下大概会用到哪些知识点：\n\n使用和管理第三方依赖包(go mod)\n协程的运用。\n多平台打包。\n\n下面开始具体操作，我觉得即便是没怎么接触过 Go 的朋友看完之后也能快速上手实现一个小工具。\n使用和管理第三方依赖\n还没有安装 Go 的朋友请参考官网自行安装。\n\n首先介绍一下 Go 的依赖管理，在版本 1.11 之后官方就自带了依赖管理模块，所以在当下最新版 1.15 中已经强烈推荐使用。\n它的目的和作用与 Java 中的 maven，Python 中的 pip 类似，但使用起来比 maven 简单许多。\n\n根据它的使用参考，需要首先在项目目录下执行 go mod init 用于初始化一个 go.mod 文件，当然如果你使用的是 GoLang 这样的 IDE，在新建项目时会自动帮我们创建好目录结构，当然也包含 go.mod 这个文件。\n在这个文件中我们引入我们需要的第三方包：\nmodule btbgo 1.15require (\tgithub.com/cheggaaa/pb/v3 v3.0.5\tgithub.com/fatih/color v1.10.0\tgithub.com/urfave/cli/v2 v2.3.0)\n\n我这里使用了三个包，分别是：\n\npb: progress bar，用于在控制台输出进度条。\ncolor: 用于在控制台输出不同颜色的文本。\ncli: 命令行工具开发包。\n\n\nimport (\t&quot;btb/constants&quot;\t&quot;btb/service&quot;\t&quot;github.com/urfave/cli/v2&quot;\t&quot;log&quot;\t&quot;os&quot;)func main() &#123;\tvar model string\tdownloadPath := constants.DownloadPath\tmarkdownPath := constants.MarkdownPath\tapp := &amp;cli.App&#123;\t\tFlags: []cli.Flag&#123;\t\t\t&amp;cli.StringFlag&#123;\t\t\t\tName:        &quot;model&quot;,\t\t\t\tUsage:       &quot;operating mode; r:replace, b:backup&quot;,\t\t\t\tDefaultText: &quot;b&quot;,\t\t\t\tAliases:     []string&#123;&quot;m&quot;&#125;,\t\t\t\tRequired:    true,\t\t\t\tDestination: &amp;model,\t\t\t&#125;,\t\t\t&amp;cli.StringFlag&#123;\t\t\t\tName:        &quot;download-path&quot;,\t\t\t\tUsage:       &quot;The path where the image is stored&quot;,\t\t\t\tAliases:     []string&#123;&quot;dp&quot;&#125;,\t\t\t\tDestination: &amp;downloadPath,\t\t\t\tRequired:    true,\t\t\t\tValue:       constants.DownloadPath,\t\t\t&#125;,\t\t\t&amp;cli.StringFlag&#123;\t\t\t\tName:        &quot;markdown-path&quot;,\t\t\t\tUsage:       &quot;The path where the markdown file is stored&quot;,\t\t\t\tAliases:     []string&#123;&quot;mp&quot;&#125;,\t\t\t\tDestination: &amp;markdownPath,\t\t\t\tRequired:    true,\t\t\t\tValue:       constants.MarkdownPath,\t\t\t&#125;,\t\t&#125;,\t\tAction: func(c *cli.Context) error &#123;\t\t\tservice.DownLoadPic(markdownPath, downloadPath)\t\t\treturn nil\t\t&#125;,\t\tName:  &quot;btb&quot;,\t\tUsage: &quot;Help you backup and replace your blog&#x27;s images&quot;,\t&#125;\terr := app.Run(os.Args)\tif err != nil &#123;\t\tlog.Fatal(err)\t&#125;&#125;\n\n代码非常简单，无非就是使用了 cli 所提供的 api 创建了几个命令，将用户输入的 -dp、-mp 参数映射到 downloadPath、markdownPath 变量中。\n之后便利用这两个数据扫描所有的图片，以及将图片下载到对应的目录中。\n更多使用指南可以直接参考官方文档。\n可以看到部分语法与 Java 完全不同，比如：\n\n申明变量时类型是放在后边，先定义变量名称；方法参数类似。\n类型推导，可以不指定变量类型（新版本的 Java 也支持）\n方法支持同时返回多个值，这点非常好用。\n公共、私用函数利用首字母大小写来区分。\n还有其他的就不一一列举了。\n\n\n协程紧接着命令执行处调用了 service.DownLoadPic(markdownPath, downloadPath) 处理业务逻辑。\n这里包含的文件扫描、图片下载之类的代码就不分析了；官方 SDK 写的很清楚，也比较简单。\n重点看看 Go 里的 goroutine 也就是协程。\n我这里使用的场景是每扫描到一个文件就利用一个协程去解析和下载图片，从而可以提高整体的运行效率。\nfunc DownLoadPic(markdownPath, downloadPath string) &#123;\twg := sync.WaitGroup&#123;&#125;\tallFile, err := util.GetAllFile(markdownPath)\twg.Add(len(*allFile))\tif err != nil &#123;\t\tlog.Fatal(&quot;read file error&quot;)\t&#125;\tfor _, filePath := range *allFile &#123;\t\tgo func(filePath string) &#123;\t\t\tallLine, err := util.ReadFileLine(filePath)\t\t\tif err != nil &#123;\t\t\t\tlog.Fatal(err)\t\t\t&#125;\t\t\tavailableImgs := util.MatchAvailableImg(allLine)\t\t\tbar := pb.ProgressBarTemplate(constants.PbTmpl).Start(len(*availableImgs))\t\t\tbar.Set(&quot;fileName&quot;, filePath).\t\t\t\tSetWidth(120)\t\t\tfor _, url := range *availableImgs &#123;\t\t\t\tif err != nil &#123;\t\t\t\t\tlog.Fatal(err)\t\t\t\t&#125;\t\t\t\terr := util.DownloadFile(url, *genFullFileName(downloadPath, filePath, &amp;url))\t\t\t\tif err != nil &#123;\t\t\t\t\tlog.Fatal(err)\t\t\t\t&#125;\t\t\t\tbar.Increment()\t\t\t&#125;\t\t\tbar.Finish()\t\t\twg.Done()\t\t&#125;(filePath)\t&#125;\twg.Wait()\tcolor.Green(&quot;Successful handling of [%v] files.\\n&quot;, len(*allFile))\tif err != nil &#123;\t\tlog.Fatal(err)\t&#125;&#125;\n\n就代码使用层面看起来是不是要比 Java 简洁许多，我们不用像 Java 那样需要维护一个 executorService，也不需要考虑这个线程池的大小，一切都交给 Go 自己去调度。\n使用时只需要在调用函数之前加上 go 关键字，只不过这里是一个匿名函数。\n而且由于 goroutine 非常轻量，与 Java 中的 thread 相比占用非常少的内存，所以我们也不需要精准的控制创建数量。\n\n不过这里也用到了一个和 Java 非常类似的东西：WaitGroup。\n它的用法与作用都与 Java 中的 CountDownLatch 非常相似；主要用于等待所有的 goroutine 执行完毕，在这里自然是等待所有的图片都下载完毕然后退出程序。\n使用起来主要分为三步：\n\n创建和初始化 goruntime 的数量：wg.Add(len(number)\n每当一个 goruntime 执行完毕调用 wg.Done() 让计数减一。\n最终调用 wg.Wait() 等待WaitGroup 的数量减为0。\n\n对于协程 Go 推荐使用 chanel 来互相通信，这点今后有机会再讨论。 \n打包核心逻辑也就这么多，下面来讲讲打包与运行；这点和 Java 的区别就比较大了。\n众所周知，Java 有一句名言：write once run anywhere\n这是因为有了 JVM 虚拟机，所以我们不管代码最终运行于哪个平台都只需要打出一个包；但 Go 没有虚拟机它是怎么做到在个各平台运行呢。\n简单来说 Go 可以针对不同平台打包出不同的二进制文件，这个文件包含了所有运行所需要的依赖，甚至都不需要在目标平台安装 Go 环境。\n\n虽说 Java 最终只需要打一个包，但也得在各个平台安装兼容的 Java 运行环境。\n\n我在这里编写了一个 Makefile 用于执行打包：make release\n# Binary nameBINARY=btbGOBUILD=go build -ldflags &quot;-s -w&quot; -o $&#123;BINARY&#125;GOCLEAN=go cleanRMTARGZ=rm -rf *.gzVERSION=0.0.1release:\t# Clean\t$(GOCLEAN)\t$(RMTARGZ)\t# Build for mac\tCGO_ENABLED=0 GOOS=darwin GOARCH=amd64 $(GOBUILD)\ttar czvf $&#123;BINARY&#125;-mac64-$&#123;VERSION&#125;.tar.gz ./$&#123;BINARY&#125;\t# Build for arm\t$(GOCLEAN)\tCGO_ENABLED=0 GOOS=linux GOARCH=arm64 $(GOBUILD)\ttar czvf $&#123;BINARY&#125;-arm64-$&#123;VERSION&#125;.tar.gz ./$&#123;BINARY&#125;\t# Build for linux\t$(GOCLEAN)\tCGO_ENABLED=0 GOOS=linux GOARCH=amd64 $(GOBUILD)\ttar czvf $&#123;BINARY&#125;-linux64-$&#123;VERSION&#125;.tar.gz ./$&#123;BINARY&#125;\t# Build for win\t$(GOCLEAN)\tCGO_ENABLED=0 GOOS=windows GOARCH=amd64 $(GOBUILD).exe\ttar czvf $&#123;BINARY&#125;-win64-$&#123;VERSION&#125;.tar.gz ./$&#123;BINARY&#125;.exe\t$(GOCLEAN)\n\n可以看到我们只需要在 go build 之前指定系统变量即可打出不同平台的包，比如我们为 Linux 系统的 arm64 架构打包文件：\nCGO_ENABLED=0 GOOS=linux GOARCH=arm64 go build main.go -o btb\n便可以直接在目标平台执行 ./btb  运行程序。\n总结本文所有代码都已上传 Github: https://github.com/crossoverJie/btb\n感兴趣的也可以直接运行安装脚本体验。\ncurl -fsSL https://raw.githubusercontent.com/crossoverJie/btb/master/install.sh | bash\n\n\n目前这个版本只实现了图片下载备份，后续会完善图床替换及其他功能。\n\n\n这段时间接触 Go 之后给我的感触颇深，对于年纪 25 岁的 Java 来说，Go 确实是后生可畏，更气人的是还赶上了云原生这个浪潮，就更惹不起了。\n一些以前看来不那么重要的小毛病也被重点放大，比如启动慢、占用内存多、语法啰嗦等；不过我依然对这位赏饭吃的祖师爷保持期待，从新版本的 Java 可以看出也在积极改变，更不用说它还有无人撼动的庞大生态。\n更多 Java 后续内容可以参考周志明老师的文章：云原生时代，Java危矣？\n","categories":["Golang"],"tags":["Golang","cli"]},{"title":"不同语言对单例模式的不同实现","url":"/2020/10/09/go/other-lang-singleton-pattern/","content":"\n前言前段时间在用 Python 实现业务的时候发现一个坑，准确的来说是对于 Python 门外汉容易踩的坑；\n大概代码如下：\nclass Mom(object):    name = &#x27;&#x27;    sons = []if __name__ == &#x27;__main__&#x27;:    m1 = Mom()    m1.name = &#x27;m1&#x27;    m1.sons.append([&#x27;s1&#x27;, &#x27;s2&#x27;])    print &#x27;&#123;&#125; sons=&#123;&#125;&#x27;.format(m1.name, m1.sons)    m2 = Mom()    m2.name = &#x27;m2&#x27;    m2.sons.append([&#x27;s3&#x27;, &#x27;s4&#x27;])    print &#x27;&#123;&#125; sons=&#123;&#125;&#x27;.format(m2.name, m2.sons)\n\n首先定义了一个 Mom 的类，它包含了一个字符串类型的 name 与列表类型的 sons 属性；\n\n\n在使用时首先创建了该类的一个实例 m1 并往 sons 中写入一个列表数据；紧接着又创建了一个实例 m2 ，也往 sons 中写入了另一个列表数据。\n如果是一个 Javaer 很少写 Python 看到这样的代码首先想到的输出应该是：\nm1 sons=[[&#x27;s1&#x27;, &#x27;s2&#x27;]]m2 sons=[[&#x27;s3&#x27;, &#x27;s4&#x27;]]\n\n但其实最终的输出结果是：\nm1 sons=[[&#x27;s1&#x27;, &#x27;s2&#x27;]]m2 sons=[[&#x27;s1&#x27;, &#x27;s2&#x27;], [&#x27;s3&#x27;, &#x27;s4&#x27;]]\n\n如果想要达到期望值需要稍微修改一下：\nclass Mom(object):    name = &#x27;&#x27;    def __init__(self):        self.sons = []\n\n只需要修改类的定义就可以了，我相信即使没有 Python 相关经验对比这两个代码应该也能猜到原因：\n在 Python 中如果需要将变量作为实例变量（也就是每个我们期望的输出）时，需要将变量定义到构造函数中，通过 self 访问。\n如果只放在类中，和 Java 中的 static 静态变量效果类似；这些数据由类共享，也就能解释为什么会出现第一种情况，因为其中的 sons 是由 Mom 类共享，所以每次都会累加。\nPython 单例既然 Python 可以通过类变量达到变量在同一个类中共享的效果，那是否可以实现单例模式呢？\n可以利用 Python 的 metaclass 的特性，动态的控制类的创建。\nclass Singleton(type):    _instances = &#123;&#125;    def __call__(cls, *args, **kwargs):        if cls not in cls._instances:            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)        return cls._instances[cls]\n\n首先创建一个 Singleton 的基类，然后我们在我们需要实现单例的类中将其作为 metaclass\nclass MySQLDriver:    __metaclass__ = Singleton    def __init__(self):        print &#x27;MySQLDriver init.....&#x27;\n\n这样Singleton 就可以控制 MySQLDriver 这个类的创建了；其实在 Singleton 中的 __call__ 可以很容易理解这个单例创建的过程：\n\n定义一个私有的类属性 _instances 的字典（也就是 Java 中的 map）可以做到在整个类中共享，无论创建多少个实例。\n当我们自定义类使用了 __metaclass__ = Singleton 后，便可以控制自定义类的创建了；如果已经创建了实例，那就直接从 _instances 取出对象返回，不然就创建一个实例并写回到 _instances ，有点 Spring 容器的感觉。\n\nif __name__ == &#x27;__main__&#x27;:    m1 = MySQLDriver()    m2 = MySQLDriver()    m3 = MySQLDriver()    m4 = MySQLDriver()    print m1    print m2    print m3    print m4MySQLDriver init.....&lt;__main__.MySQLDriver object at 0x10d848790&gt;&lt;__main__.MySQLDriver object at 0x10d848790&gt;&lt;__main__.MySQLDriver object at 0x10d848790&gt;&lt;__main__.MySQLDriver object at 0x10d848790&gt;\n\n最后我们通过实验结果可以看到单例创建成功。\nGo 单例由于最近团队中有部分业务开始在用 go ，所以也想看看在 go 中如何实现单例。\ntype MySQLDriver struct &#123;\tusername string&#125;\n\n在这样一个简单的结构体（可以简单理解为 Java 中的 class）中是没法类似于 Python 和 Java 一样可以声明类共享变量的；go 语言中不存在 static 的概念。\n但我们可以在包中声明一个全局变量来达到同样的效果：\nimport &quot;fmt&quot;type MySQLDriver struct &#123;\tusername string&#125;var mySQLDriver *MySQLDriverfunc GetDriver() *MySQLDriver &#123;\tif mySQLDriver == nil &#123;\t\tmySQLDriver = &amp;MySQLDriver&#123;&#125;\t&#125;\treturn mySQLDriver&#125;\n\n这样在使用时：\nfunc main() &#123;\tdriver := GetDriver()\tdriver.username = &quot;cj&quot;\tfmt.Println(driver.username)\tdriver2 := GetDriver()\tfmt.Println(driver2.username)&#125;\n\n就不需要直接构造 MySQLDriver  ，而是通过GetDriver() 函数来获取，通过 debug 也能看到 driver 和 driver1 引用的是同一个内存地址。\n\n这样的实现常规情况是没有什么问题的，机智的朋友一定能想到和 Java 一样，一旦并发访问就没那么简单了。\n在 go 中，如果有多个 goroutine 同时访问GetDriver() ，那大概率会创建多个 MySQLDriver 实例。\n这里说的没那么简单其实是相对于 Java 来说的，go 语言中提供了简单的 api 便可实现临界资源的访问。\nvar lock sync.Mutexfunc GetDriver() *MySQLDriver &#123;\tlock.Lock()\tdefer lock.Unlock()\tif mySQLDriver == nil &#123;\t\tfmt.Println(&quot;create instance......&quot;)\t\tmySQLDriver = &amp;MySQLDriver&#123;&#125;\t&#125;\treturn mySQLDriver&#125;func main() &#123;\tfor i := 0; i &lt; 100; i++ &#123;\t\tgo GetDriver()\t&#125;\ttime.Sleep(2000 * time.Millisecond)&#125;\n\n稍加改造上文的代码，加入了\nlock.Lock()defer lock.Unlock()\n\n代码就能简单的控制临界资源的访问，即便我们开启了100个协程并发执行，mySQLDriver 实例也只会被初始化一次。\n\n这里的 defer 类似于 Java 中的 finally ，在方法调用前加上 go 关键字即可开启一个协程。\n\n虽说能满足并发要求了，但其实这样的实现也不够优雅；仔细想想这里\nmySQLDriver = &amp;MySQLDriver&#123;&#125;\n\n创建实例只会调用一次，但后续的每次调用都需要加锁从而带来了不必要的开销。\n这样的场景每个语言都是相同的，拿 Java 来说是不是经常看到这样的单例实现：\npublic class Singleton &#123;    private Singleton() &#123;&#125;   private volatile static Singleton instance = null;   public static Singleton getInstance() &#123;        if (instance == null) &#123;              synchronized (Singleton.class)&#123;           if (instance == null) &#123;                 instance = new Singleton();               &#125;            &#125;         &#125;        return instance;    &#125;&#125;\n\n这是一个典型的双重检查的单例，这里做了两次检查便可以避免后续其他线程再次访问锁。\n同样的对于 go 来说也类似：\nfunc GetDriver() *MySQLDriver &#123;\tif mySQLDriver == nil &#123;\t\tlock.Lock()\t\tdefer lock.Unlock()\t\tif mySQLDriver == nil &#123;\t\t\tfmt.Println(&quot;create instance......&quot;)\t\t\tmySQLDriver = &amp;MySQLDriver&#123;&#125;\t\t&#125;\t&#125;\treturn mySQLDriver&#125;\n\n和 Java 一样，在原有基础上额外做一次判断也能达到同样的效果。\n但有没有觉得这样的代码非常繁琐，这一点 go 提供的 api 就非常省事了：\nvar once sync.Oncefunc GetDriver() *MySQLDriver &#123;\tonce.Do(func() &#123;\t\tif mySQLDriver == nil &#123;\t\t\tfmt.Println(&quot;create instance......&quot;)\t\t\tmySQLDriver = &amp;MySQLDriver&#123;&#125;\t\t&#125;\t&#125;)\treturn mySQLDriver&#125;\n\n本质上我们只需要不管在什么情况下  MySQLDriver 实例只初始化一次就能达到单例的目的，所以利用 once.Do() 就能让代码只执行一次。\n\n查看源码会发现 once.Do() 也是通过锁来实现，只是在加锁之前利用底层的原子操作做了一次校验，从而避免每次都要加锁，性能会更好。\n总结相信大家日常开发中很少会碰到需要自己实现一个单例；首先大部分情况下我们都不需要单例，即使是需要，框架通常也都有集成。\n类似于 go 这样框架较少，需要我们自己实现时其实也不需要过多考虑并发的问题；摸摸自己肚子左上方的位置想想，自己写的这个对象真的同时有几百上千的并发来创建嘛？\n不过通过这个对比会发现 go 的语法确实要比 Java 简洁太多，同时轻量级的协程以及简单易用的并发工具支持看起来都要比 Java 优雅许多；后续有机会再接着深入。\n参考链接：\nCreating a singleton in Python\nHow to implement Singleton Pattern in Go\n","categories":["设计模式"],"tags":["Java","Python","Golang"]},{"title":"切片 slice 原理分析","url":"/2021/07/28/go/slice%20pointer/","content":"\n前言作为一个 Go 语言新手，看到一切”诡异“的代码都会感到好奇；比如我最近看到的几个方法；伪代码如下：\nfunc FindA() ([]*T,error) &#123;&#125;func FindB() ([]T,error) &#123;&#125;func SaveA(data *[]T) error &#123;&#125;func SaveB(data *[]*T) error &#123;&#125;\n\n\n\n相信大部分刚入门 Go 的新手看到这样的代码也是一脸懵逼，其中最让人疑惑的就是：\n[]*T*[]T*[]*T\n这样对切片的声明，先不看后面两种写法；单独看 []*T 还是很好理解的：该切片中存放的是所有 T 的内存地址，会比存放 T 本身来说要更省空间，同时 []*T 在方法内部是可以修改 T 的值，而[]T 是修改不了。\nfunc TestSaveSlice(t *testing.T) &#123;\ta := []T&#123;&#123;Name: &quot;1&quot;&#125;, &#123;Name: &quot;2&quot;&#125;&#125;\tfor _, t2 := range a &#123;\t\tfmt.Println(t2)\t&#125;\t_ = SaveB(a)\tfor _, t2 := range a &#123;\t\tfmt.Println(t2)\t&#125;&#125;func SaveB(data []T) error &#123;\tt := data[0]\tt.Name = &quot;1233&quot;\treturn nil&#125;type T struct &#123;\tName string&#125;\n\n比如以上例子打印的是\n&#123;1&#125;&#123;2&#125;&#123;1&#125;&#123;2&#125;\n只有将方法修改为\nfunc SaveB(data []*T) error &#123;\tt := data[0]\tt.Name = &quot;1233&quot;\treturn nil&#125;\n才能修改 T 的值:\n&amp;&#123;1&#125;&amp;&#123;2&#125;&amp;&#123;1233&#125;&amp;&#123;2&#125;\n\n示例下面重点来看看 []*T 与 *[]T 的区别，这里写了两个 append 函数：\nfunc TestAppendA(t *testing.T) &#123;\tx:=[]int&#123;1,2,3&#125;\tappendA(x)\tfmt.Printf(&quot;main %v\\n&quot;, x)&#125;func appendA(x []int) &#123;\tx[0]= 100\tfmt.Printf(&quot;appendA %v\\n&quot;, x)&#125;\n先看第一种，输出是结果是：\nappendA [1000 2 3]main [1000 2 3]\n说明在函数传递过程中，函数内部的修改能够影响到外部。\n\n下面我们再看一个例子：\nfunc appendB(x []int) &#123;\tx = append(x, 4)\tfmt.Printf(&quot;appendA %v\\n&quot;, x)&#125;\n最终结果却是：\nappendA [1 2 3 4]main [1 2 3]\n没有影响到外部。\n而当我们再调整一下会发现又有所不同：\nfunc TestAppendC(t *testing.T) &#123;\tx:=[]int&#123;1,2,3&#125;\tappendC(&amp;x)\tfmt.Printf(&quot;main %v\\n&quot;, x)&#125;func appendC(x *[]int) &#123;\t*x = append(*x, 4)\tfmt.Printf(&quot;appendA %v\\n&quot;, x)&#125;\n最终的结果：\nappendA &amp;[1 2 3 4]main [1 2 3 4]\n\n可以发现如果传递切片的指针时，使用 append 函数追加数据时会影响到外部。\nslice 原理在分析上面三种情况之前，我们先来了解下 slice 的数据结构。\n直接查看源码会发现 slice 其实就是一个结构体，只是不能直接对外访问。\n\n\n源码地址 runtime/slice.go\n\n其中有三个重要的属性：\n\n\n\n属性\n含义\n\n\n\narray\n底层存放数据的数组，是一个指针。\n\n\nlen\n切片长度\n\n\ncap\n切片容量 cap&gt;=len\n\n\n提到切片就不得不想到数组，可以这么理解：\n\n切片是对数组的抽象，而数组则是切片的底层实现。\n\n其实通过切片这个名字也不难看出，它就是从数组中切了一部分；相对于数组的固定大小，切片可以根据实际使用情况进行扩容。\n所以切片也可以通过对数组”切一刀”获得：\nx1:=[6]int&#123;0,1,2,3,4,5&#125;x2 := x[1:4]fmt.Println(len(x2), cap(x2))\n\n其中 x1 的长度与容量都是6。\nx2 的长度与容量则为3和5。\n\nx2 的长度很容易理解。\n容量等于5可以理解为，当前这个切片最多可以使用的长度。\n\n因为切片 x2 是对数组 x1 的引用，所以底层数组排除掉左边一个没有被引用的位置则是该切片最大的容量，也就是5。\n同一个底层数组以刚才的代码为例：\nfunc TestAppendA(t *testing.T) &#123;\tx:=[]int&#123;1,2,3&#125;\tappendA(x)\tfmt.Printf(&quot;main %v\\n&quot;, x)&#125;func appendA(x []int) &#123;\tx[0]= 100\tfmt.Printf(&quot;appendA %v\\n&quot;, x)&#125;\n\n\n在函数传递过程中，main 中的 x 与 appendA 函数中的 x 切片所引用的是同个数组。\n所以在函数中对 x[0]=100，main函数中也能获取到。\n\n本质上修改的就是同一块内存数据。\n值传递带来的误会在上述例子中，在 appendB 中调用 append 函数追加数据后会发现 main 函数中并没有受到影响，这里我稍微调整了一下示例代码：\nfunc TestAppendB(t *testing.T) &#123;\t//x:=[]int&#123;1,2,3&#125;\tx := make([]int, 3,5)\tx[0] = 1\tx[1] = 2\tx[2] = 3\tappendB(x)\tfmt.Printf(&quot;main %v len=%v,cap=%v\\n&quot;, x,len(x),cap(x))&#125;func appendB(x []int) &#123;\tx = append(x, 444)\tfmt.Printf(&quot;appendB %v len=%v,cap=%v\\n&quot;, x,len(x),cap(x))&#125;\n\n\n主要是修改了切片初始化方式，使得容量大于了长度，具体原因后续会说明。\n\n输出结果如下：\nappendB [1 2 3 444] len=4,cap=5main [1 2 3] len=3,cap=5\nmain 函数中的数据看样子确实没有受到影响；但细心的朋友应该会注意到  appendB 函数中的 x 在 append() 之后长度 +1 变为了4。\n而在 main 函数中长度又变回了3.\n这个细节区别就是为什么 append() “看似” 没有生效的原因；至于为什么要说“看似”，再次调整了代码：\nfunc TestAppendB(t *testing.T) &#123;\t//x:=[]int&#123;1,2,3&#125;\tx := make([]int, 3,5)\tx[0] = 1\tx[1] = 2\tx[2] = 3\tappendB(x)\tfmt.Printf(&quot;main %v len=%v,cap=%v\\n&quot;, x,len(x),cap(x))\ty:=x[0:cap(x)]\tfmt.Printf(&quot;y %v len=%v,cap=%v\\n&quot;, y,len(y),cap(y))&#125;\n在刚才的基础之上，以 append 之后的 x 为基础再做了一个切片；该切片的范围为 x 所引用数组的全部数据。\n再来看看执行结果如何：\nappendB [1 2 3 444] len=4,cap=5main [1 2 3] len=3,cap=5y [1 2 3 444 0] len=5,cap=5\n会神奇的发现 y 将所有数据都打印出来，在 appendB 函数中追加的数据其实已经写入了数组中，但为什么 x 本身没有获取到呢？\n\n看图就很容易理解了：\n\n在appendB中确实是对原始数组追加了数据，同时长度也增加了。\n但由于是值传递，所以 slice 这个结构体即便是修改了长度为4，也只是对复制的那个对象修改了长度，main 中的长度依然为3.\n由于底层数组是同一个，所以基于这个底层数组重新生成了一个完整长度的切片便能看到追加的数据了。\n\n所以这里本质的原因是因为 slice 是一个结构体，传递的是值，不管方法里如何修改长度也不会影响到原有的数据（这里指的是长度和容量这两个属性）。\n切片扩容还有一个需要注意：\n刚才特意提到这里的例子稍有改变，主要是将切片的容量设置超过了数组的长度；\n如果不做这个特殊设置会怎么样呢？\nfunc TestAppendB(t *testing.T) &#123;\tx:=[]int&#123;1,2,3&#125;\t//x := make([]int, 3,5)\tx[0] = 1\tx[1] = 2\tx[2] = 3\tappendB(x)\tfmt.Printf(&quot;main %v len=%v,cap=%v\\n&quot;, x,len(x),cap(x))\ty:=x[0:cap(x)]\tfmt.Printf(&quot;y %v len=%v,cap=%v\\n&quot;, y,len(y),cap(y))&#125;func appendB(x []int) &#123;\tx = append(x, 444)\tfmt.Printf(&quot;appendB %v len=%v,cap=%v\\n&quot;, x,len(x),cap(x))&#125;\n\n输出结果：\nappendB [1 2 3 444] len=4,cap=6main [1 2 3] len=3,cap=3y [1 2 3] len=3,cap=3\n\n这时会发现 main 函数中的 y 切片数据也没有发生变化，这是为什么呢？\n\n这是因为初始化 x 切片时长度和容量都为3，当在 appendB 函数中追加数据时，会发现没有位置了。\n这时便会进行扩容：\n\n将老数据复制一份到新的数组中。\n追加数据。\n将新的数据内存地址返回给 appendB 中的 x .\n\n同样的由于是值传递，所以 appendB 中的切片换了底层数组对 main 函数中的切片没有任何影响，也就导致最终 main 函数的数据没有任何变化了。\n传递切片指针有没有什么办法即便是在扩容时也能对外部产生影响呢？\nfunc TestAppendC(t *testing.T) &#123;\tx:=[]int&#123;1,2,3&#125;\tappendC(&amp;x)\tfmt.Printf(&quot;main %v len=%v,cap=%v\\n&quot;, x,len(x),cap(x))&#125;func appendC(x *[]int) &#123;\t*x = append(*x, 4)\tfmt.Printf(&quot;appendC %v\\n&quot;, x)&#125;\n\n输出结果为：\nappendC &amp;[1 2 3 4]main [1 2 3 4] len=4,cap=6\n\n这时外部的切片就能受到影响了，其实原因也很简单；\n刚才也说了，因为 slice 本身是一个结构体，所以当我们传递指针时，就和平时自定义的 struct 在函数内部通过指针修改数据原理相同。\n最终在 appendC 中的 x 的指针指向了扩容后的结构体，因为传递的是 main 函数中 x 的指针，所以同样的 main 函数中的 x 也指向了该结构体。\n总结所以总结一下：\n\n切片是对数组的抽象，同时切片本身也是一个结构体。\n参数传递时函数内部与外部引用的是同一个数组，所以对切片的修改会影响到函数外部。\n如果发生扩容，情况会发生变化，同时扩容会导致数据拷贝；所以要尽量预估切片大小，避免数据拷贝。\n对切片或数组重新生成切片时，由于共享的是同一个底层数组，所以数据会互相影响，这点需要注意。\n切片也可以传递指针，但场景很少，还会带来不必要的误解；建议值传值就好，长度和容量占用不了多少内存。\n\n相信使用过切片会发现非常类似于  Java  中的 ArrayList，同样是基于数组实现，也会扩容发生数据拷贝；这样看来语言只是上层使用的选择，一些通用的底层实现大家都差不多。\n这时我们再看标题中的 []*T *[]T *[]*T 就会发现这几个并没有什么联系，只是看起来很像容易唬人。\n","categories":["Go"],"tags":["Go","切片","数组","slice"]},{"title":"几百行代码实现一个脚本解释器","url":"/2022/05/30/gscript/gscript01/","content":"\n前言最近又在重新学习编译原理了，其实两年前也复习过，当初是为了能实现通过 MySQL 的 DDL 生成 Python 中 sqlalchemy 的 model。\n相关文章在这里：手写一个词法分析器\n\n\n虽然完成了相关功能，但现在看来其实实现的比较糙的，而且也只运用到了词法分析；所以这次我的目的是可以通过词法分析-&gt;语法分析-&gt;语义分析 最终能实现一个功能完善的脚本”语言”。\n效果现在也有了一些阶段性的成果，如下图所示：\n目前具备以下基本功能：\n\n变量声明与赋值（只支持 int)\n二次运算（优先级支持）\n语法检查\ndebug 模式，可以打印 AST\n\n感兴趣的朋友可以在这里查看源码：https://github.com/crossoverJie/gscript\n本地有 go 环境的话也可以安装运行。\ngo get github.com/crossoverJie/gscriptgscript -h\n\n或者直接下载二进制文件运行：https://github.com/crossoverJie/gscript/releases\n实现当前版本是使用 go 编写的，确实也如标题所说，核心代码还不到 1k 行代码，当然这也和目前功能简陋有关。\n不过麻雀虽小五脏俱全，从当前版本还是运用到了编译原理中的部分知识：词法、语法分析。\n\n基本实现流程如上图：\n\n通过词法分析器将源码中解析出 token\n再通过对 token  推导生成出抽象语法树（AST）\n如果语法语法出现错误，这一步骤便会抛出编译失败，比如 2*(1+ 少了一个括号。\n\n\n\n因为没有使用类似于 ANTLR 这样工具来辅助生成代码（不然功能也不会只有这么点），所以其中的词法、语法分析都是手写的，代码量并不大，对于想要调试的朋友可以直接查看源码。\n词法分析器：token/token.go:39语法分析器：syntax/syntax.go\n其中会涉及到一些概念，比如有限状态机、递归下降算法等知识点就没在本文讨论了，后续这个项目功能更加完善后也会重头整理。\n规划最后是画饼阶段了，不出意外后续会继续新增如下功能：\n\n更多的基础类型，string&#x2F;long 之类的。\n变量作用域、函数。\n甚至是闭包。\nOOP 肯定也少不了。\n\n这些特性都实现后那也算是一个”现代”的脚本语言了，后续我也会继续更新学习和实现过程中的有趣内容。\n源码地址：https://github.com/crossoverJie/gscript\n","categories":["gscript","compiler"],"tags":["go"]},{"title":"用 Antlr 重构脚本解释器","url":"/2022/08/08/gscript/gscript02-antlr-statement/","content":"\n前言在上一个版本实现的脚本解释器 GScript 中实现了基本的四则运算以及 AST 的生成。\n\n当我准备再新增一个 % 取模的运算符时，会发现工作很繁琐而且几乎都是重复的；主要是两步：\n\n需要在词法解析器中新增对 % 符号的支持。\n在语法解析器遍历 AST 时对 % token 实现具体逻辑。\n\n其中的词法解析和遍历 AST 完全是重复工作，所以我们可否能够简化这两步呢？\n\n\nAntlrAntlr 就是做帮我们解决这些问题的常用工具，利用它我们只需要编写词法文件，然后就可以自动生成词法、语法解析器，并且可以生成不同语言的代码。\n下面以 GScript 的示例来看看 antlr 是如何帮我们生成词法分析器的。\nfunc TestGScriptVisitor_Visit_Lexer(t *testing.T) &#123;\texpression := &quot;(2+3) * 2&quot;\tinput := antlr.NewInputStream(expression)\tlexer := parser.NewGScriptLexer(input)\tfor &#123;\t\tt := lexer.NextToken()\t\tif t.GetTokenType() == antlr.TokenEOF &#123;\t\t\tbreak\t\t&#125;\t\tfmt.Printf(&quot;%s (%q) %d\\n&quot;,\t\t\tlexer.SymbolicNames[t.GetTokenType()], t.GetText(),t.GetColumn())\t&#125;&#125;\n\n//output: (&quot;(&quot;) 0DECIMAL_LITERAL (&quot;2&quot;) 1PLUS (&quot;+&quot;) 2DECIMAL_LITERAL (&quot;3&quot;) 3 (&quot;)&quot;) 4MULT (&quot;*&quot;) 6DECIMAL_LITERAL (&quot;2&quot;) 8\n\nAntlr  会自动将我们的表达式解析为 token，遍历 token 时还能拿到该 token 所在的代码行数、位置等信息，在编译期间做语法检查非常有用。\n要实现这些我们只需要编写词法、语法规则文件即可。\n刚才的示例所对应的词法、语法规则如下：\nexpr    : &#x27;(&#x27; expr &#x27;)&#x27;                        #NestedExpr    | liter=literal #Liter    | lhs=expr bop=( MULT | DIV ) rhs=expr #MultDivExpr    | lhs=expr bop=MOD rhs=expr            #ModExpr    | lhs=expr bop=( PLUS | SUB ) rhs=expr #PlusSubExpr    | expr bop=(LE | GE | GT | LT ) expr # GLe    | expr bop=(EQUAL | NOTEQUAL) expr # EqualOrNot    ;DECIMAL_LITERAL:    (&#x27;0&#x27; | [1-9] (Digits? | &#x27;_&#x27;+ Digits)) [lL]?;    \n\n\n完整规则：https://github.com/crossoverJie/gscript/blob/main/GScript.g4\n\n运行：\nantlr -Dlanguage=Go -o parser -visitor -no-listener GScript.g4\n\n就可以帮我们生成 Go 的代码（默认是 Java），关于 Antlr 的词法、文法规则以及安装步骤请参考官网。\n而我们要实现具体的语法逻辑时只需要实现相关的接口，Antlr 会自动遍历 AST（当然也可以手动控制），同时在访问不同的 AST 节点时会回调我们自己实现的接口，这样我们就能编写自己的语法规则了。\n以这里的新增的取模运算为例：\nfunc (v *GScriptVisitor) VisitModExpr(ctx *parser.ModExprContext) interface&#123;&#125; &#123;\tlhs := v.Visit(ctx.GetLhs())\trhs := v.Visit(ctx.GetRhs())\treturn lhs.(int) % rhs.(int)&#125;\n\n当 Antlr 回调 VisitModExpr 方法时，便能获取到 % 符号左右两侧的数据，这时只需要做相关运算即可。\n基于这个模式这次新增了一个 statement，具体语法如下：\nfunc TestGScriptVisitor_VisitIfElse8(t *testing.T) &#123;\texpression := `if(3!=(1+2))&#123;\treturn 1+3&#125; else &#123;\treturn false&#125;`\tinput := antlr.NewInputStream(expression)\tlexer := parser.NewGScriptLexer(input)\tstream := antlr.NewCommonTokenStream(lexer, 0)\tparser := parser.NewGScriptParser(stream)\tparser.BuildParseTrees = true\ttree := parser.Prog()\tvisitor := GScriptVisitor&#123;&#125;\tvar result = visitor.Visit(tree)\tfmt.Println(expression, &quot; result:&quot;, result)\tassert.Equal(t, result, false)&#125;\n\nAntlr 还有其他各种优势，比如可以解决：\n\n左递归。\n二义性。\n优先级。\n\n等问题。\n这里也推荐在 IDE 中安装 Antlr 的插件，这样就可以直观的查看  AST 语法树，可以帮我们更好的调试代码。\n\n升级 xjson借助 GScript 提供的 statement，xjson 也提供了有些有意思的写法：\n因为 xjson 的四则运算语法没有使用 Antlr 生成，所以为了能支持 GScript 提供的 statement 需要手写许多词法代码。\n\n这也体现了 Antlr 这类前端工具的重要性，效率提升是非常明显的。\n总结借助于 Antlr 后续 GScript 会继续支持函数调用、更完善的类型系统、面向对象等特性；感兴趣的朋友请持续关注。\n源码地址：https://github.com/crossoverJie/gscripthttps://github.com/crossoverJie/xjson\n","categories":["gscript","compiler"],"tags":["go","antlr"]},{"title":"自己动手写脚本解释器--实现作用域与函数调用","url":"/2022/08/17/gscript/gscript03-scope-func/","content":"\n前言上次利用 Antlr 重构一版 用 Antlr 重构脚本解释器 之后便着手新增其他功能，也就是现在看到的支持了作用域以及函数调用。\nint b= 10;int foo(int age)&#123;\tfor(int i=0;i&lt;10;i++)&#123;\t\tage++;\t&#125;\treturn b+age;&#125;int add(int a,int b) &#123;\tint e = foo(10);\te = e+10;\treturn a+b+3+e;&#125;add(2,20);// Output:65\n\n\n\n整个语法规则大部分参考了 Java，现阶段支持了：\n\n函数声明与调用。\n函数调用的入栈和出栈，保证了函数局部变量在函数退出时销毁。\n作用域支持，内部作用域可以访问外部作用域的变量。\n基本的表达式语句，如 i++, !=,==\n\n这次实现的重点与难点则是作用域与函数调用，实现之后也算是满足了我的好奇心，不过在讲作用域与函数调用之前先来看看一个简单的变量声明与访问语句是如何实现的，这样后续的理解会更加容易。\n变量声明int a=10;a;\n\n由于还没有实现内置函数，比如控制台输出函数 print()，所以这里就直接访问变量也能拿到数据\n\n运行后结果如下：\n首先看看变量声明语句的语法：\nvariableDeclarators    : typeType variableDeclarator (&#x27;,&#x27; variableDeclarator)*    ;variableDeclarator    : variableDeclaratorId (&#x27;=&#x27; variableInitializer)?    ;typeList    : typeType (&#x27;,&#x27; typeType)*    ;typeType    : (functionType | primitiveType) (&#x27;[&#x27; &#x27;]&#x27;)*    ;primitiveType    : INT    | STRING    | FLOAT    | BOOLEAN    ;        \n 只看语法不太直观，直接看下生成的 AST 树就明白了：  编译期 左边这棵 BlockVardeclar 树对应的就是  int a=10;，右边的 blockStm 对应的就是变量访问 a。\n 整个程序的运行过程分为编译期和运行期，对应的流程：\n\n遍历 AST 树，做语义分析，生成对应的符号表、类型表、引用消解、还有一些语法校验，比如变量名、函数名是否重复、是否能访问私有变量等。\n运行期：从编译期中生成的符号表、类型表中获取数据，执行具体的代码逻辑。\n\n访问 AST对于刚才提到的编译期和运行期其实分别对应两种访问 AST 的方式，这也是 Antlr 所提供两种方式。\nListener 模式第一种是 Listener 模式，就这名字也能猜到是如何运行的；我们需要实现 Antlr 所提供的接口，这些接口分别对应 AST 树中的不同节点。\n接着 Antlr 会自动遍历这棵树，当访问和退出某个节点时变会回调我们自定义的方法，这些接口都是没有返回值的，所以我们需要将遍历过程中的数据自行存放起来。\n这点非常适合上文提到的编译期，遍历过程中产生的数据自然就会存放到符号表、类型表这些容器中。\n以这段代码为例，我们实现了程序根节点、for循环节点的进入和退出 Listener，当 Antlr 运行到这些节点时便会执行其中的逻辑。\nhttps://github.com/crossoverJie/gscript/blob/main/resolver/type_scope_resolver.go\nVisitor 模式Visitor 模式正好和 Listener 相反，这是由我们自行控制需要访问哪个 AST 节点，同时需要在每次访问之后返回数据，这点非常适合来做程序运行期。\n配合在编译期中存放的数据，便可以实现各种特性了。\n\n以上图为例，在访问 Prog 节点时便可以从编译期中拿到当前节点所对应的作用域 scope，同时我们可以自行控制访问下一个节点 VisitBlockStms，访问其他节点当然也是可以的，不过通常我们还是按照语法中定义的结构进行访问。\n作用域即便是同一个语法生成的 AST 是相同的，但我们在遍历 AST 时实现不同也就会导致不同的语义，这就是各个语言语义分析的不同之处。\n\n比如 Java 不允许在子作用域中声明和父作用域中相同的变量，但 JavaScript 却是可以的。\n\n有了上面的基础下面我们来看看作用域是如何实现的。\nint a=10;a;\n\n还是以这段代码为例：\n\n这里我简单画了下流程：\n在编译期间会会为当前节点写入一个 scope，以及在 scope 中写入变量 “a”。\n\n这里的写入 scope 和写入变量是分为两次 Listener 进行的，具体代码实现在下面查看源码。\n\n第一次：https://github.com/crossoverJie/gscript/blob/main/resolver/type_scope_resolver.go#L21\n第二次：https://github.com/crossoverJie/gscript/blob/main/resolver/type_resolver.go#L59\n接着是运行期，从编译期中生成的数据拿到 scope 以及其中的变量，获取变量时有一个细节：当前 scope 中如果获取不到需要尝试从父级 scope 中获取，比如如下情况：\nint b= 10;int foo()&#123;\treturn b;&#125;\n这里的 b 在当前函数作用域中是获取不到的，只能在父级 scope 中获取。\n\n父级 scope 的关系是在创建 scope 的时候维护进去的，默认当前 scope 就是写入时 scope 的父级。\n\n关键代码试下如下图：\n第四步获取变量的值也是需要访问到 AST 中的字面量节点获取值即可，核心代码如下：\n函数函数的调用最核心的就是在运行时需要把当前函数中的所有数据入栈，访问完毕后出栈，这样才能实现函数退出后自动释放函数体类的数据。\n核心代码如下：\nint b= 10;int foo()&#123;\treturn b;&#125;int func(int a,int b) &#123;\tint e = foo();\treturn a+b+3+e;&#125;func(2,20);\n\n即便是有上面这类函数类调其他函数情况也不必担心，无非就是在执行函数体的时候再往栈中写入数据而已，函数退出后会依次退出栈帧。\n\n有点类似于匹配括号的算法 &#123;[()]&#125;，本质上就是递归调用。\n总结限于篇幅其中的许多细节没有仔细讨论，感兴趣的朋友可以直接跑跑单测，debug 试试。\nhttps://github.com/crossoverJie/gscript/blob/main/compiler_test.go\n目前的版本还比较初级，比如基本类型还只有 int，也没有一些常用的内置函数。\n后续会逐步完善，比如新增：\n\n函数多返回值。\n自定义类型\n闭包\n\n等特性，这个坑会一直填下去，希望在年底可以用 gscript 写一个 web 服务端那就算是里程碑完成了。\n现阶段也实现了一个简易的 REPL 工具，大家可以安装试用：\n源码地址：https://github.com/crossoverJie/gscript\n","categories":["gscript","compiler"],"tags":["go","antlr"]},{"title":"终于实现了一门属于自己的编程语言","url":"/2022/09/07/gscript/gscript04-preview/","content":"\n前言都说程序员的三大浪漫是：操作系统、编译原理、图形学；最后的图形学确实是特定的专业领域，我们几乎接触不到，所以对我来说换成网络更合适一些，最后再加上一个数据库。\n这四项技术如果都能掌握的话那岂不是在 IT 行业横着走了，加上这几年互联网行业越来越不景气，越底层的技术就越不可能被替代；所以为了给自己的 30+ 危机留点出路，从今年上半年开始我就逐渐开始从头学习编译原理。\n功夫不负有心人，经过近一个月的挑灯夜战，每晚都在老婆的催促下才休息，克服了中途好几次想放弃的冲动，终于现在完成了 GScript 一个预览版。\n\n预览版的意思是语法结构与整体设计基本完成，后续更新也不太会改动这部分内容、但还缺少一些易用功能。\n\n\n特性首先来看看保留环节， GScript 是如何编写 hello world 的。\nhello_world.gs:\nprintln(&quot;hello world&quot;);\n\n❯ gscript hello_world.gshello world\n\n废话说完了接下来重点聊聊 GScript 所支持的特性了。\n后文会重点说明每一个特性。\n例子除了刚才提到的 hello world，再来看一个也是示例代码经常演示的打印斐波那契数列。\nfunc int() fib()&#123;    int a = 0;    int b = 1;    int fibonacci()&#123;        int c = a;        a = b;        b = a+c;        return c;    &#125;    return fibonacci;&#125;func int() f = fib();for (int i = 0; i &lt; 5; i++)&#123;    println(f());&#125;\n\n输出结果如下：\n01123\n\n整体写法与 Go 官方推荐的类似：https://go.dev/play/p/NeGuDahW2yP\n// fib returns a function that returns// successive Fibonacci numbers.func fib() func() int &#123;\ta, b := 0, 1\treturn func() int &#123;\t\ta, b = b, a+b\t\treturn a\t&#125;&#125;func main() &#123;\tf := fib()\t// Function calls are evaluated left-to-right.\tfmt.Println(f(), f(), f(), f(), f())&#125;\n\n都是通过闭包变量实现的，同时也展示了 GScript 对闭包、函数的使用，后文详细介绍闭包的用法。\n语法GScript 的语法与常见的 Java/Go 类似，所以上手非常简单。\n基本类型先来看看基本类型，目前支持 int/string/float/bool 四种基本类型以及 nil 特殊类型。\n变量声明语法和 Java 类似：\nint a=10;string b,c;float e = 10.1;bool f = false;\n\n个人觉得将类型放在前面，代码阅读起来会更清晰一些，当然这也是个人喜好。\n数组// 声明并初始化int[] a=&#123;1,2,3&#125;;println(a);// 声明一个空数组并指定大小int[] table = [4]&#123;&#125;;println();// 向数组 append 数据a = append(a,4);println(a);for(int i=0;i&lt;len(a);i++)&#123;\tprintln(a[i]);&#125;// 通过下标获取数组数据int b=a[2];println(b);\n\n其实严格来讲这并不算是数组，因为它的底层是用 Go 切片实现的，所以可以动态扩容。\n以这段代码为例：\nint[] a=[2]&#123;&#125;;println(&quot;数组大小:&quot;+len(a));a = append(a,1);println(&quot;数组大小:&quot;+len(a));println(a);a[0]=100;println(a);\n输出：\n数组大小:2数组大小:3[&lt;nil&gt; &lt;nil&gt; 1][100 &lt;nil&gt; 1]\n\nClass类的支持非常重要，是实现面向对象的基础，目前还未完全实现面向对象，只实现了数据与函数的封装。\nclass ListNode&#123;    int value;    ListNode next;    ListNode(int v, ListNode n)&#123;        value =v;        next = n;    &#125;&#125;// 调用构造函数时不需要使用 new 关键字。ListNode l1 = ListNode(1, nil);// 使用 . 调用对象属性或函数。println(l1.value);\n\n缺省情况下 class 具有无参构造函数：\nclass Person&#123;\tint age=10;\tstring name=&quot;abc&quot;;\tint getAge()&#123;\t\treturn 100+age;\t&#125;&#125;// 无参构造函数Person xx= Person();println(xx.age);assertEqual(xx.age, 10);println(xx.getAge());assertEqual(xx.getAge(), 110);\n\n得益于 class 的实现，结合刚才的数组也可以定义出自定义类型的数组：\n// 大小为 16 的 Person 数组Person[] personList = [16]&#123;&#125;;\n\n函数函数其实分为两类：\n\n普通的全局函数。\n类的函数。\n\n本质上没有任何区别，只是所属范围不同而已。\n// 判断链表是否有环bool hasCycle(ListNode head)&#123;    if (head == nil)&#123;        return false;    &#125;    if (head.next == nil)&#123;        return false;    &#125;    ListNode fast = head.next;    ListNode slow = head;    bool ret = false;    for (fast.next != nil)&#123;        if (fast.next == nil)&#123;            return false;        &#125;        if (fast.next.next == nil)&#123;            return false;        &#125;        if (slow.next == nil)&#123;            return false;        &#125;        if (fast == slow)&#123;            ret = true;            return true;        &#125;        fast = fast.next.next;        slow = slow.next;    &#125;    return ret;&#125;ListNode l1 = ListNode(1, nil);bool b1 =hasCycle(l1);println(b1);assertEqual(b1, false);ListNode l4 = ListNode(4, nil);ListNode l3 = ListNode(3, l4);ListNode l2 = ListNode(2, l3);bool b2 = hasCycle(l2);println(b2);assertEqual(b2, false);l4.next = l2;bool b3 = hasCycle(l2);println(b3);assertEqual(b3, true);\n\n这里演示了链表是否有环的一个函数，只要有其他语言的使用基础，相信阅读起来没有任何问题。\nadd(int a)&#123;&#125;\n\n\n当函数没有返回值时，可以声明为 void 或直接忽略返回类型。\n\n闭包闭包我认为是非常有意思的一个特性，可以实现很灵活的设计，也是函数式编程的基础。\n所以在 GScript 中函数是作为一等公民存在；因此 GScript 也支持函数类型的变量。\n函数变量声明语法如下：func typeTypeOrVoid &#39;(&#39; typeList? &#39;)&#39;\n// 外部变量，全局共享。int varExternal =10;func int(int) f1()&#123;\t// 闭包变量对每个闭包单独可见\tint varInner = 20;\tint innerFun(int a)&#123;\t\tprintln(a);\t\tint c=100;\t\tvarExternal++;\t\tvarInner++;\t\treturn varInner;\t&#125;\t// 返回函数\treturn innerFun;&#125;// f2 作为一个函数类型，接收的是一个返回值和参数都是 int 的函数。func int(int) f2 = f1();for(int i=0;i&lt;2;i++)&#123;\tprintln(&quot;varInner=&quot; + f2(i) + &quot;, varExternal=&quot; + varExternal);&#125;println(&quot;=======&quot;);func int(int) f3 = f1();for(int i=0;i&lt;2;i++)&#123;\tprintln(&quot;varInner=&quot; + f3(i) + &quot;, varExternal=&quot; + varExternal);&#125;\n\n最终输出如下：\n0varInner=21, varExternal=111varInner=22, varExternal=12=======0varInner=21, varExternal=131varInner=22, varExternal=14\n\nfunc int(int) f2 = f1();\n\n以这段代码为例：f2 是一个返回值，入参都为 int 的函数类型；所以后续可以直接当做函数调用 f2(i).\n例子中将闭包分别赋值给 f2 和 f3 变量，这两个变量中的闭包数据也是互相隔离、互不影响的，所有基于这个特性甚至还是实现面向对象。\n\n关于闭包的实现，后续会单独更新一篇。\n\n更多样例请参考：https://github.com/crossoverJie/gscript/tree/main/example\n标准库标准库源码：https://github.com/crossoverJie/gscript/tree/main/internal\n目前实现的标准库并不多，这完全是一个体力活；基于现有的语法和基础数据类型，几乎可以实现大部分的数据结构了，所以感兴趣的朋友也欢迎来贡献标准库代码；比如 Stack、Set 之类的数据结构。\nMapString以这个 MapString 为例：键值对都为 string 的 HashMap。\nint count =100;MapString m1 = MapString();for (int i=0;i&lt;count;i++)&#123;\tstring key = i+&quot;&quot;;\tstring value = key;\tm1.put(key,value);&#125;println(m1.getSize());assertEqual(m1.getSize(),count);for (int i=0;i&lt;count;i++)&#123;\tstring key = i+&quot;&quot;;\tstring value = m1.get(key);\tprintln(&quot;key=&quot;+key+ &quot;:&quot;+ value);\tassertEqual(key,value);&#125;\n\n使用起来和 Java 的 HashMap 类似，当然他的实现源码也是参考的 jdk1.7 的 HashMap。\n\n由于目前并有一个类似于 Java 的 object 或者是 go 中的 interface&#123;&#125;, 所以如果需要存放 int，那还得实现一个 MapInt，不过这个通用类型很快会实现。\n\n内置函数int[] a=&#123;1,2,3&#125;;// len 返回数组大小println(len(a));// 向数组追加数据a = append(a,4);println(a);// output: [1,2,3,4]// 断言函数，不相等时会抛出运行时异常，并中断程序。assertEqual(len(a),4);// 返回 hashcodeint hashcode = hash(key);\n也内置了一些基本函数，当然也这不是由 GScript 源码实现的，而是编译器实现的，所以新增起来要稍微麻烦一些；后续会逐步完善，比如和 IO 相关的内置函数。\n总结现阶段的 GScript 还有许多功能没有完善，比如 JSON、网络库、更完善的语法检查、编译报错信息等；现在拿来刷刷 LeetCode 还是没有问题的。\n\n从这 65 个 todo 就能看出还有很长的路要走，我对它的终极目标就是可以编写一个网站那就算是一个成熟的语言了。\n目前还有一个问题是没有集成开发环境，现在的开发体验和白板上写代码相差无异，所以后续有时间的话尝试写一个 VS Code 的插件，至少能有语法高亮与提示。\n最后对 GScript 或者是编译原理感兴趣的小伙伴可以加我微信一起交流。\n项目源码：https://github.com/crossoverJie/gscript下载地址：https://github.com/crossoverJie/gscript/releases/tag/v0.0.6\n","categories":["gscript","compiler"],"tags":["go","antlr"]},{"title":"用自己的编程语言实现了一个网站","url":"/2022/09/14/gscript/gscript05-write-site/","content":"\n前言在上一篇《终于实现了一门属于自己的编程语言》 介绍了自己写的编程语言 GScript ，在文中提到希望最终可以使用 GScript 开发一个网站。\n到目前为止确实是做到了，首页地址：\nhttps://gscript.crossoverjie.top/index\n\n要称为一个网站确实有点勉强，不过也是一个动态网页，因为返回的是 HTML，所以在当前阶段只要不嫌麻烦其实也能写一个“合格”的网站，有点像以前我们学习 Java 时的 servlet。\n\n\n该页面的源码地址在这里：https://github.com/crossoverjie/gscript-homepage\n其实总共也就40来行代码：\nclass GScript&#123;    string author;    string[] features;    string since;    GScript(string a, string[] f, string s)&#123;        author = a;        features = f;        since = s;    &#125;&#125;func (HttpContext) index(HttpContext ctx)&#123;    string[] features = &#123;&quot;statically&quot;, &quot;strongly&quot;&#125;;    GScript gs = GScript(&quot;crossoverJie&quot;,features, &quot;2022&quot;);    string j = JSON(gs);    println(j);    string local = getCurrentTime(&quot;Asia/Shanghai&quot;,&quot;2006-01-02 15:04:05&quot;);    println(&quot;local=&quot; + local);    string html = ^        &lt;html&gt;            &lt;title&gt;GScript&lt;/title&gt;            &lt;pre&gt;                 _     _    ___ ___ ___ ___|_|___| |_ | . |_ -|  _|  _| | . |  _||_  |___|___|_| |_|  _|_|  |___|             |_|   v0.0.7   ^+ j +^            &lt;/pre&gt;            &lt;h1&gt;current ^+ local +^&lt;/h1&gt;            &lt;p&gt;&lt;a href=&quot;https://github.com/crossoverjie/gscript-homepage&quot;&gt;GScript-homepace source code&lt;/a&gt;&lt;/p&gt;        &lt;/html&gt;    ^;    ctx.HTML(200, html);&#125;httpHandle(&quot;GET&quot;, &quot;/index&quot;, index);string[] args = getOSArgs();if (len(args) ==3)&#123;    httpRun(&quot;:&quot; + args[2]);&#125;else &#123;    httpRun(&quot;:8000&quot;);&#125;\n\n全是利用 GScript 所提供的标准库实现的，后文会详细聊聊内置 HTTP 包。\n更新内容下面重点来看看 v0.0.8 这个版本相较于上一个更新了哪些地方。\n因为我是把自己当做一个开发者的角度去实现了一个 http 服务，同时还用 GScript 刷了两道简单的 LeetCode；为了让这个过程更流畅，更符合一个现代语言的使用方式，所以本次真的更新不少东西。\n\n刷题源码：https://github.com/crossoverJie/gscript/tree/main/example/leetcode\n\n大概如下：\n\nany 类型的支持，简化标准库的实现。\n可以用 ^^ 来声明多行字符串，方便声明复杂字符串。\n更完善的类型推导，修复了上个版本中某些情况推导不出类型的bug。\n支持运算符重载。\n基本的 http 包，可以开发出 http 服务，目前能响应 JSON 以及 HTML。\n新增内置函数：根据时区获取当前时间、获取应用启动参数等。\nJSON 的序列表以及查询，语法级适配了 XJSON。\n修复了在多个 block 嵌套情况下不能正确 return 的 bug。\n\n其实从这些更新中也能看出，上个版本只是一个简单能用的状态，而现在这个版本已经可以拿来写复杂逻辑了，当然目前还缺乏一些更友好的编译提示以及运行时错误。\n下面仔细聊聊一些更新内容。\nany 类型首先是 any 通用类型，这个类似于 Java 中的 Object 和 Go 中的 interface&#123;&#125;,极大的方便了我们编写一些标准库。\n\n以之前内置的 hash 和 len 函数为例，需要对每种类型都实现一遍，非常麻烦而且毫无必要；现在只需要定义一次即可，代码量直接省几倍。\n同理，之前实现的 Map 只支持存放 string 类型，现在便能存放任何类型的数据。\n\n对 any 的实现过程感兴趣的朋友，今后可以单独分享一下。\n\n运算符重载写 go 或者是 Java 的朋友应该知道，这两门语言都无法对两个对象进行运算，编译器会直接报错。\n但在一些特殊场景下还是蛮好用的，于是我参考了 C# 的语法在 GScript 中也实现了。\nclass Person&#123;\tint age;\tPerson(int a)&#123;\t\tage = a;\t&#125;&#125;Person operator + (Person p1, Person p2)&#123;\tPerson pp = Person(p1.age+p2.age);\treturn pp;&#125;Person operator - (Person p1, Person p2)&#123;\tPerson pp = Person(p1.age-p2.age);\treturn pp;&#125;Person p1 = Person(10);Person p2 = Person(20);Person p3 = p1+p2;println(&quot;p3.age=&quot;+p3.age);assertEqual(p3.age, 30);\n\n声明的函数名称必须为 operator，之后跟上运算符便实现了重载。\n支持的运算符有：+-*/ &lt; &gt;= &lt;= &gt; ==。\nJSON支持当前版本中支持将对象、基本类型进行序列化，暂不支持反序列化为对象，但可以根据 JSON 字符串通过一定的语法查询数据。\n内置了两个 JSON 相关函数：\n// return JSON stringstring JSON(any a)&#123;&#125;// JSON query with pathany JSONGet(string json, string path)&#123;&#125;\n\nclass Person&#123;\tint age;\tstring name;\tfloat weight;\tbool man;\tPerson(string n, int a, float w, bool m)&#123;\t\tname = n;\t\tage = a;\t\tweight = w;\t\tman =m;\t&#125;&#125;Person p1 = Person(&quot;abc&quot;,10,99.99,true);Person p2 = Person(&quot;a&quot;,11,999.99,false);string json = JSON(p1);println(json);// output:&#123;&quot;age&quot;:10,&quot;man&quot;:true,&quot;name&quot;:&quot;abc&quot;,&quot;weight&quot;:99.99&#125;\n以这段代码为例，调用 JSON 函数可以将对象序列化为 JSON 字符串。\n\nclass Person&#123;\tint age;\tstring name;\tfloat weight;\tbool man;\tPerson(string n, int a, float w, bool m)&#123;\t\tname = n;\t\tage = a;\t\tweight = w;\t\tman =m;\t&#125;&#125;Person p1 = Person(&quot;abc&quot;,10,99.99,true);string json = JSON(p1);println(json);int age = JSONGet(json, &quot;age&quot;);println(age);assertEqual(age,10);\n\n使用 JSONGet 函数可以在一个 JSON 字符串中查询任意的数据，这个功能是通过适配 XJSON 实现的，所以 XJSON 支持的查询语法都能实现。\nstring j=^&#123;&quot;age&quot;:10, &quot;abc&quot;:&#123;&quot;def&quot;:&quot;def&quot;&#125;,&quot;list&quot;:[1,2,3]&#125;^;String def = JSONGet(j, &quot;abc.def&quot;);println(def);assertEqual(def,&quot;def&quot;);int l1 = JSONGet(j, &quot;list[0]&quot;);println(l1);assertEqual(l1,1);string str=^&#123;    &quot;name&quot;: &quot;bob&quot;,    &quot;age&quot;: 20,    &quot;skill&quot;: &#123;        &quot;lang&quot;: [            &#123;                &quot;go&quot;: &#123;                    &quot;feature&quot;: [                        &quot;goroutine&quot;,                        &quot;channel&quot;,                        &quot;simple&quot;,                        true                    ]                &#125;            &#125;        ]    &#125;&#125;^;String g = JSONGet(str, &quot;skill.lang[0].go.feature[0]&quot;);println(g);assertEqual(g,&quot;goroutine&quot;);\n\n比如这样复杂的嵌套 JSON，也能通过查询语法获取数据。\nHTTP 包HTTP 包是本次升级的重点，标准库中提供了以下函数和类：\n// http lib// Response jsonFprintfJSON(int code, string path, string json)&#123;&#125;// Resonse htmlFprintfHTML(int code, string path, string html)&#123;&#125;// path (relative paths may omit leading slash)string QueryPath(string path)&#123;&#125;string FormValue(string path, string key)&#123;&#125;class HttpContext&#123;    string path;    JSON(int code, any v)&#123;        string json = JSON(v);        FprintfJSON(code, path, json);    &#125;    HTML(int code, any v) &#123;        string html = v;        FprintfHTML(code, path, html);    &#125;    string queryPath() &#123;        string p = QueryPath(path);        return p;    &#125;    string formValue(string key)&#123;        string v = FormValue(path, key);        return v;    &#125;&#125;// Bind routehttpHandle(string method, string path, func (HttpContext) handle)&#123;    // println(&quot;path=&quot;+path);    HttpContext ctx = HttpContext();    handle(ctx);&#125;// Run http server.httpRun(string addr)&#123;&#125;\n\n具体的使用流程：\n\n通过定义一个函数变量实现自己的业务逻辑。\n注册路由。\n启动 HTTP 服务。\n\n在自己的 handle 中可以通过 HttpContext 对象拿到请求上下文，可以获取请求参数以及响应数据。具体使用示例可以参考这份代码。\n总结本次更新比我预期的要顺利一些，因为语法树和编译器已经基本实现完毕，不会怎么改了，现在新增的特性无非就是运行时实现一些语法糖，大部分都是体力劳动；可能是新鲜感带来的兴奋剂效果，大部分时间都是痛并快乐着。\n比如这两天主要就是在修复多层 block 嵌套时遇到 return 语句无法正确返回的 bug，死活折腾了两夜；终于在无数次分析 AST 找到了解决方案，现在想想确实还是相关经验太少。\n\n对这个 Bug 感兴趣的朋友可以点个赞，后面可以分享一下。\n\n下一阶段重点就是将编译信息好好整理，让开发体验更好。之后抽空再把 SQL 标准库实现了，这样就能愉快的 CURD了。\n最后希望对该项目或者是编译原理感兴趣的朋友可以下载使用，提出宝贵意见，欢迎加我微信交流。\nv0.0.8 下载地址：https://github.com/crossoverJie/gscript/releases/tag/v0.0.8\n","categories":["gscript","compiler"],"tags":["go","antlr"]},{"title":"手写编程语言-递归函数是如何实现的？","url":"/2022/09/27/gscript/gscript07-return/","content":"\n前言本篇文章主要是记录一下在 GScript 中实现递归调用时所遇到的坑，类似的问题在中文互联网上我几乎没有找到相关的内容，所以还是很有必要记录一下。\n在开始之前还是简单介绍下本次更新的 GScript v0.0.9 所包含的内容：\n\n支持可变参数\n优化 append 函数语义\n优化编译错误信息\n最后一个就是支持递归调用\n\n\n\n\n先看第一个可变参数：\n//formats according to a format specifier and writes to standard output.printf(string format, any ...a)&#123;&#125;//formats according to a format specifier and returns the resulting string.string sprintf(string format, any ...a)&#123;&#125;\n\n以上是随着本次更新新增的两个标准函数，均支持可变参数，其中使用 ... 表示可变参数，调用时如下：\nprintf(&quot;hello %s &quot;,&quot;123&quot;);printf(&quot;hello-%s-%s &quot;,&quot;123&quot;,&quot;abc&quot;);printf(&quot;hello-%s-%d &quot;,&quot;123&quot;,123);string format = &quot;this is %s &quot;;printf(format, &quot;gscript&quot;);string s = sprintf(&quot;nice to meet %s&quot;, &quot;you&quot;);assertEqual(s,&quot;nice to meet you&quot;);\n\n与大部分语言类似，可变参数本质上就是一个数组，所以可以拿来循环遍历：\nint add(string s, int ...num)&#123;\tprintln(s);\tint sum = 0;\tfor(int i=0;i&lt;len(num);i++)&#123;\t\tint v = num[i];\t\tsum = sum+v;\t&#125;\treturn sum;&#125;int x = add(&quot;abc&quot;, 1,2,3,4);println(x);assertEqual(x, 10);\n\n\n// appends &quot;v&quot; to the end of a array &quot;a&quot;append(any[] a, any v)&#123;&#125;\n\n之后是优化了内置函数 append() 的语义，本次优化来自于 issue12 的建议：https://github.com/crossoverJie/gscript/issues/12\n// Beforeint[] a=&#123;1,2,3&#125;;println(a);println();a = append(a,4);println(a);// Output: [1 2 3 4]// Nowint[] a=&#123;1,2,3&#125;;println(a);println();append(a,4);int b = a[3];assertEqual(4, b);println(a);// Output: [1 2 3 4]\n\n现在 append 之后不需要再重新赋值，也会追加数据，优化后这里看起来是一个值&#x2F;引用传递的问题，但其实底层也是值传递，只是在语法上增加了这样的语法糖，帮使用者重新做了一次赋值。\n\n之后是新增了编译错误信息提示，比如下面这段代码：\na+2;b+c;\n使用没有声明的变量，现在会直接编译失败：\n1:0: undefined: a2:0: undefined: b2:2: undefined: c\n\nclass T&#123;&#125;class T&#123;&#125;// output:2:0: class T redeclared in this block\n重复声明之类的语法错误也有相关提示。\n\n最后一个才是本次讨论的重点，也就是递归函数的支持。\nint num(int x,int y)&#123;\tif (y==1 || y ==x) &#123;\t\treturn 1;\t&#125;\tint v1 = num(x - 1, y - 1);\treturn c;&#125;\n\n再上一个版本中 int v1 = num(x - 1, y - 1); 这行代码是不会执行的，具体原因后文会分析。\n现在利用递归便可以实现类似于打印杨辉三角之类的程序了：\nint num(int x,int y)&#123;\tif (y==1 || y ==x) &#123;\t\treturn 1;\t&#125;    int v1 = num(x - 1, y - 1);    int v2 = num(x - 1, y);\tint c = v1+v2;    // int c = num(x - 1, y - 1)+num(x - 1, y);\treturn c;&#125;printTriangle(int row)&#123;\tfor (int i = 1; i &lt;= row; i++) &#123;        for (int j = 1; j &lt;= row - i; j++) &#123;           print(&quot; &quot;);        &#125;        for (int j = 1; j &lt;= i; j++) &#123;            print(num(i, j) + &quot; &quot;);        &#125;        println(&quot;&quot;);    &#125;&#125;printTriangle(7);// output:      1      1 1     1 2 1    1 3 3 1   1 4 6 4 1  1 5 10 10 5 1 1 6 15 20 15 6 1 \n\n函数中的 returnint num(int x,int y)&#123;\tif (y==1 || y ==x) &#123;\t\treturn 1;\t&#125;\tint v1 = num(x - 1, y - 1);\treturn c;&#125;\n\n现在我们来看看这样的代码为什么执行完 return 1 之后就不会执行后边的语句了。\n其实在此之前我首先解决的时候函数 return 后不能执行后续 statement 的需求，其实正好就是上文提到的逻辑，只是这里是递归而已。\n先把代码简化一下方便分析：\nint f1(int a)&#123;\tif (a==10)&#123;\t\treturn 10;\t&#125;\tprintln(&quot;abc&quot;);&#125;\n\n当参数 a 等于 10 的时候确实不能执行后续的打印语句了，那么如何实现该需求呢？\n以正常人类的思考方式：当我们执行完 return 语句的时候，就应该标记该语句所属的函数直接返回，不能在执行后续的 statement。\n可是这应该如何实操呢？\n其实看看 AST 就能明白了：\n\n当碰到 return 语句的时，会递归向上遍历语法树，标记上所有 block 节点表明这个 block 后续的语句不再执行了，同时还得把返回值记录下来。\n这样当执行到下一个 statement 时，也就是 \tprintln(&quot;abc&quot;); 则会判断他所属的 block 是否有被标记，如果有则直接返回，这样便实现了 return 语句不执行后续代码。\n部分实现代码如下：\n// 在 return 的时候递归向上扫描所有的 Block，并打上标记，用于后面执行 return 的时候直接返回。func (v *Visitor) scanBlockStatementCtx(tree antlr.ParseTree, value interface&#123;&#125;) &#123;\tcontext, ok := tree.(*parser.BlockContext)\tif ok &#123;\t\tif v.blockCtx2Mark == nil &#123;\t\t\tv.blockCtx2Mark = make(map[*parser.BlockContext]interface&#123;&#125;)\t\t&#125;\t\tv.blockCtx2Mark[context] = value\t&#125;\tif tree.GetParent() != nil &#123;\t\tv.scanBlockStatementCtx(tree.GetParent().(antlr.ParseTree), value)\t&#125;&#125;\n\n\n源码地址：https://github.com/crossoverJie/gscript/blob/793d196244416574bd6be641534742e57c54db7a/visitor.go#L182\n递归的问题但同时问题也来了，就是递归的时候也不会执行后续的递归代码了。\n其实解决问题的方法也很简单，就是在判断是否需要直接返回那里新增一个条件，这个 block 中不存在递归调用。\n所以我们就得先知道这个 block 中是否存在递归调用。\n整个过程有以下几步：\n\n编译期：在函数声明处记录下函数与当前 context 的映射关系。\n编译期：扫描 statement 时，取出该 statement 的 context 所对应的函数。\n编译期：扫描到的 statement 如果是一个函数调用，则判断该函数是否为该 block 中的函数，也就是第二步取出的函数。\n编译期：如果两个函数相等，则将当前 block 标记为递归调用。\n运行期：在刚才判断 return 语句处，额外多出判断当前 block 是否为递归调用，如果是则不能返回。\n\n部分代码如下：\nhttps://github.com/crossoverJie/gscript/blob/3e179f27cb30ca5c3af57b3fbf2e46075baa266b/resolver/ref_resolver.go#L70\n总结这里的递归调用其实卡了我挺长时间的，思路是有的，但是写出来的代码总是和预期不符，当天晚上坐在电脑面前到凌晨两三点，百思不得其解。\n最后受不了上床休息的时候，突然一个灵光乍现让我想到了解决方案，于是第二天起了个早床赶忙实践，还真给解决了。\n所以有些时候碰到棘手问题时给自己放松一下，往往会有出其不意的效果。\n最后是目前的递归在某些情况下性能还有些问题，后续会尽量将这些标记过程都放在编译期，编译慢点没事，但运行时慢那就有问题了。\n之后还会继续优化运行时的异常，目前是直接 panic，堆栈也没有，体感非常不好；欢迎感兴趣的朋友试用反馈bug。\n源码地址：\nhttps://github.com/crossoverJie/gscript\n","categories":["gscript","compiler"],"tags":["递归","go","antlr"]},{"title":"手写编程语言-实现运算符重载","url":"/2022/09/18/gscript/gscript06-operator-overloading/","content":"\n前言先带来日常的 GScript 更新：新增了可变参数的特性，语法如下：\nint add(string s, int ...num)&#123;\tprintln(s);\tint sum = 0;\tfor(int i=0;i&lt;len(num);i++)&#123;\t\tint v = num[i];\t\tsum = sum+v;\t&#125;\treturn sum;&#125;int x = add(&quot;abc&quot;, 1,2,3,4);println(x);assertEqual(x, 10);\n\n\n\n得益于可变参数，所以新增了格式化字符串的内置函数：\n//formats according to a format specifier and writes to standard output.printf(string format, any ...a)&#123;&#125;//formats according to a format specifier and returns the resulting string.string sprintf(string format, any ...a)&#123;&#125;\n\n下面重点看看 GScript 所支持的运算符重载是如何实现的。\n使用运算符重载其实也是多态的一种表现形式，我们可以重写运算符的重载函数，从而改变他们的计算规则。\nprintln(100+2*2);\n以这段代码的运算符为例，输出的结果自然是：104.\n但如果我们是对两个对象进行计算呢，举个例子:\nclass Person&#123;\tint age;\tPerson(int a)&#123;\t\tage = a;\t&#125;&#125;Person p1 = Person(10);Person p2 = Person(20);Person p3 = p1+p2;\n\n这样的写法在 Java/Go 中都会报编译错误，这是因为他们两者都不支持运算符重载；\n但 Python/C# 是支持的，相比之下我觉得 C# 的实现方式更符合 GScript 语法，所以参考 C# 实现了以下的语法规则。\nPerson operator + (Person p1, Person p2)&#123;\tPerson pp = Person(p1.age+p2.age);\treturn pp;&#125;Person p3 = p1+p2;println(&quot;p3.age=&quot;+p3.age);assertEqual(p3.age, 30);\n\n有几个硬性条件：\n\n函数名必须是 operator \n名称后跟上运算符即可。\n\n\n目前支持的运算符有：+-*&#x2F;   &#x3D;&#x3D; !&#x3D; &lt; &lt;&#x3D; &gt; &gt;&#x3D;\n\n实现以前在使用 Python 运算符重载时就有想过它是如何实现的？但没有深究，这次借着自己实现相关功能从而需要深入理解。\n其中重点就为两步：\n\n编译期间：记录所有的重载函数和运算符的关系。\n运行期：根据当前的运算找到声明的函数，直接运行即可。\n\n第一步的重点是扫描所有的重载函数，将重载函数与运算符存放起来，需要关注的是函数的返回值与运算符类型。\n// OpOverload 重载符type OpOverload struct &#123;\tfunction  *Func\ttokenType int&#125;// 运算符重载自定义函数opOverloads []*symbol.OpOverload\n\n在编译器中使用一个切片存放。\n而在运行期中当两个入参类型相同时，则需要查找重载函数。\n// GetOpFunction 获取运算符重载函数// 通过返回值以及运算符号(+-*/) 匹配重载函数func (a *AnnotatedTree) GetOpFunction(returnType symbol.Type, tokenType int) *symbol.Func &#123;\tfor _, overload := range a.opOverloads &#123;\t\tisType := overload.GetFunc().GetReturnType().IsType(returnType)\t\tif isType &amp;&amp; overload.GetTokenType() == tokenType &#123;\t\t\treturn overload.GetFunc()\t\t&#125;\t&#125;\treturn nil&#125;\n\n查找方式就是通过编译期存放的数据进行匹配，拿到重载函数后自动调用便实现了重载。\n感兴趣的朋友可以查看相关代码：\n\n编译期：https://github.com/crossoverJie/gscript/blob/ae729ce7d4cf39fe115121993fcd2222716755e5/resolver/type_scope_resolver.go#L127\n\n运行期：https://github.com/crossoverJie/gscript/blob/499236af549be47ff827c6d55de1fc8e5600b9b3/visitor.go#L387\n\n\n总结运算符重载其实并不是一个常用的功能；因为会改变运算符的语义，比如明明是加法却在重载函数中写为减法。\n这会使得代码阅读起来困难，但在某些情况下我们又非常希望语言本身能支持运算符重载。\n比如在 Go 中常用的一个第三方精度库decimal.Decimal，进行运算时只能使用 d1.Add(d2) 这样的函数，当运算复杂时：\na5 = (a1.Add(a2).Add(a3)).Mul(a4);\na5 = (a1+a2+a3)*a4;\n\n就不如下面这种直观，所以有利有弊吧，多一个选项总不是坏事。\nGScript 源码：https://github.com/crossoverJie/gscript\n","categories":["gscript","compiler"],"tags":["go","antlr","运算符重载"]},{"title":"用自己的编程语言实现了一个网站（增强版）","url":"/2022/10/08/gscript/gscript08-write-site-enhance/","content":"\n前言前段时间在《用自己的编程语言实现了一个网站》用介绍了用 GScript 写的一个简单“网站”，虽然是打上引号的；页面长这样：\n\n看起来确实非常的挫，其实之前一直也想做一个 GScript 的在线 playground ，于是国庆期间学了一点  皮毛 Vue 加上老弟的帮忙（他是前端开发），最终完成了下面这个网站：\nhttps://gscript.crossoverjie.top/\n\n\n❤打印源码参考了：https://wa-lang.org/playground/\n\n\n\n在这里可以在线运行 GScript 代码，借助于前端的代码编辑器插件甚至还能有一些语法提示。\n\n不过有些提示与 GScript 的语法不兼容，毕竟编辑器的提示是基于 JavaScript 的语法。\n\n\n内置了几个 demo，可以选择运行试试。\n同时也支持查看 AST 树和 symbol 符号表。\n\n虽然显示上还有待优化。\n\n\n整个后端接口全都是用 GScript 原生代码编写的，所以这也算是 GScript 的一个实际应用案例。\n代码示例func (HttpContext) run(HttpContext ctx) &#123;    string body = ctx.postFormValue(&quot;body&quot;);    string local = d.getCurrentTime(&quot;Asia/Shanghai&quot;,&quot;2006-01-02 15:04:05&quot;);    println(&quot;===&quot; + local);    println(body);    println(&quot;===&quot;);    RunResponse r = RunResponse();    if (body == &quot;&quot;)&#123;        r.body = &quot;empty code&quot;;        ctx.JSON(200, r);        return;    &#125;    string fileName = d.unix(&quot;Asia/Shanghai&quot;) + &quot;temp.gs&quot; ;    s.writeFile(fileName, body, 438);    string pwd = s.getwd();    // string res = s.command(&quot;gscript&quot;, fileName);    string res = s.command(&quot;docker&quot;,&quot;run&quot;,&quot;--rm&quot;,&quot;-v&quot;, pwd+&quot;:/usr/src/gscript&quot;,&quot;-w&quot;,&quot;/usr/src/gscript&quot;, &quot;crossoverjie/gscript&quot;,&quot;gscript&quot;, fileName);    s.remove(fileName);    r.body = res;    r.ast = dumpAST(body);    r.symbol=dumpSymbol(body);    ctx.JSON(200, r);&#125;httpHandle(&quot;GET&quot;, &quot;/index&quot;, index);httpHandle(&quot;POST&quot;, &quot;/run&quot;, run);string[] args = s.getOSArgs();if (len(args) ==3)&#123;    httpRun(&quot;:&quot; + args[2]);&#125;else &#123;    httpRun(&quot;:8000&quot;);&#125;\n\n实际代码量也并不多，将前端输入的代码写入到一个临时文件，再调用 OS 的 command api 在本地执行执行 docker，最后将标准输出和错误返回即可。\n版本更新为了能实现上述的需求，所以本次也更新了 GScript 的版本，新增了一些内置 API。\n\n主要是新增了 playground 需要的一些 OS api、文件写入、执行系统命令、日期相关等。\n同时将同一类的 API 合并到一个 class 中，方便后期维护与调用。\n编译错误除此之外也新增了一些易用功能，比如现在提供了更友好的编译错误信息：\n运行时错误运行时的异常现在也有对应提示：\n只不过目前的显示还不太友好，打印的堆栈还是 Go 的，之后会优化为只显示 GScript 的堆栈。\n总结有了在线的 playground 后使得对 GScript 感兴趣的门槛更低了一些，欢迎大家试用。\n\n经过最近几个版本的迭代，GScript 也逐步完善了，基本完成了第一版本的需求。\n后续会继续完善第二阶段的，比如：\n\nnamespace\n包管理\n并发\n\n等内容，每一项看起来都不是那么容易啊。\n之前有不少人问我 GScript 是拿来解决什么问题的？当时我确实没仔细想过；不过现在经过几个版本的开发有了一点想法：\n是否可以将 GScript 作为一个脚本化的 Go 语言，毕竟 Go 是编译类型的，每次修改发布都需要经过编译才能运行，但如果可以像脚本语言，比如 Python、Bash 修改后就可以直接运行，这在写一些不是那么重的业务时非常有用。\n同时借助于语法糖甚至可以优化掉 Go 本身被人“吐槽”的地方，比如异常、append 函数、没有运算符重载、三目运算符等。\n\n部分吐槽 GScript 已经支持。\n\n同时因为得益于本身也是 Go 编写的，所以 Go 的一些优点也能继承过来，比如轻量级的协程等。\nGo 本身也有大量的第三方库，后续甚至也能直接使用这些现成的库。\n以上只是一些美好的畅想，就像老板画的大饼一样，具体在实现过程中可能又有不一样的想法，欢迎提供建议。\nplayground 地址：https://gscript.crossoverjie.top/\n源码地址：https://github.com/crossoverjie/gscript\n","categories":["gscript","compiler"],"tags":["go"]},{"title":"如何为 GScript 编写标准库","url":"/2022/10/15/gscript/gscript10-write-native-lib/","content":"\n版本更新最近 GScript 更新了 v0.0.11 版本，重点更新了：\n\nDocker 运行环境\n新增了 byte 原始类型\n新增了一些字符串标准库 Strings/StringBuilder\n数组切片语法：int[] b = a[1: len(a)];\n\n\n具体更新内容请看下文。\n\n前言前段时间发布了 GScript 的在线 playground，\n\n\n这是一个可以在线运行 GScript 脚本的网站，其本质原理是接收用户的输入源码从而在服务器上运行的服务；这简直就是后门大开的 XSS 攻击，为保住服务器我设置了运行 API 的后端服务的用户权限，这样可以避免执行一些恶意的请求。\n但也避免不了一些用户执行了一些耗时操作，比如一个死循环、或者是我提供 demo 里的打印杨辉三角。\n这本质上是一个递归函数，当打印的三角层数过高时便会非常耗时，同时也非常消耗 CPU。\n有几次我去检查服务器时发现了几个 CPU 过高的进程，基本上都是这样的耗时操作，不可避免的会影响到服务器的性能。\n使用 Docker为了解决这类问题，很自然的就能想到可以使用 Docker，所有的资源都和宿主机是隔离开的，无论怎么瞎折腾也不会影响到宿主机。\n说干就干，最后修改了 API 执行脚本的地方：\nstring fileName = d.unix(&quot;Asia/Shanghai&quot;) + &quot;temp.gs&quot; ;s.writeFile(fileName, body, 438);string pwd = s.getwd();// string res = s.command(&quot;gscript&quot;, fileName);string res = s.command(&quot;docker&quot;,&quot;run&quot;,&quot;--rm&quot;,&quot;-v&quot;, pwd+&quot;:/usr/src/gscript&quot;,&quot;-w&quot;,&quot;/usr/src/gscript&quot;, &quot;crossoverjie/gscript&quot;,&quot;gscript&quot;, fileName);s.remove(fileName);r.body = res;r.ast = dumpAST(body);r.symbol=dumpSymbol(body);ctx.JSON(200, r);\n\n主要修改的就是将直接执行的 GScript 命令修改为了调用 docker 执行。\n\n但其实也还有改进空间，后续新增协程之后可以便可监控运行时间，超时后便会自动 kill 进程。\n\n我也将该 Docker 上传到了 DockerHub，现在大家想在本地体验 GScript 的 REPL 时也只需要运行Docker 就能使用。\ndocker pull crossoverjie/gscriptdocker run --rm -it  crossoverjie/gscript:latest gscript\n\n当然也可以执行用 Docker 执行 GScript 脚本：\ndocker run --rm -v $PWD:/usr/src/gscript -w /usr/src/gscript crossoverjie/gscript gscript &#123;yourpath&#125;/temp.gs\n\n编写 GScript 标准库接下来重点聊聊 GScript 标准库的事情，其实编写标准库是一个费时费力的事情。现在编译器已经提供了一些可用的内置函数，借由这些内置函数写一些常见的工具类是完全没有问题的。\n对写 GScript 标准库感谢的朋友可以当做一个参考，这里我打了一个样，先看下运行效果：\n// 字符串工具类StringBuilder b = StringBuilder();b.writeString(&quot;10&quot;);b.writeString(&quot;20&quot;);int l = b.writeString(&quot;30&quot;);string s = b.String();printf(&quot;s:%s, len=%d &quot;,s,l);assertEqual(s,&quot;102030&quot;);byte[] b2 = toByteArray(&quot;40&quot;);b.WriteBytes(b2);s = b.String();assertEqual(s,&quot;10203040&quot;);println(s);// Strings 工具类Strings s = Strings();string[] elems = &#123;&quot;name=xxx&quot;,&quot;age=xx&quot;&#125;;string ret = s.join(elems, &quot;&amp;&quot;);println(ret);assertEqual(ret, &quot;name=xxx&amp;age=xx&quot;);bool b = s.hasPrefix(&quot;http://www.xx.com&quot;, &quot;http&quot;);println(b);assertEqual(b,true);b = s.hasPrefix(&quot;http://www.xx.com&quot;, &quot;https&quot;);println(b);assertEqual(b,false);\n\n其中的实现源码基本上是借鉴了 Go 的标准库，先来看看 StringBuilder 的源码：\nclass StringBuilder&#123;    byte[] buf = [0]&#123;&#125;;    // append contents to buf, it returns the length of s    int writeString(string s)&#123;        byte[] temp = toByteArray(s);        append(buf, temp);        return len(temp);    &#125;        // append b to buf, it returns the length of b.    int WriteBytes(byte[] b)&#123;        append(buf, b);        return len(b);    &#125;    // copies the buffer to a new.    grow(int n)&#123;        if (n &gt; 0) &#123;            // when there is not enough space left.            if (cap(buf) - len(buf) &lt; n) &#123;                byte[] newBuf = [len(buf), 2*cap(buf)+n]&#123;&#125;;                copy(newBuf, buf);                buf = newBuf;            &#125;        &#125;       &#125;    string String()&#123;        return toString(buf);    &#125;&#125;\n主要就是借助了原始的数组类型以及 toByteArray/toString 字节数组和字符串的转换函数实现的。\nclass Strings&#123;    // concatenates the elements of its first argument to create a single string. The separator    // string sep is placed between elements in the resulting string.    string join(string[] elems, string sep)&#123;        if (len(elems) == 0) &#123;            return &quot;&quot;;        &#125;        if (len(elems) == 1) &#123;            return elems[0];        &#125;                byte[] bs = toByteArray(sep);        int n = len(bs) * (len(elems) -1);        for (int i=0; i &lt; len(elems); i++) &#123;            string s = elems[i];            byte[] bs = toByteArray(s);            n = n + len(bs);        &#125;                StringBuilder sb = StringBuilder();        sb.grow(n);        string first = elems[0];        sb.writeString(first);        string[] remain = elems[1:len(elems)];        for(int i=0; i &lt; len(remain); i++)&#123;            sb.writeString(sep);            string r = remain[i];            sb.writeString(r);        &#125;        return sb.String();    &#125;        // tests whether the string s begins with prefix.    bool hasPrefix(string s, string prefix)&#123;        byte[] bs = toByteArray(s);        byte[] bp = toByteArray(prefix);            return len(bs) &gt;= len(bp) &amp;&amp; toString(bs[0:len(bp)]) == prefix;    &#125;&#125;\n\nStrings 工具类也是类似的，都是一些内置函数的组合运用；\n在写标准库的过程中还会有额外收获，可以再次阅读一遍 Go 标准库的实现流程，换了一种语法实现出来，会加深对 Go 标准库的理解。\n所以欢迎感兴趣的朋友向 GScript 贡献标准库，由于我个人精力有限，实现过程中可能会发现缺少某些内置函数或数据结构，这也没关系，反馈 issue 后我会尽快处理。\n\n由于目前 GScript 还不支持包管理，所以新增的函数可以创建 Class 来实现，后续支持包或者是 namespace 之后直接将该 Class 迁移过去即可。\n\n\n本文相关资源链接\n\nGScript 源码：https://github.com/crossoverJie/gscript\nPlayground 源码：https://github.com/crossoverJie/gscript-homepage\nGScript Docker地址：https://hub.docker.com/r/crossoverjie/gscript\n\n","categories":["gscript","compiler"],"tags":["go"]},{"title":"深入理解闭包实现原理","url":"/2022/10/24/gscript/gscript11-closure/","content":"\n前言闭包对于一个长期写 Java 的开发者来说估计鲜有耳闻，我在写 Python 和 Go 之前也是没怎么了解，光这名字感觉就有点”神秘莫测”，这篇文章的主要目的就是从编译器的角度来分析闭包，彻底搞懂闭包的实现原理。\n\n\n函数一等公民一门语言在实现闭包之前首先要具有的特性就是：First class function 函数是第一公民。\n简单来说就是函数可以像一个普通的值一样在函数中传递，也能对变量赋值。\n先来看看在 Go 里是如何编写的：\npackage mainimport &quot;fmt&quot;var varExternal intfunc f1() func(int) int &#123;\tvarInner := 20\tinnerFun := func(a int) int &#123;\t\tfmt.Println(a)\t\tvarExternal++\t\tvarInner++\t\treturn varInner\t&#125;\treturn innerFun&#125;func main() &#123;\tvarExternal = 10\tf2 := f1()\tfor i := 0; i &lt; 2; i++ &#123;\t\tfmt.Printf(&quot;varInner=%d, varExternal=%d \\n&quot;, f2(i), varExternal)\t&#125;\tfmt.Println(&quot;======&quot;)\tf3 := f1()\tfor i := 0; i &lt; 2; i++ &#123;\t\tfmt.Printf(&quot;varInner=%d, varExternal=%d \\n&quot;, f3(i), varExternal)\t&#125;&#125;// Output:0varInner=21, varExternal=11 1varInner=22, varExternal=12 ======0varInner=21, varExternal=13 1varInner=22, varExternal=14 \n\n这里体现了闭包的两个重要特性，第一个自然就是函数可以作为值返回，同时也能赋值给变量。\n第二个就是在闭包函数 f1() 对闭包变量 varInner 的访问，每个闭包函数的引用都会在自己的函数内部保存一份闭包变量 varInner，这样在调用过程中就不会互相影响。\n从打印的结果中也能看出这个特性。\n作用域闭包之所以不太好理解的主要原因是它不太符合自觉。\n\n本质上就是作用域的关系，当我们调用 f1() 函数的时候，会在栈中分配变量 varInner，正常情况下调用完毕后 f1 的栈会弹出，里面的变量 varInner 自然也会销毁才对。\n但在后续的 f2() 和 f3() 调用的时，却依然能访问到 varInner，就这点不符合我们对函数调用的直觉。\n但其实换个角度来看，对 innerFun 来说，他能访问到 varExternal 和 varInner 变量，最外层的 varExternal 就不用说了，一定是可以访问的。\n但对于 varInner 来说就不一定了，这里得分为两种情况；重点得看该语言是静态&#x2F;动态作用域。\n就静态作用域来说，每个符号在编译器就确定好了树状关系，运行时不会发生变化；也就是说 varInner 对于 innerFun 这个函数来说在编译期已经确定可以访问了，在运行时自然也是可以访问的。\n但对于动态作用域来说，完全是在运行时才确定访问的变量是哪一个。\n恰好 Go 就是一个静态作用域的语言，所以返回的 innerFun 函数可以一直访问到 varInner 变量。\n实现闭包但 Go 是如何做到在 f1() 函数退出之后依然能访问到 f1() 中的变量呢？\n这里我们不妨大胆假设一下：\n首先在编译期扫描出哪些是闭包变量，也就是这里的 varInner，需要将他保存到函数 innerFun() 中。\nf2 := f1()f2()\n\n运行时需要判断出 f2 是一个函数，而不是一个变量，同时得知道它所包含的函数体是 innerFun() 所定义的。\n接着便是执行函数体的 statement 即可。\n而当 f3 := f1() 重新赋值给 f3 时，在 f2 中累加的 varInner 变量将不会影响到 f3，这就得需要在给 f3 赋值的重新赋值一份闭包变量到  f3 中，这样便能达到互不影响的效果。\n闭包扫描GScript 本身也是支持闭包的，所以把 Go 的代码翻译过来便长这样：\nint varExternal =10;func int(int) f1()&#123;\tint varInner = 20;\tint innerFun(int a)&#123;\t\tprintln(a);\t\tint c=100;\t\tvarExternal++;\t\tvarInner++;\t\treturn varInner;\t&#125;\treturn innerFun;&#125;func int(int) f2 = f1();for(int i=0;i&lt;2;i++)&#123;\tprintln(&quot;varInner=&quot; + f2(i) + &quot;, varExternal=&quot; + varExternal);&#125;println(&quot;=======&quot;);func int(int) f3 = f1();for(int i=0;i&lt;2;i++)&#123;\tprintln(&quot;varInner=&quot; + f3(i) + &quot;, varExternal=&quot; + varExternal);&#125;// Output:0varInner=21, varExternal=111varInner=22, varExternal=12=======0varInner=21, varExternal=131varInner=22, varExternal=14\n\n可以看到运行结果和 Go 的一样，所以我们来看看 GScript 是如何实现的便也能理解 Go 的原理了。\n\n先来看看第一步扫描闭包变量：\nallVariable := c.allVariable(function)查询所有的变量，包括父 scope 的变量。\nscopeVariable := c.currentScopeVariable(function)查询当前 scope 包含下级所有 scope 中的变量，这样一减之后就能知道闭包变量了，然后将所有的闭包变量存放进闭包函数中。\n闭包赋值之后在 return innerFun 处，将闭包变量的数据赋值到变量中。\n\n闭包函数调用func int(int) f2 = f1();func int(int) f3 = f1();\n\n在这里每一次赋值时，都会把 f1() 返回函数复制到变量 f2/f3 中，这样两者所包含的闭包变量就不会互相影响。\n\n在调用函数变量时，判断到该变量是一个函数，则直接返回函数。\n之后直接调用该函数即可。\n函数式编程接下来便可以利用 First class function 来试试函数式编程：\nclass Test&#123;\tint value=0;\tTest(int v)&#123;\t\tvalue=v;\t&#125;\tint map(func int(int) f)&#123;\t\treturn f(value);\t&#125;&#125;int square(int v)&#123;\treturn v*v; &#125;int add(int v)&#123;\treturn v++; &#125;int add2(int v)&#123;\tv=v+2;\treturn v; &#125;Test t =Test(100);func int(int) s= square;func int(int) a= add;func int(int) a2= add2;println(t.map(s));assertEqual(t.map(s),10000);println(t.map(a));assertEqual(t.map(a),101);println(t.map(a2));assertEqual(t.map(a2),102);\n\n这个有点类似于 Java 中流的 map 函数，将函数作为值传递进去，后续支持匿名函数后会更像是函数式编程，现在必须得先定义一个函数变量再进行传递。\n\n除此之外在 GScript 中的 http 标准库也利用了函数是一等公民的特性：\n// 标准库：Bind routehttpHandle(string method, string path, func (HttpContext) handle)&#123;    HttpContext ctx = HttpContext();    handle(ctx);&#125;\n\n在绑定路由时，handle 便是一个函数，使用的时候直接传递业务逻辑的 handle 即可：\nfunc (HttpContext) handle (HttpContext ctx)&#123;    Person p = Person();    p.name = &quot;abc&quot;;    println(&quot;p.name=&quot; + p.name);    println(&quot;ctx=&quot; + ctx);    ctx.JSON(200, p);&#125;httpHandle(&quot;get&quot;, &quot;/p&quot;, handle);\n\n\n\n总结总的来说闭包具有以下特性：\n\n函数需要作为一等公民。\n编译期扫描出所有的闭包变量。\n在返回闭包函数时，为闭包变量赋值。\n每次创建新的函数变量时，需要将闭包数据复制进去，这样闭包变量才不会互相影响。\n调用函数变量时，需要判断为函数，而不是变量。\n\n可以在 Playground 中体验闭包函数打印裴波那切数列的运用。\n本文相关资源链接\n\nGScript 源码：https://github.com/crossoverJie/gscript\n\nPlayground 源码：https://github.com/crossoverJie/gscript-homepage\n\n\n","categories":["gscript","compiler"],"tags":["go","闭包","closure"]},{"title":"如何判断一个元素在亿级数据中是否存在？","url":"/2018/11/26/guava/guava-bloom-filter/","content":"\n前言最近有朋友问我这么一个面试题目：\n\n现在有一个非常庞大的数据，假设全是 int 类型。现在我给你一个数，你需要告诉我它是否存在其中(尽量高效)。\n\n需求其实很清晰，只是要判断一个数据是否存在即可。\n但这里有一个比较重要的前提：非常庞大的数据。\n\n\n常规实现先不考虑这个条件，我们脑海中出现的第一种方案是什么？\n我想大多数想到的都是用 HashMap 来存放数据，因为它的写入查询的效率都比较高。\n写入和判断元素是否存在都有对应的 API，所以实现起来也比较简单。\n为此我写了一个单测，利用 HashSet 来存数据（底层也是 HashMap ）；同时为了后面的对比将堆内存写死：\n-Xms64m -Xmx64m -XX:+PrintHeapAtGC -XX:+HeapDumpOnOutOfMemoryError \n\n为了方便调试加入了 GC 日志的打印，以及内存溢出后 Dump 内存。\n@Testpublic void hashMapTest()&#123;    long star = System.currentTimeMillis();    Set&lt;Integer&gt; hashset = new HashSet&lt;&gt;(100) ;    for (int i = 0; i &lt; 100; i++) &#123;        hashset.add(i) ;    &#125;    Assert.assertTrue(hashset.contains(1));    Assert.assertTrue(hashset.contains(2));    Assert.assertTrue(hashset.contains(3));    long end = System.currentTimeMillis();    System.out.println(&quot;执行时间：&quot; + (end - star));&#125;\n\n当我只写入 100 条数据时自然是没有问题的。\n还是在这个基础上，写入 1000W 数据试试：\n\n执行后马上就内存溢出。\n\n可见在内存有限的情况下我们不能使用这种方式。\n实际情况也是如此；既然要判断一个数据是否存在于集合中，考虑的算法的效率以及准确性肯定是要把数据全部 load 到内存中的。\nBloom Filter基于上面分析的条件，要实现这个需求最需要解决的是如何将庞大的数据 load 到内存中。\n而我们是否可以换种思路，因为只是需要判断数据是否存在，也不是需要把数据查询出来，所以完全没有必要将真正的数据存放进去。\n伟大的科学家们已经帮我们想到了这样的需求。\nBurton Howard Bloom 在 1970 年提出了一个叫做 Bloom Filter（中文翻译：布隆过滤）的算法。\n它主要就是用于解决判断一个元素是否在一个集合中，但它的优势是只需要占用很小的内存空间以及有着高效的查询效率。\n所以在这个场景下在合适不过了。\nBloom Filter 原理下面来分析下它的实现原理。\n\n官方的说法是：它是一个保存了很长的二级制向量，同时结合 Hash 函数实现的。\n\n听起来比较绕，但是通过一个图就比较容易理解了。\n\n如图所示：\n\n首先需要初始化一个二进制的数组，长度设为 L（图中为 8），同时初始值全为 0 。\n当写入一个 A1=1000 的数据时，需要进行 H 次 hash 函数的运算（这里为 2 次）；与 HashMap 有点类似，通过算出的 HashCode 与 L 取模后定位到 0、2 处，将该处的值设为 1。\nA2=2000 也是同理计算后将 4、7 位置设为 1。\n当有一个 B1=1000 需要判断是否存在时，也是做两次 Hash 运算，定位到 0、2 处，此时他们的值都为 1 ，所以认为 B1=1000 存在于集合中。\n当有一个 B2=3000 时，也是同理。第一次 Hash 定位到 index=4 时，数组中的值为 1，所以再进行第二次 Hash 运算，结果定位到 index=5 的值为 0，所以认为 B2=3000 不存在于集合中。\n\n整个的写入、查询的流程就是这样，汇总起来就是：\n\n对写入的数据做 H 次 hash 运算定位到数组中的位置，同时将数据改为 1 。当有数据查询时也是同样的方式定位到数组中。一旦其中的有一位为 0 则认为数据肯定不存在于集合，否则数据可能存在于集合中。\n\n所以布隆过滤有以下几个特点：\n\n只要返回数据不存在，则肯定不存在。\n返回数据存在，但只能是大概率存在。\n同时不能清除其中的数据。\n\n第一点应该都能理解，重点解释下 2、3 点。\n为什么返回存在的数据却是可能存在呢，这其实也和 HashMap 类似。\n在有限的数组长度中存放大量的数据，即便是再完美的 Hash 算法也会有冲突，所以有可能两个完全不同的 A、B 两个数据最后定位到的位置是一模一样的。\n这时拿 B 进行查询时那自然就是误报了。\n删除数据也是同理，当我把 B 的数据删除时，其实也相当于是把 A 的数据删掉了，这样也会造成后续的误报。\n基于以上的 Hash 冲突的前提，所以 Bloom Filter 有一定的误报率，这个误报率和 Hash 算法的次数 H，以及数组长度 L 都是有关的。\n自己实现一个布隆过滤算法其实很简单不难理解，于是利用 Java 实现了一个简单的雏形。\npublic class BloomFilters &#123;    /**     * 数组长度     */    private int arraySize;    /**     * 数组     */    private int[] array;    public BloomFilters(int arraySize) &#123;        this.arraySize = arraySize;        array = new int[arraySize];    &#125;    /**     * 写入数据     * @param key     */    public void add(String key) &#123;        int first = hashcode_1(key);        int second = hashcode_2(key);        int third = hashcode_3(key);        array[first % arraySize] = 1;        array[second % arraySize] = 1;        array[third % arraySize] = 1;    &#125;    /**     * 判断数据是否存在     * @param key     * @return     */    public boolean check(String key) &#123;        int first = hashcode_1(key);        int second = hashcode_2(key);        int third = hashcode_3(key);        int firstIndex = array[first % arraySize];        if (firstIndex == 0) &#123;            return false;        &#125;        int secondIndex = array[second % arraySize];        if (secondIndex == 0) &#123;            return false;        &#125;        int thirdIndex = array[third % arraySize];        if (thirdIndex == 0) &#123;            return false;        &#125;        return true;    &#125;    /**     * hash 算法1     * @param key     * @return     */    private int hashcode_1(String key) &#123;        int hash = 0;        int i;        for (i = 0; i &lt; key.length(); ++i) &#123;            hash = 33 * hash + key.charAt(i);        &#125;        return Math.abs(hash);    &#125;    /**     * hash 算法2     * @param data     * @return     */    private int hashcode_2(String data) &#123;        final int p = 16777619;        int hash = (int) 2166136261L;        for (int i = 0; i &lt; data.length(); i++) &#123;            hash = (hash ^ data.charAt(i)) * p;        &#125;        hash += hash &lt;&lt; 13;        hash ^= hash &gt;&gt; 7;        hash += hash &lt;&lt; 3;        hash ^= hash &gt;&gt; 17;        hash += hash &lt;&lt; 5;        return Math.abs(hash);    &#125;    /**     *  hash 算法3     * @param key     * @return     */    private int hashcode_3(String key) &#123;        int hash, i;        for (hash = 0, i = 0; i &lt; key.length(); ++i) &#123;            hash += key.charAt(i);            hash += (hash &lt;&lt; 10);            hash ^= (hash &gt;&gt; 6);        &#125;        hash += (hash &lt;&lt; 3);        hash ^= (hash &gt;&gt; 11);        hash += (hash &lt;&lt; 15);        return Math.abs(hash);    &#125;&#125;\n\n\n首先初始化了一个 int 数组。\n写入数据的时候进行三次 hash 运算，同时把对应的位置置为 1。\n查询时同样的三次 hash 运算，取到对应的值，一旦值为 0 ，则认为数据不存在。\n\n实现逻辑其实就和上文描述的一样。\n下面来测试一下，同样的参数：\n-Xms64m -Xmx64m -XX:+PrintHeapAtGC\n\n@Testpublic void bloomFilterTest()&#123;    long star = System.currentTimeMillis();    BloomFilters bloomFilters = new BloomFilters(10000000) ;    for (int i = 0; i &lt; 10000000; i++) &#123;        bloomFilters.add(i + &quot;&quot;) ;    &#125;    Assert.assertTrue(bloomFilters.check(1+&quot;&quot;));    Assert.assertTrue(bloomFilters.check(2+&quot;&quot;));    Assert.assertTrue(bloomFilters.check(3+&quot;&quot;));    Assert.assertTrue(bloomFilters.check(999999+&quot;&quot;));    Assert.assertFalse(bloomFilters.check(400230340+&quot;&quot;));    long end = System.currentTimeMillis();    System.out.println(&quot;执行时间：&quot; + (end - star));&#125;\n\n执行结果如下：\n\n只花了 3 秒钟就写入了 1000W 的数据同时做出来准确的判断。\n\n\n当让我把数组长度缩小到了 100W 时就出现了一个误报，400230340 这个数明明没在集合里，却返回了存在。\n这也体现了 Bloom Filter 的误报率。\n我们提高数组长度以及 hash 计算次数可以降低误报率，但相应的 CPU、内存的消耗就会提高；这就需要根据业务需要自行权衡。\nGuava 实现\n刚才的方式虽然实现了功能，也满足了大量数据。但其实观察 GC 日志非常频繁，同时老年代也使用了 90%，接近崩溃的边缘。\n总的来说就是内存利用率做的不好。\n其实 Google Guava 库中也实现了该算法，下面来看看业界权威的实现。\n-Xms64m -Xmx64m -XX:+PrintHeapAtGC \n\n\n@Testpublic void guavaTest() &#123;    long star = System.currentTimeMillis();    BloomFilter&lt;Integer&gt; filter = BloomFilter.create(            Funnels.integerFunnel(),            10000000,            0.01);    for (int i = 0; i &lt; 10000000; i++) &#123;        filter.put(i);    &#125;    Assert.assertTrue(filter.mightContain(1));    Assert.assertTrue(filter.mightContain(2));    Assert.assertTrue(filter.mightContain(3));    Assert.assertFalse(filter.mightContain(10000000));    long end = System.currentTimeMillis();    System.out.println(&quot;执行时间：&quot; + (end - star));&#125;\n\n\n也是同样写入了 1000W 的数据，执行没有问题。\n\n观察 GC 日志会发现没有一次 fullGC，同时老年代的使用率很低。和刚才的一对比这里明显的要好上很多，也可以写入更多的数据。\n源码分析那就来看看 Guava 它是如何实现的。\n构造方法中有两个比较重要的参数，一个是预计存放多少数据，一个是可以接受的误报率。我这里的测试 demo 分别是 1000W 以及 0.01。\n\nGuava 会通过你预计的数量以及误报率帮你计算出你应当会使用的数组大小 numBits 以及需要计算几次 Hash 函数 numHashFunctions 。\n这个算法计算规则可以参考维基百科。\nput 写入函数真正存放数据的 put 函数如下：\n\n\n根据 murmur3_128 方法的到一个 128 位长度的 byte[]。\n分别取高低 8 位的到两个 hash 值。\n再根据初始化时的到的执行 hash 的次数进行 hash 运算。\n\nbitsChanged |= bits.set((combinedHash &amp; Long.MAX_VALUE) % bitSize);\n\n其实也是 hash取模拿到 index 后去赋值 1.\n重点是 bits.set() 方法。\n\n其实 set 方法是 BitArray 中的一个函数，BitArray 就是真正存放数据的底层数据结构。\n利用了一个 long[] data 来存放数据。\n所以 set() 时候也是对这个 data 做处理。\n\n\n在 set 之前先通过 get() 判断这个数据是否存在于集合中，如果已经存在则直接返回告知客户端写入失败。\n接下来就是通过位运算进行位或赋值。\nget() 方法的计算逻辑和 set 类似，只要判断为 0 就直接返回存在该值。\n\nmightContain 是否存在函数\n前面几步的逻辑都是类似的，只是调用了刚才的 get() 方法判断元素是否存在而已。\n总结布隆过滤的应用还是蛮多的，比如数据库、爬虫、防缓存击穿等。\n特别是需要精确知道某个数据不存在时做点什么事情就非常适合布隆过滤。\n这段时间的研究发现算法也挺有意思的，后续应该会继续分享一些类似的内容。\n如果对你有帮助那就分享一下吧。\n本问的示例代码参考这里：\nhttps://github.com/crossoverJie/JCSprout\n你的点赞与分享是对我最大的支持\n","categories":["Guava"],"tags":["Bloom Filter","算法","Hash"]},{"title":"Guava 源码分析（Cache 原理【二阶段】）","url":"/2018/07/16/guava/guava-cache-2/","content":"\n前言在上文「Guava 源码分析（Cache 原理）」中分析了 Guava Cache 的相关原理。\n文末提到了回收机制、移除时间通知等内容，许多朋友也挺感兴趣，这次就这两个内容再来分析分析。\n\n在开始之前先补习下 Java 自带的两个特性，Guava 中都有具体的应用。\n\nJava 中的引用首先是 Java 中的引用。\n在之前分享过 JVM 是根据可达性分析算法找出需要回收的对象，判断对象的存活状态都和引用有关。\n在 JDK1.2 之前这点设计的非常简单：一个对象的状态只有引用和没被引用两种区别。\n\n\n这样的划分对垃圾回收不是很友好，因为总有一些对象的状态处于这两之间。\n因此 1.2 之后新增了四种状态用于更细粒度的划分引用关系：\n\n强引用（Strong Reference）:这种对象最为常见，比如 **A a = new A();**这就是典型的强引用；这样的强引用关系是不能被垃圾回收的。\n软引用（Soft Reference）:这样的引用表明一些有用但不是必要的对象，在将发生垃圾回收之前是需要将这样的对象再次回收。\n弱引用（Weak Reference）:这是一种比软引用还弱的引用关系，也是存放非必须的对象。当垃圾回收时，无论当前内存是否足够，这样的对象都会被回收。\n虚引用（Phantom Reference）:这是一种最弱的引用关系，甚至没法通过引用来获取对象，它唯一的作用就是在被回收时可以获得通知。\n\n事件回调事件回调其实是一种常见的设计模式，比如之前讲过的 Netty 就使用了这样的设计。\n这里采用一个 demo，试下如下功能：\n\nCaller 向 Notifier 提问。\n提问方式是异步，接着做其他事情。\nNotifier 收到问题执行计算然后回调 Caller 告知结果。\n\n在 Java 中利用接口来实现回调，所以需要定义一个接口：\npublic interface CallBackListener &#123;    /**     * 回调通知函数     * @param msg     */    void callBackNotify(String msg) ;&#125;\n\nCaller 中调用 Notifier 执行提问，调用时将接口传递过去：\npublic class Caller &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(Caller.class);    private CallBackListener callBackListener ;    private Notifier notifier ;    private String question ;    /**     * 使用     */    public void call()&#123;        LOGGER.info(&quot;开始提问&quot;);\t\t//新建线程，达到异步效果         new Thread(new Runnable() &#123;            @Override            public void run() &#123;                try &#123;                    notifier.execute(Caller.this,question);                &#125; catch (InterruptedException e) &#123;                    e.printStackTrace();                &#125;            &#125;        &#125;).start();        LOGGER.info(&quot;提问完毕，我去干其他事了&quot;);    &#125;        //隐藏 getter/setter    &#125;    \n\nNotifier 收到提问，执行计算（耗时操作），最后做出响应（回调接口，告诉 Caller 结果）。\npublic class Notifier &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(Notifier.class);    public void execute(Caller caller, String msg) throws InterruptedException &#123;        LOGGER.info(&quot;收到消息=【&#123;&#125;】&quot;, msg);        LOGGER.info(&quot;等待响应中。。。。。&quot;);        TimeUnit.SECONDS.sleep(2);        caller.getCallBackListener().callBackNotify(&quot;我在北京！&quot;);    &#125;&#125;\n\n\n模拟执行：\npublic static void main(String[] args) &#123;    Notifier notifier = new Notifier() ;    Caller caller = new Caller() ;    caller.setNotifier(notifier) ;    caller.setQuestion(&quot;你在哪儿！&quot;);    caller.setCallBackListener(new CallBackListener() &#123;        @Override        public void callBackNotify(String msg) &#123;            LOGGER.info(&quot;回复=【&#123;&#125;】&quot; ,msg);        &#125;    &#125;);    caller.call();&#125;\n\n最后执行结果：\n2018-07-15 19:52:11.105 [main] INFO  c.crossoverjie.guava.callback.Caller - 开始提问2018-07-15 19:52:11.118 [main] INFO  c.crossoverjie.guava.callback.Caller - 提问完毕，我去干其他事了2018-07-15 19:52:11.117 [Thread-0] INFO  c.c.guava.callback.Notifier - 收到消息=【你在哪儿！】2018-07-15 19:52:11.121 [Thread-0] INFO  c.c.guava.callback.Notifier - 等待响应中。。。。。2018-07-15 19:52:13.124 [Thread-0] INFO  com.crossoverjie.guava.callback.Main - 回复=【我在北京！】\n\n这样一个模拟的异步事件回调就完成了。\nGuava 的用法Guava 就是利用了上文的两个特性来实现了引用回收及移除通知。\n引用可以在初始化缓存时利用：\n\nCacheBuilder.weakKeys()\nCacheBuilder.weakValues()\nCacheBuilder.softValues()\n\n来自定义键和值的引用关系。\n\n在上文的分析中可以看出 Cache 中的 ReferenceEntry 是类似于 HashMap 的 Entry 存放数据的。\n来看看 ReferenceEntry 的定义：\n  interface ReferenceEntry&lt;K, V&gt; &#123;    /**     * Returns the value reference from this entry.     */    ValueReference&lt;K, V&gt; getValueReference();    /**     * Sets the value reference for this entry.     */    void setValueReference(ValueReference&lt;K, V&gt; valueReference);    /**     * Returns the next entry in the chain.     */    @Nullable    ReferenceEntry&lt;K, V&gt; getNext();    /**     * Returns the entry&#x27;s hash.     */    int getHash();    /**     * Returns the key for this entry.     */    @Nullable    K getKey();    /*     * Used by entries that use access order. Access entries are maintained in a doubly-linked list.     * New entries are added at the tail of the list at write time; stale entries are expired from     * the head of the list.     */    /**     * Returns the time that this entry was last accessed, in ns.     */    long getAccessTime();    /**     * Sets the entry access time in ns.     */    void setAccessTime(long time);&#125;\n\n包含了很多常用的操作，如值引用、键引用、访问时间等。\n根据 ValueReference&lt;K, V&gt; getValueReference(); 的实现：\n\n具有强引用和弱引用的不同实现。\nkey 也是相同的道理：\n\n当使用这样的构造方式时，弱引用的 key 和 value 都会被垃圾回收。\n当然我们也可以显式的回收：\n/** * Discards any cached value for key &#123;@code key&#125;. * 单个回收 */void invalidate(Object key);/** * Discards any cached values for keys &#123;@code keys&#125;. * * @since 11.0 */void invalidateAll(Iterable&lt;?&gt; keys);/** * Discards all entries in the cache. */void invalidateAll();\n\n回调改造了之前的例子：\nloadingCache = CacheBuilder.newBuilder()        .expireAfterWrite(2, TimeUnit.SECONDS)        .removalListener(new RemovalListener&lt;Object, Object&gt;() &#123;            @Override            public void onRemoval(RemovalNotification&lt;Object, Object&gt; notification) &#123;                LOGGER.info(&quot;删除原因=&#123;&#125;，删除 key=&#123;&#125;,删除 value=&#123;&#125;&quot;,notification.getCause(),notification.getKey(),notification.getValue());            &#125;        &#125;)        .build(new CacheLoader&lt;Integer, AtomicLong&gt;() &#123;            @Override            public AtomicLong load(Integer key) throws Exception &#123;                return new AtomicLong(0);            &#125;        &#125;);\n\n执行结果：\n2018-07-15 20:41:07.433 [main] INFO  c.crossoverjie.guava.CacheLoaderTest - 当前缓存值=0,缓存大小=12018-07-15 20:41:07.442 [main] INFO  c.crossoverjie.guava.CacheLoaderTest - 缓存的所有内容=&#123;1000=0&#125;2018-07-15 20:41:07.443 [main] INFO  c.crossoverjie.guava.CacheLoaderTest - job running times=102018-07-15 20:41:10.461 [main] INFO  c.crossoverjie.guava.CacheLoaderTest - 删除原因=EXPIRED，删除 key=1000,删除 value=12018-07-15 20:41:10.462 [main] INFO  c.crossoverjie.guava.CacheLoaderTest - 当前缓存值=0,缓存大小=12018-07-15 20:41:10.462 [main] INFO  c.crossoverjie.guava.CacheLoaderTest - 缓存的所有内容=&#123;1000=0&#125;\n\n可以看出当缓存被删除的时候会回调我们自定义的函数，并告知删除原因。\n那么 Guava 是如何实现的呢？\n\n根据 LocalCache 中的 getLiveValue() 中判断缓存过期时，跟着这里的调用关系就会一直跟到：\n\nremoveValueFromChain() 中的：\n\nenqueueNotification() 方法会将回收的缓存（包含了 key，value）以及回收原因包装成之前定义的事件接口加入到一个本地队列中。\n\n这样一看也没有回调我们初始化时候的事件啊。\n不过用过队列的同学应该能猜出，既然这里写入队列，那就肯定就有消费。\n我们回到获取缓存的地方：\n\n在 finally 中执行了 postReadCleanup() 方法；其实在这里面就是对刚才的队列进行了消费：\n\n一直跟进来就会发现这里消费了队列，将之前包装好的移除消息调用了我们自定义的事件，这样就完成了一次事件回调。\n总结以上所有源码：\nhttps://github.com/crossoverJie/Java-Interview/blob/master/src/main/java/com/crossoverjie/guava/callback/Main.java\n通过分析 Guava 的源码可以让我们学习到顶级的设计及实现方式，甚至自己也能尝试编写。\nGuava 里还有很多强大的增强实现，值得我们再好好研究。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n欢迎关注公众号一起交流：\n","categories":["Guava"],"tags":["Cache"]},{"title":"Guava 源码分析（Cache 原理）","url":"/2018/06/13/guava/guava-cache/","content":"\n前言Google 出的 Guava 是 Java 核心增强的库，应用非常广泛。\n我平时用的也挺频繁，这次就借助日常使用的 Cache 组件来看看 Google 大牛们是如何设计的。\n缓存\n本次主要讨论缓存。\n\n缓存在日常开发中举足轻重，如果你的应用对某类数据有着较高的读取频次，并且改动较小时那就非常适合利用缓存来提高性能。\n缓存之所以可以提高性能是因为它的读取效率很高，就像是 CPU 的 L1、L2、L3 缓存一样，级别越高相应的读取速度也会越快。\n但也不是什么好处都占，读取速度快了但是它的内存更小资源更宝贵，所以我们应当缓存真正需要的数据。\n\n其实也就是典型的空间换时间。\n\n下面谈谈 Java 中所用到的缓存。\n\n\nJVM 缓存首先是 JVM 缓存，也可以认为是堆缓存。\n其实就是创建一些全局变量，如 Map、List 之类的容器用于存放数据。\n这样的优势是使用简单但是也有以下问题：\n\n只能显式的写入，清除数据。\n不能按照一定的规则淘汰数据，如 LRU，LFU，FIFO 等。\n清除数据时的回调通知。\n其他一些定制功能等。\n\nEhcache、Guava Cache所以出现了一些专门用作 JVM 缓存的开源工具出现了，如本文提到的 Guava Cache。\n它具有上文 JVM 缓存不具有的功能，如自动清除数据、多种清除算法、清除回调等。\n但也正因为有了这些功能，这样的缓存必然会多出许多东西需要额外维护，自然也就增加了系统的消耗。\n分布式缓存刚才提到的两种缓存其实都是堆内缓存，只能在单个节点中使用，这样在分布式场景下就招架不住了。\n于是也有了一些缓存中间件，如 Redis、Memcached，在分布式环境下可以共享内存。\n具体不在本次的讨论范围。\nGuava Cache 示例之所以想到 Guava 的 Cache，也是最近在做一个需求，大体如下：\n\n从 Kafka 实时读取出应用系统的日志信息，该日志信息包含了应用的健康状况。如果在时间窗口 N 内发生了 X 次异常信息，相应的我就需要作出反馈（报警、记录日志等）。\n\n对此 Guava 的 Cache 就非常适合，我利用了它的 N 个时间内不写入数据时缓存就清空的特点，在每次读取数据时判断异常信息是否大于 X 即可。\n伪代码如下：\n@Value(&quot;$&#123;alert.in.time:2&#125;&quot;)private int time ;@Beanpublic LoadingCache buildCache()&#123;    return CacheBuilder.newBuilder()            .expireAfterWrite(time, TimeUnit.MINUTES)            .build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123;                @Override                public AtomicLong load(Long key) throws Exception &#123;                    return new AtomicLong(0);                &#125;            &#125;);&#125;/** * 判断是否需要报警 */public void checkAlert() &#123;    try &#123;        if (counter.get(KEY).incrementAndGet() &gt;= limit) &#123;            LOGGER.info(&quot;***********报警***********&quot;);            //将缓存清空            counter.get(KEY).getAndSet(0L);        &#125;    &#125; catch (ExecutionException e) &#123;        LOGGER.error(&quot;Exception&quot;, e);    &#125;&#125;   \n\n首先是构建了 LoadingCache 对象，在 N 分钟内不写入数据时就回收缓存（当通过 Key 获取不到缓存时，默认返回 0）。\n然后在每次消费时候调用 checkAlert() 方法进行校验，这样就可以达到上文的需求。\n我们来设想下 Guava 它是如何实现过期自动清除数据，并且是可以按照 LRU 这样的方式清除的。\n大胆假设下：\n\n内部通过一个队列来维护缓存的顺序，每次访问过的数据移动到队列头部，并且额外开启一个线程来判断数据是否过期，过期就删掉。有点类似于我之前写过的 动手实现一个 LRU cache\n\n胡适说过：大胆假设小心论证\n下面来看看 Guava 到底是怎么实现。\n原理分析看原理最好不过是跟代码一步步走了：\n示例代码在这里：\nhttps://github.com/crossoverJie/Java-Interview/blob/master/src/main/java/com/crossoverjie/guava/CacheLoaderTest.java\n\n为了能看出 Guava 是怎么删除过期数据的在获取缓存之前休眠了 5 秒钟，达到了超时条件。\n\n最终会发现在 com.google.common.cache.LocalCache 类的 2187 行比较关键。\n再跟进去之前第 2182 行会发现先要判断 count 是否大于 0，这个 count 保存的是当前缓存的数量，并用 volatile 修饰保证了可见性。\n\n更多关于 volatile 的相关信息可以查看 你应该知道的 volatile 关键字\n\n接着往下跟到：\n\n2761 行，根据方法名称可以看出是判断当前的 Entry 是否过期，该 entry 就是通过 key 查询到的。\n\n这里就很明显的看出是根据根据构建时指定的过期方式来判断当前 key 是否过期了。\n\n如果过期就往下走，尝试进行过期删除（需要加锁，后面会具体讨论）。\n\n到了这里也很清晰了：\n\n获取当前缓存的总数量\n自减一（前面获取了锁，所以线程安全）\n删除并将更新的总数赋值到 count。\n\n其实大体上就是这个流程，Guava 并没有按照之前猜想的另起一个线程来维护过期数据。\n应该是以下原因：\n\n新起线程需要资源消耗。\n维护过期数据还要获取额外的锁，增加了消耗。\n\n而在查询时候顺带做了这些事情，但是如果该缓存迟迟没有访问也会存在数据不能被回收的情况，不过这对于一个高吞吐的应用来说也不是问题。\n总结最后再来总结下 Guava 的 Cache。\n其实在上文跟代码时会发现通过一个 key 定位数据时有以下代码：\n\n如果有看过 ConcurrentHashMap 的原理 应该会想到这其实非常类似。\n其实 Guava Cache 为了满足并发场景的使用，核心的数据结构就是按照 ConcurrentHashMap 来的，这里也是一个 key 定位到一个具体位置的过程。\n\n先找到 Segment，再找具体的位置，等于是做了两次 Hash 定位。\n\n上文有一个假设是对的，它内部会维护两个队列 accessQueue,writeQueue 用于记录缓存顺序，这样才可以按照顺序淘汰数据（类似于利用 LinkedHashMap 来做 LRU 缓存）。\n同时从上文的构建方式来看，它也是构建者模式来创建对象的。\n因为作为一个给开发者使用的工具，需要有很多的自定义属性，利用构建则模式再合适不过了。\nGuava 其实还有很多东西没谈到，比如它利用 GC 来回收内存，移除数据时的回调通知等。之后再接着讨论。\n扫码关注微信公众号，第一时间获取消息。\n","categories":["Guava"],"tags":["Cache"]},{"title":"Istio 升级后踩的坑","url":"/2023/02/20/istio/istio1.12-upgrade-fix/","content":"\n背景前段时间我们将 istio 版本升级到 1.12 后导致现有的应用监控有部分数据丢失（页面上显示不出来）。\n\n一个是应用基础信息丢失。\n再一个是应用 JVM 数据丢失。\n接口维度的监控数据丢失。\n\n\n\n\n\n\n\n\n修复基础信息首先是第一个基础信息丢失的问题，页面上其实显示的是我们的一个聚合指标istio_requests_total:source:rate1m。\n\n聚合后可以将多个指标合并为一个，减少系统压力\n\n具体可以参考 Istio 的最佳实践 Observability Best Practices 有详细说明。\nspec:  groups:    - interval: 30s      name: istio.service.source.istio_requests_total      rules:        - expr: |            sum(irate(istio_requests_total&#123;reporter=&quot;source&quot;&#125;[1m]))            by (              destination_app,              source_workload_namespace,              response_code,              source_app            )          record: istio_requests_total:source:rate1m\n\n本质上是通过以上四个维度进行统计 istio_requests_total；但在升级之后查看原始数据发现丢失了 destination_app, source_app 这两个 tag。\n至于为啥丢失，查了许久，最后在升级后的资源文件 stats-filter-1.12.yaml 中找到了答案:升级后新增了 tags_to_remove 标记，将我们所需要的两个 tag 直接删掉了。\n后续在当前 namespace 下重新建一个 EnvoyFilter 资源覆盖掉默认的便能恢复这两个 tag，修复后监控页面也显示正常了。\n\nEnvoyFilter 是实时生效的，并不需要重建应用 Pod。\n\nJVM 监控JVM 数据丢失的这个应用，直接进入 Pod 查看暴露出的 metric，发现数据都有，一切正常。\njvm_memory_pool_bytes_used&#123;pool=&quot;Code Cache&quot;,&#125; 1.32126784E8jvm_memory_pool_bytes_used&#123;pool=&quot;Metaspace&quot;,&#125; 2.74250552E8jvm_memory_pool_bytes_used&#123;pool=&quot;Compressed Class Space&quot;,&#125; 3.1766024E7jvm_memory_pool_bytes_used&#123;pool=&quot;G1 Eden Space&quot;,&#125; 1.409286144E9jvm_memory_pool_bytes_used&#123;pool=&quot;G1 Survivor Space&quot;,&#125; 2.01326592E8jvm_memory_pool_bytes_used&#123;pool=&quot;G1 Old Gen&quot;,&#125; 2.583691248E9\n\n说明不是数据源的问题，那就可能是数据采集节点的问题了。\n进入VictoriaMetrics 的 target 页面发现应用确实已经下线，原来是采集的端口不通导致的。\n\n我们使用 VictoriaMetrics 代替了 Prometheus。\n\n\n而这个端口 15020 之前并未使用，我们使用的是另外一个自定义端口和端点来采集数据。\n经过查阅发现 15020 是 istio 默认的端口：\n原来在默认情况下 Istio 会为所有的数据面 Pod 加上：\nmetadata:  annotations:    prometheus.io/path: /stats/prometheus    prometheus.io/port: &quot;15020&quot;\n\n这个注解用于采集数据，由于我们是自定义的端点，所以需要修改默认行为：\n在控制面将 --set meshConfig.enablePrometheusMerge=false 设置为 false，其实官方文档已经说明，如果不是使用的标准 prometheus.io 注解，需要将这个设置为 false。\n\n修改后需要重建应用 Pod 方能生效。\n\n有了 url 这个 tag 后，接口监控页也恢复了正常。\n接口维度接口维度的数据丢失和基本数据丢失的原因类似，本质上也是原始数据中缺少了 url 这个 tag，因为我们所聚合的指标使用了 url：\n- interval: 30s  name: istio.service.source.url.istio_requests_total  rules:    - expr: |        sum(irate(istio_requests_total&#123;reporter=&quot;source&quot;&#125;[1m]))        by (          destination_app,          source_workload_namespace,          response_code,          source_app,          url        )\n最终参考了 MetricConfig 自定义了 URL 的tag.\n&#123;&quot;dimensions&quot;: &#123;  &quot;url&quot;: &quot;request.url_path&quot;&#125;,\n但这也有个大前提，当我们 tag 的指标没有在默认 tag 列表中时，需要在 Deployment 或者是 Istio 控制面中全局加入我们自定义的 tag 声明。\n比如这里新增了 url 的 tag，那么就需要在控制面中加入：\nmeshConfig:  defaultConfig:    extraStatTags:     - url\n\n\n修改了控制面后需要重新构建 Pod 后才会生效。\n\nEnvoyFilter的问题查看MetricConfig的配置后发现是可以直接去掉指标以及去掉指标中的 tag ，这个很有用，能够大大减低指标采集系统 VictoriaMetrics 的系统负载。\n于是参考了官方的示例，去掉了一些 tag，同时还去掉了指标：istio_request_messages_total。\n&#123;      &quot;tags_to_remove&quot;: [        &quot;source_principal&quot;,        &quot;source_version&quot;,        &quot;destination_principal&quot;,        &quot;destination_version&quot;,        &quot;source_workload&quot;,        &quot;source_cluster&quot;,      ]&#125;,&#123;\t&quot;name&quot;: &quot;istio_request_messages_total&quot;,\t&quot;drop&quot;: true&#125;\n但并没有生效，于是换成了在 v1.12 中新增的 Telemetry API。\n使用 Telemetry API\napiVersion: telemetry.istio.io/v1alpha1kind: Telemetrymetadata:  name: mesh-istio-test  namespace: istio-testspec:  # no selector specified, applies to all workloads  metrics:    - overrides:        - match:            metric: GRPC_REQUEST_MESSAGES            mode: CLIENT_AND_SERVER          disabled: true\n\n\n但是参考了官方文档后发现依然不能生效，GRPC_REQUEST_MESSAGES 所对应的 istio_request_messages_total 指标依然存在。\n接着在我领导查看 Istio 源码以及相关 issue 后发现 Telemetry API 和 EnvoyFilter 是不能同时存在的，也就是说会优先使用 EnvoyFilter；这也就是为什么我之前配置没有生效的原因。\n\n后初始化 EnvoyFilter\n\n正如这个 issue 中所说，需要删掉现在所有的 EnvoyFilter；删除后果然就生效了。\n新的 Telemetry API 不但语义更加清晰，功能也一样没少，借助他我们依然可以自定义、删除指标、tag 等。\napiVersion: telemetry.istio.io/v1alpha1kind: Telemetrymetadata:  name: mesh-istio-telemetry-test  namespace: testspec:  metrics:    - overrides:        - match:            metric: GRPC_RESPONSE_MESSAGES            mode: CLIENT_AND_SERVER          disabled: true        - tagOverrides:            url:              value: &quot;request.url_path&quot;        - match:            metric: ALL_METRICS          tagOverrides:            source_workload:              operation: REMOVE\n\n比如以上配置便可以删除掉 GRPC_RESPONSE_MESSAGES 指标，新增一个 url 的指标，同时在所有指标中删除了 source_workload 这个 tag。\n借助于这一个声明文件便能满足我们多个需求。\n裁剪指标后续根据我们实际需求借助于 Telemetry API 裁剪掉了许多指标和 tag，使得指标系统负载下降了一半左右。效果相当明显。\n总结本次定位修复 Istio 升级后带来的指标系统问题收获巨大，之前对 Istio 一直只停留在理论阶段，只知道他可以实现传统微服务中对接口粒度的控制，完美弥补了 k8s 只有服务层级的粗粒度控制；\n这两周下来对一个现代云原生监控系统也有了系统的认识，从 App-&gt;Pod-&gt;sidecar-&gt;VictoriaMetrics(Prometheus)-&gt;Grafana 这一套流程中每个环节都可能会出错；\n所以学无止境吧，幸好借助公司业务场景后续还有更多机会参与实践。\n","categories":["Istio"],"tags":["K8s","云原生"]},{"title":"常见的集合容器应当避免的坑","url":"/2019/07/04/java-senior/ArrayList%20VS%20LinkedList/","content":"\n前言前不久帮同事一起 review 一个 job 执行缓慢的问题时发现不少朋友在撸码实现功能时还是有需要细节不够注意，于是便有了这篇文章。\nArrayList 踩坑List&lt;String&gt; temp = new ArrayList() ;//获取一批数据List&lt;String&gt; all = getData();for(String str : all) &#123;\ttemp.add(str);&#125;\n\n首先大家看看这段代码有什么问题嘛？\n\n\n其实在大部分情况下这都是没啥问题，无非就是循环的往 ArrayList 中写入数据而已。\n但在特殊情况下，比如这里的 getData() 返回数据非常巨大时后续 temp.add(str) 就会有问题了。\n比如我们在 review 代码时发现这里返回的数据有时会高达 2000W，这时 ArrayList 写入的问题就凸显出来了。\n填坑指南大家都知道 ArrayList 是由数组实现，而数据的长度有限；需要在合适的时机对数组扩容。\n\n这里以插入到尾部为例 add(E e)。\n\n\nArrayList&lt;String&gt; temp = new ArrayList&lt;&gt;(2) ;temp.add(&quot;1&quot;);temp.add(&quot;2&quot;);temp.add(&quot;3&quot;);\n\n当我们初始化一个长度为 2 的 ArrayList ，并往里边写入三条数据时 ArrayList 就得扩容了，也就是将之前的数据复制一份到新的数组长度为 3 的数组中。\n\n\n之所以是 3 ，是因为新的长度&#x3D;原有长度 * 1.5\n\n通过源码我们可以得知 ArrayList 的默认长度为 10.\n\n但其实并不是在初始化的时候就创建了 DEFAULT_CAPACITY = 10 的数组。\n\n而是在往里边 add 第一个数据的时候会扩容到 10.\n既然知道了默认的长度为 10 ，那说明后续一旦写入到第九个元素的时候就会扩容为 10*1.5 =15。这一步为数组复制，也就是要重新开辟一块新的内存空间存放这 15 个数组。\n一旦我们频繁且数量巨大的进行写入时就会导致许多的数组复制，这个效率是极低的。\n但如果我们提前预知了可能会写入多少条数据时就可以提前避免这个问题。\n比如我们往里边写入 1000W 条数据，在初始化的时候就给定数组长度与用默认 10 的长度之间性能是差距巨大的。\n\n我用 JMH 基准测试验证如下：\n\n@Warmup(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)public class CollectionsTest &#123;    private static final int TEN_MILLION = 10000000;    @Benchmark    @BenchmarkMode(Mode.AverageTime)    @OutputTimeUnit(TimeUnit.MICROSECONDS)    public void arrayList() &#123;        List&lt;String&gt; array = new ArrayList&lt;&gt;();        for (int i = 0; i &lt; TEN_MILLION; i++) &#123;            array.add(&quot;123&quot;);        &#125;    &#125;    @Benchmark    @BenchmarkMode(Mode.AverageTime)    @OutputTimeUnit(TimeUnit.MICROSECONDS)    public void arrayListSize() &#123;        List&lt;String&gt; array = new ArrayList&lt;&gt;(TEN_MILLION);        for (int i = 0; i &lt; TEN_MILLION; i++) &#123;            array.add(&quot;123&quot;);        &#125;    &#125;    public static void main(String[] args) throws RunnerException &#123;        Options opt = new OptionsBuilder()                .include(CollectionsTest.class.getSimpleName())                .forks(1)                .build();        new Runner(opt).run();    &#125;&#125;\n\n\n根据结果可以看出预设长度的效率会比用默认的效率高上很多（这里的 Score 指执行完函数所消耗的时间）。\n所以这里强烈建议大家：在有大量数据写入 ArrayList 时，一定要初始化指定长度。\n\n再一个是一定要慎用 add(int index, E element) 向指定位置写入数据。\n\n通过源码我们可以看出，每一次写入都会将 index 后的数据往后移动一遍，其实本质也是要复制数组；\n但区别于往常规的往数组尾部写入数据，它每次都会进行数组复制，效率极低。\nLinkedList提到 ArrayList 就不得不聊下 LinkedList 这个孪生兄弟；虽说都是 List 的容器，但本质实现却完全不同。\n\nLinkedList 是由链表组成，每个节点又有头尾两个节点分别引用了前后两个节点；因此它也是一个双向链表。\n所以理论上来说它的写入非常高效，将不会有 ArrayList 中效率极低的数组复制，每次只需要移动指针即可。\n\n这里偷懒就不画图了，大家自行脑补下。\n\n对比测试坊间一直流传：\n\nLinkedList 的写入效率高于 ArrayList，所以在写大于读的时候非常适用于 LinkedList 。\n\n@Benchmark@BenchmarkMode(Mode.AverageTime)@OutputTimeUnit(TimeUnit.MICROSECONDS)public void linkedList() &#123;    List&lt;String&gt; array = new LinkedList&lt;&gt;();    for (int i = 0; i &lt; TEN_MILLION; i++) &#123;        array.add(&quot;123&quot;);    &#125;&#125;\n\n\n这里测试看下结论是否符合；同样的也是对 LinkedList 写入 1000W 次数据，通过结果来看初始化数组长度的 ArrayList 效率明显是要高于 LinkedList 。\n但这里的前提是要提前预设 ArrayList 的数组长度，避免数组扩容，这样 ArrayList 的写入效率是非常高的，而 LinkedList 的虽然不需要复制内存，但却需要创建对象，变换指针等操作。\n而查询就不用多说了，ArrayList 可以支持下标随机访问，效率非常高。\n LinkedList 由于底层不是数组，不支持通过下标访问，而是需要根据查询 index 所在的位置来判断是从头还是从尾进行遍历。\n\n但不管是哪种都得需要移动指针来一个个遍历，特别是 index 靠近中间位置时将会非常慢。\n总结高性能应用都是从小细节一点点堆砌起来的，就如这里提到的 ArrayList 的坑一样，日常使用没啥大问题，一旦数据量起来所有的小问题都会成为大问题。\n所以再总结下：\n\n再使用 ArrayList 时如果能提前预测到数据量大小，比较大时一定要指定其长度。\n尽可能避免使用 add(index,e) api，会导致复制数组，降低效率。\n再额外提一点，我们常用的另一个 Map 容器 HashMap 也是推荐要初始化长度从而避免扩容。\n\n本文所有测试代码：\nhttps://github.com/crossoverJie/JCSprout/blob/master/src/main/java/com/crossoverjie/basic/CollectionsTest.java\n你的点赞与分享是对我最大的支持\n","categories":["Java 进阶"],"tags":["Java","ArrayList","LinkedList"]},{"title":"HashMap? ConcurrentHashMap? 相信看完这篇没人能难住你！","url":"/2018/07/23/java-senior/ConcurrentHashMap/","content":"\n前言Map 这样的 Key Value 在软件开发中是非常经典的结构，常用于在内存中存放数据。\n本篇主要想讨论 ConcurrentHashMap 这样一个并发容器，在正式开始之前我觉得有必要谈谈 HashMap，没有它就不会有后面的 ConcurrentHashMap。\nHashMap众所周知 HashMap 底层是基于 数组 + 链表 组成的，不过在 jdk1.7 和 1.8 中具体实现稍有不同。\nBase 1.71.7 中的数据结构图：\n\n\n\n先来看看 1.7 中的实现。\n\n这是 HashMap 中比较核心的几个成员变量；看看分别是什么意思？\n\n初始化桶大小，因为底层是数组，所以这是数组默认的大小。\n桶最大值。\n默认的负载因子（0.75）\ntable 真正存放数据的数组。\nMap 存放数量的大小。\n桶大小，可在初始化时显式指定。\n负载因子，可在初始化时显式指定。\n\n重点解释下负载因子：\n由于给定的 HashMap 的容量大小是固定的，比如默认初始化：\npublic HashMap() &#123;    this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);&#125;public HashMap(int initialCapacity, float loadFactor) &#123;    if (initialCapacity &lt; 0)        throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; +                                           initialCapacity);    if (initialCapacity &gt; MAXIMUM_CAPACITY)        initialCapacity = MAXIMUM_CAPACITY;    if (loadFactor &lt;= 0 || Float.isNaN(loadFactor))        throw new IllegalArgumentException(&quot;Illegal load factor: &quot; +                                           loadFactor);    this.loadFactor = loadFactor;    threshold = initialCapacity;    init();&#125;\n给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。\n因此通常建议能提前预估 HashMap 的大小最好，尽量的减少扩容带来的性能损耗。\n根据代码可以看到其实真正存放数据的是 \ntransient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE; \n这个数组，那么它又是如何定义的呢？\n\nEntry 是 HashMap 中的一个内部类，从他的成员变量很容易看出：\n\nkey 就是写入时的键。\nvalue 自然就是值。\n开始的时候就提到 HashMap 是由数组和链表组成，所以这个 next 就是用于实现链表结构。\nhash 存放的是当前 key 的 hashcode。\n\n知晓了基本结构，那来看看其中重要的写入、获取函数：\nput 方法public V put(K key, V value) &#123;    if (table == EMPTY_TABLE) &#123;        inflateTable(threshold);    &#125;    if (key == null)        return putForNullKey(value);    int hash = hash(key);    int i = indexFor(hash, table.length);    for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123;        Object k;        if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123;            V oldValue = e.value;            e.value = value;            e.recordAccess(this);            return oldValue;        &#125;    &#125;    modCount++;    addEntry(hash, key, value, i);    return null;&#125;\n\n\n判断当前数组是否需要初始化。\n如果 key 为空，则 put 一个空值进去。\n根据 key 计算出 hashcode。\n根据计算出的 hashcode 定位出所在桶。\n如果桶是一个链表则需要遍历判断里面的 hashcode、key 是否和传入 key 相等，如果相等则进行覆盖，并返回原来的值。\n如果桶是空的，说明当前位置没有数据存入；新增一个 Entry 对象写入当前位置。\n\nvoid addEntry(int hash, K key, V value, int bucketIndex) &#123;    if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123;        resize(2 * table.length);        hash = (null != key) ? hash(key) : 0;        bucketIndex = indexFor(hash, table.length);    &#125;    createEntry(hash, key, value, bucketIndex);&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123;    Entry&lt;K,V&gt; e = table[bucketIndex];    table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e);    size++;&#125;\n\n当调用 addEntry 写入 Entry 时需要判断是否需要扩容。\n如果需要就进行两倍扩充，并将当前的 key 重新 hash 并定位。\n而在 createEntry 中会将当前位置的桶传入到新建的桶中，如果当前桶有值就会在位置形成链表。\nget 方法再来看看 get 函数：\npublic V get(Object key) &#123;    if (key == null)        return getForNullKey();    Entry&lt;K,V&gt; entry = getEntry(key);    return null == entry ? null : entry.getValue();&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123;    if (size == 0) &#123;        return null;    &#125;    int hash = (key == null) ? 0 : hash(key);    for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)];         e != null;         e = e.next) &#123;        Object k;        if (e.hash == hash &amp;&amp;            ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))            return e;    &#125;    return null;&#125;\n\n\n首先也是根据 key 计算出 hashcode，然后定位到具体的桶中。\n判断该位置是否为链表。\n不是链表就根据 key、key 的 hashcode 是否相等来返回值。\n为链表则需要遍历直到 key 及 hashcode 相等时候就返回值。\n啥都没取到就直接返回 null 。\n\nBase 1.8不知道 1.7 的实现大家看出需要优化的点没有？\n其实一个很明显的地方就是：\n\n当 Hash 冲突严重时，在桶上形成的链表会变的越来越长，这样在查询时的效率就会越来越低；时间复杂度为 O(N)。\n\n因此 1.8 中重点优化了这个查询效率。\n1.8 HashMap 结构图：\n\n先来看看几个核心的成员变量：\nstatic final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f;static final int TREEIFY_THRESHOLD = 8;transient Node&lt;K,V&gt;[] table;/** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;/** * The number of key-value mappings contained in this map. */transient int size;\n\n和 1.7 大体上都差不多，还是有几个重要的区别：\n\nTREEIFY_THRESHOLD 用于判断是否需要将链表转换为红黑树的阈值。\nHashEntry 修改为 Node。\n\nNode 的核心组成其实也是和 1.7 中的 HashEntry 一样，存放的都是 key value hashcode next 等数据。\n再来看看核心方法。\nput 方法\n看似要比 1.7 的复杂，我们一步步拆解：\n\n判断当前桶是否为空，空的就需要初始化（resize 中会判断是否进行初始化）。\n根据当前 key 的 hashcode 定位到具体的桶中并判断是否为空，为空表明没有 Hash 冲突就直接在当前位置创建一个新桶即可。\n如果当前桶有值（ Hash 冲突），那么就要比较当前桶中的 key、key 的 hashcode 与写入的 key 是否相等，相等就赋值给 e,在第 8 步的时候会统一进行赋值及返回。\n如果当前桶为红黑树，那就要按照红黑树的方式写入数据。\n如果是个链表，就需要将当前的 key、value 封装成一个新节点写入到当前桶的后面（形成链表）。\n接着判断当前链表的大小是否大于预设的阈值，大于时就要转换为红黑树。\n如果在遍历过程中找到 key 相同时直接退出遍历。\n如果 e != null 就相当于存在相同的 key,那就需要将值覆盖。\n最后判断是否需要进行扩容。\n\nget 方法public V get(Object key) &#123;    Node&lt;K,V&gt; e;    return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123;    Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k;    if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;        (first = tab[(n - 1) &amp; hash]) != null) &#123;        if (first.hash == hash &amp;&amp; // always check first node            ((k = first.key) == key || (key != null &amp;&amp; key.equals(k))))            return first;        if ((e = first.next) != null) &#123;            if (first instanceof TreeNode)                return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);            do &#123;                if (e.hash == hash &amp;&amp;                    ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))                    return e;            &#125; while ((e = e.next) != null);        &#125;    &#125;    return null;&#125;\n\nget 方法看起来就要简单许多了。\n\n首先将 key hash 之后取得所定位的桶。\n如果桶为空则直接返回 null 。\n否则判断桶的第一个位置(有可能是链表、红黑树)的 key 是否为查询的 key，是就直接返回 value。\n如果第一个不匹配，则判断它的下一个是红黑树还是链表。\n红黑树就按照树的查找方式返回值。\n不然就按照链表的方式遍历匹配返回值。\n\n从这两个核心方法（get&#x2F;put）可以看出 1.8 中对大链表做了优化，修改为红黑树之后查询效率直接提高到了 O(logn)。\n但是 HashMap 原有的问题也都存在，比如在并发场景下使用时容易出现死循环。\nfinal HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();for (int i = 0; i &lt; 1000; i++) &#123;    new Thread(new Runnable() &#123;        @Override        public void run() &#123;            map.put(UUID.randomUUID().toString(), &quot;&quot;);        &#125;    &#125;).start();&#125;\n\n但是为什么呢？简单分析下。\n看过上文的还记得在 HashMap 扩容的时候会调用 resize() 方法，就是这里的并发操作容易在一个桶上形成环形链表；这样当获取一个不存在的 key 时，计算出的 index 正好是环形链表的下标就会出现死循环。\n如下图：\n\n\n遍历方式还有一个值得注意的是 HashMap 的遍历方式，通常有以下几种：\nIterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator();        while (entryIterator.hasNext()) &#123;            Map.Entry&lt;String, Integer&gt; next = entryIterator.next();            System.out.println(&quot;key=&quot; + next.getKey() + &quot; value=&quot; + next.getValue());        &#125;        Iterator&lt;String&gt; iterator = map.keySet().iterator();        while (iterator.hasNext())&#123;            String key = iterator.next();            System.out.println(&quot;key=&quot; + key + &quot; value=&quot; + map.get(key));        &#125;\n强烈建议使用第一种 EntrySet 进行遍历。\n第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低。\n\n简单总结下 HashMap：无论是 1.7 还是 1.8 其实都能看出 JDK 没有对它做任何的同步操作，所以并发会出问题，甚至 1.7 中出现死循环导致系统不可用（1.8 已经修复死循环问题）。\n\n因此 JDK 推出了专项专用的 ConcurrentHashMap ，该类位于  java.util.concurrent 包下，专门用于解决并发问题。\n\n坚持看到这里的朋友算是已经把 ConcurrentHashMap 的基础已经打牢了，下面正式开始分析。\n\nConcurrentHashMapConcurrentHashMap 同样也分为 1.7 、1.8 版，两者在实现上略有不同。\nBase 1.7先来看看 1.7 的实现，下面是他的结构图：\n\n如图所示，是由 Segment 数组、HashEntry 组成，和 HashMap 一样，仍然是数组加链表。\n它的核心成员变量：\n/** * Segment 数组，存放数据时首先需要定位到具体的 Segment 中。 */final Segment&lt;K,V&gt;[] segments;transient Set&lt;K&gt; keySet;transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;\n\n\nSegment 是 ConcurrentHashMap 的一个内部类，主要的组成如下：\n   static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123;       private static final long serialVersionUID = 2249069246763182397L;              // 和 HashMap 中的 HashEntry 作用一样，真正存放数据的桶       transient volatile HashEntry&lt;K,V&gt;[] table;       transient int count;       transient int modCount;       transient int threshold;       final float loadFactor;       &#125;\n\n\n看看其中 HashEntry 的组成：\n\n和 HashMap 非常类似，唯一的区别就是其中的核心数据如 value ，以及链表都是 volatile 修饰的，保证了获取时的可见性。\n原理上来说：ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。\n下面也来看看核心的 put get 方法。\nput 方法public V put(K key, V value) &#123;    Segment&lt;K,V&gt; s;    if (value == null)        throw new NullPointerException();    int hash = hash(key);    int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask;    if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject          // nonvolatile; recheck         (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) //  in ensureSegment        s = ensureSegment(j);    return s.put(key, hash, value, false);&#125;\n\n首先是通过 key 定位到 Segment，之后在对应的 Segment 中进行具体的 put。\nfinal V put(K key, int hash, V value, boolean onlyIfAbsent) &#123;    HashEntry&lt;K,V&gt; node = tryLock() ? null :        scanAndLockForPut(key, hash, value);    V oldValue;    try &#123;        HashEntry&lt;K,V&gt;[] tab = table;        int index = (tab.length - 1) &amp; hash;        HashEntry&lt;K,V&gt; first = entryAt(tab, index);        for (HashEntry&lt;K,V&gt; e = first;;) &#123;            if (e != null) &#123;                K k;                if ((k = e.key) == key ||                    (e.hash == hash &amp;&amp; key.equals(k))) &#123;                    oldValue = e.value;                    if (!onlyIfAbsent) &#123;                        e.value = value;                        ++modCount;                    &#125;                    break;                &#125;                e = e.next;            &#125;            else &#123;                if (node != null)                    node.setNext(first);                else                    node = new HashEntry&lt;K,V&gt;(hash, key, value, first);                int c = count + 1;                if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY)                    rehash(node);                else                    setEntryAt(tab, index, node);                ++modCount;                count = c;                oldValue = null;                break;            &#125;        &#125;    &#125; finally &#123;        unlock();    &#125;    return oldValue;&#125;\n\n虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。\n首先第一步的时候会尝试获取锁，如果获取失败肯定就有其他线程存在竞争，则利用 scanAndLockForPut() 自旋获取锁。\n\n\n尝试自旋获取锁。\n如果重试的次数达到了 MAX_SCAN_RETRIES 则改为阻塞锁获取，保证能获取成功。\n\n\n再结合图看看 put 的流程。\n\n将当前 Segment 中的 table 通过 key 的 hashcode 定位到 HashEntry。\n遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value。\n不为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容。\n最后会解除在 1 中所获取当前 Segment 的锁。\n\nget 方法public V get(Object key) &#123;    Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead    HashEntry&lt;K,V&gt;[] tab;    int h = hash(key);    long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE;    if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp;        (tab = s.table) != null) &#123;        for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile                 (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE);             e != null; e = e.next) &#123;            K k;            if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k)))                return e.value;        &#125;    &#125;    return null;&#125;\n\nget 逻辑比较简单：\n只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。\n由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。\nConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。\nBase 1.81.7 已经解决了并发问题，并且能支持 N 个 Segment 这么多次数的并发，但依然存在 HashMap 在 1.7 版本中的问题。\n\n那就是查询遍历链表效率太低。\n\n因此 1.8 做了一些数据结构上的调整。\n首先来看下底层的组成结构：\n\n看起来是不是和 1.8 HashMap 结构类似？\n其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。\n\n也将 1.7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。\n其中的 val next 都用了 volatile 修饰，保证了可见性。\nput 方法重点来看看 put 函数：\n\n\n根据 key 计算出 hashcode 。\n判断是否需要进行初始化。\nf 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。\n如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。\n如果都不满足，则利用 synchronized 锁写入数据。\n如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。\n\nget 方法\n\n根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。\n如果是红黑树那就按照树的方式获取值。\n就不满足那就按照链表的方式遍历获取值。\n\n\n1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。\n\n总结看完了整个 HashMap 和 ConcurrentHashMap 在 1.7 和 1.8 中不同的实现方式相信大家对他们的理解应该会更加到位。\n其实这块也是面试的重点内容，通常的套路是：\n\n谈谈你理解的 HashMap，讲讲其中的 get put 过程。\n1.8 做了什么优化？\n是线程安全的嘛？\n不安全会导致哪些问题？\n如何解决？有没有线程安全的并发容器？\nConcurrentHashMap 是如何实现的？ 1.7、1.8 实现有何不同？为什么这么做？\n\n这一串问题相信大家仔细看完都能怼回面试官。\n除了面试会问到之外平时的应用其实也蛮多，像之前谈到的 Guava 中 Cache 的实现就是利用 ConcurrentHashMap 的思想。\n同时也能学习 JDK 作者大牛们的优化思路以及并发解决方案。\n\n其实写这篇的前提是源于 GitHub 上的一个 Issues，也希望大家能参与进来，共同维护好这个项目。\n\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n欢迎关注公众号一起交流：\n","categories":["Java 进阶"],"tags":["Java","concurrent","ConcurrentHashMap","HashMap"]},{"title":"一次线上问题排查所引发的思考","url":"/2018/07/08/java-senior/JVM-Troubleshoot/","content":"\n前言之前或多或少分享过一些内存模型、对象创建之类的内容，其实大部分人看完都是懵懵懂懂，也不知道这些的实际意义。\n直到有一天你会碰到线上奇奇怪怪的问题，如：\n\n线程执行一个任务迟迟没有返回，应用假死。\n接口响应缓慢，甚至请求超时。\nCPU 高负载运行。\n\n这类问题并不像一个空指针、数组越界这样明显好查，这时就需要刚才提到的内存模型、对象创建、线程等相关知识结合在一起来排查问题了。\n正好这次借助之前的一次生产问题来聊聊如何排查和解决问题。\n生产现象首先看看问题的背景吧：\n我这其实是一个定时任务，在固定的时间会开启 N 个线程并发的从 Redis 中获取数据进行运算。\n业务逻辑非常简单，但应用一般涉及到多线程之后再简单的事情都要小心对待。\n果不其然这次就出问题了。\n现象:原本只需要执行几分钟的任务执行了几个小时都没退出。翻遍了所有的日志都没找到异常。\n于是便开始定位问题之路。\n\n\n\n定位问题既然没办法直接从日志中发现异常，那就只能看看应用到底在干嘛了。\n最常见的工具就是 JDK 自带的那一套。\n这次我使用了 jstack 来查看线程的执行情况，它的作用其实就是 dump 当前的线程堆栈。\n当然在 dump 之前是需要知道我应用的 pid 的，可以使用 jps -v 这样的方式列出所有的 Java 进程。\n当然如果知道关键字的话直接使用 ps aux|grep java 也是可以的。\n拿到 pid=1523 了之后就可以利用 jstack 1523 &gt; 1523.log 这样的方式将 dump 文件输出到日志文件中。\n如果应用简单不复杂，线程这些也比较少其实可以直接打开查看。\n但复杂的应用导出来的日志文件也比较大还是建议用专业的分析工具。\n我这里的日志比较少直接打开就可以了。\n因为我清楚知道应用中开启的线程名称，所以直接根据线程名就可以在日志中找到相关的堆栈：\n\n\n所以通常建议大家线程名字给的有意义，在排查问题时很有必要。\n\n其实其他几个线程都和这里的堆栈类似，很明显的看出都是在做 Redis 连接。\n于是我登录 Redis 查看了当前的连接数，发现已经非常高了。\n这样 Redis 的响应自然也就变慢了。\n接着利用 jps -v 列出了当前所以在跑的 Java 进程，果不其然有好几个应用都在查询 Redis，而且都是并发连接，问题自然就找到了。\n解决办法\n所以问题的主要原因是：大量的应用并发查询 Redis，导致 Redis 的性能降低。\n\n既然找到了问题，那如何解决呢？\n\n减少同时查询 Redis 的应用，分开时段降低 Redis 的压力。\n将 Redis 复制几个集群，各个应用分开查询。但是这样会涉及到数据的同步等运维操作，或者由程序了进行同步也会增加复杂度。\n\n目前我们选择的是第一个方案，效果很明显。\n本地模拟上文介绍的是线程相关问题，现在来分析下内存的问题。\n以这个类为例：\nhttps://github.com/crossoverJie/Java-Interview/blob/master/src/main/java/com/crossoverjie/oom/heap/HeapOOM.java\npublic class HeapOOM &#123;    public static void main(String[] args) &#123;        List&lt;String&gt; list = new ArrayList&lt;&gt;(10) ;        while (true)&#123;            list.add(&quot;1&quot;) ;        &#125;    &#125;&#125;\n\n启动参数如下：\n-Xms20m-Xmx20m-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=/Users/xx/Documents\n\n为了更快的突出内存问题将堆的最大内存固定在 20M，同时在 JVM 出现 OOM 的时候自动 dump 内存到 /Users/xx/Documents(不配路径则会生成在当前目录)。\n执行之后果不其然出现了异常：\n\n同时对应的内存 dump 文件也生成了。\n内存分析这时就需要相应的工具进行分析了，最常用的自然就是 MAT 了。\n我试了一个在线工具也不错（文件大了就不适合了）：\nhttp://heaphero.io/index.jsp\n上传刚才生成的内存文件之后：\n\n因为是内存溢出，所以主要观察下大对象：\n\n也有相应提示，这个很有可能就是内存溢出的对象，点进去之后：\n\n看到这个堆栈其实就很明显了：\n在向 ArrayList 中不停的写入数据时，会导致频繁的扩容也就是数组复制这些过程，最终达到 20M 的上限导致内存溢出了。\n更多建议上文说过，一旦使用了多线程，那就要格外小心。\n以下是一些日常建议：\n\n尽量不要在线程中做大量耗时的网络操作，如查询数据库（可以的话在一开始就将数据从从 DB 中查出准备好）。\n尽可能的减少多线程竞争锁。可以将数据分段，各个线程分别读取。\n多利用 CAS+自旋 的方式更新数据，减少锁的使用。\n应用中加上 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp 参数，在内存溢出时至少可以拿到内存日志。\n线程池监控。如线程池大小、队列大小、最大线程数等数据，可提前做好预估。\nJVM 监控，可以看到堆内存的涨幅趋势，GC 曲线等数据，也可以提前做好准备。\n\n总结线上问题定位需要综合技能，所以是需要一些基础技能。如线程、内存模型、Linux 等。\n当然这些问题没有实操过都是纸上谈兵；如果第一次碰到线上问题，不要慌张，反而应该庆幸解决之后你又会习得一项技能。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n欢迎关注公众号一起交流：\n","categories":["Java 进阶"],"tags":["Java","Thread","concurrent","JVM"]},{"title":"一次 HashSet 所引起的并发问题","url":"/2018/11/08/java-senior/JVM-concurrent-HashSet-problem/","content":"\n背景上午刚到公司，准备开始一天的摸鱼之旅时突然收到了一封监控中心的邮件。\n心中暗道不好，因为监控系统从来不会告诉我应用完美无 bug，其实系统挺猥琐。\n打开邮件一看，果然告知我有一个应用的线程池队列达到阈值触发了报警。\n由于这个应用出问题非常影响用户体验；于是立马让运维保留现场 dump 线程和内存同时重启应用，还好重启之后恢复正常。于是开始着手排查问题。\n\n\n分析首先了解下这个应用大概是做什么的。\n简单来说就是从 MQ 中取出数据然后丢到后面的业务线程池中做具体的业务处理。\n而报警的队列正好就是这个线程池的队列。\n跟踪代码发现构建线程池的方式如下：\nThreadPoolExecutor executor = new ThreadPoolExecutor(coreSize, maxSize,              0L, TimeUnit.MILLISECONDS,              new LinkedBlockingQueue&lt;Runnable&gt;());;             put(poolName,executor);\n\n采用的是默认的 LinkedBlockingQueue 并没有指定大小（这也是个坑），于是这个队列的默认大小为 Integer.MAX_VALUE。\n由于应用已经重启，只能从仅存的线程快照和内存快照进行分析。\n内存分析先利用 MAT 分析了内存，的到了如下报告。\n\n其中有两个比较大的对象，一个就是之前线程池存放任务的 LinkedBlockingQueue，还有一个则是 HashSet。\n当然其中队列占用了大量的内存，所以优先查看，HashSet 一会儿再看。\n\n 由于队列的大小给的够大，所以结合目前的情况来看应当是线程池里的任务处理较慢，导致队列的任务越堆越多，至少这是目前可以得出的结论。\n\n线程分析再来看看线程的分析，这里利用 fastthread.io 这个网站进行线程分析。\n因为从表现来看线程池里的任务迟迟没有执行完毕，所以主要看看它们在干嘛。\n正好他们都处于 RUNNABLE 状态，同时堆栈如下：\n\n发现正好就是在处理上文提到的 HashSet，看这个堆栈是在查询 key 是否存在。通过查看 312 行的业务代码确实也是如此。\n\n这里的线程名字也是个坑，让我找了好久。\n\n定位分析了内存和线程的堆栈之后其实已经大概猜出一些问题了。\n这里其实有一个前提忘记讲到：\n这个告警是凌晨三点发出的邮件，但并没有电话提醒之类的，所以大家都不知道。\n到了早上上班时才发现并立即 dump 了上面的证据。\n所有有一个很重要的事实：这几个业务线程在查询 HashSet 的时候运行了 6 7 个小时都没有返回。\n通过之前的监控曲线图也可以看出：\n\n操作系统在之前一直处于高负载中，直到我们早上看到报警重启之后才降低。\n同时发现这个应用生产上运行的是 JDK1.7 ，所以我初步认为应该是在查询 key 的时候进入了 HashMap 的环形链表导致 CPU 高负载同时也进入了死循环。\n为了验证这个问题再次 review 了代码。\n整理之后的伪代码如下：\n//线程池private ExecutorService executor;private Set&lt;String&gt; set = new hashSet();private void execute()&#123;\t\twhile(true)&#123;\t\t//从 MQ 中获取数据\t\tString key = subMQ();\t\texecutor.excute(new Worker(key)) ;\t&#125;&#125;public class Worker extends Thread&#123;\tprivate String key ;\tpublic Worker(String key)&#123;\t\tthis.key = key;\t&#125;\t@Override\tprivate void run()&#123;\t\tif(!set.contains(key))&#123;\t\t\t//数据库查询\t\t\tif(queryDB(key))&#123;\t\t\t\tset.add(key);\t\t\t\treturn;\t\t\t&#125;\t\t&#125;\t\t//达到某种条件时清空 set\t\tif(flag)&#123;\t\t\tset = null ;\t\t&#125;\t&#125;\t&#125;\n\n大致的流程如下：\n\n源源不断的从 MQ 中获取数据。\n将数据丢到业务线程池中。\n判断数据是否已经写入了 Set。\n没有则查询数据库。\n之后写入到 Set 中。\n\n这里有一个很明显的问题，那就是作为共享资源的 Set 并没有做任何的同步处理。\n这里会有多个线程并发的操作，由于 HashSet 其实本质上就是 HashMap，所以它肯定是线程不安全的，所以会出现两个问题：\n\nSet 中的数据在并发写入时被覆盖导致数据不准确。\n会在扩容的时候形成环形链表。\n\n第一个问题相对于第二个还能接受。\n通过上文的内存分析我们已经知道这个 set 中的数据已经不少了。同时由于初始化时并没有指定大小，仅仅只是默认值，所以在大量的并发写入时候会导致频繁的扩容，而在 1.7 的条件下又可能会形成环形链表。\n不巧的是代码中也有查询操作（contains()）,观察上文的堆栈情况：\n\n发现是运行在 HashMap 的 465 行，来看看 1.7 中那里具体在做什么：\n\n已经很明显了。这里在遍历链表，同时由于形成了环形链表导致这个 e.next 永远不为空，所以这个循环也不会退出了。\n到这里其实已经找到问题了，但还有一个疑问是为什么线程池里的任务队列会越堆越多。我第一直觉是任务执行太慢导致的。\n仔细查看了代码发现只有一个地方可能会慢：也就是有一个数据库的查询。\n把这个 SQL 拿到生产环境执行发现确实不快，查看索引发现都有命中。\n但我一看表中的数据发现已经快有 7000W 的数据了。同时经过运维得知 MySQL 那台服务器的 IO 压力也比较大。\n所以这个原因也比较明显了：\n\n 由于每消费一条数据都要去查询一次数据库，MySQL 本身压力就比较大，加上数据量也很高所以导致这个 IO 响应较慢，导致整个任务处理的就比较慢了。\n\n但还有一个原因也不能忽视；由于所有的业务线程在某个时间点都进入了死循环，根本没有执行完任务的机会，而后面的数据还在源源不断的进入，所以这个队列只会越堆越多！\n这其实是一个老应用了，可能会有人问为什么之前没出现问题。\n这是因为之前数据量都比较少，即使是并发写入也没有出现并发扩容形成环形链表的情况。这段时间业务量的暴增正好把这个隐藏的雷给揪出来了。所以还是得信墨菲他老人家的话。\n总结至此整个排查结束，而我们后续的调整措施大概如下：\n\nHashSet 不是线程安全的，换为 ConcurrentHashMap同时把 value 写死一样可以达到 set 的效果。\n根据我们后面的监控，初始化 ConcurrentHashMap 的大小尽量大一些，避免频繁的扩容。\nMySQL 中很多数据都已经不用了，进行冷热处理。尽量降低单表数据量。同时后期考虑分表。\n查数据那里调整为查缓存，提高查询效率。\n线程池的名称一定得取的有意义，不然是自己给自己增加难度。\n根据监控将线程池的队列大小调整为一个具体值，并且要有拒绝策略。\n升级到 JDK1.8。\n再一个是报警邮件酌情考虑为电话通知😂。\n\nHashMap 的死循环问题在网上层出不穷，没想到还真被我遇到了。现在要满足这个条件还是挺少见的，比如 1.8 以下的 JDK 这一条可能大多数人就碰不到，正好又证实了一次墨菲定律。\n同时我会将文章更到这里，方便大家阅读和查询。\nhttps://crossoverjie.top/JCSprout/\n你的点赞与分享是对我最大的支持\n","categories":["Java 进阶"],"tags":["Java","Thread","concurrent","HashMap","JVM","HashSet"]},{"title":"强如 Disruptor 也发生内存溢出？","url":"/2018/08/29/java-senior/OOM-Disruptor/","content":"\n前言OutOfMemoryError 问题相信很多朋友都遇到过，相对于常见的业务异常（数组越界、空指针等）来说这类问题是很难定位和解决的。\n本文以最近碰到的一次线上内存溢出的定位、解决问题的方式展开；希望能对碰到类似问题的同学带来思路和帮助。\n主要从表现--&gt;排查--&gt;定位--&gt;解决 四个步骤来分析和解决问题。\n\n\n表象最近我们生产上的一个应用不断的爆出内存溢出，并且随着业务量的增长出现的频次越来越高。\n该程序的业务逻辑非常简单，就是从 Kafka 中将数据消费下来然后批量的做持久化操作。\n而现象则是随着 Kafka 的消息越多，出现的异常的频次就越快。由于当时还有其他工作所以只能让运维做重启，并且监控好堆内存以及 GC 情况。\n\n重启大法虽好，可是依然不能根本解决问题。\n\n排查于是我们想根据运维之前收集到的内存数据、GC 日志尝试判断哪里出现问题。\n\n结果发现老年代的内存使用就算是发生 GC 也一直居高不下，而且随着时间推移也越来越高。\n结合 jstat 的日志发现就算是发生了 FGC 老年代也已经回收不了，内存已经到顶。\n\n甚至有几台应用 FGC 达到了上百次，时间也高的可怕。\n这说明应用的内存使用肯定是有问题的，有许多赖皮对象始终回收不掉。\n定位由于生产上的内存 dump 文件非常大，达到了几十G。也是由于我们的内存设置太大有关。\n所以导致想使用 MAT 分析需要花费大量时间。\n因此我们便想是否可以在本地复现，这样就要好定位的多。\n为了尽快的复现问题，我将本地应用最大堆内存设置为 150M。\n然后在消费 Kafka 那里 Mock 为一个 while 循环一直不断的生成数据。\n同时当应用启动之后利用 VisualVM 连上应用实时监控内存、GC 的使用情况。\n结果跑了 10 几分钟内存使用并没有什么问题。根据图中可以看出，每产生一次 GC 内存都能有效的回收，所以这样并没有复现问题。\n\n没法复现问题就很难定位了。于是我们 review 代码，发现生产的逻辑和我们用 while 循环 Mock 数据还不太一样。\n查看生产的日志发现每次从 Kafka 中取出的都是几百条数据，而我们 Mock 时每次只能产生一条。\n为了尽可能的模拟生产情况便在服务器上跑着一个生产者程序，一直源源不断的向 Kafka 中发送数据。\n果然不出意外只跑了一分多钟内存就顶不住了，观察左图发现 GC 的频次非常高，但是内存的回收却是相形见拙。\n\n同时后台也开始打印内存溢出了，这样便复现出问题。\n解决从目前的表现来看就是内存中有许多对象一直存在强引用关系导致得不到回收。\n于是便想看看到底是什么对象占用了这么多的内存，利用 VisualVM 的 HeapDump 功能可以立即 dump 出当前应用的内存情况。\n\n结果发现 com.lmax.disruptor.RingBuffer 类型的对象占用了将近 50% 的内存。\n看到这个包自然就想到了 Disruptor 环形队列。\n再次 review 代码发现：从 Kafka 里取出的 700 条数据是直接往 Disruptor 里丢的。\n这里也就能说明为什么第一次模拟数据没复现问题了。\n模拟的时候是一个对象放进队列里，而生产的情况是 700 条数据放进队列里。这个数据量是 700 倍的差距。\n而 Disruptor 作为一个环形队列，再对象没有被覆盖之前是一直存在的。\n我也做了一个实验，证明确实如此。\n\n我设置队列大小为 8 ，从 0~9 往里面写 10 条数据，当写到 8 的时候就会把之前 0 的位置覆盖掉，后面的以此类推（类似于 HashMap 的取模定位）。\n所以在生产上假设我们的队列大小是 1024，那么随着系统的运行最终肯定会导致 1024 个位置上装满了对象，而且每个位置是 700 个！\n于是查看了生产上 Disruptor 的 RingBuffer 配置，结果是：1024*1024。\n这个数量级就非常吓人了。\n为了验证是否是这个问题，我在本地将该值换为 2 ，一个最小值试试。\n同样的 128M 内存，也是通过 Kafka 一直源源不断的取出数据。通过监控如下：\n\n跑了 20 几分钟系统一切正常，每当一次 GC 都能回收大部分内存，最终呈现锯齿状。\n这样问题就找到了，不过生产上这个值具体设置多少还得根据业务情况测试才能知道，但原有的 1024*1024 是绝对不能再使用了。\n总结虽然到了最后也就改了一行代码(还没改，直接修改配置)，但这排查过程我觉得是有意义的。\n也会让大部分觉得 JVM 这样的黑盒难以下手的同学有一个直观的感受。\n同时也得感叹 Disruptor 东西虽好，也不能乱用哦！\n相关演示代码查看：\nhttps://github.com/crossoverJie/JCSprout/tree/master/src/main/java/com/crossoverjie/disruptor\n你的点赞与转发是最大的支持。\n","categories":["Java 进阶"],"tags":["Java","Thread","concurrent","JVM","OOM"]},{"title":"如何优雅的使用和理解线程池","url":"/2018/07/29/java-senior/ThreadPool/","content":"\n前言平时接触过多线程开发的童鞋应该都或多或少了解过线程池，之前发布的《阿里巴巴 Java 手册》里也有一条：\n\n可见线程池的重要性。\n简单来说使用线程池有以下几个目的：\n\n线程是稀缺资源，不能频繁的创建。\n解耦作用；线程的创建于执行完全分开，方便维护。\n应当将其放入一个池子中，可以给其他任务进行复用。\n\n线程池原理谈到线程池就会想到池化技术，其中最核心的思想就是把宝贵的资源放到一个池子中；每次使用都从里面获取，用完之后又放回池子供其他人使用，有点吃大锅饭的意思。\n那在 Java 中又是如何实现的呢？\n在 JDK 1.5 之后推出了相关的 api，常见的创建线程池方式有以下几种：\n\nExecutors.newCachedThreadPool()：无限线程池。\nExecutors.newFixedThreadPool(nThreads)：创建固定大小的线程池。\nExecutors.newSingleThreadExecutor()：创建单个线程的线程池。\n\n\n\n其实看这三种方式创建的源码就会发现：\npublic static ExecutorService newCachedThreadPool() &#123;    return new ThreadPoolExecutor(0, Integer.MAX_VALUE,                                  60L, TimeUnit.SECONDS,                                  new SynchronousQueue&lt;Runnable&gt;());&#125;\n\n实际上还是利用 ThreadPoolExecutor 类实现的。\n所以我们重点来看下 ThreadPoolExecutor 是怎么玩的。\n首先是创建线程的 api：\nThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) \n\n这几个核心参数的作用：\n\ncorePoolSize 为线程池的基本大小。\nmaximumPoolSize 为线程池最大线程大小。\nkeepAliveTime 和 unit 则是线程空闲后的存活时间。\nworkQueue 用于存放任务的阻塞队列。\nhandler 当队列和最大线程池都满了之后的饱和策略。\n\n了解了这几个参数再来看看实际的运用。\n通常我们都是使用:\nthreadPool.execute(new Job());\n\n这样的方式来提交一个任务到线程池中，所以核心的逻辑就是 execute() 函数了。\n在具体分析之前先了解下线程池中所定义的状态，这些状态都和线程的执行密切相关：\n\n\nRUNNING 自然是运行状态，指可以接受任务执行队列里的任务\nSHUTDOWN 指调用了 shutdown() 方法，不再接受新任务了，但是队列里的任务得执行完毕。\nSTOP 指调用了 shutdownNow() 方法，不再接受新任务，同时抛弃阻塞队列里的所有任务并中断所有正在执行任务。\nTIDYING 所有任务都执行完毕，在调用 shutdown()/shutdownNow() 中都会尝试更新为这个状态。\nTERMINATED 终止状态，当执行 terminated() 后会更新为这个状态。\n\n用图表示为：\n\n然后看看 execute() 方法是如何处理的：\n\n\n获取当前线程池的状态。\n当前线程数量小于 coreSize 时创建一个新的线程运行。\n如果当前线程处于运行状态，并且写入阻塞队列成功。\n双重检查，再次获取线程状态；如果线程状态变了（非运行状态）就需要从阻塞队列移除任务，并尝试判断线程是否全部执行完毕。同时执行拒绝策略。\n如果当前线程池为空就新创建一个线程并执行。\n如果在第三步的判断为非运行状态，尝试新建线程，如果失败则执行拒绝策略。\n\n这里借助《聊聊并发》的一张图来描述这个流程：\n\n如何配置线程流程聊完了再来看看上文提到了几个核心参数应该如何配置呢？\n有一点是肯定的，线程池肯定是不是越大越好。\n通常我们是需要根据这批任务执行的性质来确定的。\n\nIO 密集型任务：由于线程并不是一直在运行，所以可以尽可能的多配置线程，比如 CPU 个数 * 2 \nCPU 密集型任务（大量复杂的运算）应当分配较少的线程，比如 CPU 个数相当的大小。\n\n当然这些都是经验值，最好的方式还是根据实际情况测试得出最佳配置。\n优雅的关闭线程池有运行任务自然也有关闭任务，从上文提到的 5 个状态就能看出如何来关闭线程池。\n其实无非就是两个方法 shutdown()/shutdownNow()。\n但他们有着重要的区别：\n\nshutdown() 执行后停止接受新任务，会把队列的任务执行完毕。\nshutdownNow() 也是停止接受新任务，但会中断所有的任务，将线程池状态变为 stop。\n\n\n两个方法都会中断线程，用户可自行判断是否需要响应中断。\n\nshutdownNow() 要更简单粗暴，可以根据实际场景选择不同的方法。\n我通常是按照以下方式关闭线程池的：\nlong start = System.currentTimeMillis();for (int i = 0; i &lt;= 5; i++) &#123;    pool.execute(new Job());&#125;pool.shutdown();while (!pool.awaitTermination(1, TimeUnit.SECONDS)) &#123;    LOGGER.info(&quot;线程还在执行。。。&quot;);&#125;long end = System.currentTimeMillis();LOGGER.info(&quot;一共处理了【&#123;&#125;】&quot;, (end - start));\n\npool.awaitTermination(1, TimeUnit.SECONDS) 会每隔一秒钟检查一次是否执行完毕（状态为 TERMINATED），当从 while 循环退出时就表明线程池已经完全终止了。\nSpringBoot 使用线程池2018 年了，SpringBoot 盛行；来看看在 SpringBoot 中应当怎么配置和使用线程池。\n既然用了 SpringBoot ，那自然得发挥 Spring 的特性，所以需要 Spring 来帮我们管理线程池：\n@Configurationpublic class TreadPoolConfig &#123;    /**     * 消费队列线程     * @return     */    @Bean(value = &quot;consumerQueueThreadPool&quot;)    public ExecutorService buildConsumerQueueThreadPool()&#123;        ThreadFactory namedThreadFactory = new ThreadFactoryBuilder()                .setNameFormat(&quot;consumer-queue-thread-%d&quot;).build();        ExecutorService pool = new ThreadPoolExecutor(5, 5, 0L, TimeUnit.MILLISECONDS,                new ArrayBlockingQueue&lt;Runnable&gt;(5),namedThreadFactory,new ThreadPoolExecutor.AbortPolicy());        return pool ;    &#125;&#125;\n\n使用时：\n@Resource(name = &quot;consumerQueueThreadPool&quot;)private ExecutorService consumerQueueThreadPool;@Overridepublic void execute() &#123;    //消费队列    for (int i = 0; i &lt; 5; i++) &#123;        consumerQueueThreadPool.execute(new ConsumerQueueThread());    &#125;&#125;\n\n其实也挺简单，就是创建了一个线程池的 bean，在使用时直接从 Spring 中取出即可。\n监控线程池谈到了 SpringBoot，也可利用它 actuator 组件来做线程池的监控。\n线程怎么说都是稀缺资源，对线程池的监控可以知道自己任务执行的状况、效率等。\n关于 actuator 就不再细说了，感兴趣的可以看看这篇，有详细整理过如何暴露监控端点。\n其实 ThreadPool 本身已经提供了不少 api 可以获取线程状态：\n\n很多方法看名字就知道其含义，只需要将这些信息暴露到 SpringBoot 的监控端点中，我们就可以在可视化页面查看当前的线程池状态了。\n甚至我们可以继承线程池扩展其中的几个函数来自定义监控逻辑：\n\n\n看这些名称和定义都知道，这是让子类来实现的。\n可以在线程执行前、后、终止状态执行自定义逻辑。\n线程池隔离\n线程池看似很美好，但也会带来一些问题。\n\n如果我们很多业务都依赖于同一个线程池,当其中一个业务因为各种不可控的原因消耗了所有的线程，导致线程池全部占满。\n这样其他的业务也就不能正常运转了，这对系统的打击是巨大的。\n比如我们 Tomcat 接受请求的线程池，假设其中一些响应特别慢，线程资源得不到回收释放；线程池慢慢被占满，最坏的情况就是整个应用都不能提供服务。\n所以我们需要将线程池进行隔离。\n通常的做法是按照业务进行划分：\n\n比如下单的任务用一个线程池，获取数据的任务用另一个线程池。这样即使其中一个出现问题把线程池耗尽，那也不会影响其他的任务运行。\n\nhystrix 隔离这样的需求 Hystrix 已经帮我们实现了。\n\nHystrix 是一款开源的容错插件，具有依赖隔离、系统容错降级等功能。\n\n下面来看看 Hystrix 简单的应用：\n首先需要定义两个线程池，分别用于执行订单、处理用户。\n/** * Function:订单服务 * * @author crossoverJie *         Date: 2018/7/28 16:43 * @since JDK 1.8 */public class CommandOrder extends HystrixCommand&lt;String&gt; &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(CommandOrder.class);    private String orderName;    public CommandOrder(String orderName) &#123;        super(Setter.withGroupKey(                //服务分组                HystrixCommandGroupKey.Factory.asKey(&quot;OrderGroup&quot;))                //线程分组                .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(&quot;OrderPool&quot;))                //线程池配置                .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter()                        .withCoreSize(10)                        .withKeepAliveTimeMinutes(5)                        .withMaxQueueSize(10)                        .withQueueSizeRejectionThreshold(10000))                .andCommandPropertiesDefaults(                        HystrixCommandProperties.Setter()                                .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.THREAD))        )        ;        this.orderName = orderName;    &#125;    @Override    public String run() throws Exception &#123;        LOGGER.info(&quot;orderName=[&#123;&#125;]&quot;, orderName);        TimeUnit.MILLISECONDS.sleep(100);        return &quot;OrderName=&quot; + orderName;    &#125;&#125;/** * Function:用户服务 * * @author crossoverJie *         Date: 2018/7/28 16:43 * @since JDK 1.8 */public class CommandUser extends HystrixCommand&lt;String&gt; &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(CommandUser.class);    private String userName;    public CommandUser(String userName) &#123;        super(Setter.withGroupKey(                //服务分组                HystrixCommandGroupKey.Factory.asKey(&quot;UserGroup&quot;))                //线程分组                .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(&quot;UserPool&quot;))                //线程池配置                .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter()                        .withCoreSize(10)                        .withKeepAliveTimeMinutes(5)                        .withMaxQueueSize(10)                        .withQueueSizeRejectionThreshold(10000))                //线程池隔离                .andCommandPropertiesDefaults(                        HystrixCommandProperties.Setter()                                .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.THREAD))        )        ;        this.userName = userName;    &#125;    @Override    public String run() throws Exception &#123;        LOGGER.info(&quot;userName=[&#123;&#125;]&quot;, userName);        TimeUnit.MILLISECONDS.sleep(100);        return &quot;userName=&quot; + userName;    &#125;&#125;\n\n\napi 特别简洁易懂，具体详情请查看官方文档。\n然后模拟运行：\npublic static void main(String[] args) throws Exception &#123;    CommandOrder commandPhone = new CommandOrder(&quot;手机&quot;);    CommandOrder command = new CommandOrder(&quot;电视&quot;);    //阻塞方式执行    String execute = commandPhone.execute();    LOGGER.info(&quot;execute=[&#123;&#125;]&quot;, execute);    //异步非阻塞方式    Future&lt;String&gt; queue = command.queue();    String value = queue.get(200, TimeUnit.MILLISECONDS);    LOGGER.info(&quot;value=[&#123;&#125;]&quot;, value);    CommandUser commandUser = new CommandUser(&quot;张三&quot;);    String name = commandUser.execute();    LOGGER.info(&quot;name=[&#123;&#125;]&quot;, name);&#125;\n\n\n运行结果：\n\n可以看到两个任务分成了两个线程池运行，他们之间互不干扰。\n获取任务任务结果支持同步阻塞和异步非阻塞方式，可自行选择。\n它的实现原理其实容易猜到：\n\n利用一个 Map 来存放不同业务对应的线程池。\n\n通过刚才的构造函数也能证明：\n\n还要注意的一点是：\n\n自定义的 Command 并不是一个单例，每次执行需要 new 一个实例，不然会报  This instance can only be executed once. Please instantiate a new instance. 异常。\n\n总结池化技术确实在平时应用广泛，熟练掌握能提高不少效率。\n文末的 hystrix 源码：\nhttps://github.com/crossoverJie/Java-Interview/tree/master/src/main/java/com/crossoverjie/hystrix\n最后插播个小广告：\nJava-Interview 截止目前将近 8K star。\n\n这次定个小目标：争取冲击 1W star。\n感谢各位老铁的支持与点赞。\n欢迎关注公众号一起交流：\n","categories":["Java 进阶"],"tags":["Java","SpringBoot","ThreadPool","Hystirx"]},{"title":"不改一行代码定位线上性能问题","url":"/2018/11/12/java-senior/coding-online-analysis/","content":"\n背景最近时运不佳，几乎天天被线上问题骚扰。前几天刚解决了一个 HashSet 的并发问题，周六又来了一个性能问题。\n大致的现象是：\n\n我们提供出去的一个 OpenAPI 反应时快时慢，快的时候几十毫秒，慢的时候几秒钟才响应。\n\n尝试解决由于这种也不是业务问题，不能直接定位。所以尝试在测试环境复现，但遗憾的测试环境贼快。\n没办法只能硬着头皮上了。\n中途有抱着侥幸心里让运维查看了 Nginx 里 OpenAPI 的响应时间，想把锅扔给网络。结果果然打脸了；Nginx 里的日志也表明确实响应时间确实有问题。\n\n\n为了清晰的了解这个问题，我简单梳理了这个调用过程。\n\n整个的流程算是比较常见的分层架构：\n\n客户端请求到 Nginx。\nNginx 负载了后端的 web 服务。\nweb 服务通过 RPC 调用后端的 Service 服务。\n\n日志大法我们首先想到的是打日志，在可能会慢的方法或接口处记录处理时间来判断哪里有问题。\n但通过刚才的调用链来说，这个请求流程不短。加日志涉及的改动较多而且万一加漏了还有可能定位不到问题。\n再一个是改动代码之后还会涉及到发版上线。\n工具分析所以最好的方式就是不改动一行代码把这个问题分析出来。\n这时就需要一个 agent 工具了。我们选用了阿里以前开源的  Tprofile 来使用。\n只需要在启动参数中加入 -javaagent:/xx/tprofiler.jar 即可监控你想要监控的方法耗时，并且可以给你输出报告，非常方便。对代码没有任何侵入性同时性能影响也较小。\n工具使用下面来简单展示下如何使用这个工具。\n首先第一步自然是 clone 源码然后打包，可以克隆我修改过的源码。\n\n因为这个项目阿里多年没有维护了，还残留一些 bug,我在它原有的基础上修复了个影响使用的 bug，同时做了一些优化。\n\n执行以下脚本即可。\ngit clone https://github.com/crossoverJie/TProfilermvn assembly:assembly\n\n到这里之后会在项目的 TProfiler/pkg/TProfiler/lib/tprofiler-1.0.1.jar 中生成好我们要使用的 jar 包。\n接下来只需要将这个 jar 包配置到启动参数中，同时再配置一个配置文件路径即可。\n这个配置文件我 copy 官方的解释。\n#log file namelogFileName = tprofiler.logmethodFileName = tmethod.logsamplerFileName = tsampler.log#basic configuration items# 开始取样时间startProfTime = 1:00:00# 结束取样时间endProfTime = 23:00:00# 取样的时间长度eachProfUseTime = 10# 每次取样的时间间隔eachProfIntervalTime = 1samplerIntervalTime = 20# 端口，主要不要冲突了port = 50000debugMode = falseneedNanoTime = false# 是否忽略 get set 方法ignoreGetSetMethod = true#file paths 日志路径logFilePath = /data/work/logs/tprofile/$&#123;logFileName&#125;methodFilePath =/data/work/logs/tprofile/$&#123;methodFileName&#125;samplerFilePath =/data/work/logs/tprofile/$&#123;samplerFileName&#125;#include &amp; excludes itemsexcludeClassLoader = org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader# 需要监控的包includePackageStartsWith = top.crossoverjie.cicada.example.action# 不需要监控的包excludePackageStartsWith = com.taobao.sketch;org.apache.velocity;com.alibaba;com.taobao.forest.domain.dataobject\n\n最终的启动参数如下：\n-javaagent:/TProfiler/lib/tprofiler-1.0.1.jar-Dprofile.properties=/TProfiler/profile.properties\n\n为了模拟排查接口响应慢的问题，我用 cicada 实现了一个 HTTP 接口。其中调用了两个耗时方法：\n\n这样当我启动应用时，Tprofile 就会在我配置的目录记录它所收集的方法信息。\n我访问接口 http://127.0.0.1:5688/cicada-example/demoAction?name=test&amp;id=10 几次后它就会把每个方法的明细响应写入 tprofile.log。\n\n由左到右每列分别代表为：\n线程ID、方法栈深度、方法编号、耗时（毫秒）。\n但 tmethod.log 还是空的；\n这时我们只需要执行这个命令即可把最新的方法采样信息刷到 tmethod.log 文件中。\njava -cp /TProfiler/tprofiler.jar com.taobao.profile.client.TProfilerClient 127.0.0.1 50000 flushmethodflushmethod success\n\n其实就是访问了 Tprofile 暴露出的一个服务，他会读取、解析 tprofile.log 同时写入 tmethod.log.\n\n其中的端口就是配置文件中的 port。\n\n再打开 tmethod.log ：\n\n其中会记录方法的信息。\n\n第一行数字为方法的编号。可以通过这个编号去 tprofile.log(明细)中查询每次的耗时情况。\n行末的数字则是这个方法在源码中最后一行的行号。\n\n其实大部分的性能分析都是统计某个方法的平均耗时。\n所以还需要执行下面的命令，通过 tmethod.log tprofile.log 来生成每个方法的平均耗时。\njava -cp /TProfiler/tprofiler.jar com.taobao.profile.analysis.ProfilerLogAnalysis tprofiler.log tmethod.log topmethod.log topobject.logprint result success\n\n打开 topmethod.log 就是所有方法的平均耗时。\n\n\n4 为请求次数。\n205 为平均耗时。\n818 则为总耗时。\n\n和实际情况是相符的。\n方法的明细耗时这是可能还会有其他需求；比如说我想查询某个方法所有的明细耗时怎么办呢？\n官方没有提供，但也是可以的，只是要麻烦一点。\n比如我想查看 selectDB() 的耗时明细：\n首先得知道这个方法的编号，在 tmethod.log 中可以看查到。\n2 top/crossoverjie/cicada/example/action/DemoAction:selectDB:84\n\n编号为 2.\n之前我们就知道 tprofile.log 记录的是明细，所以通过下面的命令即可查看。\ngrep 2 tprofiler.log\n\n\n通过第三列方法编号为 2 的来查看每次执行的明细。\n但这样的方式显然不够友好，需要人为来过滤干扰，步骤也多；所以我也准备加上这样一个功能。\n只需要传入一个方法名称即可查询采集到的所有方法耗时明细。\n总结回到之前的问题；线上通过这个工具分析我们得到了如下结果。\n\n有些方法确实执行时快时慢，但都是和数据库相关的。由于目前数据库压力较大，准备在接下来进行冷热数据分离，以及分库分表。\n在第一步操作还没实施之前将部分写数据库的操作改为异步，减小响应时间。\n考虑接入 pinpoint 这样的 APM工具。\n\n类似于 Tprofile 的工具确实挺多的，找到适合自己的就好。\n在还没有使用类似于 pinpoint 这样的分布式跟踪工具之前应该会大量依赖于这个工具，所以后续说不定也会做一些定制，比如增加一些可视化界面等，可以提高排查效率。\n你的点赞与分享是对我最大的支持\n","categories":["Java 进阶"],"tags":["Java","Thread","Tprofile"]},{"title":"一份针对于新手的多线程实践","url":"/2018/10/29/java-senior/concurrent-in-action/","content":"\n前言\n前段时间在某个第三方平台看到我写作字数居然突破了 10W 字，难以想象高中 800 字作文我都得巧妙的利用换行来完成(懂的人肯定也干过😏)。\n干了这行养成了一个习惯：能撸码验证的事情都自己验证一遍。\n于是在上周五通宵加班的空余时间写了一个工具：\nhttps://github.com/crossoverJie/NOWS\n利用 SpringBoot 只需要一行命令即可统计自己写了多少个字。\njava -jar nows-0.0.1-SNAPSHOT.jar /xx/Hexo/source/_posts\n\n\n\n传入需要扫描的文章目录即可输出结果（目前只支持 .md 结尾 Markdown 文件）\n\n当然结果看个乐就行（40 几万字），因为早期的博客我喜欢大篇的贴代码，还有一些英文单词也没有过滤，所以导致结果相差较大。\n\n如果仅仅只是中文文字统计肯定是准的，并且该工具内置灵活的扩展方式，使用者可以自定义统计策略，具体请看后文。\n\n其实这个工具挺简单的，代码量也少，没有多少可以值得拿出来讲的。但经过我回忆不管是面试还是和网友们交流都发现一个普遍的现象：\n\n大部分新手开发都会去看多线程、但几乎都没有相关的实践。甚至有些都不知道多线程拿来在实际开发中有什么用。\n\n为此我想基于这个简单的工具为这类朋友带来一个可实践、易理解的多线程案例。\n至少可以让你知道：\n\n为什么需要多线程？\n怎么实现一个多线程程序？\n多线程带来的问题及解决方案？\n\n单线程统计再谈多线程之前先来聊聊单线程如何实现。\n本次的需求也很简单，只是需要扫描一个目录读取下面的所有文件即可。\n所有我们的实现有以下几步：\n\n读取某个目录下的所有文件。\n将所有文件的路径保持到内存。\n遍历所有的文件挨个读取文本记录字数即可。\n\n先来看前两个如何实现，并且当扫描到目录时需要继续读取当前目录下的文件。\n这样的场景就非常适合递归：\n    public List&lt;String&gt; getAllFile(String path)&#123;        File f = new File(path) ;        File[] files = f.listFiles();        for (File file : files) &#123;            if (file.isDirectory())&#123;                String directoryPath = file.getPath();                getAllFile(directoryPath);            &#125;else &#123;                String filePath = file.getPath();                if (!filePath.endsWith(&quot;.md&quot;))&#123;                    continue;                &#125;                allFile.add(filePath) ;            &#125;        &#125;        return allFile ;    &#125;&#125;\n\n读取之后将文件的路径保持到一个集合中。\n\n需要注意的是这个递归次数需要控制下，避免出现栈溢出(StackOverflow)。\n\n最后读取文件内容则是使用 Java8 中的流来进行读取，这样代码可以更简洁：\nStream&lt;String&gt; stringStream = Files.lines(Paths.get(path), StandardCharsets.UTF_8);List&lt;String&gt; collect = stringStream.collect(Collectors.toList());\n\n接下来便是读取字数，同时要过滤一些特殊文本（比如我想过滤掉所有的空格、换行、超链接等）。\n扩展能力简单处理可在上面的代码中遍历 collect 然后把其中需要过滤的内容替换为空就行。\n但每个人的想法可能都不一样。比如我只想过滤掉空格、换行、超链接就行了，但有些人需要去掉其中所有的英文单词，甚至换行还得留着（就像写作文一样可以充字数）。\n所有这就需要一个比较灵活的处理方式。\n看过上文《利用责任链模式设计一个拦截器》应该很容易想到这样的场景责任链模式再合适不过了。\n关于责任链模式具体的内容就不在详述了，感兴趣的可以查看上文。\n这里直接看实现吧：\n定义责任链的抽象接口及处理方法：\npublic interface FilterProcess &#123;    /**     * 处理文本     * @param msg     * @return     */    String process(String msg) ;&#125;\n\n\n处理空格和换行的实现：\npublic class WrapFilterProcess implements FilterProcess&#123;    @Override    public String process(String msg) &#123;        msg = msg.replaceAll(&quot;\\\\s*&quot;, &quot;&quot;);        return msg ;    &#125;&#125;\n\n\n处理超链接的实现：\npublic class HttpFilterProcess implements FilterProcess&#123;    @Override    public String process(String msg) &#123;        msg = msg.replaceAll(&quot;^((https|http|ftp|rtsp|mms)?:\\\\/\\\\/)[^\\\\s]+&quot;,&quot;&quot;);        return msg ;    &#125;&#125;\n\n\n这样在初始化时需要将这些处理 handle 都加入责任链中，同时提供一个 API 供客户端执行即可。\n\n这样一个简单的统计字数的工具就完成了。\n多线程模式在我本地一共就几十篇博客的条件下执行一次还是很快的，但如果我们的文件是几万、几十万甚至上百万呢。\n虽然功能可以实现，但可以想象这样的耗时绝对是成倍的增加。\n这时多线程就发挥优势了，由多个线程分别去读取文件最后汇总结果即可。\n这样实现的过程就变为：\n\n读取某个目录下的所有文件。\n将文件路径交由不同的线程自行处理。\n最终汇总结果。\n\n多线程带来的问题也不是使用多线程就万事大吉了，先来看看第一个问题：共享资源。\n简单来说就是怎么保证多线程和单线程统计的总字数是一致的。\n基于我本地的环境先看看单线程运行的结果：\n\n总计为：414142 字。\n接下来换为多线程的方式：\nList&lt;String&gt; allFile = scannerFile.getAllFile(strings[0]);logger.info(&quot;allFile size=[&#123;&#125;]&quot;,allFile.size());for (String msg : allFile) &#123;\texecutorService.execute(new ScanNumTask(msg,filterProcessManager));&#125;public class ScanNumTask implements Runnable &#123;    private static Logger logger = LoggerFactory.getLogger(ScanNumTask.class);    private String path;    private FilterProcessManager filterProcessManager;    public ScanNumTask(String path, FilterProcessManager filterProcessManager) &#123;        this.path = path;        this.filterProcessManager = filterProcessManager;    &#125;    @Override    public void run() &#123;        Stream&lt;String&gt; stringStream = null;        try &#123;            stringStream = Files.lines(Paths.get(path), StandardCharsets.UTF_8);        &#125; catch (Exception e) &#123;            logger.error(&quot;IOException&quot;, e);        &#125;        List&lt;String&gt; collect = stringStream.collect(Collectors.toList());        for (String msg : collect) &#123;            filterProcessManager.process(msg);        &#125;    &#125;&#125;\n\n\n\n使用线程池管理线程，更多线程池相关的内容请看这里：《如何优雅的使用和理解线程池》\n\n执行结果：\n我们会发现无论执行多少次，这个值都会小于我们的预期值。\n来看看统计那里是怎么实现的。\n@Componentpublic class TotalWords &#123;    private long sum = 0 ;    public void sum(int count)&#123;        sum += count;    &#125;    public long total()&#123;        return sum;    &#125;&#125;\n\n可以看到就是对一个基本类型进行累加而已。那导致这个值比预期小的原因是什么呢？\n我想大部分人都会说：多线程运行时会导致有些线程把其他线程运算的值覆盖。\n\n但其实这只是导致这个问题的表象，根本原因还是没有讲清楚。\n\n内存可见性核心原因其实是由 Java 内存模型（JMM）的规定导致的。\n这里引用一段之前写的《你应该知道的 volatile 关键字》一段解释：\n\n由于 Java 内存模型(JMM)规定，所有的变量都存放在主内存中，而每个线程都有着自己的工作内存(高速缓存)。\n\n\n线程在工作时，需要将主内存中的数据拷贝到工作内存中。这样对数据的任何操作都是基于工作内存(效率提高)，并且不能直接操作主内存以及其他线程工作内存中的数据，之后再将更新之后的数据刷新到主内存中。\n\n\n\n这里所提到的主内存可以简单认为是堆内存，而工作内存则可以认为是栈内存。\n\n\n\n如下图所示：\n\n\n\n所以在并发运行时可能会出现线程 B 所读取到的数据是线程 A 更新之前的数据。\n\n更多相关内容就不再展开了，感兴趣的朋友可以翻翻以前的博文。\n直接来说如何解决这个问题吧，JDK 其实已经帮我们想到了这些问题。\n在 java.util.concurrent 并发包下有许多你可能会使用到的并发工具。\n这里就非常适合 AtomicLong，它可以原子性的对数据进行修改。\n来看看修改后的实现：\n@Componentpublic class TotalWords &#123;    private AtomicLong sum = new AtomicLong() ;        public void sum(int count)&#123;        sum.addAndGet(count) ;    &#125;    public  long total()&#123;        return sum.get() ;    &#125;&#125;\n\n只是使用了它的两个 API 而已。再来运行下程序会发现结果居然还是不对。\n\n甚至为 0 了。\n线程间通信这时又出现了一个新的问题，来看看获取总计数据是怎么实现的。\nList&lt;String&gt; allFile = scannerFile.getAllFile(strings[0]);logger.info(&quot;allFile size=[&#123;&#125;]&quot;,allFile.size());for (String msg : allFile) &#123;\texecutorService.execute(new ScanNumTask(msg,filterProcessManager));&#125;executorService.shutdown();long total = totalWords.total();long end = System.currentTimeMillis();logger.info(&quot;total sum=[&#123;&#125;],[&#123;&#125;] ms&quot;,total,end-start);\n\n不知道大家看出问题没有，其实是在最后打印总数时并不知道其他线程是否已经执行完毕了。\n因为 executorService.execute() 会直接返回，所以当打印获取数据时还没有一个线程执行完毕，也就导致了这样的结果。\n关于线程间通信之前我也写过相关的内容：《深入理解线程通信》\n大概的方式有以下几种：\n\n这里我们使用线程池的方式：\n在停用线程池后加上一个判断条件即可：\nexecutorService.shutdown();while (!executorService.awaitTermination(100, TimeUnit.MILLISECONDS)) &#123;\tlogger.info(&quot;worker running&quot;);&#125;long total = totalWords.total();long end = System.currentTimeMillis();logger.info(&quot;total sum=[&#123;&#125;],[&#123;&#125;] ms&quot;,total,end-start);\n\n这样我们再次尝试，发现无论多少次结果都是正确的了：\n\n效率提升可能还会有朋友问，这样的方式也没见提升多少效率啊。\n这其实是由于我本地文件少，加上一个文件处理的耗时也比较短导致的。\n甚至线程数开的够多导致频繁的上下文切换还是让执行效率降低。\n为了模拟效率的提升，每处理一个文件我都让当前线程休眠 100 毫秒来模拟执行耗时。\n先看单线程运行需要耗时多久。\n\n总共耗时：[8404] ms\n接着在线程池大小为 4 的情况下耗时：\n\n\n总共耗时：[2350] ms\n可见效率提升还是非常明显的。\n更多思考这只是多线程其中的一个用法，相信看到这里的朋友应该多它的理解更进一步了。\n再给大家留个阅后练习，场景也是类似的：\n\n在 Redis 或者其他存储介质中存放有上千万的手机号码数据，每个号码都是唯一的，需要在最快的时间内把这些号码全部都遍历一遍。\n\n有想法感兴趣的朋友欢迎在文末留言参与讨论🤔🤨。\n总结希望看完的朋友心中能对文初的几个问题能有自己的答案：\n\n为什么需要多线程？\n怎么实现一个多线程程序？\n多线程带来的问题及解决方案？\n\n文中的代码都在此处。\nhttps://github.com/crossoverJie/NOWS\n你的点赞与转发是最大的支持。\n","categories":["Java 进阶"],"tags":["Java","SpringBoot","ThreadPool"]},{"title":"一份针对于新手的多线程实践--进阶篇","url":"/2018/10/31/java-senior/concurrent-in-action2/","content":"\n前言在上文《一份针对于新手的多线程实践》留下了一个问题：\n\n这只是多线程其中的一个用法，相信看到这里的朋友应该多它的理解更进一步了。\n再给大家留个阅后练习，场景也是类似的：\n\n在 Redis 或者其他存储介质中存放有上千万的手机号码数据，每个号码都是唯一的，需要在最快的时间内把这些号码全部都遍历一遍。\n\n有想法感兴趣的朋友欢迎在文末留言参与讨论🤔🤨。\n\n网友们的方案\n\n我在公众号以及其他一些平台收到了大家的回复，果然是众人拾柴火焰高啊。\n\n\n\n\n\n\n感谢每一位参与的朋友。\n其实看了大家的方案大多都想到了数据肯定要分段，因为大量的数据肯定没法一次性 load 到内存。\n但怎么加载就要考虑清楚了，有些人说放在数据库中通过分页的方式进行加载，然后将每页的数据丢到一个线程里去做遍历。\n其实想法挺不错的，但有个问题就是：\n\n这样肯定会导致有一个主线程去遍历所有的号码，即便是分页查询的那也得全部查询一遍，效率还是很低。\n即便是分页加载号码用多线程，那就会涉及到锁的问题，怎么保证每个线程读取的数据是互不冲突的。\n\n但如果存储换成 Redis 的 String 结构这样就更行不通了。\n遍历数据方案有没有一种利用多线程加载效率高，并且线程之间互相不需要竞争锁的方案呢？\n下面来看看这个方案：\n首先在存储这千万号码的时候我们把它的号段单独提出来并冗余存储一次。\n比如有个号码是 18523981123 那么就还需要存储一个号段：1852398。\n这样当我们有以下这些号码时：\n18523981123 18523981124 18523981125 13123874321 13123874322 13123874323\n我们就还会维护一个号段数据为：\n1852398 1312387\n这样我想大家应该明白下一步应当怎么做了吧。\n在需要遍历时：\n\n通过主线程先把所有的号段加载到内存，即便是千万的号码号段也顶多几千条数据。\n遍历这个号段，将每个号段提交到一个 task 线程中。\n由这个线程通过号段再去查询真正的号码进行遍历。\n最后所有的号段都提交完毕再等待所有的线程执行完毕即可遍历所有的号码。\n\n这样做的根本原因其实是避免了线程之间加锁，通过号段可以让每个线程只取自己那一部分数据。\n可能会有人说，如果号码持续增多导致号段的数据也达到了上万甚至几十万这怎么办呢？\n那其实也是同样的思路，可以再把号段进行拆分。\n比如之前是 1852398 的号段，那我继续拆分为 1852 。\n这样只需要在之前的基础上再启动一个线程去查询子号段即可，有点 fork/join 的味道。\n\n这样的思路其实也和 JDK1.7 中的 ConcurrentHashMap 类似，定位一个真正的数据需要两次定位。\n\n\n分布式方案上面的方案也是由局限性的，毕竟说到底还是一个单机应用。没法扩展；处理的数据始终是有上限。\n这个上限就和服务器的配置以及线程数这些相关，说的高大上一点其实就是垂直扩展增加单机的处理性能。\n因此随着数据量的提升我们肯定得需要通过水平扩展的方式才能达到最好的性能，这就是分布式的方案。\n假设我现在有上亿的数据需要遍历，但我当前的服务器配置只能支撑一个应用启动 N 个线程 5 分钟跑5000W 的数据。\n于是我水平扩展，在三台服务器上启动了三个独立的进程。假设一个应用能跑 5000W ，那么理论上来说三个应用就可以跑1.5亿的数据了。\n但这个的前提还是和上文一样：每个应用只能处理自己的数据，不能出现加锁的情况（这样会大大的降低性能）。\n所以我们得对刚才的号段进行分组。\n先通过一张图来直观的表示这个逻辑：\n\n假设现在我有 9 个号段，那么我就得按照图中的方式把数据隔离开来。\n第一个数据给应用0，第二个数据给应用1，第三个数据给应用2。后面的数据以此类推（就是一个简单的取模运算）。\n这样就可以将号段均匀的分配给不同的应用来进行处理，然后每个应用再按照上文提到的将分配给自己的号段丢到线程池中由具体的线程去查询、遍历即可。\n分布式带来的问题这样看似没啥问题，但一旦引入了分布式之后就不可避免的会出现 CAP 的取舍，这里不做过多讨论，感兴趣的朋友可以自行搜索。\n首先要解决的一个问题就是：\n这三个应用怎么知道它自己应该取哪些号段的数据呢？比如 0 号应用就取 0 3 6（这个相当于号段的下标），难道在配置文件里配置嘛？\n那如果数据量又增大了，对应的机器数也增加到了 5 台，那自然 0 号应用就不是取 0 3 6 了（取模之后数据会变）。\n\n所以我们得需要一个统一的调度来分配各个应用他们应当取哪些号段，这也就是数据分片。\n\n假设我这里有一个统一的分配中心，他知道现在有多少个应用来处理数据。还是假设上文的三个应用吧。\n在真正开始遍历数据的时候，这个分配中心就会去告诉这三个应用：\n\n你们要开始工作了啊，0 号应用你的工作内容是 0 3 6，1 号应用你的工作内容是 1 4 7，2 号应用你的工作内容是 2 5 8。\n\n这样各个应用就知道他们所应当处理的数据了。\n当我们新增了一个应用来处理数据时也很简单，同样这个分配中心知道现在有多少台应用会工作。\n他会再拿着现有的号段对 4(3+1台应用) 进行取模然后对数据进行重新分配，这样就可以再次保证数据分配均匀了。\n\n只是分配中心如何知道有多少应用呢，其实也简单，只要中心和应用之间通信就可以了。比如启动的时候调用分配中心的接口即可。\n\n上面提到的这个分配中心其实就是一个常见的定时任务的分布式调度中心，由它来统一发起调度，当然分片只是它其中的一个功能而已（关于调度中心之后有兴趣再细说）。\n总结本次探讨了多线程的更多应用方式，如要是如何高效的运行。最主要的一点其实就是尽量的避免加锁。\n同时对分布式水平扩展谈了一些处理建议，本次也是难得的一行代码都没贴，大家感兴趣的话在后面更新相关代码。\n\n也欢迎大家留言讨论。😄\n\n你的点赞与转发是最大的支持。\n","categories":["Java 进阶"],"tags":["Java","SpringBoot","ThreadPool"]},{"title":"利用策略模式优化过多 if else 代码","url":"/2019/01/30/java-senior/design-if-else/","content":"\n前言不出意外，这应该是年前最后一次分享，本次来一点实际开发中会用到的小技巧。\n\n\n比如平时大家是否都会写类似这样的代码：\nif(a)&#123;\t//dosomething&#125;else if(b)&#123;\t//doshomething&#125;else if(c)&#123;\t//doshomething&#125; else&#123;\t////doshomething&#125;\n\n条件少还好，一旦 else if 过多这里的逻辑将会比较混乱，并很容易出错。\n比如这样：\n\n\n摘自 cim 中的一个客户端命令的判断条件。\n\n刚开始条件较少，也就没管那么多直接写的；现在功能多了导致每次新增一个 else 条件我都得仔细核对，生怕影响之前的逻辑。\n这次终于忍无可忍就把他重构了，重构之后这里的结构如下：\n\n最后直接变为两行代码，简洁了许多。\n而之前所有的实现逻辑都单独抽取到其他实现类中。\n\n这样每当我需要新增一个 else 逻辑，只需要新增一个类实现同一个接口便可完成。每个处理逻辑都互相独立互不干扰。\n实现\n按照目前的实现画了一个草图。\n整体思路如下：\n\n定义一个 InnerCommand 接口，其中有一个 process 函数交给具体的业务实现。\n根据自己的业务，会有多个类实现 InnerCommand 接口；这些实现类都会注册到 Spring Bean 容器中供之后使用。\n通过客户端输入命令，从 Spring Bean 容器中获取一个 InnerCommand 实例。\n执行最终的 process 函数。\n\n主要想实现的目的就是不在有多个判断条件，只需要根据当前客户端的状态动态的获取 InnerCommand 实例。\n从源码上来看最主要的就是 InnerCommandContext 类，他会根据当前客户端命令动态获取 InnerCommand 实例。\n\n\n第一步是获取所有的 InnerCommand 实例列表。\n根据客户端输入的命令从第一步的实例列表中获取类类型。\n根据类类型从 Spring 容器中获取具体实例对象。\n\n因此首先第一步需要维护各个命令所对应的类类型。\n\n所以在之前的枚举中就维护了命令和类类型的关系，只需要知道命令就能知道他的类类型。\n这样才能满足只需要两行代码就能替换以前复杂的 if else，同时也能灵活扩展。\nInnerCommand instance = innerCommandContext.getInstance(msg);instance.process(msg) ;\n\n总结当然还可以做的更灵活一些，比如都不需要显式的维护命令和类类型的对应关系。\n只需要在应用启动时扫描所有实现了 InnerCommand 接口的类即可，在 cicada 中有类似实现，感兴趣的可以自行查看。\n这样一些小技巧希望对你有所帮助。\n以上所有源码可以在这里查看：\nhttps://github.com/crossoverJie/cim\n你的点赞与分享是对我最大的支持\n","categories":["Java 进阶","设计模式"],"tags":["Java","策略模式"]},{"title":"没错，老板让我写个 BUG！","url":"/2018/12/12/java-senior/java-memary-allocation/","content":"\n前言标题没有看错，真的是让我写个 bug！\n刚接到这个需求时我内心没有丝毫波澜，甚至还有点激动。这可是我特长啊；终于可以光明正大的写 bug 了🙄。\n先来看看具体是要干啥吧，其实主要就是要让一些负载很低的服务器额外消耗一些内存、CPU 等资源（至于背景就不多说了），让它的负载可以提高一些。\n\nJVM 内存分配回顾于是我刷刷一把梭的就把代码写好了，大概如下：\n\n写完之后我就在想一个问题，代码中的 mem 对象在方法执行完之后会不会被立即回收呢？我想肯定会有一部分人认为就是在方法执行完之后回收。\n我也正儿八经的去调研了下，问了一些朋友；果不其然确实有一部分认为是在方法执行完毕之后回收。\n那事实情况如何呢？我做了一个试验。\n我用以下的启动参数将刚才这个应用启动起来。\njava -Djava.rmi.server.hostname=10.xx.xx.xx -Djava.security.policy=jstatd.all.policy -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=8888  -Xms4g -Xmx4g  -jar bug-0.0.1-SNAPSHOT.jar\n\n这样我就可以通过 JMX 端口远程连接到这个应用观察内存、GC 情况了。\n\n\n如果是方法执行完毕就回收 mem 对象，当我分配 250M 内存时；内存就会有一个明显的曲线，同时 GC 也会执行。\n\n\n这时观察内存曲线。\n\n会发现确实有明显的涨幅，但是之后并没有立即回收，而是一直保持在这个水位。同时左边的 GC 也没有任何的反应。\n用 jstat 查看内存布局也是同样的情况。\n\n不管是 YGC,FGC 都没有，只是 Eden 区的使用占比有所增加，毕竟分配了 250M 内存嘛。\n那怎样才会回收呢？\n我再次分配了两个 250M 之后观察内存曲线。\n\n\n发现第三个 250M 的时候 Eden 区达到了 98.83% 于是再次分配时就需要回收 Eden 区产生了 YGC。\n同时内存曲线也得到了下降。\n整个的换算过程如图：\n\n由于初始化的堆内存为 4G，所以算出来的 Eden 区大概为 1092M 内存。\n加上应用启动 Spring 之类消耗的大约 20% 内存，所以分配 3 次 250M 内存就会导致 YGC。\n再来回顾下刚才的问题：\n\nmem 对象既然在方法执行完毕后不会回收，那什么时候回收呢。\n其实只要记住一点即可：对象都需要垃圾回收器发生 GC 时才能回收；不管这个对象是局部变量还是全局变量。\n通过刚才的实验也发现了，当 Eden 区空间不足产生 YGC 时才会回收掉我们创建的 mem 对象。\n但这里其实还有一个隐藏条件：那就是这个对象是局部变量。如果该对象是全局变量那依然不能被回收。\n也就是我们常说的对象不可达，这样不可达的对象在 GC 发生时就会被认为是需要回收的对象从而进行回收。\n在多考虑下，为什么有些人会认为方法执行完毕后局部变量会被回收呢？\n我想这应当是记混了，其实方法执行完毕后回收的是栈帧。\n它最直接的结果就是导致 mem 这个对象没有被引用了。但没有引用并不代表会被马上回收，也就是上面说到的需要产生 GC 才会回收。\n所以使用的是上面提到的对象不可达所采用的可达性分析算法来表明哪些对象需要被回收。\n当对象没有被引用后也就认为不可达了。\n这里有一张动图比较清晰：\n\n当方法执行完之后其中的 mem 对象就相当于图中的 Object 5，所以在 GC 时候就会回收掉。\n优先在 Eden 区分配对象其实从上面的例子中可以看出对象是优先分配在新生代中 Eden 区的，但有个前提就是对象不能太大。\n以前也写过相关的内容：\n\n大对象直接进入老年代而大对象则是直接分配到老年代中（至于多大算大，可以通过参数配置）。\n\n\n当我直接分配 1000M 内存时，由于 Eden 区不能直接装下，所以改为分配在老年代中。\n\n\n可以看到 Eden 区几乎没有变动，但是老年代却涨了 37% ，根据之前计算的老年代内存 2730M 算出来也差不多是 1000M 的内存。\nLinux 内存查看回到这次我需要完成的需求：增加服务器内存和 CPU 的消耗。\nCPU 还好，本身就有一定的使用，同时每创建一个对象也会消耗一些 CPU。\n\n主要是内存,先来看下没启动这个应用之前的内存情况。\n\n大概只使用了 3G 的内存。\n启动应用之后大概只消耗了 600M 左右的内存。\n\n为了满足需求我需要分配一些内存，但这里有点需要讲究。\n不能一直分配内存，这样会导致 CPU 负载太高了，同时内存也会由于 GC 回收导致占用也不是特别多。\n所以我需要少量的分配，让大多数对象在新生代中，为了不被回收需要保持在百分之八九十。\n同时也需要分配一些大对象到老年代中，也要保持老年代的使用在百分之八九十。\n这样才能最大限度的利用这 4G 的堆内存。\n于是我做了以下操作：\n\n先分配一些小对象在新生代中（800M）保持新生代在90%\n接着又分配了老年代内 *（100%-已使用的28%）；也就是 2730*60%=1638M 让老年代也在 90% 左右。\n\n\n\n效果如上。\n最主要的是一次 GC 都没有发生这样也就达到了我的目的。\n最终内存消耗了 3.5G 左右。\n\n总结虽说这次的需求是比较奇葩，但想要精确的控制 JVM 的内存分配还是没那么容易。\n需要对它的内存布局，回收都要有一定的了解，写这个 Bug 的过程确实也加深了印象，如果对你有所帮助请不要吝啬你的点赞与分享。\n你的点赞与分享是对我最大的支持\n","categories":["Java 进阶"],"tags":["Java","JVM"]},{"title":"深入理解线程通信","url":"/2018/03/16/java-senior/thread-communication/","content":"\n前言开发中不免会遇到需要所有子线程执行完毕通知主线程处理某些逻辑的场景。\n或者是线程 A 在执行到某个条件通知线程 B 执行某个操作。\n可以通过以下几种方式实现：\n等待通知机制\n等待通知模式是 Java 中比较经典的线程通信方式。\n\n两个线程通过对同一对象调用等待 wait() 和通知 notify() 方法来进行通讯。\n如两个线程交替打印奇偶数：\n\n\npublic class TwoThreadWaitNotify &#123;    private int start = 1;    private boolean flag = false;    public static void main(String[] args) &#123;        TwoThreadWaitNotify twoThread = new TwoThreadWaitNotify();        Thread t1 = new Thread(new OuNum(twoThread));        t1.setName(&quot;A&quot;);        Thread t2 = new Thread(new JiNum(twoThread));        t2.setName(&quot;B&quot;);        t1.start();        t2.start();    &#125;    /**     * 偶数线程     */    public static class OuNum implements Runnable &#123;        private TwoThreadWaitNotify number;        public OuNum(TwoThreadWaitNotify number) &#123;            this.number = number;        &#125;        @Override        public void run() &#123;            while (number.start &lt;= 100) &#123;                synchronized (TwoThreadWaitNotify.class) &#123;                    System.out.println(&quot;偶数线程抢到锁了&quot;);                    if (number.flag) &#123;                        System.out.println(Thread.currentThread().getName() + &quot;+-+偶数&quot; + number.start);                        number.start++;                        number.flag = false;                        TwoThreadWaitNotify.class.notify();                    &#125;else &#123;                        try &#123;                            TwoThreadWaitNotify.class.wait();                        &#125; catch (InterruptedException e) &#123;                            e.printStackTrace();                        &#125;                    &#125;                &#125;            &#125;        &#125;    &#125;    /**     * 奇数线程     */    public static class JiNum implements Runnable &#123;        private TwoThreadWaitNotify number;        public JiNum(TwoThreadWaitNotify number) &#123;            this.number = number;        &#125;        @Override        public void run() &#123;            while (number.start &lt;= 100) &#123;                synchronized (TwoThreadWaitNotify.class) &#123;                    System.out.println(&quot;奇数线程抢到锁了&quot;);                    if (!number.flag) &#123;                        System.out.println(Thread.currentThread().getName() + &quot;+-+奇数&quot; + number.start);                        number.start++;                        number.flag = true;                        TwoThreadWaitNotify.class.notify();                    &#125;else &#123;                        try &#123;                            TwoThreadWaitNotify.class.wait();                        &#125; catch (InterruptedException e) &#123;                            e.printStackTrace();                        &#125;                    &#125;                &#125;            &#125;        &#125;    &#125;&#125;\n\n输出结果：\nt2+-+奇数93t1+-+偶数94t2+-+奇数95t1+-+偶数96t2+-+奇数97t1+-+偶数98t2+-+奇数99t1+-+偶数100\n\n这里的线程 A 和线程 B 都对同一个对象 TwoThreadWaitNotify.class 获取锁，A 线程调用了同步对象的 wait() 方法释放了锁并进入 WAITING 状态。\nB 线程调用了 notify() 方法，这样 A 线程收到通知之后就可以从 wait() 方法中返回。\n这里利用了 TwoThreadWaitNotify.class 对象完成了通信。\n有一些需要注意:\n\nwait() 、nofify() 、nofityAll() 调用的前提都是获得了对象的锁(也可称为对象监视器)。\n调用 wait() 方法后线程会释放锁，进入 WAITING 状态，该线程也会被移动到等待队列中。\n调用 notify() 方法会将等待队列中的线程移动到同步队列中，线程状态也会更新为 BLOCKED\n从 wait() 方法返回的前提是调用 notify() 方法的线程释放锁，wait() 方法的线程获得锁。\n\n等待通知有着一个经典范式：\n线程 A 作为消费者：\n\n获取对象的锁。\n进入 while(判断条件)，并调用 wait() 方法。\n当条件满足跳出循环执行具体处理逻辑。\n\n线程 B 作为生产者:\n\n获取对象锁。\n更改与线程 A 共用的判断条件。\n调用 notify() 方法。\n\n伪代码如下:\n//Thread Asynchronized(Object)&#123;    while(条件)&#123;        Object.wait();    &#125;    //do something&#125;//Thread Bsynchronized(Object)&#123;    条件=false;//改变条件    Object.notify();&#125;\n\n\njoin() 方法private static void join() throws InterruptedException &#123;    Thread t1 = new Thread(new Runnable() &#123;        @Override        public void run() &#123;            LOGGER.info(&quot;running&quot;);            try &#123;                Thread.sleep(3000);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;) ;    Thread t2 = new Thread(new Runnable() &#123;        @Override        public void run() &#123;            LOGGER.info(&quot;running2&quot;);            try &#123;                Thread.sleep(4000);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;) ;    t1.start();    t2.start();    //等待线程1终止    t1.join();    //等待线程2终止    t2.join();    LOGGER.info(&quot;main over&quot;);&#125;\n\n输出结果:\n2018-03-16 20:21:30.967 [Thread-1] INFO  c.c.actual.ThreadCommunication - running22018-03-16 20:21:30.967 [Thread-0] INFO  c.c.actual.ThreadCommunication - running2018-03-16 20:21:34.972 [main] INFO  c.c.actual.ThreadCommunication - main over\n\n在  t1.join() 时会一直阻塞到 t1 执行完毕，所以最终主线程会等待 t1 和 t2 线程执行完毕。\n其实从源码可以看出，join() 也是利用的等待通知机制：\n核心逻辑:\nwhile (isAlive()) &#123;    wait(0);&#125;\n\n在 join 线程完成后会调用 notifyAll() 方法，是在 JVM 实现中调用，所以这里看不出来。\nvolatile 共享内存因为 Java 是采用共享内存的方式进行线程通信的，所以可以采用以下方式用主线程关闭 A 线程:\npublic class Volatile implements Runnable&#123;    private static volatile boolean flag = true ;    @Override    public void run() &#123;        while (flag)&#123;            System.out.println(Thread.currentThread().getName() + &quot;正在运行。。。&quot;);        &#125;        System.out.println(Thread.currentThread().getName() +&quot;执行完毕&quot;);    &#125;    public static void main(String[] args) throws InterruptedException &#123;        Volatile aVolatile = new Volatile();        new Thread(aVolatile,&quot;thread A&quot;).start();        System.out.println(&quot;main 线程正在运行&quot;) ;        TimeUnit.MILLISECONDS.sleep(100) ;        aVolatile.stopThread();    &#125;    private void stopThread()&#123;        flag = false ;    &#125;&#125;\n\n输出结果：\nthread A正在运行。。。thread A正在运行。。。thread A正在运行。。。thread A正在运行。。。thread A执行完毕\n\n这里的 flag 存放于主内存中，所以主线程和线程 A 都可以看到。\nflag 采用 volatile 修饰主要是为了内存可见性，更多内容可以查看这里。\nCountDownLatch 并发工具CountDownLatch 可以实现 join 相同的功能，但是更加的灵活。\nprivate static void countDownLatch() throws Exception&#123;    int thread = 3 ;    long start = System.currentTimeMillis();    final CountDownLatch countDown = new CountDownLatch(thread);    for (int i= 0 ;i&lt;thread ; i++)&#123;        new Thread(new Runnable() &#123;            @Override            public void run() &#123;                LOGGER.info(&quot;thread run&quot;);                try &#123;                    Thread.sleep(2000);                    countDown.countDown();                    LOGGER.info(&quot;thread end&quot;);                &#125; catch (InterruptedException e) &#123;                    e.printStackTrace();                &#125;            &#125;        &#125;).start();    &#125;    countDown.await();    long stop = System.currentTimeMillis();    LOGGER.info(&quot;main over total time=&#123;&#125;&quot;,stop-start);&#125;\n\n输出结果:\n2018-03-16 20:19:44.126 [Thread-0] INFO  c.c.actual.ThreadCommunication - thread run2018-03-16 20:19:44.126 [Thread-2] INFO  c.c.actual.ThreadCommunication - thread run2018-03-16 20:19:44.126 [Thread-1] INFO  c.c.actual.ThreadCommunication - thread run2018-03-16 20:19:46.136 [Thread-2] INFO  c.c.actual.ThreadCommunication - thread end2018-03-16 20:19:46.136 [Thread-1] INFO  c.c.actual.ThreadCommunication - thread end2018-03-16 20:19:46.136 [Thread-0] INFO  c.c.actual.ThreadCommunication - thread end2018-03-16 20:19:46.136 [main] INFO  c.c.actual.ThreadCommunication - main over total time=2012\n\nCountDownLatch 也是基于 AQS(AbstractQueuedSynchronizer) 实现的，更多实现参考 ReentrantLock 实现原理\n\n初始化一个 CountDownLatch 时告诉并发的线程，然后在每个线程处理完毕之后调用 countDown() 方法。\n该方法会将 AQS 内置的一个 state 状态 -1 。\n最终在主线程调用 await() 方法，它会阻塞直到 state == 0 的时候返回。\n\nCyclicBarrier 并发工具private static void cyclicBarrier() throws Exception &#123;    CyclicBarrier cyclicBarrier = new CyclicBarrier(3) ;    new Thread(new Runnable() &#123;        @Override        public void run() &#123;            LOGGER.info(&quot;thread run&quot;);            try &#123;                cyclicBarrier.await() ;            &#125; catch (Exception e) &#123;                e.printStackTrace();            &#125;            LOGGER.info(&quot;thread end do something&quot;);        &#125;    &#125;).start();    new Thread(new Runnable() &#123;        @Override        public void run() &#123;            LOGGER.info(&quot;thread run&quot;);            try &#123;                cyclicBarrier.await() ;            &#125; catch (Exception e) &#123;                e.printStackTrace();            &#125;            LOGGER.info(&quot;thread end do something&quot;);        &#125;    &#125;).start();    new Thread(new Runnable() &#123;        @Override        public void run() &#123;            LOGGER.info(&quot;thread run&quot;);            try &#123;                Thread.sleep(5000);                cyclicBarrier.await() ;            &#125; catch (Exception e) &#123;                e.printStackTrace();            &#125;            LOGGER.info(&quot;thread end do something&quot;);        &#125;    &#125;).start();    LOGGER.info(&quot;main thread&quot;);&#125;\n\nCyclicBarrier 中文名叫做屏障或者是栅栏，也可以用于线程间通信。\n它可以等待 N 个线程都达到某个状态后继续运行的效果。\n\n首先初始化线程参与者。\n调用 await() 将会在所有参与者线程都调用之前等待。\n直到所有参与者都调用了 await() 后，所有线程从 await() 返回继续后续逻辑。\n\n运行结果:\n2018-03-18 22:40:00.731 [Thread-0] INFO  c.c.actual.ThreadCommunication - thread run2018-03-18 22:40:00.731 [Thread-1] INFO  c.c.actual.ThreadCommunication - thread run2018-03-18 22:40:00.731 [Thread-2] INFO  c.c.actual.ThreadCommunication - thread run2018-03-18 22:40:00.731 [main] INFO  c.c.actual.ThreadCommunication - main thread2018-03-18 22:40:05.741 [Thread-0] INFO  c.c.actual.ThreadCommunication - thread end do something2018-03-18 22:40:05.741 [Thread-1] INFO  c.c.actual.ThreadCommunication - thread end do something2018-03-18 22:40:05.741 [Thread-2] INFO  c.c.actual.ThreadCommunication - thread end do something\n\n可以看出由于其中一个线程休眠了五秒，所有其余所有的线程都得等待这个线程调用 await() 。\n该工具可以实现 CountDownLatch 同样的功能，但是要更加灵活。甚至可以调用 reset() 方法重置 CyclicBarrier (需要自行捕获 BrokenBarrierException 处理) 然后重新执行。\n线程响应中断public class StopThread implements Runnable &#123;    @Override    public void run() &#123;        while ( !Thread.currentThread().isInterrupted()) &#123;            // 线程执行具体逻辑            System.out.println(Thread.currentThread().getName() + &quot;运行中。。&quot;);        &#125;        System.out.println(Thread.currentThread().getName() + &quot;退出。。&quot;);    &#125;    public static void main(String[] args) throws InterruptedException &#123;        Thread thread = new Thread(new StopThread(), &quot;thread A&quot;);        thread.start();        System.out.println(&quot;main 线程正在运行&quot;) ;        TimeUnit.MILLISECONDS.sleep(10) ;        thread.interrupt();    &#125;&#125;\n\n输出结果:\nthread A运行中。。thread A运行中。。thread A退出。。\n\n可以采用中断线程的方式来通信，调用了 thread.interrupt() 方法其实就是将 thread 中的一个标志属性置为了 true。\n并不是说调用了该方法就可以中断线程，如果不对这个标志进行响应其实是没有什么作用(这里对这个标志进行了判断)。\n但是如果抛出了 InterruptedException 异常，该标志就会被 JVM 重置为 false。\n线程池 awaitTermination() 方法如果是用线程池来管理线程，可以使用以下方式来让主线程等待线程池中所有任务执行完毕:\nprivate static void executorService() throws Exception&#123;    BlockingQueue&lt;Runnable&gt; queue = new LinkedBlockingQueue&lt;&gt;(10) ;    ThreadPoolExecutor poolExecutor = new ThreadPoolExecutor(5,5,1, TimeUnit.MILLISECONDS,queue) ;    poolExecutor.execute(new Runnable() &#123;        @Override        public void run() &#123;            LOGGER.info(&quot;running&quot;);            try &#123;                Thread.sleep(3000);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;);    poolExecutor.execute(new Runnable() &#123;        @Override        public void run() &#123;            LOGGER.info(&quot;running2&quot;);            try &#123;                Thread.sleep(2000);            &#125; catch (InterruptedException e) &#123;                e.printStackTrace();            &#125;        &#125;    &#125;);    poolExecutor.shutdown();    while (!poolExecutor.awaitTermination(1,TimeUnit.SECONDS))&#123;        LOGGER.info(&quot;线程还在执行。。。&quot;);    &#125;    LOGGER.info(&quot;main over&quot;);&#125;\n\n输出结果:\n2018-03-16 20:18:01.273 [pool-1-thread-2] INFO  c.c.actual.ThreadCommunication - running22018-03-16 20:18:01.273 [pool-1-thread-1] INFO  c.c.actual.ThreadCommunication - running2018-03-16 20:18:02.273 [main] INFO  c.c.actual.ThreadCommunication - 线程还在执行。。。2018-03-16 20:18:03.278 [main] INFO  c.c.actual.ThreadCommunication - 线程还在执行。。。2018-03-16 20:18:04.278 [main] INFO  c.c.actual.ThreadCommunication - main over\n\n使用这个 awaitTermination() 方法的前提需要关闭线程池，如调用了 shutdown() 方法。\n调用了 shutdown() 之后线程池会停止接受新任务，并且会平滑的关闭线程池中现有的任务。\n管道通信public static void piped() throws IOException &#123;    //面向于字符 PipedInputStream 面向于字节    PipedWriter writer = new PipedWriter();    PipedReader reader = new PipedReader();    //输入输出流建立连接    writer.connect(reader);    Thread t1 = new Thread(new Runnable() &#123;        @Override        public void run() &#123;            LOGGER.info(&quot;running&quot;);            try &#123;                for (int i = 0; i &lt; 10; i++) &#123;                    writer.write(i+&quot;&quot;);                    Thread.sleep(10);                &#125;            &#125; catch (Exception e) &#123;            &#125; finally &#123;                try &#123;                    writer.close();                &#125; catch (IOException e) &#123;                    e.printStackTrace();                &#125;            &#125;        &#125;    &#125;);    Thread t2 = new Thread(new Runnable() &#123;        @Override        public void run() &#123;            LOGGER.info(&quot;running2&quot;);            int msg = 0;            try &#123;                while ((msg = reader.read()) != -1) &#123;                    LOGGER.info(&quot;msg=&#123;&#125;&quot;, (char) msg);                &#125;            &#125; catch (Exception e) &#123;            &#125;        &#125;    &#125;);    t1.start();    t2.start();&#125;\n\n输出结果:\n2018-03-16 19:56:43.014 [Thread-0] INFO  c.c.actual.ThreadCommunication - running2018-03-16 19:56:43.014 [Thread-1] INFO  c.c.actual.ThreadCommunication - running22018-03-16 19:56:43.130 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=02018-03-16 19:56:43.132 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=12018-03-16 19:56:43.132 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=22018-03-16 19:56:43.133 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=32018-03-16 19:56:43.133 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=42018-03-16 19:56:43.133 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=52018-03-16 19:56:43.133 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=62018-03-16 19:56:43.134 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=72018-03-16 19:56:43.134 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=82018-03-16 19:56:43.134 [Thread-1] INFO  c.c.actual.ThreadCommunication - msg=9\n\nJava 虽说是基于内存通信的，但也可以使用管道通信。\n需要注意的是，输入流和输出流需要首先建立连接。这样线程 B 就可以收到线程 A 发出的消息了。\n实际开发中可以灵活根据需求选择最适合的线程通信方式。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["Java 进阶"],"tags":["Java","Thread","concurrent"]},{"title":"云原生背景下如何配置 JVM 内存","url":"/2023/05/15/k8s/cloudnative-java/","content":"\n背景前段时间业务研发反馈说是他的应用内存使用率很高，导致频繁的重启，让我排查下是怎么回事；\n在这之前我也没怎么在意过这个问题，正好这次排查分析的过程做一个记录。\n\n\n首先我查看了监控面板里的 Pod 监控：\n发现确实是快满了，而此时去查看应用的 JVM 占用情况却只有30%左右；说明并不是应用内存满了导致 JVM 的 OOM，而是 Pod 的内存满了，导致 Pod 的内存溢出，从而被 k8s 杀掉了。\n而 k8s 为了维持应用的副本数量就得重启一个 Pod，所以看起来就是应用运行一段时间后就被重启。\n\n而这个应用配置的是 JVM 8G，容器申请的内存是16G，所以 Pod 的内存占用看起来也就 50% 左右。\n容器的原理在解决这个问题之前还是先简单了解下容器的运行原理，因为在 k8s 中所有的应用都是运行在容器中的，而容器本质上也是运行在宿主机上的一个个进程而已。\n但我们使用 Docker 的时候会感觉每个容器启动的应用之间互不干扰，从文件系统、网络、CPU、内存这些都能完全隔离开来，就像两个运行在不同的服务器中的应用。\n其实这一点也不是啥黑科技，Linux 早就支持 2.6.x 的版本就已经支持 namespace 隔离了，使用 namespace 可以将两个进程完全隔离。\n仅仅将资源隔离还不够，还需要限制对资源的使用，比如 CPU、内存、磁盘、带宽这些也得做限制；这点也可以使用 cgroups 进行配置。\n它可以限制某个进程的资源，比如宿主机是 4 核 CPU，8G 内存，为了保护其他容器，必须给这个容器配置使用上限：1核 CPU，2G内存。\n\n这张图就很清晰的表示了 namespace  和 cgroups 在容器技术中的作用，简单来说就是：\n\nnamespace 负责隔离\ncgroups 负责限制\n\n在 k8s 中也有对应的提现：\nresources:  requests:    memory: 1024Mi    cpu: 0.1  limits:    memory: 1024Mi    cpu: 4\n\n这个资源清单表示该应用至少需要为一个容器分配一个 0.1 核和 1024M 的资源，CPU 的最高上限为 4 个核心。\n不同的OOM回到本次的问题，可以确认是容器发生了 OOM 从而导致被 k8s 重启，这也是我们配置 limits 的作用。\n\nk8s 内存溢出导致容器退出会出现 exit code 137 的一个 event 日志。\n\n因为该应用的 JVM 内存配置和容器的配置大小是一样的，都是8GB，但 Java 应用还有一些非 JVM 管理的内存，比如堆外内存之类的，这样很容易就导致容器内存大小超过了限制的 8G 了，也就导致了容器内存溢出。\n云原生背景的优化因为这个应用本身使用的内存不多，所以建议将堆内存限制到 4GB，这样就避免了容器内存超限，从而解决了问题。\n当然之后我们也会在应用配置栏里加上建议：推荐 JVM 的配置小于容器限制的 2&#x2F;3，预留一些内存。\n其实本质上还是开发模式没有转变过来，以传统的 Java 应用开发模式甚至都不会去了解容器的内存大小，因为以前大家的应用都是部署在一个内存较大的虚拟机上，所以感知不到容器内存的限制。\n从而误以为将两者画了等号，这一点可能在 Java 应用中尤为明显，毕竟多了一个 JVM；甚至在老版本的 JDK 中如果没有设置堆内存大小，无法感知到容器的内存限制，从而自动生成的 Xmx 大于了容器的内存大小，以致于 OOM。\n","categories":["cloudnative"],"tags":["Java","K8s","JVM"]},{"title":"Grafana 变量转义处理","url":"/2023/06/26/k8s/grafana-variable/","content":"Grafana 是一款强大的可视化工具，不止是用于 Prometheus 做数据源，还可以集成数据库、日志等作为数据源整体使用。\n最近我在配置一个监控面板，其中的数据由 Prometheus 和 MySQL 组成；简单来说就是一个指标的查询条件是从数据库中来的。\n\n\npulsar_subscription_back_log_no_delayed&#123;topic=~&quot;$topic&quot;,subscription=~&quot;$subscription&quot;&#125;\n\n其中的 topic 数据是从  MySQL 中来的，其实就是在 Grafana 声明一个变量，从数据库返回了一个列表。\n\n因为我们的查询条件是 topic=~&quot;$topic&quot;是正则匹配，所以理论上应该把所有的 topic 关联的数据都查询出来。\n\n但实际情况是任何数据都查不到。\n查看发出去的原始请求后才发现问题出在哪里：\n\n原来是选择所有 topic 后 grafana 会~~~~自动对参数转义，这个我查了好多资料包括咨询 ChatGPT 都没有得到解决。\n经过多次测试，发现只要开启多选 grafana 就会自动转义。\n最后我只能想到一个不需要生成多行记录的办法：将所有数据合并成一条记录。\n\n这样的话就只会生成一条数据，其中包含了所有的 topic，也就避免了被转义。\n\nSQL 中的 CONCAT 函数其实我也不知道怎么使用，还是 ChatGPT 告诉我的。\n\n\n最后便能完美的查询出数据了。\n有碰到类似问题的朋友可以尝试这个方法，我估计用到这个场景的并不多，不然 ChatGPT 也不会不知道。\n","categories":["cloudnative"],"tags":["Grafana"]},{"title":"分享一些 Kafka 消费数据的小经验","url":"/2018/11/20/kafka/kafka-consumer/","content":"\n前言之前写过一篇《从源码分析如何优雅的使用 Kafka 生产者》 ，有生产者自然也就有消费者。\n\n建议对 Kakfa 还比较陌生的朋友可以先看看。\n\n就我的使用经验来说，大部分情况都是处于数据下游的消费者角色。也用 Kafka 消费过日均过亿的消息（不得不佩服 Kakfa 的设计），本文将借助我使用 Kakfa 消费数据的经验来聊聊如何高效的消费数据。\n\n\n单线程消费以之前生产者中的代码为例，事先准备好了一个 Topic:data-push，3个分区。\n先往里边发送 100 条消息，没有自定义路由策略，所以消息会均匀的发往三个分区。\n先来谈谈最简单的单线程消费，如下图所示：\n\n由于数据散列在三个不同分区，所以单个线程需要遍历三个分区将数据拉取下来。\n单线程消费的示例代码：\n\n这段代码大家在官网也可以找到：将数据取出放到一个内存缓冲中最后写入数据库的过程。\n\n先不讨论其中的 offset 的提交方式。\n\n\n\n通过消费日志可以看出：\n取出的 100 条数据确实是分别遍历了三个分区。\n单线程消费虽然简单，但存在以下几个问题：\n\n效率低下。如果分区数几十上百个，单线程无法高效的取出数据。\n可用性很低。一旦消费线程阻塞，甚至是进程挂掉，那么整个消费程序都将出现问题。\n\n多线程消费既然单线程有诸多问题，那是否可以用多线程来提高效率呢？\n在多线程之前不得不将消费模式分为两种进行探讨：消费组、独立消费者。\n这两种消费模式对应的处理方式有着很大的不同，所以很有必要单独来讲。\n独立消费者模式先从独立消费者模式谈起，这种模式相对于消费组来说用的相对小众一些。\n看一个简单示例即可知道它的用法：\n\n\n值得注意的是：独立消费者可以不设置 group.id 属性。\n\n也是发送100条消息，消费结果如下：\n\n通过 API 可以看出：我们可以手动指定需要消费哪些分区。\n比如 data-push Topic 有三个分区，我可以手动只消费其中的 1 2 分区，第三个可以视情况来消费。\n同时它也支持多线程的方式，每个线程消费指定分区进行消费。\n\n\n为了直观，只发送了 10 条数据。\n\n根据消费结果可以看出：\nc1 线程只取 0 分区；c2 只取 1 分区；c3 只取 2 分区的数据。\n甚至我们可以将消费者多进程部署，这样的消费方式如下：\n\n假设 Topic:data-push 的分区数为 4 个，那我们就可以按照图中的方式创建两个进程。\n每个进程内有两个线程，每个线程再去消费对应的分区。\n这样当我们性能不够新增 Topic 的分区数时，消费者这边只需要这样水平扩展即可，非常的灵活。\n这种自定义分区消费的方式在某些场景下还是适用的，比如生产者每次都将某一类的数据只发往一个分区。这样我们就可以只针对这一个分区消费。\n但这种方式有一个问题：可用性不高，当其中一个进程挂掉之后；该进程负责的分区数据没法转移给其他进程处理。\n消费组模式消费组模式应当是使用最多的一种消费方式。\n我们可以创建 N 个消费者实例（new KafkaConsumer()）,当这些实例都用同一个 group.id 来创建时，他们就属于同一个消费组。\n在同一个消费组中的消费实例可以收到消息，但一个分区的消息只会发往一个消费实例。\n还是借助官方的示例图来更好的理解它。\n\n某个 Topic 有四个分区 p0 p1 p2 p3，同时创建了两个消费组 groupA，groupB。\n\nA 消费组中有两个消费实例 C1、C2。\nB 消费组中有四个消费实例 C3、C4、C5、C6。\n\n这样消息是如何划分到每个消费实例的呢？\n通过图中可以得知：\n\nA 组中的 C1 消费了 P0 和 P3 分区；C2 消费 P1、P2 分区。\nB 组有四个实例，所以每个实例消费一个分区；也就是消费实例和分区是一一对应的。\n\n需要注意的是：\n\n这里的消费实例简单的可以理解为 new KafkaConsumer，它和进程没有关系。\n\n\n比如说某个 Topic 有三个分区，但是我启动了两个进程来消费它。\n其中每个进程有两个消费实例，那其实就相当于有四个实例了。\n这时可能就会问 4 个实例怎么消费 3 个分区呢？\n消费组自平衡这个 Kafka 已经帮我做好了，它会来做消费组里的 Rebalance。\n比如上面的情况，3 个分区却有 4 个消费实例；最终肯定只有三个实例能取到消息。但至于是哪三个呢，这点 Kakfa 会自动帮我们分配好。\n看个例子，还在之前的 data-push 这个 Topic，其中有三个分区。\n当其中一个进程（其中有三个线程，每个线程对应一个消费实例）时，消费结果如下：\n\n里边的 20 条数据都被这个进程的三个实例消费掉。\n这时我新启动了一个进程，程序和上面那个一模一样；这样就相当于有两个进程，同时就是 6 个实例。\n我再发送 10 条消息会发现：\n进程1 只取到了分区 1 里的两条数据（之前是所有数据都是进程1里的线程获取的）。\n\n\n同时进程2则消费了剩下的 8 条消息，分别是分区 0、2 的数据（总的还是只有三个实例取到了数据，只是分别在不同的进程里）。\n\n\n当我关掉进程2，再发送10条数据时会发现所有数据又被进程1里的三个线程消费了。\n\n通过这些测试相信大家已经可以看到消费组的优势了。\n\n我们可以在一个消费组中创建多个消费实例来达到高可用、高容错的特性，不会出现单线程以及独立消费者挂掉之后数据不能消费的情况。同时基于多线程的方式也极大的提高了消费效率。\n\n而当新增消费实例或者是消费实例挂掉时 Kakfa 会为我们重新分配消费实例与分区的关系就被称为消费组 Rebalance。\n发生这个的前提条件一般有以下几个：\n\n消费组中新增消费实例。\n消费组中消费实例 down 掉。\n订阅的 Topic 分区数发生变化。\n如果是正则订阅 Topic 时，匹配的 Topic 数发生变化也会导致 Rebalance。\n\n所以推荐使用这样的方式消费数据，同时扩展性也非常好。当性能不足新增分区时只需要启动新的消费实例加入到消费组中即可。\n总结本次只分享了几个不同消费数据的方式，并没有着重研究消费参数、源码；这些内容感兴趣的话可以在下次分享。\n文中提到的部分源码可以在这里查阅：\nhttps://github.com/crossoverJie/JCSprout\n欢迎关注公众号一起交流：\n","categories":["Kafka","Java 进阶"],"tags":["Kafka"]},{"title":"从源码分析如何优雅的使用 Kafka 生产者","url":"/2018/10/11/kafka/kafka-product/","content":"\n前言在上文 设计一个百万级的消息推送系统 中提到消息流转采用的是 Kafka 作为中间件。\n其中有朋友咨询在大量消息的情况下 Kakfa 是如何保证消息的高效及一致性呢？\n正好以这个问题结合 Kakfa 的源码讨论下如何正确、高效的发送消息。\n\n内容较多，对源码感兴趣的朋友请系好安全带😏(源码基于 v0.10.0.0 版本分析)。同时最好是有一定的 Kafka 使用经验，知晓基本的用法。\n\n\n\n简单的消息发送在分析之前先看一个简单的消息发送是怎么样的。\n\n以下代码基于 SpringBoot 构建。\n\n首先创建一个 org.apache.kafka.clients.producer.Producer 的 bean。\n\n主要关注 bootstrap.servers，它是必填参数。指的是 Kafka 集群中的 broker 地址，例如 127.0.0.1:9094。\n\n其余几个参数暂时不做讨论，后文会有详细介绍。\n\n接着注入这个 bean 即可调用它的发送函数发送消息。\n\n这里我给某一个 Topic 发送了 10W 条数据，运行程序消息正常发送。\n但这仅仅只是做到了消息发送，对消息是否成功送达完全没管，等于是纯异步的方式。\n同步那么我想知道消息到底发送成功没有该怎么办呢？\n其实 Producer 的 API 已经帮我们考虑到了，发送之后只需要调用它的 get() 方法即可同步获取发送结果。\n\n发送结果：\n\n这样的发送效率其实是比较低下的，因为每次都需要同步等待消息发送的结果。 \n异步为此我们应当采取异步的方式发送，其实 send() 方法默认则是异步的，只要不手动调用  get() 方法。\n但这样就没法获知发送结果。\n所以查看 send() 的 API 可以发现还有一个参数。\nFuture&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; producer, Callback callback);\n\nCallback 是一个回调接口，在消息发送完成之后可以回调我们自定义的实现。\n\n执行之后的结果：\n\n同样的也能获取结果，同时发现回调的线程并不是上文同步时的主线程，这样也能证明是异步回调的。\n同时回调的时候会传递两个参数：\n\nRecordMetadata 和上文一致的消息发送成功后的元数据。\nException 消息发送过程中的异常信息。\n\n但是这两个参数并不会同时都有数据，只有发送失败才会有异常信息，同时发送元数据为空。\n所以正确的写法应当是：\n\n\n至于为什么会只有参数一个有值，在下文的源码分析中会一一解释。\n\n源码分析现在只掌握了基本的消息发送，想要深刻的理解发送中的一些参数配置还是得源码说了算。\n首先还是来谈谈消息发送时的整个流程是怎么样的，Kafka 并不是简单的把消息通过网络发送到了 broker 中，在 Java 内部还是经过了许多优化和设计。\n发送流程为了直观的了解发送的流程，简单的画了几个在发送过程中关键的步骤。\n\n从上至下依次是：\n\n初始化以及真正发送消息的 kafka-producer-network-thread IO 线程。\n将消息序列化。\n得到需要发送的分区。\n写入内部的一个缓存区中。\n初始化的 IO 线程不断的消费这个缓存来发送消息。\n\n步骤解析接下来详解每个步骤。\n初始化\n调用该构造方法进行初始化时，不止是简单的将基本参数写入 KafkaProducer。比较麻烦的是初始化 Sender 线程进行缓冲区消费。\n初始化 IO 线程处：\n\n可以看到 Sender 线程有需要成员变量，比如：\nacks,retries,requestTimeout\n\n等，这些参数会在后文分析。\n序列化消息在调用 send() 函数后其实第一步就是序列化，毕竟我们的消息需要通过网络才能发送到 Kafka。\n\n其中的 valueSerializer.serialize(record.topic(), record.value()); 是一个接口，我们需要在初始化时候指定序列化实现类。\n\n我们也可以自己实现序列化，只需要实现 org.apache.kafka.common.serialization.Serializer 接口即可。\n路由分区接下来就是路由分区，通常我们使用的 Topic 为了实现扩展性以及高性能都会创建多个分区。\n如果是一个分区好说，所有消息都往里面写入即可。\n但多个分区就不可避免需要知道写入哪个分区。\n通常有三种方式。\n指定分区可以在构建 ProducerRecord 为每条消息指定分区。\n\n这样在路由时会判断是否有指定，有就直接使用该分区。\n\n这种一般在特殊场景下会使用。\n自定义路由策略\n如果没有指定分区，则会调用 partitioner.partition 接口执行自定义分区策略。\n而我们也只需要自定义一个类实现 org.apache.kafka.clients.producer.Partitioner 接口，同时在创建 KafkaProducer 实例时配置 partitioner.class 参数。\n\n通常需要自定义分区一般是在想尽量的保证消息的顺序性。\n或者是写入某些特有的分区，由特别的消费者来进行处理等。\n默认策略最后一种则是默认的路由策略，如果我们啥都没做就会执行该策略。\n该策略也会使得消息分配的比较均匀。\n来看看它的实现：\n\n简单的来说分为以下几步：\n\n获取 Topic 分区数。\n将内部维护的一个线程安全计数器 +1。\n与分区数取模得到分区编号。\n\n其实这就是很典型的轮询算法，所以只要分区数不频繁变动这种方式也会比较均匀。\n写入内部缓存在 send() 方法拿到分区后会调用一个 append() 函数：\n\n该函数中会调用一个 getOrCreateDeque() 写入到一个内部缓存中 batches。\n\n消费缓存在最开始初始化的 IO 线程其实是一个守护线程，它会一直消费这些数据。\n\n通过图中的几个函数会获取到之前写入的数据。这块内容可以不必深究，但其中有个 completeBatch 方法却非常关键。\n\n调用该方法时候肯定已经是消息发送完毕了，所以会调用 batch.done() 来完成之前我们在 send() 方法中定义的回调接口。\n\n\n从这里也可以看出为什么之前说发送完成后元数据和异常信息只会出现一个。\n\nProducer 参数解析发送流程讲完了再来看看 Producer 中比较重要的几个参数。\nacksacks 是一个影响消息吞吐量的一个关键参数。\n\n主要有 [all、-1, 0, 1] 这几个选项，默认为 1。\n由于 Kafka 不是采取的主备模式，而是采用类似于 Zookeeper 的主备模式。\n\n前提是 Topic 配置副本数量 replica &gt; 1。 \n\n当 acks = all/-1 时：\n意味着会确保所有的 follower 副本都完成数据的写入才会返回。\n这样可以保证消息不会丢失！\n\n但同时性能和吞吐量却是最低的。\n\n当 acks = 0 时：\nproducer 不会等待副本的任何响应，这样最容易丢失消息但同时性能却是最好的！\n当 acks = 1 时：\n这是一种折中的方案，它会等待副本 Leader 响应，但不会等到 follower 的响应。\n一旦 Leader 挂掉消息就会丢失。但性能和消息安全性都得到了一定的保证。\nbatch.size这个参数看名称就知道是内部缓存区的大小限制，对他适当的调大可以提高吞吐量。\n但也不能极端，调太大会浪费内存。小了也发挥不了作用，也是一个典型的时间和空间的权衡。\n\n\n上图是几个使用的体现。\nretriesretries 该参数主要是来做重试使用，当发生一些网络抖动都会造成重试。\n这个参数也就是限制重试次数。\n但也有一些其他问题。\n\n因为是重发所以消息顺序可能不会一致，这也是上文提到就算是一个分区消息也不会是完全顺序的情况。\n还是由于网络问题，本来消息已经成功写入了但是没有成功响应给 producer，进行重试时就可能会出现消息重复。这种只能是消费者进行幂等处理。\n\n高效的发送方式如果消息量真的非常大，同时又需要尽快的将消息发送到 Kafka。一个 producer 始终会收到缓存大小等影响。\n那是否可以创建多个 producer 来进行发送呢？\n\n配置一个最大 producer 个数。\n发送消息时首先获取一个 producer，获取的同时判断是否达到最大上限，没有就新建一个同时保存到内部的 List 中，保存时做好同步处理防止并发问题。\n获取发送者时可以按照默认的分区策略使用轮询的方式获取（保证使用均匀）。\n\n这样在大量、频繁的消息发送场景中可以提高发送效率减轻单个 producer 的压力。\n关闭 Producer最后则是 Producer 的关闭，Producer 在使用过程中消耗了不少资源（线程、内存、网络等）因此需要显式的关闭从而回收这些资源。\n\n默认的 close() 方法和带有超时时间的方法都是在一定的时间后强制关闭。\n但在过期之前都会处理完剩余的任务。\n所以使用哪一个得视情况而定。\n总结本文内容较多，从实例和源码的角度分析了 Kafka 生产者。\n希望看完的朋友能有收获，同时也欢迎留言讨论。\n不出意外下期会讨论 Kafka 消费者。\n\n如果对你有帮助还请分享让更多的人看到。\n\n欢迎关注公众号一起交流：\n","categories":["Kafka","Java 进阶"],"tags":["Kafka"]},{"title":"从源码彻底理解 Prometheus/VictoriaMetrics 中的 relabel_configs/metric_relabel_configs 配置","url":"/2023/03/13/metrics/relabel_configs_%20metric_relabel_configs/","content":"\n背景最近接手维护了公司的指标监控系统，之后踩到坑就没站起来过。。\n\n\n本次问题的起因是我们配置了一些指标的删除策略没有生效：\n- action: drop_metrics  regex: &quot;^envoy_.*|^url\\_\\_\\_\\_.*|istio_request_bytes_sum&quot;\n\n与这两个容易引起误解的配置relabel_configs/metric_relabel_configs有关。\n他们都是对抓取的数据进行重命名、过滤、新增、删除等操作，但应用场景却完全不同。\n\n我们使用了 VictoriaMetrics 替换了 Prometheus，VM 完全兼容 Prometheus ，所以本文也对 Prometheus 同样适用。\n\n理解错误1但这里其实是有一个错误理解的，我是通过 VM 的服务发现页面的指标响应页面查询指标的，打开之后确实能搜到需要被删除的相关指标。\n但其实即便是真的删除了数据这个页面也会有数据存在，删除的数据只是不会写入 VM 的时序数据库中。\n\n这一点是在后续查源码时才发现；后面我配置对了依然在这里查看数据，发现还是没有删除，这个错误理解浪费了不少时间😂。\n\n理解错误2为了解决问题，通过 drop metrics 这类关键字在 VM 的官方文档中查询，最终找到一篇文章。https://www.robustperception.io/dropping-metrics-at-scrape-time-with-prometheus/\n按照这里的介绍，将删除的配置加入到 metric_relabel_configs 配置下，经过测试确实有效。\n不过为啥将同样的配置：\nrelabel_configs:    - action: drop_metrics      regex: &quot;^envoy_.*|^url\\_\\_\\_\\_.*|istio_request_bytes_sum&quot;\n\n加入到 relabel_configs 未能生效呢？\n估计确实容易令人误导，在文档中也找到了相关的解释：https://www.robustperception.io/relabel_configs-vs-metric_relabel_configs&#x2F;这篇文章主要是表达几个重点：\n\nrelabel_configs 用于配置哪个目标需要被抓取，发生在指标抓取之前。\nmetric_relabel_configs 发生在指标抓取之后，写入存储之前。\n如果其中一个没生效，就换一个（这句话很容易让人犯迷糊）\n\n但说实话当时我看到这里还是一脸懵，为了彻底了解两则的区别还是看源码来的直接。\n阅读源码理解本质原因metric_relabel_configsmetric_relabel_configs:    - action: drop_metrics      regex: &quot;^envoy_.*|^url\\_\\_\\_\\_.*|istio_request_bytes_sum&quot;\n首先看下metric_relabel_configs配置生效的原因。\n\nmetric_relabel_configs 配置的整体流程如上图：\n\n启动 VM 时加载配置到内存\n根据配置的抓取间隔时间(scrape_interval)抓取数据，拿到的每一条数据都需要通过 metric_relabel_configs 的应用。\n针对于这里的 drop_metrics 来说，就是判断是否需要删除掉所有的 Label。\n如果可以匹配删除，那就不会写入存储。\n\n其中的关键代码如下：\n这里还有一个小细节，源码里判断的 action 是 drop，而我们配置的是 drop_metrics，其实 drop_metrics 也是 drop 的一个封装而已。\n在解析配置的时候会进行转换。\n与这个写法是等价的：\n- source_labels: [ __name__ ]  regex: &quot;^envoy_.*|^url\\_\\_\\_\\_.*|istio_request_bytes_sum&quot;  action: drop\n\nrelabel_configs然后来看看 relabel_configs 没有按照预期生效的原因。\n\n其实核心的应用配置就是同一份代码，只是触发点不一样。\nrelabel_configs 是在应用启动的时候根据我们配置的抓取目标的数据当做数据源，所以这里的 action: drop 删除的是抓取目标，而不是真正的抓取数据。\n而且它的目的是在应用启动的时候，用于生成抓取目标的任务，只会运行一次。\n假设我这里改写为：\nrelabel_configs:    - source_labels: [ __address__ ]      regex: &#x27;192.xx.xx.xx:443&#x27;      action: drop\n那么我这个抓取任务就会被删除掉，而不是删除这个指标了。\n因此之前我在这里配置的是一些业务指标 regex: &quot;^envoy_.*|^url\\_\\_\\_\\_.*|istio_request_bytes_sum&quot;，在所有元数据里自然是没有任何一个可以匹配了，所以也就无事发生。\n\n元数据都是以 __ 开头。\n\n\n其实 VM 也有提供一个 Debug 页面用于调试 relabel_configs，但如果知道怎么用这个调试页面其实也理解了他的运行原理😂\n总结https://www.robustperception.io/relabelling-can-discard-targets-timeseries-and-alerts/ \n后面我查到这篇文章也有相关解释，理解了两者的区别后再看这里的分析会更加容易理解。\n总的来说：\n\nrelabel_configs 用于对抓取目标元数据的增删改；如果删除后连后续的抓取任务也会被取消。\nmetric_relabel_configs 用于对抓取到的数据增删改，对于不需要的业务指标可以在这里配置。\n\n也就是前文讲到的 relabel_configs 应用于指标抓取前，metric_relabel_configs 应用于指标抓取后。\n","categories":["metrics"],"tags":["K8s","Prometheus","VictoriaMetrics"]},{"title":"Netty(一) SpringBoot 整合长连接心跳机制","url":"/2018/05/24/netty/Netty(1)TCP-Heartbeat/","content":"\n前言Netty 是一个高性能的 NIO 网络框架，本文基于 SpringBoot 以常见的心跳机制来认识 Netty。\n最终能达到的效果：\n\n客户端每隔 N 秒检测是否需要发送心跳。\n服务端也每隔 N 秒检测是否需要发送心跳。\n服务端可以主动 push 消息到客户端。\n基于 SpringBoot 监控，可以查看实时连接以及各种应用信息。\n\n效果如下：\n\n\n\nIdleStateHandlerNetty 可以使用 IdleStateHandler 来实现连接管理，当连接空闲时间太长（没有发送、接收消息）时则会触发一个事件，我们便可在该事件中实现心跳机制。\n客户端心跳当客户端空闲了 N 秒没有给服务端发送消息时会自动发送一个心跳来维持连接。\n核心代码代码如下：\npublic class EchoClientHandle extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(EchoClientHandle.class);    @Override    public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123;        if (evt instanceof IdleStateEvent)&#123;            IdleStateEvent idleStateEvent = (IdleStateEvent) evt ;            if (idleStateEvent.state() == IdleState.WRITER_IDLE)&#123;                LOGGER.info(&quot;已经 10 秒没有发送信息！&quot;);                //向服务端发送消息                CustomProtocol heartBeat = SpringBeanFactory.getBean(&quot;heartBeat&quot;, CustomProtocol.class);                ctx.writeAndFlush(heartBeat).addListener(ChannelFutureListener.CLOSE_ON_FAILURE) ;            &#125;        &#125;        super.userEventTriggered(ctx, evt);    &#125;    @Override    protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf in) throws Exception &#123;        //从服务端收到消息时被调用        LOGGER.info(&quot;客户端收到消息=&#123;&#125;&quot;,in.toString(CharsetUtil.UTF_8)) ;    &#125;&#125;    \n\n实现非常简单，只需要在事件回调中发送一个消息即可。\n由于整合了 SpringBoot ，所以发送的心跳信息是一个单例的 Bean。\n@Configurationpublic class HeartBeatConfig &#123;    @Value(&quot;$&#123;channel.id&#125;&quot;)    private long id ;    @Bean(value = &quot;heartBeat&quot;)    public CustomProtocol heartBeat()&#123;        return new CustomProtocol(id,&quot;ping&quot;) ;    &#125;&#125;\n\n这里涉及到了自定义协议的内容，请继续查看下文。\n当然少不了启动引导：\n@Componentpublic class HeartbeatClient &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(HeartbeatClient.class);    private EventLoopGroup group = new NioEventLoopGroup();    @Value(&quot;$&#123;netty.server.port&#125;&quot;)    private int nettyPort;    @Value(&quot;$&#123;netty.server.host&#125;&quot;)    private String host;    private SocketChannel channel;    @PostConstruct    public void start() throws InterruptedException &#123;        Bootstrap bootstrap = new Bootstrap();        bootstrap.group(group)                .channel(NioSocketChannel.class)                .handler(new CustomerHandleInitializer())        ;        ChannelFuture future = bootstrap.connect(host, nettyPort).sync();        if (future.isSuccess()) &#123;            LOGGER.info(&quot;启动 Netty 成功&quot;);        &#125;        channel = (SocketChannel) future.channel();    &#125;    &#125;public class CustomerHandleInitializer extends ChannelInitializer&lt;Channel&gt; &#123;    @Override    protected void initChannel(Channel ch) throws Exception &#123;        ch.pipeline()                //10 秒没发送消息 将IdleStateHandler 添加到 ChannelPipeline 中                .addLast(new IdleStateHandler(0, 10, 0))                .addLast(new HeartbeatEncode())                .addLast(new EchoClientHandle())        ;    &#125;&#125;    \n\n所以当应用启动每隔 10 秒会检测是否发送过消息，不然就会发送心跳信息。\n\n服务端心跳服务器端的心跳其实也是类似，也需要在 ChannelPipeline 中添加一个 IdleStateHandler 。\npublic class HeartBeatSimpleHandle extends SimpleChannelInboundHandler&lt;CustomProtocol&gt; &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(HeartBeatSimpleHandle.class);    private static final ByteBuf HEART_BEAT =  Unpooled.unreleasableBuffer(Unpooled.copiedBuffer(new CustomProtocol(123456L,&quot;pong&quot;).toString(),CharsetUtil.UTF_8));    /**     * 取消绑定     * @param ctx     * @throws Exception     */    @Override    public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123;        NettySocketHolder.remove((NioSocketChannel) ctx.channel());    &#125;    @Override    public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123;        if (evt instanceof IdleStateEvent)&#123;            IdleStateEvent idleStateEvent = (IdleStateEvent) evt ;            if (idleStateEvent.state() == IdleState.READER_IDLE)&#123;                LOGGER.info(&quot;已经5秒没有收到信息！&quot;);                //向客户端发送消息                ctx.writeAndFlush(HEART_BEAT).addListener(ChannelFutureListener.CLOSE_ON_FAILURE) ;            &#125;        &#125;        super.userEventTriggered(ctx, evt);    &#125;    @Override    protected void channelRead0(ChannelHandlerContext ctx, CustomProtocol customProtocol) throws Exception &#123;        LOGGER.info(&quot;收到customProtocol=&#123;&#125;&quot;, customProtocol);        //保存客户端与 Channel 之间的关系        NettySocketHolder.put(customProtocol.getId(),(NioSocketChannel)ctx.channel()) ;    &#125;&#125;\n\n这里有点需要注意：\n当有多个客户端连上来时，服务端需要区分开，不然响应消息就会发生混乱。\n所以每当有个连接上来的时候，我们都将当前的 Channel 与连上的客户端 ID 进行关联（因此每个连上的客户端 ID 都必须唯一）。\n这里采用了一个 Map 来保存这个关系，并且在断开连接时自动取消这个关联。\npublic class NettySocketHolder &#123;    private static final Map&lt;Long, NioSocketChannel&gt; MAP = new ConcurrentHashMap&lt;&gt;(16);    public static void put(Long id, NioSocketChannel socketChannel) &#123;        MAP.put(id, socketChannel);    &#125;    public static NioSocketChannel get(Long id) &#123;        return MAP.get(id);    &#125;    public static Map&lt;Long, NioSocketChannel&gt; getMAP() &#123;        return MAP;    &#125;    public static void remove(NioSocketChannel nioSocketChannel) &#123;        MAP.entrySet().stream().filter(entry -&gt; entry.getValue() == nioSocketChannel).forEach(entry -&gt; MAP.remove(entry.getKey()));    &#125;&#125;\n\n启动引导程序：\nComponentpublic class HeartBeatServer &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(HeartBeatServer.class);    private EventLoopGroup boss = new NioEventLoopGroup();    private EventLoopGroup work = new NioEventLoopGroup();    @Value(&quot;$&#123;netty.server.port&#125;&quot;)    private int nettyPort;    /**     * 启动 Netty     *     * @return     * @throws InterruptedException     */    @PostConstruct    public void start() throws InterruptedException &#123;        ServerBootstrap bootstrap = new ServerBootstrap()                .group(boss, work)                .channel(NioServerSocketChannel.class)                .localAddress(new InetSocketAddress(nettyPort))                //保持长连接                .childOption(ChannelOption.SO_KEEPALIVE, true)                .childHandler(new HeartbeatInitializer());        ChannelFuture future = bootstrap.bind().sync();        if (future.isSuccess()) &#123;            LOGGER.info(&quot;启动 Netty 成功&quot;);        &#125;    &#125;    /**     * 销毁     */    @PreDestroy    public void destroy() &#123;        boss.shutdownGracefully().syncUninterruptibly();        work.shutdownGracefully().syncUninterruptibly();        LOGGER.info(&quot;关闭 Netty 成功&quot;);    &#125;&#125;    public class HeartbeatInitializer extends ChannelInitializer&lt;Channel&gt; &#123;    @Override    protected void initChannel(Channel ch) throws Exception &#123;        ch.pipeline()                //五秒没有收到消息 将IdleStateHandler 添加到 ChannelPipeline 中                .addLast(new IdleStateHandler(5, 0, 0))                .addLast(new HeartbeatDecoder())                .addLast(new HeartBeatSimpleHandle());    &#125;&#125;\n\n也是同样将IdleStateHandler 添加到 ChannelPipeline 中，也会有一个定时任务，每5秒校验一次是否有收到消息，否则就主动发送一次请求。\n\n因为测试是有两个客户端连上所以有两个日志。\n自定义协议上文其实都看到了：服务端与客户端采用的是自定义的 POJO 进行通讯的。\n所以需要在客户端进行编码，服务端进行解码，也都只需要各自实现一个编解码器即可。\nCustomProtocol：\npublic class CustomProtocol implements Serializable&#123;    private static final long serialVersionUID = 4671171056588401542L;    private long id ;    private String content ;    //省略 getter/setter&#125;\n\n客户端的编码器：\npublic class HeartbeatEncode extends MessageToByteEncoder&lt;CustomProtocol&gt; &#123;    @Override    protected void encode(ChannelHandlerContext ctx, CustomProtocol msg, ByteBuf out) throws Exception &#123;        out.writeLong(msg.getId()) ;        out.writeBytes(msg.getContent().getBytes()) ;    &#125;&#125;\n\n也就是说消息的前八个字节为 header，剩余的全是 content。\n服务端的解码器：\npublic class HeartbeatDecoder extends ByteToMessageDecoder &#123;    @Override    protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123;        long id = in.readLong() ;        byte[] bytes = new byte[in.readableBytes()] ;        in.readBytes(bytes) ;        String content = new String(bytes) ;        CustomProtocol customProtocol = new CustomProtocol() ;        customProtocol.setId(id);        customProtocol.setContent(content) ;        out.add(customProtocol) ;    &#125;&#125;\n\n只需要按照刚才的规则进行解码即可。\n实现原理其实联想到 IdleStateHandler 的功能，自然也能想到它实现的原理：\n\n应该会存在一个定时任务的线程去处理这些消息。\n\n来看看它的源码：\n首先是构造函数:\npublic IdleStateHandler(        int readerIdleTimeSeconds,        int writerIdleTimeSeconds,        int allIdleTimeSeconds) &#123;    this(readerIdleTimeSeconds, writerIdleTimeSeconds, allIdleTimeSeconds,         TimeUnit.SECONDS);&#125;\n\n其实就是初始化了几个数据：\n\nreaderIdleTimeSeconds：一段时间内没有数据读取\nwriterIdleTimeSeconds：一段时间内没有数据发送\nallIdleTimeSeconds：以上两种满足其中一个即可\n\n因为 IdleStateHandler 也是一种 ChannelHandler，所以会在 channelActive 中初始化任务：\n@Overridepublic void channelActive(ChannelHandlerContext ctx) throws Exception &#123;    // This method will be invoked only if this handler was added    // before channelActive() event is fired.  If a user adds this handler    // after the channelActive() event, initialize() will be called by beforeAdd().    initialize(ctx);    super.channelActive(ctx);&#125;private void initialize(ChannelHandlerContext ctx) &#123;    // Avoid the case where destroy() is called before scheduling timeouts.    // See: https://github.com/netty/netty/issues/143    switch (state) &#123;    case 1:    case 2:        return;    &#125;    state = 1;    initOutputChanged(ctx);    lastReadTime = lastWriteTime = ticksInNanos();    if (readerIdleTimeNanos &gt; 0) &#123;        readerIdleTimeout = schedule(ctx, new ReaderIdleTimeoutTask(ctx),                readerIdleTimeNanos, TimeUnit.NANOSECONDS);    &#125;    if (writerIdleTimeNanos &gt; 0) &#123;        writerIdleTimeout = schedule(ctx, new WriterIdleTimeoutTask(ctx),                writerIdleTimeNanos, TimeUnit.NANOSECONDS);    &#125;    if (allIdleTimeNanos &gt; 0) &#123;        allIdleTimeout = schedule(ctx, new AllIdleTimeoutTask(ctx),                allIdleTimeNanos, TimeUnit.NANOSECONDS);    &#125;&#125;    \n\n也就是会按照我们给定的时间初始化出定时任务。\n接着在任务真正执行时进行判断：\nprivate final class ReaderIdleTimeoutTask extends AbstractIdleTask &#123;    ReaderIdleTimeoutTask(ChannelHandlerContext ctx) &#123;        super(ctx);    &#125;    @Override    protected void run(ChannelHandlerContext ctx) &#123;        long nextDelay = readerIdleTimeNanos;        if (!reading) &#123;            nextDelay -= ticksInNanos() - lastReadTime;        &#125;        if (nextDelay &lt;= 0) &#123;            // Reader is idle - set a new timeout and notify the callback.            readerIdleTimeout = schedule(ctx, this, readerIdleTimeNanos, TimeUnit.NANOSECONDS);            boolean first = firstReaderIdleEvent;            firstReaderIdleEvent = false;            try &#123;                IdleStateEvent event = newIdleStateEvent(IdleState.READER_IDLE, first);                channelIdle(ctx, event);            &#125; catch (Throwable t) &#123;                ctx.fireExceptionCaught(t);            &#125;        &#125; else &#123;            // Read occurred before the timeout - set a new timeout with shorter delay.            readerIdleTimeout = schedule(ctx, this, nextDelay, TimeUnit.NANOSECONDS);        &#125;    &#125;&#125;\n\n如果满足条件则会生成一个 IdleStateEvent 事件。\nSpringBoot 监控由于整合了 SpringBoot 之后不但可以利用 Spring 帮我们管理对象，也可以利用它来做应用监控。\nactuator 监控当我们为引入了:\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;\n\n就开启了 SpringBoot 的 actuator 监控功能，他可以暴露出很多监控端点供我们使用。\n如一些应用中的一些统计数据：\n存在的 Beans：\n更多信息请查看：https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html\n但是如果我想监控现在我的服务端有多少客户端连上来了，分别的 ID 是多少？\n其实就是实时查看我内部定义的那个关联关系的 Map。\n这就需要暴露自定义端点了。\n自定义端点暴露的方式也很简单：\n继承 AbstractEndpoint 并复写其中的 invoke 函数：\npublic class CustomEndpoint extends AbstractEndpoint&lt;Map&lt;Long,NioSocketChannel&gt;&gt; &#123;    /**     * 监控端点的 访问地址     * @param id     */    public CustomEndpoint(String id) &#123;        //false 表示不是敏感端点        super(id, false);    &#125;    @Override    public Map&lt;Long, NioSocketChannel&gt; invoke() &#123;        return NettySocketHolder.getMAP();    &#125;&#125;\n\n其实就是返回了 Map 中的数据。\n再配置一个该类型的 Bean 即可：\n@Configurationpublic class EndPointConfig &#123;    @Value(&quot;$&#123;monitor.channel.map.key&#125;&quot;)    private String channelMap;    @Bean    public CustomEndpoint buildEndPoint()&#123;        CustomEndpoint customEndpoint = new CustomEndpoint(channelMap) ;        return customEndpoint ;    &#125;&#125;\n\n这样我们就可以通过配置文件中的 monitor.channel.map.key 来访问了：\n一个客户端连接时：\n两个客户端连接时：\n整合 SBA这样其实监控功能已经可以满足了，但能不能展示的更美观、并且多个应用也可以方便查看呢？\n有这样的开源工具帮我们做到了：\nhttps://github.com/codecentric/spring-boot-admin\n简单来说我们可以利用该工具将 actuator 暴露出来的接口可视化并聚合的展示在页面中：\n\n接入也很简单，首先需要引入依赖：\n&lt;dependency&gt;    &lt;groupId&gt;de.codecentric&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-admin-starter-client&lt;/artifactId&gt;&lt;/dependency&gt;        \n\n并在配置文件中加入：\n# 关闭健康检查权限management.security.enabled=false# SpringAdmin 地址spring.boot.admin.url=http://127.0.0.1:8888\n\n在启动应用之前先讲 SpringBootAdmin 部署好：\n这个应用就是一个纯粹的 SpringBoot ，只需要在主函数上加入 @EnableAdminServer 注解。\n@SpringBootApplication@Configuration@EnableAutoConfiguration@EnableAdminServerpublic class AdminApplication &#123;\tpublic static void main(String[] args) &#123;\t\tSpringApplication.run(AdminApplication.class, args);\t&#125;&#125;\n\n引入：\n&lt;dependency&gt;\t&lt;groupId&gt;de.codecentric&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-admin-starter-server&lt;/artifactId&gt;\t&lt;version&gt;1.5.7&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;de.codecentric&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-admin-server-ui&lt;/artifactId&gt;\t&lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt;\n\n之后直接启动就行了。\n这样我们在 SpringBootAdmin 的页面中就可以查看很多应用信息了。\n\n更多内容请参考官方指南：\nhttp://codecentric.github.io/spring-boot-admin/1.5.6/\n自定义监控数据其实我们完全可以借助 actuator 以及这个可视化页面帮我们监控一些简单的度量信息。\n比如我在客户端和服务端中写了两个 Rest 接口用于向对方发送消息。\n只是想要记录分别发送了多少次：\n客户端：\n@Controller@RequestMapping(&quot;/&quot;)public class IndexController &#123;    /**     * 统计 service     */    @Autowired    private CounterService counterService;    @Autowired    private HeartbeatClient heartbeatClient ;    /**     * 向服务端发消息     * @param sendMsgReqVO     * @return     */    @ApiOperation(&quot;客户端发送消息&quot;)    @RequestMapping(&quot;sendMsg&quot;)    @ResponseBody    public BaseResponse&lt;SendMsgResVO&gt; sendMsg(@RequestBody SendMsgReqVO sendMsgReqVO)&#123;        BaseResponse&lt;SendMsgResVO&gt; res = new BaseResponse();        heartbeatClient.sendMsg(new CustomProtocol(sendMsgReqVO.getId(),sendMsgReqVO.getMsg())) ;        // 利用 actuator 来自增        counterService.increment(Constants.COUNTER_CLIENT_PUSH_COUNT);        SendMsgResVO sendMsgResVO = new SendMsgResVO() ;        sendMsgResVO.setMsg(&quot;OK&quot;) ;        res.setCode(StatusEnum.SUCCESS.getCode()) ;        res.setMessage(StatusEnum.SUCCESS.getMessage()) ;        res.setDataBody(sendMsgResVO) ;        return res ;    &#125;&#125;\n\n只要我们引入了 actuator 的包，那就可以直接注入 counterService ，利用它来帮我们记录数据。\n当我们调用该接口时：\n\n\n在监控页面中可以查询刚才的调用情况：\n\n服务端主动 push 消息也是类似，只是需要在发送时候根据客户端的 ID 查询到具体的 Channel 发送：\n\n\n\n总结以上就是一个简单 Netty 心跳示例，并演示了 SpringBoot 的监控，之后会继续更新 Netty 相关内容，欢迎关注及指正。\n本文所有代码：\nhttps://github.com/crossoverJie/netty-action\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["Netty"],"tags":["SpringBoot","TCP","Heartbeat"]},{"title":"Netty(二) 从线程模型的角度看 Netty 为什么是高性能的？","url":"/2018/07/04/netty/Netty(2)Thread-model/","content":"\n前言在之前的 SpringBoot 整合长连接心跳机制 一文中认识了 Netty。\n但其实只是能用，为什么要用 Netty？它有哪些优势？这些其实都不清楚。\n本文就来从历史源头说道说道。\n传统 IO在 Netty 以及 NIO 出现之前，我们写 IO 应用其实用的都是用 java.io.* 下所提供的包。  \n比如下面的伪代码：\nServeSocket serverSocket = new ServeSocket(8080);Socket socket = serverSocket.accept() ;BufferReader in = .... ;String request ; while((request = in.readLine()) != null)&#123;\tnew Thread(new Task()).start()&#125;\n\n\n\n大概是这样，其实主要想表达的是：这样一个线程只能处理一个连接。\n如果是 100 个客户端连接那就得开 100 个线程，1000 那就得 1000 个线程。\n要知道线程资源非常宝贵，每次的创建都会带来消耗，而且每个线程还得为它分配对应的栈内存。\n即便是我们给 JVM 足够的内存，大量线程所带来的上下文切换也是受不了的。\n\n并且传统 IO 是阻塞模式，每一次的响应必须的是发起 IO 请求，处理请求完成再同时返回，直接的结果就是性能差，吞吐量低。\n\nReactor 模型因此业界常用的高性能 IO 模型是 Reactor。\n它是一种异步、非阻塞的事件驱动模型。\n通常也表现为以下三种方式：\n单线程\n从图中可以看出：\n它是由一个线程来接收客户端的连接，并将该请求分发到对应的事件处理 handler 中，整个过程完全是异步非阻塞的；并且完全不存在共享资源的问题。所以理论上来说吞吐量也还不错。\n\n但由于是一个线程，对多核 CPU 利用率不高，一旦有大量的客户端连接上来性能必然下降，甚至会有大量请求无法响应。最坏的情况是一旦这个线程哪里没有处理好进入了死循环那整个服务都将不可用！\n\n多线程\n因此产生了多线程模型。\n其实最大的改进就是将原有的事件处理改为了多线程。\n可以基于 Java 自身的线程池实现，这样在大量请求的处理上性能提示是巨大的。\n虽然如此，但理论上来说依然有一个地方是单点的；那就是处理客户端连接的线程。\n因为大多数服务端应用或多或少在连接时都会处理一些业务，如鉴权之类的，当连接的客户端越来越多时这一个线程依然会存在性能问题。\n于是又有了下面的线程模型。\n主从多线程\n该模型将客户端连接那一块的线程也改为多线程，称为主线程。\n同时也是多个子线程来处理事件响应，这样无论是连接还是事件都是高性能的。\nNetty 实现以上谈了这么多其实 Netty 的线程模型与之的类似。\n我们回到之前 SpringBoot 整合长连接心跳机制 中的服务端代码：\nprivate EventLoopGroup boss = new NioEventLoopGroup();private EventLoopGroup work = new NioEventLoopGroup();/** * 启动 Netty * * @return * @throws InterruptedException */@PostConstructpublic void start() throws InterruptedException &#123;    ServerBootstrap bootstrap = new ServerBootstrap()            .group(boss, work)            .channel(NioServerSocketChannel.class)            .localAddress(new InetSocketAddress(nettyPort))            //保持长连接            .childOption(ChannelOption.SO_KEEPALIVE, true)            .childHandler(new HeartbeatInitializer());    ChannelFuture future = bootstrap.bind().sync();    if (future.isSuccess()) &#123;        LOGGER.info(&quot;启动 Netty 成功&quot;);    &#125;&#125;\n\n其实这里的 boss 就相当于 Reactor 模型中处理客户端连接的线程池。\nwork 自然就是处理事件的线程池了。\n那么如何来实现上文的三种模式呢？其实也很简单：\n单线程模型：\nprivate EventLoopGroup group = new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap()                .group(group)                .childHandler(new HeartbeatInitializer());\n\n多线程模型：\nprivate EventLoopGroup boss = new NioEventLoopGroup(1);private EventLoopGroup work = new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap()                .group(boss,work)                .childHandler(new HeartbeatInitializer());\n\n主从多线程：\nprivate EventLoopGroup boss = new NioEventLoopGroup();private EventLoopGroup work = new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap()                .group(boss,work)                .childHandler(new HeartbeatInitializer());\n\n相信大家一看也明白。\n总结其实看过了 Netty 的线程模型之后能否对我们平时做高性能应用带来点启发呢？\n我认为是可以的：\n\n接口同步转异步处理。\n回调通知结果。\n多线程提高并发效率。\n\n无非也就是这些，只是做了这些之后就会带来其他问题：\n\n异步之后事务如何保证？\n回调失败的情况？\n多线程所带来的上下文切换、共享资源的问题。\n\n这就是一个博弈的过程，想要做到一个尽量高效的应用是需要不断磨合试错的。\n上文相关的代码：\nhttps://github.com/crossoverJie/netty-action\n欢迎关注公众号一起交流：\n","categories":["Netty"],"tags":["内存模型"]},{"title":"Netty(三) 什么是 TCP 拆、粘包？如何解决？","url":"/2018/08/03/netty/Netty(3)TCP-Sticky/","content":"\n前言记得前段时间我们生产上的一个网关出现了故障。\n这个网关逻辑非常简单，就是接收客户端的请求然后解析报文最后发送短信。\n但这个请求并不是常见的 HTTP ，而是利用 Netty 自定义的协议。\n\n有个前提是：网关是需要读取一段完整的报文才能进行后面的逻辑。\n\n问题是有天突然发现网关解析报文出错，查看了客户端的发送日志也没发现问题，最后通过日志发现收到了许多不完整的报文，有些还多了。\n于是想会不会是 TCP 拆、粘包带来的问题，最后利用 Netty 自带的拆包工具解决了该问题。\n这便有了此文。\n\n\nTCP 协议问题虽然解决了，但还是得想想原因，为啥会这样？打破砂锅问到底才是一个靠谱的程序员。\n这就得从 TCP 这个协议说起了。\nTCP 是一个面向字节流的协议，它是性质是流式的，所以它并没有分段。就像水流一样，你没法知道什么时候开始，什么时候结束。\n所以他会根据当前的套接字缓冲区的情况进行拆包或是粘包。\n下图展示了一个 TCP 协议传输的过程：\n\n发送端的字节流都会先传入缓冲区，再通过网络传入到接收端的缓冲区中，最终由接收端获取。\n当我们发送两个完整包到接收端的时候：\n\n正常情况会接收到两个完整的报文。\n\n但也有以下的情况：\n\n接收到的是一个报文，它是由发送的两个报文组成的，这样对于应用程序来说就很难处理了（这样称为粘包）。\n\n\n还有可能出现上面这样的虽然收到了两个包，但是里面的内容却是互相包含，对于应用来说依然无法解析（拆包）。\n对于这样的问题只能通过上层的应用来解决，常见的方式有：\n\n在报文末尾增加换行符表明一条完整的消息，这样在接收端可以根据这个换行符来判断消息是否完整。\n将消息分为消息头、消息体。可以在消息头中声明消息的长度，根据这个长度来获取报文（比如 808 协议）。\n规定好报文长度，不足的空位补齐，取的时候按照长度截取即可。\n\n以上的这些方式我们在 Netty 的 pipline 中里加入对应的解码器都可以手动实现。\n但其实 Netty 已经帮我们做好了，完全可以开箱即用。\n比如：\n\nLineBasedFrameDecoder 可以基于换行符解决。\nDelimiterBasedFrameDecoder 可基于分隔符解决。\nFixedLengthFrameDecoder 可指定长度解决。\n\n字符串拆、粘包下面来模拟一下最简单的字符串传输。\n还是在之前的\nhttps://github.com/crossoverJie/netty-action\n进行演示。\n在 Netty 客户端中加了一个入口可以循环发送 100 条字符串报文到接收端：\n/** * 向服务端发消息 字符串 * @param stringReqVO * @return */@ApiOperation(&quot;客户端发送消息，字符串&quot;)@RequestMapping(value = &quot;sendStringMsg&quot;, method = RequestMethod.POST)@ResponseBodypublic BaseResponse&lt;NULLBody&gt; sendStringMsg(@RequestBody StringReqVO stringReqVO)&#123;    BaseResponse&lt;NULLBody&gt; res = new BaseResponse();    for (int i = 0; i &lt; 100; i++) &#123;        heartbeatClient.sendStringMsg(stringReqVO.getMsg()) ;    &#125;    // 利用 actuator 来自增    counterService.increment(Constants.COUNTER_CLIENT_PUSH_COUNT);    SendMsgResVO sendMsgResVO = new SendMsgResVO() ;    sendMsgResVO.setMsg(&quot;OK&quot;) ;    res.setCode(StatusEnum.SUCCESS.getCode()) ;    res.setMessage(StatusEnum.SUCCESS.getMessage()) ;    return res ;&#125;/** * 发送消息字符串 * * @param msg */public void sendStringMsg(String msg) &#123;    ByteBuf message = Unpooled.buffer(msg.getBytes().length) ;    message.writeBytes(msg.getBytes()) ;    ChannelFuture future = channel.writeAndFlush(message);    future.addListener((ChannelFutureListener) channelFuture -&gt;            LOGGER.info(&quot;客户端手动发消息成功=&#123;&#125;&quot;, msg));&#125;\n\n服务端直接打印即可：\n@Overrideprotected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123;    LOGGER.info(&quot;收到msg=&#123;&#125;&quot;, msg);&#125;\n\n顺便提一下，这里加的有一个字符串的解码器：.addLast(new StringDecoder()) 其实就是把消息解析为字符串。\n@Overrideprotected void decode(ChannelHandlerContext ctx, ByteBuf msg, List&lt;Object&gt; out) throws Exception &#123;    out.add(msg.toString(charset));&#125;\n\n\n在 Swagger 中调用了客户端的接口用于给服务端发送了 100 次消息：\n\n正常情况下接收端应该打印 100 次 hello 才对，但是查看日志会发现：\n\n收到的内容有完整的、多的、少的、拼接的；这也就对应了上面提到的拆包、粘包。\n该怎么解决呢？这便可采用之前提到的 LineBasedFrameDecoder 利用换行符解决。\n利用 LineBasedFrameDecoder 解决问题LineBasedFrameDecoder 解码器使用非常简单，只需要在 pipline 链条上添加即可。\n//字符串解析,换行防拆包.addLast(new LineBasedFrameDecoder(1024)).addLast(new StringDecoder())\n\n构造函数中传入了 1024 是指报的长度最大不超过这个值，具体可以看下文的源码分析。\n然后我们再进行一次测试看看结果：\n\n注意，由于 LineBasedFrameDecoder 解码器是通过换行符来判断的，所以在发送时，一条完整的消息需要加上 \\n。\n\n\n最终的结果：\n仔细观察日志，发现确实没有一条被拆、粘包。\nLineBasedFrameDecoder 的原理目的达到了，来看看它的实现原理：\n\n\n第一步主要就是 findEndOfLine 方法去找到当前报文中是否存在分隔符，存在就会返回分隔符所在的位置。\n判断是否需要丢弃，默认为 false ，第一次走这个逻辑（下文会判断是否需要改为 true）。\n如果报文中存在换行符，就会将数据截取到那个位置。\n如果不存在换行符（有可能是拆包、粘包），就看当前报文的长度是否大于预设的长度。大于则需要缓存这个报文长度，并将 discarding 设为 true。\n如果是需要丢弃时，判断是否找到了换行符，存在则需要丢弃掉之前记录的长度然后截取数据。\n如果没有找到换行符，则将之前缓存的报文长度进行累加，用于下次抛弃。\n\n从这个逻辑中可以看出就是寻找报文中是否包含换行符，并进行相应的截取。\n由于是通过缓冲区读取的，所以即使这次没有换行符的数据，只要下一次的报文存在换行符，上一轮的数据也不会丢。\n高效的编码方式 Google Protocol上面提到的其实就是在解码中进行操作，我们也可以自定义自己的拆、粘包工具。\n编解码的主要目的就是为了可以编码成字节流用于在网络中传输、持久化存储。\nJava 中也可以实现 Serializable 接口来实现序列化，但由于它性能等原因在一些 RPC 调用中用的很少。\n而 Google Protocol 则是一个高效的序列化框架，下面来演示在 Netty 中如何使用。\n安装首先第一步自然是安装：\n在官网下载对应的包。\n本地配置环境变量：\n\n当执行 protoc --version 出现以下结果表明安装成功：\n\n定义自己的协议格式接着是需要按照官方要求的语法定义自己的协议格式。\n比如我这里需要定义一个输入输出的报文格式：\nBaseRequestProto.proto:\nsyntax = &quot;proto2&quot;;package protocol;option java_package = &quot;com.crossoverjie.netty.action.protocol&quot;;option java_outer_classname = &quot;BaseRequestProto&quot;;message RequestProtocol &#123;  required int32 requestId = 2;  required string reqMsg = 1;  &#125;\n\nBaseResponseProto.proto:\nsyntax = &quot;proto2&quot;;package protocol;option java_package = &quot;com.crossoverjie.netty.action.protocol&quot;;option java_outer_classname = &quot;BaseResponseProto&quot;;message ResponseProtocol &#123;  required int32 responseId = 2;  required string resMsg = 1;  &#125;\n\n再通过\nprotoc --java_out=/dev BaseRequestProto.proto BaseResponseProto.proto\n\nprotoc 命令将刚才定义的协议格式转换为 Java 代码，并生成在 /dev 目录。\n只需要将生成的代码拷贝到我们的项目中，同时引入依赖：\n&lt;dependency&gt;\t&lt;groupId&gt;com.google.protobuf&lt;/groupId&gt;\t&lt;artifactId&gt;protobuf-java&lt;/artifactId&gt;\t&lt;version&gt;3.4.0&lt;/version&gt;&lt;/dependency&gt;\n\n利用 Protocol 的编解码也非常简单：\npublic class ProtocolUtil &#123;    public static void main(String[] args) throws InvalidProtocolBufferException &#123;        BaseRequestProto.RequestProtocol protocol = BaseRequestProto.RequestProtocol.newBuilder()                .setRequestId(123)                .setReqMsg(&quot;你好啊&quot;)                .build();        byte[] encode = encode(protocol);        BaseRequestProto.RequestProtocol parseFrom = decode(encode);        System.out.println(protocol.toString());        System.out.println(protocol.toString().equals(parseFrom.toString()));    &#125;    /**     * 编码     * @param protocol     * @return     */    public static byte[] encode(BaseRequestProto.RequestProtocol protocol)&#123;        return protocol.toByteArray() ;    &#125;    /**     * 解码     * @param bytes     * @return     * @throws InvalidProtocolBufferException     */    public static BaseRequestProto.RequestProtocol decode(byte[] bytes) throws InvalidProtocolBufferException &#123;        return BaseRequestProto.RequestProtocol.parseFrom(bytes);    &#125;&#125;\n\n利用 BaseRequestProto 来做一个演示，先编码再解码最后比较最终的结果是否相同。答案肯定是一致的。\n利用 protoc 命令生成的 Java 文件里已经帮我们把编解码全部都封装好了，只需要简单调用就行了。\n可以看出 Protocol 创建对象使用的是构建者模式，对使用者来说清晰易读，更多关于构建器的内容可以参考这里。\n更多关于 Google Protocol 内容请查看官方开发文档。\n结合 NettyNetty 已经自带了对 Google protobuf 的编解码器，也是只需要在 pipline 中添加即可。\nserver 端：\n// google Protobuf 编解码.addLast(new ProtobufDecoder(BaseRequestProto.RequestProtocol.getDefaultInstance())).addLast(new ProtobufEncoder())\n\n\n\n客户端：\n// google Protobuf 编解码.addLast(new ProtobufDecoder(BaseResponseProto.ResponseProtocol.getDefaultInstance())).addLast(new ProtobufEncoder())\n\n\n稍微注意的是，在构建 ProtobufDecoder 时需要显式指定解码器需要解码成什么类型。\n\n我这里服务端接收的是 BaseRequestProto，客户端收到的是服务端响应的 BaseResponseProto 所以就设置了对应的实例。\n同样的提供了一个接口向服务端发送消息，当服务端收到了一个特殊指令时也会向客户端返回内容：\n@Overrideprotected void channelRead0(ChannelHandlerContext ctx, BaseRequestProto.RequestProtocol msg) throws Exception &#123;    LOGGER.info(&quot;收到msg=&#123;&#125;&quot;, msg.getReqMsg());    if (999 == msg.getRequestId())&#123;        BaseResponseProto.ResponseProtocol responseProtocol = BaseResponseProto.ResponseProtocol.newBuilder()                .setResponseId(1000)                .setResMsg(&quot;服务端响应&quot;)                .build();        ctx.writeAndFlush(responseProtocol) ;    &#125;&#125;\n\n在 swagger 中调用相关接口：\n\n在日志可以看到服务端收到了消息，同时客户端也收到了返回：\n\n\n虽说 Netty 封装了 Google Protobuf 相关的编解码工具，其实查看它的编码工具就会发现也是利用上文提到的 api 实现的。\n\nProtocol 拆、粘包Google Protocol 的使用确实非常简单，但还是有值的注意的地方，比如它依然会有拆、粘包问题。\n不妨模拟一下：\n\n连续发送 100 次消息看服务端收到的怎么样：\n\n会发现服务端在解码的时候报错，其实就是被拆、粘包了。\n这点 Netty 自然也考虑到了，所以已经提供了相关的工具。\n//拆包解码.addLast(new ProtobufVarint32FrameDecoder()).addLast(new ProtobufVarint32LengthFieldPrepender())\n\n只需要在服务端和客户端加上这两个编解码工具即可，再来发送一百次试试。\n查看日志发现没有出现一次异常，100 条信息全部都接收到了。\n\n这个编解码工具可以简单理解为是在消息体中加了一个 32 位长度的整形字段，用于表明当前消息长度。\n总结网络这块同样是计算机的基础，由于近期在做相关的工作所以接触的比较多，也算是给大学补课了。\n后面会接着更新 Netty 相关的内容，最后会产出一个高性能的 HTTP 以及 RPC 框架，敬请期待。\n上文相关的代码：\nhttps://github.com/crossoverJie/netty-action\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n欢迎关注公众号一起交流：\n","categories":["Netty"],"tags":["拆包","粘包","protobuf"]},{"title":"为自己搭建一个分布式 IM(即时通讯) 系统","url":"/2019/01/02/netty/cim01-started/","content":"\n前言大家新年快乐！\n新的一年第一篇技术文章希望开个好头，所以元旦三天我也没怎么闲着，希望给大家带来一篇比较感兴趣的干货内容。\n老读者应该还记得我在去年国庆节前分享过一篇《设计一个百万级的消息推送系统》；虽然我在文中有贴一些伪代码，依然有些朋友希望能直接分享一些可以运行的源码；这么久了是时候把坑填上了。\n\n本文较长，高能预警；带好瓜子板凳。\n\n\n\n\n于是在之前的基础上我完善了一些内容，先来看看这个项目的介绍吧：\nCIM(CROSS-IM) 一款面向开发者的 IM(即时通讯)系统；同时提供了一些组件帮助开发者构建一款属于自己可水平扩展的 IM 。\n借助 CIM 你可以实现以下需求：\n\nIM 即时通讯系统。\n适用于 APP 的消息推送中间件。\nIOT 海量连接场景中的消息透传中间件。\n\n完整源码托管在 GitHub : https://github.com/crossoverJie/cim\n演示本次主要涉及到 IM 即时通讯，所以特地录了两段视频演示（群聊、私聊）。\n\n点击下方链接可以查看视频版 Demo。\n\n\n\n\nYouTube\nBilibili\n\n\n\n群聊 私聊\n群聊 私聊\n\n\n\n\n\n\n也在公网部署了一套演示环境，想要试一试的可以联系我加入内测群获取账号一起尬聊😋。\n架构设计下面来看看具体的架构设计。\n\n\nCIM 中的各个组件均采用 SpringBoot 构建。\n采用 Netty + Google Protocol Buffer 构建底层通信。\nRedis 存放各个客户端的路由信息、账号信息、在线状态等。\nZookeeper 用于 IM-server 服务的注册与发现。\n\n整体主要由以下模块组成：\ncim-serverIM 服务端；用于接收 client 连接、消息透传、消息推送等功能。\n支持集群部署。\ncim-forward-route消息路由服务器；用于处理消息路由、消息转发、用户登录、用户下线以及一些运营工具（获取在线用户数等）。\ncim-clientIM 客户端；给用户使用的消息终端，一个命令即可启动并向其他人发起通讯（群聊、私聊）；同时内置了一些常用命令方便使用。\n流程图整体的流程也比较简单，流程图如下：\n\n\n客户端向 route 发起登录。\n登录成功从 Zookeeper 中选择可用 IM-server 返回给客户端，并保存登录、路由信息到 Redis。\n客户端向 IM-server 发起长连接，成功后保持心跳。\n客户端下线时通过 route 清除状态信息。\n\n所以当我们自己部署时需要以下步骤：\n\n搭建基础中间件 Redis、Zookeeper。\n部署 cim-server，这是真正的 IM 服务器，为了满足性能需求所以支持水平扩展，只需要注册到同一个 Zookeeper 即可。\n部署 cim-forward-route，这是路由服务器，所有的消息都需要经过它。由于它是无状态的，所以也可以利用 Nginx 代理提高可用性。\ncim-client 真正面向用户的客户端；启动之后会自动连接 IM 服务器便可以在控制台收发消息了。\n\n更多使用介绍可以参考快速启动。\n详细设计接下来重点看看具体的实现，比如群聊、私聊消息如何流转；IM 服务端负载均衡；服务如何注册发现等等。\nIM 服务端先来看看服务端；主要是实现客户端上下线、消息下发等功能。\n首先是服务启动：\n\n由于是在 SpringBoot 中搭建的，所以在应用启动时需要启动 Netty 服务。\n从 pipline 中可以看出使用了 Protobuf 的编解码（具体报文在客户端中分析）。\n注册发现需要满足 IM 服务端的水平扩展需求，所以 cim-server 是需要将自身数据发布到注册中心的。\n这里参考之前分享的《搞定服务注册与发现》有具体介绍。\n所以在应用启动成功后需要将自身数据注册到 Zookeeper 中。\n\n最主要的目的就是将当前应用的 ip + cim-server-port+ http-port 注册上去。\n\n上图是我在演示环境中注册的两个 cim-server 实例（由于在一台服务器，所以只是端口不同）。\n这样在客户端（监听这个 Zookeeper 节点）就能实时的知道目前可用的服务信息。\n登录当客户端请求 cim-forward-route 中的登录接口（详见下文）做完业务验证（就相当于日常登录其他网站一样）之后，客户端会向服务端发起一个长连接，如之前的流程所示：\n\n这时客户端会发送一个特殊报文，表明当前是登录信息。\n服务端收到后就需要将该客户端的 userID 和当前 Channel 通道关系保存起来。\n\n同时也缓存了用户的信息，也就是 userID 和 用户名。\n离线当客户端断线后也需要将刚才缓存的信息清除掉。\n\n同时也需要调用 route 接口清除相关信息（具体接口看下文）。\nIM 路由\n从架构图中可以看出，路由层是非常重要的一环；它提供了一系列的 HTTP 服务承接了客户端和服务端。\n目前主要是以下几个接口。\n注册接口\n由于每一个客户端都是需要登录才能使用的，所以第一步自然是注册。\n这里就设计的比较简单，直接利用 Redis 来存储用户信息；用户信息也只有 ID 和 userName 而已。\n只是为了方便查询在 Redis 中的 KV 又反过来存储了一份 VK，这样 ID 和 userName 都必须唯一。\n登录接口这里的登录和 cim-server 中的登录不一样，具有业务性质，\n\n\n登录成功之后需要判断是否是重复登录（一个用户只能运行一个客户端）。\n登录成功后需要从 Zookeeper 中获取服务列表（cim-server）并根据某种算法选择一台服务返回给客户端。\n登录成功之后还需要保存路由信息，也就是当前用户分配的服务实例保存到 Redis 中。\n\n为了实现只能一个用户登录，使用了 Redis 中的 set 来保存登录信息；利用 userID 作为 key ，重复的登录就会写入失败。\n\n\n类似于 Java 中的 HashSet，只能去重保存。\n\n获取一台可用的路由实例也比较简单：\n\n\n先从 Zookeeper 获取所有的服务实例做一个内部缓存。\n轮询选择一台服务器（目前只有这一种算法，后续会新增）。\n\n当然要获取 Zookeeper 中的服务实例前自然是需要监听 cim-server 之前注册上去的那个节点。\n具体代码如下：\n\n也是在应用启动之后监听 Zookeeper 中的路由节点，一旦发生变化就会更新内部缓存。\n\n这里使用的是 Guava 的 cache，它基于 ConcurrentHashMap，所以可以保证清除、新增缓存的原子性。\n\n群聊接口这是一个真正发消息的接口，实现的效果就是其中一个客户端发消息，其余所有客户端都能收到！\n流程肯定是客户端发送一条消息到服务端，服务端收到后在上文介绍的 SessionSocketHolder 中遍历所有 Channel（通道）然后下发消息即可。\n服务端是单机倒也可以，但现在是集群设计。所以所有的客户端会根据之前的轮询算法分配到不同的 cim-server 实例中。\n因此就需要路由层来发挥作用了。\n\n路由接口收到消息后首先遍历出所有的客户端和服务实例的关系。\n路由关系在 Redis 中的存放如下：\n\n由于 Redis 单线程的特质，当数据量大时；一旦使用 keys 匹配所有 cim-route:* 数据，会导致 Redis 不能处理其他请求。\n所以这里改为使用 scan 命令来遍历所有的 cim-route:*。\n\n接着会挨个调用每个客户端所在的服务端的 HTTP 接口用于推送消息。\n在 cim-server 中的实现如下：\n\ncim-server 收到消息后会在内部缓存中查询该 userID 的通道，接着只需要发消息即可。\n在线用户接口这是一个辅助接口，可以查询出当前在线用户信息。\n\n实现也很简单，也就是查询之前保存 ”用户登录状态的那个去重 set “即可。\n私聊接口之所以说获取在线用户是一个辅助接口，其实就是用于辅助私聊使用的。\n一般我们使用私聊的前提肯定得知道当前哪些用户在线，接着你才会知道你要和谁进行私聊。\n类似于这样：\n\n在我们这个场景中，私聊的前提就是需要获得在线用户的 userID。\n\n所以私聊接口在收到消息后需要查询到接收者所在的 cim-server 实例信息，后续的步骤就和群聊一致了。调用接收者所在实例的 HTTP 接口下发信息。\n只是群聊是遍历所有的在线用户，私聊只发送一个的区别。\n下线接口一旦客户端下线，我们就需要将之前存放在 Redis 中的一些信息删除掉（路由信息、登录状态）。\n\nIM 客户端客户端中的一些逻辑其实在上文已经谈到一些了。\n登录第一步也就是登录，需要在启动时调用 route 的登录接口，获得 cim-server 信息再创建连接。\n\n\n\n登录过程中 route 接口会判断是否为重复登录，重复登录则会直接退出程序。\n\n接下来是利用 route 接口返回的 cim-server 实例信息（ip+port）创建连接。\n最后一步就是发送一个登录标志的信息到服务端，让它保持客户端和 Channel 的关系。\n\n自定义协议上文提到的一些登录报文、真正的消息报文这些其实都是在我们自定义协议中可以区别出来的。\n由于是使用 Google Protocol Buffer 编解码，所以先看看原始格式。\n\n其实这个协议中目前一共就三个字段：\n\nrequestId 可以理解为 userId。\nreqMsg 就是真正的消息。\ntype 也就是上文提到的消息类别。\n\n目前主要是三种类型，分别对应不同的业务：\n\n心跳为了保持客户端和服务端的连接，每隔一段时间没有发送消息都需要自动的发送心跳。\n目前的策略是每隔一分钟就是发送一个心跳包到服务端：\n\n这样服务端每隔一分钟没有收到业务消息时就会收到 ping 的心跳包：\n\n内置命令客户端也内置了一些基本命令来方便使用。\n\n\n\n命令\n描述\n\n\n\n:q\n退出客户端\n\n\n:olu\n获取所有在线用户信息\n\n\n:all\n获取所有命令\n\n\n:\n更多命令正在开发中。。\n\n\n\n比如输入 :q 就会退出客户端，同时会关闭一些系统资源。\n\n当输入 :olu(onlineUser 的简写)就会去调用 route 的获取所有在线用户接口。\n\n群聊群聊的使用非常简单，只需要在控制台输入消息回车即可。\n这时会去调用 route 的群聊接口。\n\n私聊私聊也是同理，但前提是需要触发关键字；使用 userId;;消息内容 这样的格式才会给某个用户发送消息，所以一般都需要先使用 :olu 命令获取所以在线用户才方便使用。\n\n消息回调为了满足一些定制需求，比如消息需要保存之类的。\n所以在客户端收到消息之后会回调一个接口，在这个接口中可以自定义实现。\n\n因此先创建了一个 caller 的 bean，这个 bean 中包含了一个 CustomMsgHandleListener 接口，需要自行处理只需要实现此接口即可。\n自定义界面由于我自己不怎么会写界面，但保不准有其他大牛会写。所以客户端中的群聊、私聊、获取在线用户、消息回调等业务(以及之后的业务)都是以接口形式提供。\n也方便后面做页面集成，只需要调这些接口就行了；具体实现不用怎么关心。\n总结cim 目前只是第一版，BUG 多，功能少（只拉了几个群友做了测试）；不过后续还会接着完善，至少这一版会给那些没有相关经验的朋友带来一些思路。\n后续计划：\n\n完整源码：\nhttps://github.com/crossoverJie/cim\n如果这篇对你有所帮助还请不吝转发。\n","categories":["Netty","cim"],"tags":["Redis","Zookeeper","推送","IM","IOT"]},{"title":"为自己搭建一个分布式 IM 系统二【从查找算法聊起】","url":"/2019/01/14/netty/cim02-v1.0.1/","content":"\n前言\n最近这段时间确实有点忙，这篇的目录还是在飞机上敲出来了的。\n\n言归正传，上周更新了 cim 第一版：为自己搭建一个分布式 IM(即时通讯) 系统；没想到反响热烈，最高时上了 GitHub Trending  Java 版块的首位，一天收到了 300+ 的 star。\n\n\n\n现在总共也有 1.3K+ 的 star，有几十个朋友参加了测试，非常感谢大家的支持。\n在这过程中也收到一些 bug 反馈，feature 建议；因此这段时间我把一些影响较大的 bug 以及需求比较迫切的 feature 调整了，本次更新的 v1.0.1 版本：\n\n客户端超时自动下线。\n新增 AI 模式。\n聊天记录查询。\n在线用户前缀模糊匹配。\n\n下面谈下几个比较重点的功能。\n客户端超时自动下线 这个功能涉及到客户端和服务端的心跳设计，比较有意思，也踩了几个坑；所以准备留到下次单独来聊。\nAI 模式大家应该还记得这个之前刷爆朋友圈的 估值两个一个亿的 AI 核心代码。\n和我这里的场景再合适不过了。\n于是我新增了一个命令用于一键开启 AI 模式，使用情况大概如下。\n\n欢迎大家更新源码体验，融资的请私聊我🤣。\n聊天记录聊天记录也是一个比较迫切的功能。\n\n使用命令 :q 关键字 即可查询与个人相关的聊天记录。\n这个功能其实比较简单，只需要在消息发送及接收消息时保存即可。\n但要考虑的一点是，这个保存消息是 IO 操作，不可避免的会有耗时；需要尽量避免对消息发送、接收产生影响。\n异步写入消息因此我把消息写入的过程异步完成，可以不影响真正的业务。\n实现起来也挺简单，就是一个典型的生产者消费者模式。\n\n主线程收到消息之后直接写入队列，另外再有一个线程一直源源不断的从队列中取出数据后保存聊天记录。\n大概的代码如下：\n\n\n写入消息的同时会把消费消息的线程打开：\n\n而最终存放消息记录的策略，考虑后还是以最简单的方式存放在客户端，可以降低复杂度。\n\n简单来说就是根据当前日期+用户名写入到磁盘里。\n当客户端关闭时利用线程中断的方式停止了消费队列的线程。\n\n这点的设计其实和 logback 写日志的方式比较类似，感兴趣的可以去翻翻 logback 的源码，更加详细。\n回调接口至于收到其他客户端发来的消息时则是利用之前预留的消息回调接口来写入日志。\n\n收到消息后会执行自定义的回调接口。\n\n于是在这个回调方法中实现写入逻辑即可，当后续还有其他的消息处理逻辑时也能在这里直接添加。\n\n当处理逻辑增多时最好是改为责任链模式，更加清晰易维护。\n\n查找算法接下来是本文着重要讨论的一个查找算法，准确的说是一个前缀模糊匹配的算法。\n实现的效果如下：\n\n使用命令 :qu prefix 可以按照前缀的方式搜索用户信息。\n当然在命令行中其实意义不大，但是在移动端中确是比较有用的。类似于微信按照用户名匹配：\n\n\n因为后期打算出一个移动端 APP，所以就先把这个功能实现了。\n\n从效果也看得出来：就是按照输入的前缀匹配字符串（目前只支持英文）。\n在没有任何限制的条件下最快、最简单的实现方式可以直接把所有的字符串存放在一个容器中 （List、Set），查询时则挨个遍历；利用 String.startsWith(&quot;prefix&quot;) 进行匹配。\n但这样会有几个问题：\n\n存储资源比较浪费，不管是 list 还是 Set 都会有额外的损耗。\n查询效率较低，需要遍历集合后再遍历字符串的 char 数组（String.startsWith 的实现方式）。\n\n字典树基于以上的问题我们可以考虑下：\n假设我需要存放 java,javascript,jsp,php 这些字符串时在 ArrayList 中会怎么存放？\n\n很明显，会是这样完整的存放在一个数组中；同时这个数组还可能存在浪费，没有全部使用完。\n但其实仔细观察这些数据会发现有一些共同特点，比如 java,javascript 有共同的前缀 java;和 jsp 有共同的前缀 j。\n那是否可以把这些前缀利用起来呢？这样就可以少存储一份。\n比如写入 java,javascript 这两个字符串时存放的结构如下：\n\n当再存入一个 jsp 时：\n\n最后再存入 jsf 时：\n\n相信大家应该已经看明白了，按照这样的存储方式可以节省很多内存，同时查询效率也比较高。\n比如查询以 jav 开头的数据，只需要从头结点 j 开始往下查询，最后会查询到 ava 以及 script 这两个个结点，所以整个查询路径所经历的字符拼起来就是查询到的结果java+javascript。\n如果以 b 开头进行查询，那第一步就会直接返回，这样比在 list 中的效率高很多。\n但这个图还不完善，因为不知道查询到啥时候算是匹配到了一个之前写入的字符串。\n\n比如在上图中怎么知道 j+ava 是一个我们之前写入的 java 这个字符呢。\n\n因此我们需要对这种是一个完整字符串的数据打上一个标记：\n\n比如这样，我们将 ava、script、p、f 这几个节点都换一个颜色表示。表明查询到这个字符时就算是匹配到了一个结果。\n而查到 s 这个字符颜色不对，代表还需要继续往下查。\n比如输入关键字 js 进行匹配时，当它的查询路径走到 s 这里时判断到 s 的颜色不对，所以不会把 js 作为一个匹配结果。而是继续往下查，发现有两个子节点 p、f 颜色都正确，于是把查询的路径 jsp 和 jsf 都作为一个匹配结果。\n而只输入 j，则会把下面所有有色的字符拼起来作为结果集合。\n\n这其实就一个典型的字典树。\n\n具体实现下面则是具体的代码实现，其实算法不像是实现一个业务功能这样好用文字分析；具体还是看源码多调试就明白了。\n谈下几个重点的地方吧：\n\n字典树的节点实现，其中的 isEnd 相当于图中的上色。\n利用一个 Node[] children 来存放子节点。\n\n为了可以区分大小写查询，所以子节点的长度相当于是 26*2。\n写入数据\n这里以一个单测为例，写入了三个字符串，那最终形成的数据结构如下：\n\n图中有与上图有几点不同：\n\n每个节点都是一个字符，这样树的高度最高为52。\n每个节点的子节点都是长度为 52 的数组；所以可以利用数组的下标表示他代表的字符值。比如 0 就是大 A,26 则是小 a，以此类推。\n有点类似于之前提到的布隆过滤器，可以节省内存。\n\ndebug 时也能看出符合上图的数据结构：\n\n所以真正的写入步骤如下：\n\n\n把字符串拆分为 char 数组，并判断大小写计算它所存放在数组中的位置 index。\n将当前节点的子节点数组的 index 处新增一个节点。\n如果是最后一个字符就将新增的节点置为最后一个节点，也就是上文的改变节点颜色。\n最后将当前节点指向下一个节点方便继续写入。\n\n\n\n查询总的来说要麻烦一些，其实就是对树进行深度遍历；最终的思想看图就能明白。\n所以在 cim 中进行模糊匹配时就用到了这个结构。\n\n字典树的源码在此处：\nhttps://github.com/crossoverJie/cim/blob/master/cim-common/src/main/java/com/crossoverjie/cim/common/data/construct/TrieTree.java\n\n其实利用这个结构还能实现判断某个前缀的单词是否在某堆数据里、某个前缀的单词出现的次数等。\n\n总结目前 cim 还在火热内测中（虽然群里只有20几人）,感兴趣的朋友可以私聊我拉你入伙☺️ \n\n再没有新的 BUG 产生前会着重把这些功能完成了，不出意外下周更新 cim 的心跳重连等机制。\n完整源码：\nhttps://github.com/crossoverJie/cim\n如果这篇对你有所帮助还请不吝转发。\n","categories":["Netty","cim"],"tags":["IM","TrieTree"]},{"title":"长连接的心跳及重连设计","url":"/2019/01/23/netty/cim03-heartbeat/","content":"\n前言说道“心跳”这个词大家都不陌生，当然不是指男女之间的心跳，而是和长连接相关的。\n顾名思义就是证明是否还活着的依据。\n什么场景下需要心跳呢？\n目前我们接触到的大多是一些基于长连接的应用需要心跳来“保活”。\n由于在长连接的场景下，客户端和服务端并不是一直处于通信状态，如果双方长期没有沟通则双方都不清楚对方目前的状态；所以需要发送一段很小的报文告诉对方“我还活着”。\n\n\n同时还有另外几个目的：\n\n服务端检测到某个客户端迟迟没有心跳过来可以主动关闭通道，让它下线。\n客户端检测到某个服务端迟迟没有响应心跳也能重连获取一个新的连接。\n\n正好借着在 cim有这样两个需求来聊一聊。\n心跳实现方式心跳其实有两种实现方式：\n\nTCP 协议实现（keepalive 机制）。\n应用层自己实现。\n\n由于 TCP 协议过于底层，对于开发者来说维护性、灵活度都比较差同时还依赖于操作系统。\n所以我们这里所讨论的都是应用层的实现。\n\n\n如上图所示，在应用层通常是由客户端发送一个心跳包 ping 到服务端，服务端收到后响应一个 pong 表明双方都活得好好的。\n一旦其中一端延迟 N 个时间窗口没有收到消息则进行不同的处理。\n客户端自动重连先拿客户端来说吧，每隔一段时间客户端向服务端发送一个心跳包，同时收到服务端的响应。\n常规的实现应当是：\n\n开启一个定时任务，定期发送心跳包。\n收到服务端响应后更新本地时间。\n再有一个定时任务定期检测这个“本地时间”是否超过阈值。\n超过后则认为服务端出现故障，需要重连。\n\n这样确实也能实现心跳，但并不友好。\n在正常的客户端和服务端通信的情况下，定时任务依然会发送心跳包；这样就显得没有意义，有些多余。\n所以理想的情况应当是客户端收到的写消息空闲时才发送这个心跳包去确认服务端是否健在。\n好消息是 Netty 已经为我们考虑到了这点，自带了一个开箱即用的 IdleStateHandler 专门用于心跳处理。\n来看看 cim 中的实现：\n\n在 pipeline 中加入了一个 10秒没有收到写消息的 IdleStateHandler，到时他会回调 ChannelInboundHandler 中的 userEventTriggered 方法。\n\n所以一旦写超时就立马向服务端发送一个心跳（做的更完善应当在心跳发送失败后有一定的重试次数）；\n这样也就只有在空闲时候才会发送心跳包。\n但一旦间隔许久没有收到服务端响应进行重连的逻辑应当写在哪里呢？\n先来看这个示例：\n当收到服务端响应的 pong 消息时，就在当前 Channel 上记录一个时间，也就是说后续可以在定时任务中取出这个时间和当前时间的差额来判断是否超过阈值。\n超过则重连。\n\n同时在每次心跳时候都用当前时间和之前服务端响应绑定到 Channel 上的时间相减判断是否需要重连即可。\n也就是  heartBeatHandler.process(ctx); 的执行逻辑。\n伪代码如下：\n@Overridepublic void process(ChannelHandlerContext ctx) throws Exception &#123;    long heartBeatTime = appConfiguration.getHeartBeatTime() * 1000;        Long lastReadTime = NettyAttrUtil.getReaderTime(ctx.channel());    long now = System.currentTimeMillis();    if (lastReadTime != null &amp;&amp; now - lastReadTime &gt; heartBeatTime)&#123;        reconnect();    &#125;&#125;\n\nIdleStateHandler 误区一切看起来也没毛病，但实际上却没有这样实现重连逻辑。\n最主要的问题还是对 IdleStateHandler 理解有误。\n我们假设下面的场景：\n\n客户端通过登录连上了服务端并保持长连接，一切正常的情况下双方各发心跳包保持连接。\n这时服务端突入出现 down 机，那么理想情况下应当是客户端迟迟没有收到服务端的响应从而 userEventTriggered 执行定时任务。\n判断当前时间 - UpdateWriteTime &gt; 阈值 时进行重连。\n\n但却事与愿违，并不会执行 2、3两步。\n因为一旦服务端 down 机、或者是与客户端的网络断开则会回调客户端的 channelInactive 事件。\nIdleStateHandler 作为一个 ChannelInbound 也重写了 channelInactive() 方法。\n\n这里的 destroy() 方法会把之前开启的定时任务都给取消掉。\n所以就不会再有任何的定时任务执行了，也就不会有机会执行这个重连业务。\n靠谱实现因此我们得有一个单独的线程来判断是否需要重连，不依赖于 IdleStateHandler。\n于是 cim 在客户端感知到网络断开时就会开启一个定时任务：\n\n\n之所以不在客户端启动就开启，是为了节省一点线程消耗。网络问题虽然不可避免，但在需要的时候开启更能节省资源。\n\n\n\n在这个任务重其实就是执行了重连，限于篇幅具体代码就不贴了，感兴趣的可以自行查阅。\n同时来验证一下效果。\n\n启动两个服务端，再启动客户端连接上一台并保持长连接。这时突然手动关闭一台服务，客户端可以自动重连到可用的那台服务节点。\n\n\n启动客户端后服务端也能收到正常的 ping 消息。\n利用 :info 命令查看当前客户端的链接状态发现连的是 9000端口。\n\n\n:info 是一个新增命令，可以查看一些客户端信息。\n\n这时我关掉连接上的这台节点。\nkill -9 2142\n\n\n这时客户端会自动重连到可用的那台节点。这个节点也收到了上线日志以及心跳包。\n服务端自动剔除离线客户端现在来看看服务端，它要实现的效果就是延迟 N 秒没有收到客户端的 ping 包则认为客户端下线了，在 cim 的场景下就需要把他踢掉置于离线状态。\n消息发送误区这里依然有一个误区，在调用 ctx.writeAndFlush() 发送消息获取回调时。\n其中是 isSuccess 并不能作为消息发送成功与否的标准。\n\n也就是说即便是客户端直接断网，服务端这里发送消息后拿到的 success 依旧是 true。\n这是因为这里的 success 只是告知我们消息写入了 TCP 缓冲区成功了而已。\n和我之前有着一样错误理解的不在少数，这是 Netty 官方给的回复。\n\n相关 issue：\nhttps://github.com/netty/netty/issues/4915\n\n同时感谢 95老徐以及闪电侠的一起排查。\n\n所以我们不能依据此来关闭客户端的连接，而是要像上文一样判断 Channel 上绑定的时间与当前时间只差是否超过了阈值。\n\n以上则是 cim 服务端的实现，逻辑和开头说的一致，也和 Dubbo 的心跳机制有些类似。\n于是来做个试验：正常通信的客户端和服务端，当我把客户端直接断网时，服务端会自动剔除客户端。\n\n总结这样就实现了文初的两个要求。\n\n服务端检测到某个客户端迟迟没有心跳过来可以主动关闭通道，让它下线。\n客户端检测到某个服务端迟迟没有响应心跳也能重连获取一个新的连接。\n\n同时也踩了两个误区，坑一个人踩就可以了，希望看过本文的都有所收获避免踩坑。\n本文所有相关代码都在此处，感兴趣的可以自行查看：\nhttps://github.com/crossoverJie/cim\n如果本文对你有所帮助还请不吝转发。\n","categories":["Netty","cim"],"tags":["Heartbeat","IM"]},{"title":"设计一个百万级的消息推送系统","url":"/2018/09/25/netty/million-sms-push/","content":"\n前言首先迟到的祝大家中秋快乐。\n最近一周多没有更新了。其实我一直想憋一个大招，分享一些大家感兴趣的干货。\n鉴于最近我个人的工作内容，于是利用这三天小长假憋了一个出来（其实是玩了两天🤣）。\n\n先简单说下本次的主题，由于我最近做的是物联网相关的开发工作，其中就不免会遇到和设备的交互。\n最主要的工作就是要有一个系统来支持设备的接入、向设备推送消息；同时还得满足大量设备接入的需求。\n所以本次分享的内容不但可以满足物联网领域同时还支持以下场景：\n\n基于 WEB 的聊天系统（点对点、群聊）。\nWEB 应用中需求服务端推送的场景。\n基于 SDK 的消息推送平台。\n\n技术选型要满足大量的连接数、同时支持双全工通信，并且性能也得有保障。\n在 Java 技术栈中进行选型首先自然是排除掉了传统 IO。\n那就只有选 NIO 了，在这个层面其实选择也不多，考虑到社区、资料维护等方面最终选择了 Netty。\n最终的架构图如下：\n\n\n\n现在看着蒙没关系，下文一一介绍。\n协议解析既然是一个消息系统，那自然得和客户端定义好双方的协议格式。\n常见和简单的是 HTTP 协议，但我们的需求中有一项需要是双全工的交互方式，同时 HTTP 更多的是服务于浏览器。我们需要的是一个更加精简的协议，减少许多不必要的数据传输。\n因此我觉得最好是在满足业务需求的情况下定制自己的私有协议，在我这个场景下其实有标准的物联网协议。\n如果是其他场景可以借鉴现在流行的 RPC 框架定制私有协议，使得双方通信更加高效。\n不过根据这段时间的经验来看，不管是哪种方式都得在协议中预留安全相关的位置。\n协议相关的内容就不过讨论了，更多介绍具体的应用。\n简单实现首先考虑如何实现功能，再来思考百万连接的情况。\n注册鉴权在做真正的消息上、下行之前首先要考虑的就是鉴权问题。\n就像你使用微信一样，第一步怎么也得是登录吧，不能无论是谁都可以直接连接到平台。\n所以第一步得是注册才行。\n如上面架构图中的 注册/鉴权 模块。通常来说都需要客户端通过 HTTP 请求传递一个唯一标识，后台鉴权通过之后会响应一个 token，并将这个 token 和客户端的关系维护到 Redis 或者是 DB 中。\n客户端将这个 token 也保存到本地，今后的每一次请求都得带上这个 token。一旦这个 token 过期，客户端需要再次请求获取 token。\n鉴权通过之后客户端会直接通过TCP 长连接到图中的 push-server 模块。\n这个模块就是真正处理消息的上、下行。\n保存通道关系在连接接入之后，真正处理业务之前需要将当前的客户端和 Channel 的关系维护起来。\n假设客户端的唯一标识是手机号码，那就需要把手机号码和当前的 Channel 维护到一个 Map 中。\n这点和之前 SpringBoot 整合长连接心跳机制 类似。\n\n同时为了可以通过 Channel 获取到客户端唯一标识（手机号码），还需要在 Channel 中设置对应的属性：\npublic static void putClientId(Channel channel, String clientId) &#123;    channel.attr(CLIENT_ID).set(clientId);&#125;\n\n获取时手机号码时：\npublic static String getClientId(Channel channel) &#123;    return (String)getAttribute(channel, CLIENT_ID);&#125;\n\n这样当我们客户端下线的时便可以记录相关日志：\nString telNo = NettyAttrUtil.getClientId(ctx.channel());NettySocketHolder.remove(telNo);log.info(&quot;客户端下线，TelNo=&quot; +  telNo);\n\n\n这里有一点需要注意：存放客户端与 Channel 关系的 Map 最好是预设好大小（避免经常扩容），因为它将是使用最为频繁同时也是占用内存最大的一个对象。\n\n消息上行接下来则是真正的业务数据上传，通常来说第一步是需要判断上传消息输入什么业务类型。\n在聊天场景中，有可能上传的是文本、图片、视频等内容。\n所以我们得进行区分，来做不同的处理；这就和客户端协商的协议有关了。\n\n可以利用消息头中的某个字段进行区分。\n更简单的就是一个 JSON 消息，拿出一个字段用于区分不同消息。\n\n不管是哪种只有可以区分出来即可。\n消息解析与业务解耦消息可以解析之后便是处理业务，比如可以是写入数据库、调用其他接口等。\n我们都知道在 Netty 中处理消息一般是在 channelRead() 方法中。\n\n在这里可以解析消息，区分类型。\n但如果我们的业务逻辑也写在里面，那这里的内容将是巨多无比。\n甚至我们分为好几个开发来处理不同的业务，这样将会出现许多冲突、难以维护等问题。\n所以非常有必要将消息解析与业务处理完全分离开来。\n\n这时面向接口编程就发挥作用了。\n\n这里的核心代码和 「造个轮子」——cicada(轻量级 WEB 框架) 是一致的。\n都是先定义一个接口用于处理业务逻辑，然后在解析消息之后通过反射创建具体的对象执行其中的处理函数即可。\n这样不同的业务、不同的开发人员只需要实现这个接口同时实现自己的业务逻辑即可。\n伪代码如下：\n\n\n想要了解 cicada 的具体实现请点击这里：\nhttps://github.com/TogetherOS/cicada\n上行还有一点需要注意；由于是基于长连接，所以客户端需要定期发送心跳包用于维护本次连接。同时服务端也会有相应的检查，N 个时间间隔没有收到消息之后将会主动断开连接节省资源。\n这点使用一个 IdleStateHandler 就可实现，更多内容可以查看 Netty(一) SpringBoot 整合长连接心跳机制。\n消息下行有了上行自然也有下行。比如在聊天的场景中，有两个客户端连上了 push-server,他们直接需要点对点通信。\n这时的流程是：\n\nA 将消息发送给服务器。\n服务器收到消息之后，得知消息是要发送给 B，需要在内存中找到 B 的 Channel。\n通过 B 的 Channel 将 A 的消息转发下去。\n\n这就是一个下行的流程。\n甚至管理员需要给所有在线用户发送系统通知也是类似：\n遍历保存通道关系的 Map，挨个发送消息即可。这也是之前需要存放到 Map 中的主要原因。\n伪代码如下：\n\n具体可以参考：\nhttps://github.com/crossoverJie/netty-action/\n分布式方案单机版的实现了，现在着重讲讲如何实现百万连接。\n百万连接其实只是一个形容词，更多的是想表达如何来实现一个分布式的方案，可以灵活的水平拓展从而能支持更多的连接。\n再做这个事前首先得搞清楚我们单机版的能支持多少连接。影响这个的因素就比较多了。\n\n服务器自身配置。内存、CPU、网卡、Linux 支持的最大文件打开数等。\n应用自身配置，因为 Netty 本身需要依赖于堆外内存，但是 JVM 本身也是需要占用一部分内存的，比如存放通道关系的大 Map。这点需要结合自身情况进行调整。\n\n结合以上的情况可以测试出单个节点能支持的最大连接数。\n单机无论怎么优化都是有上限的，这也是分布式主要解决的问题。\n架构介绍在将具体实现之前首先得讲讲上文贴出的整体架构图。\n\n先从左边开始。\n上文提到的 注册鉴权 模块也是集群部署的，通过前置的 Nginx 进行负载。之前也提过了它主要的目的是来做鉴权并返回一个 token 给客户端。\n但是 push-server 集群之后它又多了一个作用。那就是得返回一台可供当前客户端使用的 push-server。\n右侧的 平台 一般指管理平台，它可以查看当前的实时在线数、给指定客户端推送消息等。\n推送消息则需要经过一个推送路由（push-server）找到真正的推送节点。\n其余的中间件如：Redis、Zookeeper、Kafka、MySQL 都是为了这些功能所准备的，具体看下面的实现。\n注册发现首先第一个问题则是 注册发现，push-server 变为多台之后如何给客户端选择一台可用的节点是第一个需要解决的。\n这块的内容其实已经在 分布式(一) 搞定服务注册与发现 中详细讲过了。\n所有的 push-server 在启动时候需要将自身的信息注册到 Zookeeper 中。\n注册鉴权 模块会订阅 Zookeeper 中的节点，从而可以获取最新的服务列表。结构如下：\n\n以下是一些伪代码：\n应用启动注册 Zookeeper。\n\n\n对于注册鉴权模块来说只需要订阅这个 Zookeeper 节点：\n\n路由策略既然能获取到所有的服务列表，那如何选择一台刚好合适的 push-server 给客户端使用呢？\n这个过程重点要考虑以下几点：\n\n尽量保证各个节点的连接均匀。\n增删节点是否要做 Rebalance。\n\n首先保证均衡有以下几种算法：\n\n轮询。挨个将各个节点分配给客户端。但会出现新增节点分配不均匀的情况。\nHash 取模的方式。类似于 HashMap，但也会出现轮询的问题。当然也可以像 HashMap 那样做一次 Rebalance，让所有的客户端重新连接。不过这样会导致所有的连接出现中断重连，代价有点大。\n由于 Hash 取模方式的问题带来了一致性 Hash算法，但依然会有一部分的客户端需要 Rebalance。\n权重。可以手动调整各个节点的负载情况，甚至可以做成自动的，基于监控当某些节点负载较高就自动调低权重，负载较低的可以提高权重。\n\n还有一个问题是：\n\n当我们在重启部分应用进行升级时，在该节点上的客户端怎么处理？\n\n由于我们有心跳机制，当心跳不通之后就可以认为该节点出现问题了。那就得重新请求注册鉴权模块获取一个可用的节点。在弱网情况下同样适用。\n如果这时客户端正在发送消息，则需要将消息保存到本地等待获取到新的节点之后再次发送。\n有状态连接在这样的场景中不像是 HTTP 那样是无状态的，我们得明确的知道各个客户端和连接的关系。\n在上文的单机版中我们将这个关系保存到本地的缓存中，但在分布式环境中显然行不通了。\n比如在平台向客户端推送消息的时候，它得首先知道这个客户端的通道保存在哪台节点上。\n借助我们以前的经验，这样的问题自然得引入一个第三方中间件用来存放这个关系。\n也就是架构图中的存放路由关系的 Redis，在客户端接入 push-server 时需要将当前客户端唯一标识和服务节点的 ip+port 存进 Redis。\n同时在客户端下线时候得在 Redis 中删掉这个连接关系。\n\n这样在理想情况下各个节点内存中的 map 关系加起来应该正好等于 Redis 中的数据。\n\n伪代码如下：\n\n这里存放路由关系的时候会有并发问题，最好是换为一个 lua 脚本。\n推送路由设想这样一个场景：管理员需要给最近注册的客户端推送一个系统消息会怎么做？\n\n结合架构图\n\n假设这批客户端有 10W 个，首先我们需要将这批号码通过平台下的 Nginx 下发到一个推送路由中。\n为了提高效率甚至可以将这批号码再次分散到每个 push-route 中。\n拿到具体号码之后再根据号码的数量启动多线程的方式去之前的路由 Redis 中获取客户端所对应的 push-server。\n再通过 HTTP 的方式调用 push-server 进行真正的消息下发（Netty 也很好的支持 HTTP 协议）。\n推送成功之后需要将结果更新到数据库中，不在线的客户端可以根据业务再次推送等。\n消息流转也许有些场景对于客户端上行的消息非常看重，需要做持久化，并且消息量非常大。\n在 push-sever 做业务显然不合适，这时完全可以选择 Kafka 来解耦。\n将所有上行的数据直接往 Kafka 里丢后就不管了。\n再由消费程序将数据取出写入数据库中即可。\n其实这块内容也很值得讨论，可以先看这篇了解下：强如 Disruptor 也发生内存溢出？\n后续谈到 Kafka 再做详细介绍。\n分布式问题分布式解决了性能问题但却带来了其他麻烦。\n应用监控比如如何知道线上几十个 push-server 节点的健康状况？\n这时就得监控系统发挥作用了，我们需要知道各个节点当前的内存使用情况、GC。\n以及操作系统本身的内存使用，毕竟 Netty 大量使用了堆外内存。\n同时需要监控各个节点当前的在线数，以及 Redis 中的在线数。理论上这两个数应该是相等的。\n这样也可以知道系统的使用情况，可以灵活的维护这些节点数量。\n日志处理日志记录也变得异常重要了，比如哪天反馈有个客户端一直连不上，你得知道问题出在哪里。\n最好是给每次请求都加上一个 traceID 记录日志，这样就可以通过这个日志在各个节点中查看到底是卡在了哪里。\n以及 ELK 这些工具都得用起来才行。\n总结本次是结合我日常经验得出的，有些坑可能在工作中并没有踩到，所有还会有一些遗漏的地方。\n就目前来看想做一个稳定的推送系统其实是比较麻烦的，其中涉及到的点非常多，只有真正做过之后才会知道。\n看完之后觉得有帮助的还请不吝转发分享。\n欢迎关注公众号一起交流：\n","categories":["Netty"],"tags":["Kafka","Redis","Zookeeper","推送","路由策略","注册发现"]},{"title":"2024年的云原生架构需要哪些技术栈","url":"/2024/04/11/ob/2024-cloud-native/","content":"背景时间过得很快啊，一转眼已经到了 2024 年，还记得 15 年刚工作那会掌握个 SSM/H(Spring/Struts2/Mybatis/Hibernate) 框架就能应付大部分面试了。\n\n现在 CS 专业的新同学估计都没听说过 SSM😢\n\n恰好从我刚开始工作时的移动互联网热潮到电商-&gt;共享经济-&gt;toB 大热-&gt;如今我都经历了一遍，技术栈也有由最开始的单体应用+物理机发展到现在的 kubernetes 云原生架构。\n当然中途也经历了几个大的阶段：SOA服务化-&gt; 微服务-&gt; 云原生-&gt; 服务网格-&gt; 无服务等几个阶段。\n最近一份工作又主要是在做基础架构，我认为了解的还算是比较全面的，所以本文我就以我的视角分享下我们在 2024 年应当使用哪些云原生技术栈，因为涉及到的技术组件比较多，就不过多讨论细节了。\n但可以保证的是提到的技术栈都是我所用过的，优缺点都会提到，主打一个真实体验。\n\n操作系统 首先是操作系统，这里有别于以往我们传统的操作系统(Linux&#x2F;Windows Server&#x2F;MacOS)，主要指的是云原生的操作系统，没有太多可以选择的余地，那就是 kubernetes。\n不过怎么维护好 kubernetes 是一个难点问题，还记得去年下半年滴滴出过一次事故，网传就是 kubernetes 升级出现的问题。\n根据我们的经验来看，对于小团队更建议直接托管给云厂商，维护 kubernetes 是一个非常复杂的工作，小团队通常都是一职多能，自己维护更容易出问题。\n当然大团队有专人维护最好，即便是出问题也能快速响应，前提是自己能 cover 住这个风险。\n\n因为我们是小团队，所以考虑到成本和稳定性，我们也只使用了云厂商的 kubernetes 能力，其余的部分可控组件由我们自己维护（具体的后文会讲到）\n\n多云的优势与好处既然都用了云厂商的容器服务，那也要考虑到云厂商故障可能带来的问题；比如去年的阿里云故障。\n所以现在一些中大厂也会选择多云方案，将同一份代码部署再多个云服务商，一旦其中一个出现问题可以快速切换。\n但具体的实施过程中也有许多挑战，比如最棘手也是最关键的数据一致性如何保证？\n当然我们可以采用一些支持分布式部署的数据库或中间件，他们本身是支持数据同步的；比如消息队列中的 Pulsar，它就可以跨级群部署以及消息同步。\n同时多云部署对应的成本也会提升，在这个“降本增效”的大背景下也得慎重考虑；所以对此还有一个折中方案：\n\n我们的技术架构需要具备快速迁移到其他云服务的能力，比如我们内部有一些工具可以定期备份资源，比如 MySQL 的 binlog，一些中间件的元数据，同时可以基于这些元数据快速恢复业务。\n\n一般遇到需要切换云服务时都是一些极端情况，所以允许部分运行时的数据丢失也是能接受的，我们只要保证最核心的数据不会丢失从而不影响业务即可。\n这个说起来简单，但也需要我们花时间进行模拟演练；具体是否实施就得看公司是否接受云服务宕机带来的损失以及演练所花的成本了。\n\n我们是具备恢复元数据能力的，但会丢失部分运行时的数据。\n\nDevOps既然我们已经选择 kubernetes 作为我们云原生的操作系统，那我们的持续集成与发布也得围绕着 kubernetes 来做。\n\n上图是一张使用 Git 配合 gitlab+ArgoCD 的流程图，我们使用 gitlab 来管理源码，同时也可以利用他的 Pipline 帮我们做持续集成，最终使用 Argo 帮我们打通 kubernetes 的流程。\n\n也就是我们常说的 GitOps\n\n同时我们的回滚历史版本，扩缩容都由 kubernetes 提供能力，我们的 DevOps 平台只需要调用 kubernetes 的 API 即可。\n当然还有现在流行 FinOps，我的理解主要是做云成本的管理和优化，对应到我的工作就是回收一些不用的资源，在不影响业务的情况下适当的降低一些配置😳。\nService Mesh\n接下来便是我认为最重要的 Service Mesh 环节了，这个的背景故事就多了，本质上我觉得这都是由 RPC(Remote Process Call) 引起的也是分布式所带来的。\n由最开始的单机的本地函数调用开始：\nlocal+------&gt;remote +------&gt; micro-service+-----&gt;service-mesh               +                  |                    +               v                  v                    v           +---+----+       +-----+------+        +----+----+           | motan  |       | SpringCloud|        | Istio   |           | dubbo  |       | Dubbo3.0   |        | Linkerd |           | gRPC   |       | SOFA       |        |         |           +--------+       +------------+        +---------+\n\n主要经历了以上三个重要的阶段，分别是 RPC 框架到微服务再到现在的服务网格。\n\nRPC 框架主要帮我们简化了分布式通信，只专注于业务本身\n微服务框架的出现可以更好的帮我们治理大批量的服务，比如一些限流、路由、降级等功能，让我们分布式应用更加健壮。\n而如今的服务网格让我们的应用程序更加适配云原生，专注于业务研发而不再需要去维护微服务框架；将这些基础功能全部下沉到我们的基础层，同时也带来了不弱于微服务框架的功能性。\n\n但使用 Istio 也有着不低的技术门槛，我觉得如果满足以下条件更推荐使用 Istio：\n\n应用已经接入 kubernetes 平台\n应用之间采用的是 gRPC 通讯框架\nAPI 网关也迁移到 Istio Gateway\n公司至少预备一个专人维护 Istio（这里的维护不一定是对代码的了解，但一定要对 Istio 本身的功能和文档足够了解）\n\n除此之外使用 SpringCloud、Dubbo、kratos、go-zero之类的微服务框架也未尝不可。\n我之前有写过两篇关于 Istio 的 文章，也可以用做参考：\n\n在 kubernetes 环境中实现 gRPC 负载均衡\n服务网格实战-入门Istio\n\n可观测性现如今可观测系统也变得越来越重要，个人觉得评价一个技术团队重要指标就是他们的可观测系统做的如何。\n一个优秀的可观测系统可以清晰得知系统的运行状态、高效的排查问题、还有及时的故障告警。\n要实现上述标准就需要我们可观测系统的三个核心指标了：\n\n\nMetrics，借助它我们可以在 Grafana 中绘制出各种直观的面板，可以更加全面的了解我们系统的运行状态\n\n\n\nTrace则是可以帮助我们构建出系统调用的全貌，通过一个 trace 就可以知道一个请求经历了哪些系统，在哪个环节出了问题。\nLogs 就比较好理解了，就是我们自己在应用里打印的一些日志；只是和以往的开发模式略有不同的是：在云原生体系中更推荐直接输出到标准输出和标准错误流中，一些第三方采集组件可以更方便的进行采集。\n\n\n我们自己的可观测系统经历过一次迭代，以往的技术栈是：\n\nMetrics 使用 VictoriaMetrics：这是一个完全兼容 Prometheus 的时序数据库，但相对 Prometheus 来说更加的节省资源。\nTrace 选择的是 SkyWalking，这也是 Java trace 领域比较流行的技术方案。\nLogs：使用 filebeat 采集日志然后输出到 ElasticSearch 中，这也是比较经典的方案。\n\n去年底我们做了一次比较大的改造，主要就是将 SkyWalking 换为了 OpenTelemetry，这是一个更加开放的社区，也逐渐成为云原生可观测的标准了。\n使用它我们的灵活性更高，不用与某些具体的技术栈进行绑定；目前 logs 还没有切换，社区也还在 beta 测试中，后续成熟后也可以直接用 OpenTelemetry 来收集日志。\n我也写的有一篇 SW 迁移到 OpenTelemetry 的文章，感兴趣的朋友可以参考：\n\n实战：如何优雅的从 SkyWalking 切换到 OpenTelemetry\n\n消息队列这里单独把消息队列拎出来是因为我目前主要是在维护公司内部的消息队列，同时业务体量大了之后消息队列变得非常重要了，通常会充当各个业务线对接的桥梁，或者是数据库同步 MySQL 的渠道，总之用处非常广泛。\n\n这里还是推荐更贴合云原生的消息队列 Pulsar，由于它存算分离的架构特性，配合kubernetes 的特性可以实现快速的扩缩容，相比 kafka 来说更易维护；同时社区活跃度也非常高，在 Bug 修复和支持新特性方面比较积极。\nPulsar官方支持的客户端也比较全面：\n\n\n\nLanguage\nDocumentation\nRelease note\nCode repo\n\n\n\nJava\nUser doc    API doc\nStandalone\nBundled\n\n\nC++\nUser doc     API doc\nStandalone\nStandalone\n\n\nPython\nUser doc  API doc\nStandalone\nStandalone\n\n\nGo client\nUser doc    API doc\nStandalone\nStandalone\n\n\nNode.js\nUser doc   API doc\nStandalone\nStandalone\n\n\nC#&#x2F;DotPulsar\nUser doc\nStandalone\nStandalone\n\n\n还有一个问题是：如何部署我们的 Pulsar 集群，是私有化部署还是购买云服务（目前 Pulsar的商业公司 streamnative 和国内的腾讯云都有类似的服务）\n我们之前有咨询过价格，相对来说还是自己部署性价比最高；和前文讲的一样，只使用云厂商的 kubernetes 服务，在这基础上部署我们的自己的服务。\n因为得益于 Pulsar 社区的活跃，即便是自己维护出现问题也可以及时得到反馈；同时自己平时踩的坑也可以反哺社区。\n之前也写过一些关于 Pulsar 的系列文章，感兴趣的可以查阅：\n\n在 kubernetes 环境下如何优雅扩缩容 Pulsar\nPulsar3.0新功能介绍\nPulsar负载均衡原理及优化\n白话 Pulsar Bookkeeper 的存储模型\nPulsar压测及优化\n\n业务框架最后是业务框架的选择，决定这个的前提是我们先要确定选择哪个语言作为主力业务语言。\n虽然这点对于 kubernetes 来说无关紧要，下面以我比较熟悉的 Java 和 Golang 进行介绍。\nJavaJava 可选的技术方案就比较多了，如果我们只是上了 kubernetes 但没有使用服务网格；那完全可以只使用 springboot 开发 http 接口，就和开发一个单体应用一样简单。\n只是这样会缺少一些服务治理的能力，更适用于中小型团队。\n如果团队人员较多，也没使用服务网格时；那就推荐使用前文介绍的微服务框架：比如 Dubbo、SpringCloud 等。\n当有专门的云原生团队时，则更推荐使用服务网格的方案，这样我们就能综合以上两种方案的优点：\n\n代码简洁，只是需要将 http 换为 gRPC。\n同时利用 Istio 也包含了微服务框架的能力。\n\nGolangGolang 其实也与 Java 类似，中小团队时我们完全可以只使用 Gin 这类 http 框架进行开发。\n而中大型团队在 Golang 生态中也有对标 Dubbo 和 SpringCloud 的框架，比如 kratos和 go-zero 等。\n得益于 Golang 的简洁特性，我觉得比使用 Java 开发业务更加简单和“无脑”。\n同样的后续也可以切换到服务网格，直接采用 gRPC 和 Golang 也非常适配，此时团队应该也比较成熟了，完全可以自己基于 gRPC 做一个开发脚手架，或者也可以使用 Kratos 或者是 go-zero 去掉他们的服务调用模块即可。\n总结以上就是个人对目前流行的技术方案的理解，也分别对不同团队规模进行了推荐；确实没有完美的技术方案，只有最合适的，也不要跟风选择一些自己不能把控的技术栈，最终吃亏的可能就是自己。\n参考链接：\n\nhttps://levelup.gitconnected.com/gitops-in-kubernetes-with-gitlab-ci-and-argocd-9e20b5d3b55b\nhttps://grpc.io/\n\n#Blog #CloudNative \n","categories":["OB"],"tags":["CloudNative","k8s"]},{"title":"我用我的270篇文章做了一个数字 AI 替身","url":"/2024/09/23/ob/Build-ower-AI-robot/","content":"23 年在 ChatGPT 刚出来的时候就在 V 站上看到有一个看到有大佬用自己的微信聊天记录和博客文章生成了一个 AI 替身：\n\n\n当时就想着自己做一个，不过当时实现起来还比较复杂，直到如今 AI 已经越来越普及，想做一个自己的 AI 替身成本也非常低了。\n于是就有了下图里的效果：\n和自己的内容这么对话还挺有意思的，现在大家就可以直接在我公众号回复消息和”他“聊天。\n\n也可以通过小程序来使用：\n\n如何搭建这里使用的数据源全都是我发布在公众号里的 260 篇文章。\n能够直接获取到微信公众号的数据一定是腾讯自己的产品，其实这个产品叫做：腾讯元器，是腾讯大模型团队基于混元大模型推出的智能创作工具。\n我们可以自定义 prompt、数据源、插件来实现自己的 AI 机器人，或者类似的交互产品。\n直接创建一个智能体，然后编写对应的提示词即可，使用起来非常简单，官方也提供了一些 prompt 的示例：\n根据自己的需求来填写就可以了。\n最主要的还是创建一个知识库，也就是你的数据源，好在这里直接整合了公众号的数据；\n直接授权就可以使用，同时还可以每天定时更新，非常方便。\n\n它会根据你的问题来判断是否用知识库的内容来回答，所以即便是问一些知识库不存在的内容也能拿到结果。\n\n除此之外还可以上传你本地的文件，所以即便是你没有写公众号也可以上传自己整理的内容。\n有兴趣的朋友可以试试尝尝鲜，后续我可以持续完善这个知识库，比如输入一些代码，之后再有向我咨询问题的朋友就可以先去问问”他“，\n大家可以直接在公众号里和”对话“，说不定还有意外收获🐶。\n","categories":["AI"],"tags":["AI"]},{"title":"白话 Pulsar Bookkeeper 的存储模型","url":"/2024/01/15/ob/Bookkeeper-storage/","content":"\n最近我们的 Pulsar 存储有很长一段时间数据一直得不到回收，但消息确实已经是 ACK 了，理论上应该是会被回收的，随着时间流逝不但没回收还一直再涨，最后在没找到原因的情况下就只有一直不停的扩容。\n\n最后磁盘是得到了回收，过程先不表，之后再讨论。\n\n为了防止类似的问题再次发生，我们希望可以监控到磁盘维度，能够列出各个日志文件的大小以及创建时间。\n这时就需要对 Pulsar 的存储模型有一定的了解，也就有了这篇文章。\n\n\n讲到 Pulsar 的存储模型，本质上就是 Bookkeeper 的存储模型。\nPulsar 所有的消息读写都是通过 Bookkeeper 实现的。\n\nBookkeeper 是一个可扩展、可容错、低延迟的日志存储数据库，基于 Append Only 模型。（数据只能追加不能修改）\n\n\n这里我利用 Pulsar 和 Bookkeeper 的 Admin API 列出了 Broker 和 BK 中 Ledger 分别占用的磁盘空间。\n\n关于这个如何获取和计算的，后续也准备提交给社区。\n\n背景但和我们实际 kubernetes 中的磁盘占用量依然对不上，所以就想看看在 BK 中实际的存储日志和 Ledger 到底差在哪里。\n\n知道 Ledger 就可以通过 Ledger 的元数据中找到对应的 topic，从而判断哪些 topic 的数据导致统计不能匹配。\n\nBookkeeper 有提提供一个Admin API 可以返回当前 BK 所使用了哪些日志文件的接口:https://bookkeeper.apache.org/docs/admin/http#endpoint-apiv1bookielist_disk_filefile_typetype\n\n从返回的结果可以看出，落到具体的磁盘上只有一个文件名称，是无法知道具体和哪些 Ledger 进行关联的，也就无法知道具体的 topic 了。\n此时只能大胆假设，应该每个文件和具体的消息 ID 有一个映射关系，也就是索引。所以需要搞清楚这个索引是如何运行的。\n存储模型\n我查阅了一些网上的文章和源码大概梳理了一个存储流程：\n\nBK 收到写入请求，数据会异步写入到 Journal&#x2F;Entrylog\nJournal 直接顺序写入，并且会快速清除已经写入的数据，所以需要的磁盘空间不多（所以从监控中其实可以看到 Journal 的磁盘占有率是很低的）。\n考虑到会随机读消息，EntryLog 在写入前进行排序，保证落盘的数据中同一个 Ledger 的数据尽量挨在一起，充分利用 PageCache.\n最终数据的索引通过 LedgerId+EntryId 生成索引信息存放到 RockDB 中（Pulsar 的场景使用的是 DbLedgerStorage 实现）。\n读取数据时先从获取索引，然后再从磁盘读取数据。\n利用 Journal 和 EntryLog 实现消息的读写分离。\n\n简单来说 BK 在存储数据的时候会进行双写，Journal 目录用于存放写的数据，对消息顺序没有要求，写完后就可以清除了。\n而 Entry 目录主要用于后续消费消息进行读取使用，大部分场景都是顺序读，毕竟我们消费消息的时候很少会回溯，所以需要充分利用磁盘的 PageCache，将顺序的消息尽量的存储在一起。\n\n同一个日志文件中可能会存放多个 Ledger 的消息，这些数据如果不排序直接写入就会导致乱序，而消费时大概率是顺序的，但具体到磁盘的表现就是随机读了，这样读取效率较低。\n\n所以我们使用 Helm 部署 Bookkeeper 的时候需要分别指定 journal 和 ledgers 的目录\nvolumes:    # use a persistent volume or emptyDir    persistence: true    journal:      name: journal      size: 20Gi      local_storage: false      multiVolumes:        - name: journal0          size: 10Gi          # storageClassName: existent-storage-class          mountPath: /pulsar/data/bookkeeper/journal0        - name: journal1          size: 10Gi          # storageClassName: existent-storage-class          mountPath: /pulsar/data/bookkeeper/journal1    ledgers:      name: ledgers      size: 50Gi      local_storage: false      storageClassName: sc    # storageClass:        # ...    useMultiVolumes: false      multiVolumes:        - name: ledgers0          size: 1000Gi          # storageClassName: existent-storage-class          mountPath: /pulsar/data/bookkeeper/ledgers0        - name: ledgers1          size: 1000Gi          # storageClassName: existent-storage-class          mountPath: /pulsar/data/bookkeeper/ledgers1\n\n\n每次在写入和读取数据的时候都需要通过消息 ID 也就是 ledgerId 和 entryId 来获取索引信息。\n\n也印证了之前索引的猜测。\n\n所以借助于 BK 读写分离的特性，我们还可以单独优化存储。\n比如写入 Journal 的磁盘因为是顺序写入，所以即便是普通的 HDD 硬盘速度也很快。\n大部分场景下都是读大于写，所以我们可以单独为 Ledger 分配高性能 SSD 磁盘，按需使用。\n\n因为在最底层的日志文件中无法直接通过 ledgerId 得知占用磁盘的大小，所以我们实际的磁盘占用率对不上的问题依然没有得到解决，这个问题我还会持续跟进，有新的进展再继续同步。\n\n#Blog #Pulsar \n","categories":["OB"],"tags":["Pulsar","Bookkeeper"]},{"title":"使用 ChatGPT 碰到的坑","url":"/2023/07/18/ob/ChatGPT-hole/","content":"\n最近在使用 ChatGPT 的时候碰到一个小坑，因为某些特殊情况我需要使用 syslog 向 logbeat 中发送日志。\n由于这是一个比较古老的协议，确实也没接触过，所以就想着让 ChatGPT 帮我生成个例子。\n\n\n原本我已经在  Go  中将这个流程跑通，所以其实只需要将代码转换为 Java 就可以了，这个我还是很信任 ChatGPT 的；\n\n现在我挺多结构化数据的转换都交给了 ChatGPT，省去了不少小工具。\n\n于是便有了这段对话：看起来挺正常的，我那过来改改确实也能用。\n\n直到快上线的时候，我发现一些元信息丢失了，比如日志生产者的 hostname, PID 等，然而这个信息在 Go 却没有丢失。\n于是我反复调试了之前生成的代码，依然没有找到问题。\n没办法，就只有去翻翻 Go 源码，想看看最终发出去的数据长什么样子，最后看到这样几行代码：\n这样一看就很清晰了，只是按照 &lt;%d&gt;%s %s %s[%d]: %s%s 的格式将生成的字符串通过网络发送出去。\n既然这样 Java 代码也很好写了:\nSocket socket = new Socket(hostname,port);socket.setKeepAlive(true);OutputStream os = socket.getOutputStream();PrintWriter pw = new PrintWriter(os, true);String format = String.format(&quot;&lt;%d&gt;%s %s %s[%d]: %s%s&quot;, 6 , rfc3164DateFormat.format(new Date()), &quot;test&quot;, &quot;test&quot;, 0, message, &quot;\\n&quot;);pw.println(format);\n经过测试数据终于对了。\n之后我就在想这么简单的一个问题 Google 上不可能没有吧，于是直接搜索了 Java syslog 关键字，结果直接就有一个现成的库。\n\n而且实现也是类似的。\n我相信应该有不少朋友也有被 ChatGPT 一本正经的胡说八道误导过，至少在当前的环境下一些简单的东西我还是决定优先 Google。\n","categories":["ChatGPT"],"tags":["Go","Syslog"]},{"title":"Golang 基础面试题 01","url":"/2023/09/12/ob/Golang-interview-01/","content":"\n背景在之前的文章中分享了 k8s 相关的面试题，本文我们重点来讨论和 k8s 密切相关的 Go 语言面试题。\n这几年随着云原生的兴起，大部分后端开发者，特别是 Java 开发者都或多或少的想学习一些 Go 相关的技能，所以今天分享的内容比较初级，适合 Go 语言初学者。\n\n本文内容依然来自于这个仓库https://github.com/bregman-arie/devops-exercises\n\n\n\n以下是具体内容：\n\n（）的内容是我的补充部分。\n\nGo 101Go 语言有哪些特点\nGo 是一种强类型静态语言，变量的类型必须在声明的时候指定（但可以使用类型推导），在运行时不能修改变量类型（与 Python 这类动态类型语言不同）。\n足够的简单，通常一个周末就能学会\n编译速度够快\n内置并发（相对于 Java 的并发来说非常简单）\n内置垃圾收集\n多平台支持\n可以打包到一个二进制文件中，所有运行时需要依赖的库都会被打包进这个二进制文件中，非常适合于分发。\n\nGo 是一种编译型的静态类型语言，正确还是错误正确✅\n为什么有些函数是以大写字母开头的这是因为 Go 语言中首字母大写的函数和变量是可以导出的，也就是可以被其他包所引用；类似于 Java 中的 public 和 private 关键字。\n变量和数据类型简洁和常规声明变量方式package mainimport &quot;fmt&quot;func main() &#123;  x := 2 // 只能在函数内使用，自动类型推导  var y int = 2  fmt.Printf(&quot;x: %v. y: %v&quot;, x, y)&#125;\n\n\n正确✅还是错误❌\n可以重复声明变量❌（强类型语言的特性）\n变量一旦声明，就必须使用✅（避免声明无效变量，增强代码可读性）\n\n下面这段代码的结果是什么？package mainimport &quot;fmt&quot;func main() &#123;    var userName    userName = &quot;user&quot;    fmt.Println(userName)&#125;\n\n编译错误，变量 userName 没有声明类型；修改为这样是可以的：\nfunc main() &#123;    var userName string    userName = &quot;user&quot;    fmt.Println(userName)&#125;\n\nvar x int = 2 and x := 2 这两种声明变量的区别结果上来说是相等的，但 x := 2  只能在函数体类声明。\n下面这段代码的结果是声明？package mainimport &quot;fmt&quot;x := 2func main() &#123;    x = 3    fmt.Println(x)&#125;\n\n编译错误，x := 2  不能在函数体外使用， x = 3 没有指定类型，除非使用 x := 3 进行类型推导。\n如何使用变量声明块（至少三个变量）package mainimport &quot;fmt&quot;var (  x bool   = false  y int    = 0  z string = &quot;false&quot;)func main() &#123;  fmt.Printf(&quot;The type of x: %T. The value of x: %v\\n&quot;, x, x)  fmt.Printf(&quot;The type of y: %T. The value of y: %v\\n&quot;, y, y)  fmt.Printf(&quot;The type of z: %T. The value of z: %v\\n&quot;, y, y)&#125;\n变量块配合 go fmt 格式化之后的代码对齐的非常工整，强迫症的福音。\nGo 的基础面试题也蛮多的，我们先从基础的开始，今后后继续更新相关面试题，难度也会逐渐提高，感兴趣的朋友请持续关注。#GO #面试 \n","categories":["Golang"],"tags":["面试"]},{"title":"【译】几个你或许并不知道 kubernetes 技巧","url":"/2024/06/03/ob/Kubernetes-tricks/","content":"\n原文链接: https://overcast.blog/13-kubernetes-tricks-you-didnt-know-647de6364472\n使用 PreStop 优雅关闭 PodapiVersion: v1kind: Podmetadata:  name: graceful-shutdown-examplespec:  containers:  - name: sample-container    image: nginx    lifecycle:      preStop:        exec:          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 30 &amp;&amp; nginx -s quit&quot;]\n\nPreStop 允许 Pod 在终止前执行一个命令或者是脚本，使用它就可以在应用退出前释放一些资源，确保应用可以优雅退出。\n比如可以在 Nginx 的 Pod 退出前将当前的请求执行完毕。\n\n\n使用临时容器调试 Pod临时容器可以不修改一个运行的容器的前提下调试容器，可以很方便的调试一些生产环境的 bug，可以避免重启应用。\nkubectl alpha debug -it podname --image=busybox --target=containername\n\n生产环境谨慎使用，只有在当前环境下无法排查问题的时候才使用。\n基于自定义的  Metrics 自动扩容Podkubernetes 是提供了 HPA 机制可以跟进 CPU 内存等标准数据进行自动扩缩容，但有时我们需要根据自定义的数据进行扩缩容。\n比如某个接口的延迟、队列大小等。\napiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata:  name: custom-metric-hpaspec:  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: your-application  minReplicas: 1  maxReplicas: 10  metrics:  - type: Pods    pods:      metric:        name: your_custom_metric      target:        type: AverageValue        averageValue: 10\n\n用 Init Containers 配置启动脚本初始化容器可以在应用容器启动前运行，我们可以使用它来初始化应用需要的配置、等待依赖的服务启动完成等工作：\napiVersion: v1kind: Podmetadata:  name: myapp-podspec:  containers:  - name: myapp-container    image: myapp  initContainers:  - name: init-myservice    image: busybox    command: [&#x27;sh&#x27;, &#x27;-c&#x27;, &#x27;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&#x27;]\n\n比如这个初始化容器会等待 myservice 可用后才会启动应用。\n需要注意的是如果初始化容器会阻塞应用启动，所以要避免在初始化容器里执行耗时操作。\nNode 亲和性调度当我们需要将某些应用部署到硬件配置较高的节点时（比如需要 SSD 硬盘），就可以使用节点亲和性来部署应用：\napiVersion: v1kind: Podmetadata:  name: with-node-affinityspec:  containers:  - name: with-node-affinity    image: nginx  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: disktype            operator: In            values:            - ssd\n\n这个 Pod 会被部署到有这个 disktype=ssd 标签的 节点上。\n动态配置：ConfigMap 和 SecretsConfigMap 和 Secrets可以动态注入到 Pod 中，避免对这些配置硬编码。\nConfigMap 适合非敏感的数据，Secrets 适合敏感的数据。\n# ConfigMap ExampleapiVersion: v1kind: ConfigMapmetadata:  name: app-configdata:  config.json: |    &#123;      &quot;key&quot;: &quot;value&quot;,      &quot;databaseURL&quot;: &quot;http://mydatabase.example.com&quot;    &#125;# Pod Spec using ConfigMapapiVersion: v1kind: Podmetadata:  name: myapp-podspec:  containers:    - name: myapp-container      image: myapp      volumeMounts:      - name: config-volume        mountPath: /etc/config  volumes:    - name: config-volume      configMap:        name: app-config\n\n这样在应用中就可以通过这路径 /etc/config/config.json 读取数据了。\n\n当然也可以把这些数据写入到环境变量中。\n\n以上这些个人技巧用的最多的是：\n\n临时容器调试 Pod，特别是业务容器缺少一些命令时。\nInit Container 等待依赖的服务启动完成。\nNode 亲和性调度。\nConfigMap 是基础操作了。\n\n","categories":["翻译","kubernetes"],"tags":["kubernetes"]},{"title":"使用 Helm 管理应用的一些 Tips","url":"/2023/10/07/ob/Helm-tips/","content":"\n背景Helm 是一个 Kubernetes 的包管理工具，有点类似于 Mac 上的 brew，Python 中的 PIP；可以很方便的帮我们直接在 kubernetes 中安装某个应用。\n比如我们可以直接使用以下命令方便的在 k8s 集群安装和卸载 MySQL：\nhelm install my-sql oci://registry-1.docker.io/bitnamicharts/mysql -n mysqlhelm uninstall my-mysql -n mysql\n\n\n对于一些复杂的应用使用 Helm 一键安装会更简单，以 Pulsar 举例：它有着多个组件，比如 bookkeeper、zookeeper、broker、proxy 等，各个组件还有着依赖关系。\n如果我们手动安装流程会比较繁琐，而使用 Helm 时便非常简单：\nhelm repo add apache https://pulsar.apache.org/chartshelm install my-pulsar apache/pulsar --version 3.0.0 -n pulsar\n\n\n当然他也只是帮我们生成了部署所需要的 yaml 文件，也没有太多黑科技。\n\n升级看似简单的工具我在实际线上使用的时候也踩过一个坑，最大的一个问题就是某次升级 Pulsar 的时候生成的 yaml 文件是空的，导致整个集群被删除了😭。\n还好最后使用 helm  rollback version 将集群恢复过来了，我们的持久化数据也还在。\n而出现这个问题的原因是我执行了下面这个命令：\nhelm upgrade pulsar ./charts/pulsar --version 2.9.2 -f charts/pulsar/values-2.10.3.yaml -n pulsar\n\n我们是将 pulsar 的 Helm-Chart 源码下载到本地，然后修改 value.yaml 的方式执行升级的。\n当时执行命令的时候没有注意，在一个没有 values-2.10.3.yaml 文件的目录下执行的，导致生成的 yaml 文件是空的，也就导致 k8s 在 pulsar 这个 namespace 下删除了所有的资源。\n模拟升级为了避免今后再次出现类似的问题，需要在升级前先模拟升级：\nhelm upgrade pulsar ./charts/pulsar --version 2.9.2 -f charts/pulsar/values-2.10.3.yaml -n pulsar --dry-run --debug &gt; debug.yaml\n\n其中关键的 dry-run 和 debug 参数可以指定模拟升级和输出详细的内容。\n这样我们就可以在升级前先查看 debug.yaml 里的内容是不是符合我们的预期。\n对比升级但这样并不能直观的看出哪些地方是我们修改的，还好社区已经有了相关的插件，可以帮我们高亮显示修改的地方。\nhelm plugin install https://github.com/databus23/helm-diff\n我们先安装好这个 helm 插件。\n然后在升级前先使用该插件：\nhelm diff upgrade pulsar ./charts/pulsar --version 2.9.2 -f charts/pulsar/values-2.10.3.yaml -n pulsar\n\n\n这样就可以高亮显示出修改的内容。\n\n不用担心这个命令会直接升级，它会自动加上 –dry-run –debug 参数。\n\n更多命令可以参考官方文档：https://github.com/databus23/helm-diff\nHelm 功能很强，在操作生产环境的时候必须得谨慎，都是血淋淋的教训啊。\n","categories":["Helm"],"tags":["CloudNative"]},{"title":"实操 OpenTelemetry：通过 Demo 掌握微服务监控的艺术","url":"/2024/05/26/ob/OTel-demo/","content":"前言在上一篇文章 OpenTelemetry 实践指南：历史、架构与基本概念中回顾了可观测性的历史以及介绍了一些 OpenTelemetry 的基础概念，同时也介绍了 OpenTelemetry 社区常用的开源项目。\n基础背景知识了解后，这篇就来介绍一下使用 OpenTelemetry 如何实战部署应用，同时在一个可视化页面查看 trace、metric 等信息。\n\n\n项目介绍我们参考官方文档构建几个 spring boot 、Golang 项目再配合 Agent 其实也可以很方便的集成 OpenTelemetry。\n但是要完整的体验 OpenTelemetry 的所有功能，包含 trace、logs、metrics，还有社区这么多语言的支持其实还是比较麻烦的。\n我们还需要单独部署 collector、存储的 backend service 等组件、包括 trace UI 展示所需要的 Jaeger，metric 所需要的 grafana 等。\n这些所有东西都自己从头弄的话还是比较费时，不过好在社区已经将这些步骤都考虑到了。\n特地为大家写了一个 opentelemetry-demo。\n这个项目模拟了一个微服务版本的电子商城，主要包含了以下一些项目：\n\n\n\nService\nLanguage\nDescription\n\n\n\naccountingservice\nGo\n处理和计算订单数据\n\n\nadservice\nJava\n广告服务\n\n\ncartservice\n.NET\n购物车服务，主要会依赖 Redis\n\n\ncheckoutservice\nGo\ncheckout\n\n\ncurrencyservice\nC++\n货币转换服务，提供了较高的 QPS 能力。\n\n\nemailservice\nRuby\n邮件服务\n\n\nfrauddetectionservice\nKotlin\n风控服务\n\n\nfrontend\nJavaScript\n前端应用\n\n\nloadgenerator\nPython&#x2F;Locust\n模拟压测服务\n\n\npaymentservice\nJavaScript\n支付服务\n\n\nproductcatalogservice\nGo\n商品服务\n\n\nquoteservice\nPHP\n成本服务\n\n\nrecommendationservice\nPython\n推荐服务\n\n\nshippingservice\nRust\nshipping service\n\n\n可以发现在这个 demo 中提供了许多的服务，而且包含了几乎所有主流的语言，可以很好的模拟我们实际的使用场景了。\n\n\n\n\n\n通过这张图可以更直观的查看各个服务之间的关系。\n整体来说前端所有的请求都会通过 front-end-proxy 这个组件代理，最终再由 front 这个服务进行转发到不同的后端服务中。\n\n除了一个项目的架构图之外，还有一个关于 OpenTelemetry 的数据流转图。\n在 OpenTelemetry 中数据流转是它的特点也是非常重要的核心，这点在上一篇文章中讲过，用户可以自由定制数据的流转以及任意的处理数据，在这个图中就将数据流转可视化了。\n\n客户端可以通过 OTLP 协议或者是 HTTP 将数据上传到 OTel Collector 中。\n在 collector 中会根据我们配置的 Process pipeline 处理数据。\nMetric 数据通过  OTLP HTTP exporter 将数据导入到 Prometheus 中。\nPrometheus 已经于 23 年七月份支持 OTLP 格式的 metric 数据导入了。\n\n\nTrace 数据则是通过 OTLP Exporter 写入到 Jaeger 中进行存储，最后通过 Jaeger 的 UI 进行查询展示。\n而存入 Prometheus 中的 metric 数据则是有 grafana 进行查询。\n\n\n关于 collector 的配置会在后文讲解。\n\n部署接下来便是安装 Demo 了，我更推荐使用 helm 安装。\n这里的版本要求是：\n\nKubernetes 1.24+\nHelm 3.9+\n\nhelm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-chartshelm repo updatehelm install my-otel-demo open-telemetry/opentelemetry-demo\n\n这样就可以很简单的将 demo 所涉及到的所有组件和服务都安装到 default 命名空间中。\nhelm show values open-telemetry/opentelemetry-demo &gt; demo.yaml\n不过在安装前还是建议先导出一份 value.yaml，之后可以使用这个 yaml 定制需要安装的组件。\n在这个 yaml 中我们可以看到有哪些组件和服务可以定制：可以看到这里包含了我们刚才提到的所有服务，以及这些服务所依赖的 Kafka、redis、Prometheus 等中间件，都可以自己进行定制修改。\n当所有的 Pod 都成功运行之后表示安装成功。\n\n正常情况下安装不会有什么问题，最大可能的问题就是镜像拉取失败，此时我们可以先在本地手动 docker pull 下来镜像后再上传到私服，然后修改 deployment 中的镜像地址即可。\n\n暴露服务为了方便使用我们可以用这个 demo 进行测试，还需要将 front-proxy 的服务暴露出来可以在本地访问：\nkubectl port-forward svc/my-otel-demo-frontendproxy 8080:8080\n\n\n\n\nComponent\nPath\n\n\n\nShop 首页\nhttp://localhost:8080\n\n\nGrafana\nhttp://localhost:8080/grafana\n\n\n压测页面\nhttp://localhost:8080/loadgen\n\n\nJaeger UI\nhttp://localhost:8080/jaeger/ui\n\n\n正常情况下就可以打开这些页面进行访问了。\n\n\n\n不过使用 port-forward 转发的方式只是临时方案，使用 ctrl+c 就会停止暴露服务，所以如果想要一个稳定的访问链接时便可以配置一个 ingress。\ncomponents:  frontendProxy:    ingress:      enabled: true      annotations: &#123;&#125;      hosts:        - host: otel-demo.my-domain.com          paths:            - path: /              pathType: Prefix              port: 8080\n\n在之前的 helm 的 value.yaml 中配置即可，本地测试的话需要将这个 host 和 ingress 暴露出来的 IP 进行绑定才可以使用这个域名机进行访问。\n更多关于 ingress 的使用可以参考我之前的文章：\n\nk8s入门到实战-使用Ingress\n\n当然简单起见也可以直接将 front-proxy 的 service 类型改为 LoadBalancer。（默认是 ClusterIP 只可以在集群内访问）\n这样就可以直接通过这个 service 的 IP 进行访问了。\ncomponents:  frontendProxy:    service:      type: LoadBalancer\n\n\n不过需要注意的是如果 demo 安装完成之后是不可以再次修改 service 的类型的，需要手动这个 service 删掉之后再次新建才可以。\n\n 临时测试使用的话还是推荐直接使用 port-forward 进行转发。\n查看 Trace通过之前的项目架构图可以得知，我们在项目首页刷新会直接请求 AdService 来获取广告。\n为了简单起见我们只查询这一链路的调用情况：\n打开 http://localhost:8080/jaeger/ui/search Jeager 的 UI 页面便可以筛选服务，之后点击查找 Traces 就可以列出一段时间内的访问 trace。\n可以看到这个请求链路是从前端访问到 adService 中的 getAds()接口，然后在这个接口中再访问了 getAdsByCategory 函数。\n最终在源码中也可以看到符合链路的调用代码。\n\n在刚才的链路图的右下角有一个 spanID，整个 trace 是由这些小的 span 组成，每一个 span 也会有唯一 spanID； trace 也会有一个 traceID 将这些 span 串联起来；更多关于 trace 的内容会在后面的文章进行分析。\n\n查看 Metrics我们再打开 grafana 便可以看到刚才访问的 adService 的延迟和接口的 QPS 情况：\n\n在opentelemetry-collector-data-flow 面板中还可以看到 OpenTelemetry 的数据流转。\n\n更多监控信息可以查看其它的面板。\n\n而刚才面板中的数据流转规则则是在我们的 collector 中进行配置的：\nreceivers:  otlp:    protocols:      grpc:      http:        cors:          allowed_origins:            - &quot;http://*&quot;            - &quot;https://*&quot;  httpcheck/frontendproxy:    targets:      - endpoint: http://frontendproxy:$&#123;env:ENVOY_PORT&#125;exporters:  debug:  otlp:    endpoint: &quot;jaeger:4317&quot;    tls:      insecure: true  otlphttp/prometheus:    endpoint: &quot;http://prometheus:9090/api/v1/otlp&quot;    tls:      insecure: true  opensearch:    logs_index: otel    http:      endpoint: &quot;http://opensearch:9200&quot;      tls:        insecure: trueprocessors:  batch:connectors:  spanmetrics:service:  pipelines:    traces:      receivers: [otlp]      processors: [batch]      exporters: [otlp, debug, spanmetrics]    metrics:      receivers: [httpcheck/frontendproxy, otlp, spanmetrics]      processors: [batch]      exporters: [otlphttp/prometheus, debug]    logs:      receivers: [otlp]      processors: [batch]      exporters: [opensearch, debug]\n重点的就是这里的 service.piplines，可以进行任意的组装。\n更多关于 collector 的配置也会在后续文章中继续讲解。\n我们也可以继续访问这个 demo 网站，模拟加入购物车、下单等行为，再结合 trace 和 metric 观察系统的变化。\n这样一个完整的 OpenTelemetry-Demo 就搭建完毕了，我们实际在生产环境使时完全可以参考这个 demo 进行配置，可以少踩很多坑。\n参考链接：\n\nhttps://github.com/open-telemetry/opentelemetry-demo/blob/main/src/adservice/Dockerfile\nhttps://github.com/open-telemetry/opentelemetry-demo\nhttps://github.com/prometheus/prometheus/pull/12571\nhttps://github.com/open-telemetry/opentelemetry-demo/blob/main/src/otelcollector/otelcol-config.yml\n\n","categories":["OB"],"tags":["OpenTelemetry"]},{"title":"OpenTelemetry 实战：从零实现分布式链路追踪","url":"/2024/08/20/ob/OpenTelemetry-01-trace/","content":"背景之前写过一篇 从 Dapper 到 OpenTelemetry：分布式追踪的演进之旅的文章，主要是从概念上讲解了 Trace 在 OpenTelemetry 的中的场景和使用。\n也写过一篇 实操 OpenTelemetry：通过 Demo 掌握微服务监控的艺术：如何从一个 demo 开始集成 OpenTelemetry。\n但还是有不少小伙伴反馈说无法快速上手（可能也是这个 demo 的项目比较多），于是我准备从 0 开始从真实的代码一步步带大家集成 OpenTelemetry，因为 OpenTelemetry 本身是跨多种语言的，所以也会以两种语言为（Java、Golang）主进行讲解。\n\n使用这两种语言主要是因为 Java 几乎全是自动埋点，而 Golang 因为语言特性，大部分都得硬编码埋点；覆盖到这两种场景后其他语言也是类似的，顶多只是 API 名称有些许区别。\n\n在这个过程中也会穿插一些 OpenTelemetry 的原理，希望整个过程下来大家可以在项目中实际运用起来，同时也能知其所以然。\n\n\n项目结构在这个过程中会涉及到以下项目：\n\n\n\n名称\n作用\n语言\n版本\n\n\n\njava-demo\n发送 gRPC 请求的客户端\nJava\nopentelemetry-agent: 2.4.0&#x2F;SpringBoot: 2.7.14\n\n\nk8s-combat\n提供 gRPC 服务的服务端\nGolang\ngo.opentelemetry.io&#x2F;otel: 1.28&#x2F; Go: 1.22\n\n\nJaeger\ntrace 存储的服务端以及 TraceUI 展示\nGolang\njaegertracing&#x2F;all-in-one:1.56\n\n\nopentelemetry-collector-contrib\nOpenTelemetry 的 collector 服务端，用于收集 trace&#x2F;metrics&#x2F;logs 然后写入到远端存储\nGolang\notel&#x2F;opentelemetry-collector-contrib:0.98.0\n\n\n\n在开始之前我们先看看实际的效果，我们需要先把 collector 和 Jaeger 部署好：\ndocker run --rm -d --name jaeger \\  -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\  -p 6831:6831/udp \\  -p 6832:6832/udp \\  -p 5778:5778 \\  -p 16686:16686 \\  -p 4317:4317 \\  -p 4318:4318 \\  -p 14250:14250 \\  -p 14268:14268 \\  -p 14269:14269 \\  -p 9411:9411 \\  jaegertracing/all-in-one:1.56docker run --rm -d -v $(pwd)/coll-config.yaml:/etc/otelcol-contrib/config.yaml --name coll \\-p 5318:4318 \\-p 5317:4317 \\otel/opentelemetry-collector-contrib:0.98.0\n\n这里有一个 coll-config 的配置文件如下：\nreceivers:  otlp:    protocols:      grpc:      http:exporters:  debug:  otlp:    endpoint: &quot;127.0.0.1:4317&quot;    tls:      insecure: trueprocessors:  batch:service:  pipelines:    traces:      receivers: [otlp]      processors: [batch]      exporters: [otlp, debug]\n\n重点是这里的 endpoint: &quot;127.0.0.1:4317&quot; 我们需要配置位 Jaeger 的 IP 和端口。\n\n更多关于这里的配置会在后续单独的 collector 章节中讲解。\n\n这两个服务都启动成功后再启动我们的 Java 客户端和  Go  服务端：\njava -javaagent:opentelemetry-javaagent-2.4.0-SNAPSHOT.jar \\-Dotel.traces.exporter=otlp \\-Dotel.metrics.exporter=otlp \\-Dotel.logs.exporter=none \\-Dotel.service.name=demo \\-Dotel.exporter.otlp.protocol=grpc \\-Dotel.propagators=tracecontext,baggage \\-Dotel.exporter.otlp.endpoint=http://127.0.0.1:5317 \\      -jar target/demo-0.0.1-SNAPSHOT.jar# Golangexport OTEL_EXPORTER_OTLP_ENDPOINT=http://127.0.0.1:5317export OTEL_RESOURCE_ATTRIBUTES=service.name=k8s-combat./k8s-combat\n\n可以看到不管是 Java 还是 Golang 应用都是需要配置 OTEL_EXPORTER_OTLP_ENDPOINT 参数，也就是 opentelemetry-collector-contrib 的地址。\n\n其余的一些配置在后面会讲到。\n\ncurl http://127.0.0.1:9191/request\\?name\\=1232\n然后我们触发一下 Java 客户端的入口，就可以在 JaegerUI 中查询到刚才的链路了。http://localhost:16686/search\n这样整个 trace 链路就串起来了。\nJava 应用下面来看看具体的应用代码里是如何编写的。\n\nJava 是基于 springboot 编写的，具体 springboot 的使用就不再赘述了。\n\n因为我们应用是使用 gRPC 通信的，所以需要提供一个 helloworld.proto 的 pb 文件：\nsyntax = &quot;proto3&quot;;    option go_package = &quot;google.golang.org/grpc/examples/helloworld/helloworld&quot;;  option java_multiple_files = true;  option java_package = &quot;io.grpc.examples.helloworld&quot;;  option java_outer_classname = &quot;HelloWorldProto&quot;;    package helloworld;    // The greeting service definition.  service Greeter &#123;    // Sends a greeting    rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;  &#125;    // The request message containing the user&#x27;s name.  message HelloRequest &#123;    string name = 1;  &#125;    // The response message containing the greetings  message HelloReply &#123;    string message = 1;  &#125;\n\n这个文件也没啥好说的，就定义了一个简单的 SayHello 接口。\n&lt;dependency&gt;    &lt;groupId&gt;net.devh&lt;/groupId&gt;    &lt;artifactId&gt;grpc-spring-boot-starter&lt;/artifactId&gt;    &lt;version&gt;3.1.0.RELEASE&lt;/version&gt;  &lt;/dependency&gt;    &lt;dependency&gt;    &lt;groupId&gt;io.grpc&lt;/groupId&gt;    &lt;artifactId&gt;grpc-stub&lt;/artifactId&gt;    &lt;version&gt;$&#123;grpc.version&#125;&lt;/version&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.grpc&lt;/groupId&gt;    &lt;artifactId&gt;grpc-protobuf&lt;/artifactId&gt;    &lt;version&gt;$&#123;grpc.version&#125;&lt;/version&gt;  &lt;/dependency&gt;\n\n在 Java 中使用了 grpc-spring-boot-starter 这个库来处理 gRPC 的客户端和服务端请求。\ngrpc:    server:      port: 9192    client:      greeter:        address: &#x27;static://127.0.0.1:50051&#x27;        enableKeepAlive: true        keepAliveWithoutCalls: true        negotiationType: plaintext\n\n然后我们定义了一个接口用于接收请求触发 gRPC 的调用：\n@RequestMapping(&quot;/request&quot;)  public String request(@RequestParam String name) &#123;     log.info(&quot;request: &#123;&#125;&quot;, request);       HelloReply abc = greeterStub.sayHello(io.grpc.examples.helloworld.HelloRequest.newBuilder().setName(request.getName()).build());      return abc.getMessage();  &#125;\n\nJava 应用的实现非常简单，和我们日常日常开发没有任何区别；唯一的区别就是在启动时需要加入一个 javaagent以及一些启动参数。\njava -javaagent:opentelemetry-javaagent-2.4.0-SNAPSHOT.jar \\-Dotel.traces.exporter=otlp \\-Dotel.metrics.exporter=otlp \\-Dotel.logs.exporter=none \\-Dotel.service.name=demo \\-Dotel.exporter.otlp.protocol=grpc \\-Dotel.propagators=tracecontext,baggage \\-Dotel.exporter.otlp.endpoint=http://127.0.0.1:5317 \\      -jar target/demo-0.0.1-SNAPSHOT.jar\n\n下面来仔细看看这些参数\n\n\n\n名称\n作用\n\n\n\njavaagent:opentelemetry-javaagent-2.4.0-SNAPSHOT.jar\n这个没啥好说的，指定一个 javaagent\n\n\notel.traces.exporter\n指定 trace 以什么格式传输（默认是这里的 otlp)；当然还有其他的值：logging/jaeger/zipkin 等，我们这里使用 otlp 会将数据传输到 collector 中。\n\n\notel.metrics.exporter\n同上，只是指定的是 metrics 的传输方式，我们在之后讲解指标的时候会用到。\n\n\notel.service.name\n定义在 trace 中的应用名称，springboot 会默认使用 spring.application.name 这个变量。\n\n\notel.exporter.otlp.protocol\n指定传输协议；除了 grpc 之外还有 http/protobuf，当然我们也可以根据 trace 和 metrics 分开指定：otel.exporter.otlp.traces.protocol/otel.exporter.otlp.metrics.protocol\n\n\notel.propagators\n指定我们跨服务传播上下文的时候使用哪种格式，默认是 W3C Trace Context,baggage，当然也有其他的- &quot;b3&quot;: B3 Single，- &quot;xray&quot;: AWS X-Ray，&quot;jaeger&quot;: Jaeger等\n\n\notel.exporter.otlp.endpoint\n指定 collector 的 endpoint\n\n\n更多细节的参数大家可以在这里找到：\n\n\n\nhttps://opentelemetry.io/docs/languages/java/configuration/\n\n\n\nGolang 应用接着我们来看看 Go 是如何集成 OpenTelemetry 的。\n在创建好项目之后我们需要添加 OpenTelemetry 所提供的包：\ngo get &quot;go.opentelemetry.io/otel&quot; \\  &quot;go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc&quot; \\  &quot;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc&quot; \\  &quot;go.opentelemetry.io/otel/propagation&quot; \\  &quot;go.opentelemetry.io/otel/sdk/metric&quot; \\  &quot;go.opentelemetry.io/otel/sdk/resource&quot; \\  &quot;go.opentelemetry.io/otel/sdk/trace&quot; \\       &quot;go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc&quot;\\\n\n然后我们需要创建一个初始化 tracer 的函数：\nfunc initTracerProvider() *sdktrace.TracerProvider &#123;\tctx := context.Background()\texporter, err := otlptracegrpc.New(ctx)\tif err != nil &#123;\t\tlog.Printf(&quot;new otlp trace grpc exporter failed: %v&quot;, err)\t&#125;\ttp := sdktrace.NewTracerProvider(\t\tsdktrace.WithBatcher(exporter),\t\tsdktrace.WithResource(initResource()),\t)\totel.SetTracerProvider(tp)\totel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.TraceContext&#123;&#125;, propagation.Baggage&#123;&#125;))\treturn tp&#125;\n\n因为我们使用的是 grpc 协议上报 otlp 数据，所以这里使用的是 exporter, err := otlptracegrpc.New(ctx)  创建了一个 exporter。\notel.SetTextMapPropagator() 这个函数里配置数据和刚才 Java 里配置的 -Dotel.propagators=tracecontext,baggage 是一样的效果。\n与此同时我们还需要提供一个 initResource() 的函数：\nfunc initResource() *sdkresource.Resource &#123;\tinitResourcesOnce.Do(func() &#123;\t\textraResources, _ := sdkresource.New(\t\t\tcontext.Background(),\t\t\tsdkresource.WithOS(),\t\t\tsdkresource.WithProcess(),\t\t\tsdkresource.WithContainer(),\t\t\tsdkresource.WithHost(),\t\t)\t\tresource, _ = sdkresource.Merge(\t\t\tsdkresource.Default(),\t\t\textraResources,\t\t)\t&#125;)\treturn resource&#125;\n\n这个函数用来告诉 trace 需要暴露那些 resource，也就是我们在这里看到进程相关的属性：比如这里的 sdkresource.WithOS(), 就会显示 OS 的类型和描述。\nfunc WithOS() Option &#123;      return WithDetectors(         osTypeDetector&#123;&#125;,         osDescriptionDetector&#123;&#125;,      )&#125;\n\n而 sdkresource.WithProcess(), 显示的数据就更多了。\nfunc WithProcess() Option &#123;      return WithDetectors(         processPIDDetector&#123;&#125;,         processExecutableNameDetector&#123;&#125;,         processExecutablePathDetector&#123;&#125;,         processCommandArgsDetector&#123;&#125;,         processOwnerDetector&#123;&#125;,         processRuntimeNameDetector&#123;&#125;,         processRuntimeVersionDetector&#123;&#125;,         processRuntimeDescriptionDetector&#123;&#125;,      )&#125;\n\n\n以上这些代码在 Java 中都是由 agent 指定创建的。\n\n\n// Init OpenTelemetry start  tp := initTracerProvider()  defer func() &#123;      if err := tp.Shutdown(context.Background()); err != nil &#123;         log.Printf(&quot;Error shutting down tracer provider: %v&quot;, err)      &#125;&#125;()     err := runtime.Start(runtime.WithMinimumReadMemStatsInterval(time.Second))  if err != nil &#123;      log.Err(err)  &#125;tracer = tp.Tracer(&quot;k8s-combat&quot;)// Init OpenTelemetry end\n\n之后我们需要在 main 函数一开始就初始化 traceProvider。\n对于 grpc 来说，OpenTelemetry 的 Go-SDK 提供了自动埋点，但我们也得手动配置一下：\ns := grpc.NewServer(      grpc.StatsHandler(otelgrpc.NewServerHandler()),  )  pb.RegisterGreeterServer(s, &amp;server&#123;&#125;)\n\n使用 grpc.StatsHandler(otelgrpc.NewServerHandler()),  将 OTel 的 serverHandle 加入进去，这个 handle 会自动创建 grpc 服务端的 span。\n\n对 trace&#x2F;span 概念还有不了解的朋友可以查看这篇文章。\n\nvar port = &quot;:50051&quot;  lis, err := net.Listen(&quot;tcp&quot;, port)  if err != nil &#123;      log.Fatal().Msgf(&quot;failed to listen: %v&quot;, err)  &#125;  s := grpc.NewServer(      grpc.StatsHandler(otelgrpc.NewServerHandler()),  )  pb.RegisterGreeterServer(s, &amp;server&#123;&#125;)  if err := s.Serve(lis); err != nil &#123;      log.Fatal().Msgf(&quot;failed to serve: %v&quot;, err)  &#125; else &#123;      log.Printf(&quot;served on %s \\n&quot;, port)  &#125;\n\n接着我们只需要启动这个 grpc 服务即可，就算完成了 Go 服务的集成。\n从这里可以看出 Java 相对于 Go 来说会简单许多，只需要配置一个 agent 就可以不该一行代码支持目前市面上流行的绝大多数框架。\n自定义  span 的 attribute我们在看链路信息的时候其实看的最多的是某个 span 里的 attribute 数据（有些地方又称为 tag)如下图所示：\n这里会展示当前 span 的各种信息，但如果我们想要额外加一些自己关心的数据应该如何添加呢？\nmessage HelloRequest &#123;    string name = 1;  &#125;\n比如我们想知道这个 grpc 接口里的 name 参数，如上图所示那样展示在 span 中。\n好在 OpenTelemetry 已经考虑到类似的需求：\nspan := trace.SpanFromContext(ctx)  span.SetAttributes(attribute.String(&quot;request.name&quot;, in.Name))\n\n我们使用 span := trace.SpanFromContext(ctx)  获取到当前的 span，然后调用 SetAttributes 就可以添加自定义的数据了。\n\n对应的 Java 也有类似的函数。\n\n除了新增 attribute 之外还可以新增 Event，Link 等数据，使用方式也是类似的。\n// AddEvent adds an event with the provided name and options.  AddEvent(name string, options ...EventOption)    // AddLink adds a link.  // Adding links at span creation using WithLinks is preferred to calling AddLink  // later, for contexts that are available during span creation, because head  // sampling decisions can only consider information present during span creation.  AddLink(link Link)\n\n\n自定义新增 span同理我们可能不局限于为某个 span 新增 attribute，也有可能想要新增一个新的 span 来记录关键的调用信息。\n\n默认情况下只有 OpenTelemetry 实现过的组件的核心函数才会有 span，自己代码里的函数调用是不会创建span 的。\n\nfunc (s *server) span(ctx context.Context) &#123;      ctx, span := tracer.Start(ctx, &quot;hello-span&quot;)      defer span.End()      // do some work      log.Printf(&quot;create span&quot;)  &#125;\n\n在  Go 中只需要手动 Start 一个 span 即可。\n对应到 Java 稍微简单一些，只需要为函数添加一个注解即可。\n@WithSpan(&quot;span&quot;)  public void span(@SpanAttribute(&quot;request.name&quot;) String name) &#123;      TimeUnit.SECONDS.sleep(1);      log.info(&quot;span:&#123;&#125;&quot;, name);  &#125;\n\n只不过得单独引入一个依赖：\n&lt;dependency&gt;    &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;    &lt;artifactId&gt;opentelemetry-api&lt;/artifactId&gt;  &lt;/dependency&gt;    &lt;dependency&gt;    &lt;groupId&gt;io.opentelemetry.instrumentation&lt;/groupId&gt;    &lt;artifactId&gt;opentelemetry-instrumentation-annotations&lt;/artifactId&gt;    &lt;version&gt;2.3.0&lt;/version&gt;  &lt;/dependency&gt;\n\n\n最终我们在 Jaeger UI 上看到的效果如下：\n\n总结\n最后总结一下，OpenTelemetry 支持许多流行的语言，主要分为两类：是否支持自动埋点。\n\n\n这里 Go 也可以零代码埋点，是使用了 eBPF，本文暂不做介绍。\n\n对于支持自动埋点的语言就很简单，只需要配置下 agent 即可；而原生的 Go 语言不支持自动埋点就得手动使用 OpenTelemetry 提供的 SDK 处理一些关键步骤；总体来说也不算复杂。\n下一期会重点讲解如何使用 Metrics。\n感兴趣的朋友可以在这里查看 Go 相关的源码：\n\nhttps://github.com/crossoverJie/k8s-combat\n\n参考链接：\n\nhttps://opentelemetry.io/docs/languages/java/configuration/\nhttps://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/docs/supported-libraries.md\nhttps://crossoverjie.top/2024/06/06/ob/OpenTelemetry-trace-concept/\n\n","categories":["OB","OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"一年时间从小白成为 OpenTelemetry Member 有感","url":"/2025/04/15/ob/OTel-member/","content":"前段时间申请成为了 OpenTelemetry 的 Member 通过了，算是完成了一个阶段性目标；从 24 年的 2 月份的第一个 issue 到现在刚好一年的时间。\n\n\n\n\n这事也挺突然的，源自于年初我发了一个 24 年的年终总结，提到了希望在今年争取成为 Member，然后谭总就提醒我可以自己去申请，只要找到两个 sponsors 支持就可以了。\n\n我之前不知道这个 Member 是自己申请的，没注意看社区的文档（之前的 Apache 社区都是邀请制）。\n\n\n于是我提交了相关的 issue，列举了自己做的一些贡献（PR 和 issue），也找到了之前经常帮我 review 的Rao 哥作为 sponsor.\n不出意外，没等两天就收到了邀请。\n参与社区OpenTelemetry 作为和厂商无关的可观测标准，非常开放和包容，也是我参与过的社区最多元的开源项目，几乎每个子项目都有上百人参与，他们都来自于不同的公司和个人，在这样的背景下社区自然就会更佳和谐，很难出现某个公司或者个人主导项目的发，风险自然也会小很多。\n\nOTel 的技术栈主要是可以分为下面三个部分：\n\n客户端：负责上报可观测数据（Trace、Metrics、Logs）\nOTel collector：处理客户端上报的数据\n数据存储：存储日志、指标、trace 等数据\n\n以上每个模块都是 OpenTelemetry 非常重要的组成部分，大家可以都挑感兴趣的部分去参与。\n作为一个可观测标准，客户端自然就需要支持大部分的技术栈，所以我们常用的语言和技术栈都有对应的实现：\n这一部分的工作量也非常大，靠个人实现和维护肯定不现实，所以社区非常欢迎大家都来做贡献。\n\n\n\n拿我常用的 Java 来说目前支持了这些框架和库，但依然没有支持全，我们可以在这里的 issue 列表里找到社区需要大家贡献的内容。\nSIG 小组社区也准备许多兴趣小组（SIG）来解决特定领域的问题：\n\n大家也可以订阅日历参与周会，基本上每个兴趣小组都会定期组织，拿 Java 的来说就是每周四的 UTC+8 的早上九点都会举行。\n\n之前参加过两次，都是 zoom 的线上会议（老外的习惯是开摄像头），如果自己口语尚可的话和社区主要的 maintainer 直接沟通效率会高很多。\n当然如果不能开口的话， zoom 也是实时字幕的功能，理解起来问题也不是很大。\n\n如果以可以成为 Member 的角度，目前我看了一些申请，提交了两个或以上的 PR 应该都可以申请通过，前提是线下提前和你找的 sponsor 达成一致就可以了。\n\n带着这个目的也挺好的，做开源项目往往就是靠爱发电，有这个 Member 的身份也可以作为正向激励，鼓励继续参与。\n\n总结当然成为 Member 只是第一步，随着社区参与的深入度后面还有其他的角色：\n比如 triager 可以分配 issue、approver 可以批准代码、maintainer 就是某个模块的具体负责人了，后面就再接再厉吧。\n","categories":["OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"OpenTelemetry 实战：从零实现应用指标监控","url":"/2024/08/27/ob/OpenTelemetry-02-metrics/","content":"前言在上一篇文章：OpenTelemetry 实战：从零实现分布式链路追踪讲解了链路相关的实战，本次我们继续跟进如何使用 OpenTelemetry 集成 metrics 监控。\n\n建议对指标监控不太熟的朋友可以先查看这篇前菜文章：从 Prometheus 到 OpenTelemetry：指标监控的演进与实践\n\n\n\n\n\n\n名称\n作用\n语言\n版本\n\n\n\njava-demo\n发送 gRPC 请求的客户端\nJava\nopentelemetry-agent: 2.4.0&#x2F;SpringBoot: 2.7.14\n\n\nk8s-combat\n提供 gRPC 服务的服务端\nGolang\ngo.opentelemetry.io&#x2F;otel: 1.28&#x2F; Go: 1.22\n\n\nJaeger\ntrace 存储的服务端以及 TraceUI 展示\nGolang\njaegertracing&#x2F;all-in-one:1.56\n\n\nopentelemetry-collector-contrib\nOpenTelemetry 的 collector 服务端，用于收集 trace&#x2F;metrics&#x2F;logs 然后写入到远端存储\nGolang\notel&#x2F;opentelemetry-collector-contrib:0.98.0\n\n\nPrometheus\n作为 metrics 的存储和展示组件，也可以用 VictoriaMetrics 等兼容 Prometheus 的存储替代。\nGolang\nquay.io&#x2F;prometheus&#x2F;prometheus:v2.49.1\n\n\n\n快速开始以上是加入 metrics 之后的流程图，在原有的基础上会新增一个 Prometheus 组件，collector 会将 metrics 指标数据通过远程的 remote write 的方式写入到 Prometheus 中。\nPrometheus 为了能兼容 OpenTelemetry 写入过来的数据，需要开启相关特性才可以。\n如果是 docker 启动的话需要传入相关参数：\ndocker run  -d -p 9292:9090 --name prometheus \\-v /prometheus/prometheus.yml:/etc/prometheus/prometheus.yml \\quay.io/prometheus/prometheus:v2.49.1 \\--config.file=/etc/prometheus/prometheus.yml \\--storage.tsdb.path=/prometheus \\--web.console.libraries=/etc/prometheus/console_libraries \\--web.console.templates=/etc/prometheus/consoles \\--enable-feature=exemplar-storage \\--enable-feature=otlp-write-receiver\n\n--enable-feature=otlp-write-receiver 最主要的就是这个参数，用于开启接收 OTLP 格式的数据。\n但使用这个 Push 特性就会丧失掉 Prometheus 的许多 Pull 特性，比如服务发现，定时抓取等，不过也还好，Push 和 Pull 可以同时使用，原本使用 Pull 抓取的组件依然不受影响。\n修改 OpenTelemetry-Collector接着我们需要修改下 Collector 的配置:\nexporters:  debug:  otlp:    endpoint: &quot;jaeger:4317&quot;    tls:      insecure: true  otlphttp/prometheus:    endpoint: http://prometheus:9292/api/v1/otlp    tls:      insecure: true      processors:  batch:service:  pipelines:    traces:      receivers:      - otlp      processors: [batch]      exporters:      - otlp      - debug            metrics:      exporters:      - otlphttp/prometheus      - debug      processors:      - batch      receivers:      - otlp\n\n这里我们在 exporter 中新增了一个 otlphttp/prometheus 的节点，用于指定导出 prometheus 的 endpoint 地址。\n同时我们还需要在 server.metrics.exporters 中配置相同的 key: otlphttp/prometheus。\n需要注意的是这里我们一定得是配置在 metrics.exporters 这个节点下，如果配置在 traces.exporters 下时，相当于是告诉 collector 讲 trace 的数据导出到 otlphttp/prometheus.endpoint 这个 endpoint 里了。\n\n所以重点是需要理解这里的配对关系。\n\n运行效果这样我们只需要将应用启动之后就可以在 Prometheus 中查询到应用上报的指标了。\njava -javaagent:opentelemetry-javaagent-2.4.0-SNAPSHOT.jar \\-Dotel.traces.exporter=otlp \\-Dotel.metrics.exporter=otlp \\-Dotel.logs.exporter=none \\-Dotel.service.name=java-demo \\-Dotel.exporter.otlp.protocol=grpc \\-Dotel.propagators=tracecontext,baggage \\-Dotel.exporter.otlp.endpoint=http://127.0.0.1:5317 -jar target/demo-0.0.1-SNAPSHOT.jar# Run go appexport OTEL_EXPORTER_OTLP_ENDPOINT=http://127.0.0.1:5317 OTEL_RESOURCE_ATTRIBUTES=service.name=k8s-combat./k8s-combat\n\n因为我们在 collector 中开启了 Debug 的 exporter，所以可以看到以下日志：\n2024-07-22T06:34:08.060Z\tinfo\tMetricsExporter\t&#123;&quot;kind&quot;: &quot;exporter&quot;, &quot;data_type&quot;: &quot;metrics&quot;, &quot;name&quot;: &quot;debug&quot;, &quot;resource metrics&quot;: 1, &quot;metrics&quot;: 18, &quot;data points&quot;: 44&#125;\n此时是可以说明指标上传成功的。\n然后我们打开 Prometheus 的地址：http://127.0.0.1:9292/graph便可以查询到 Java 应用和 Go 应用上报的指标。\n\nOpenTelemetry 的 javaagent 会自动上报 JVM 相关的指标。\n\n\n而在 Go 程序中我们还是需要显式的配置一些埋点：\nfunc initMeterProvider() *sdkmetric.MeterProvider &#123;      ctx := context.Background()        exporter, err := otlpmetricgrpc.New(ctx)      if err != nil &#123;         log.Printf(&quot;new otlp metric grpc exporter failed: %v&quot;, err)      &#125;      mp := sdkmetric.NewMeterProvider(         sdkmetric.WithReader(sdkmetric.NewPeriodicReader(exporter)),         sdkmetric.WithResource(initResource()),      )    otel.SetMeterProvider(mp)      return mp  &#125;mp := initMeterProvider()defer func() &#123;\tif err := mp.Shutdown(context.Background()); err != nil &#123;\t\tlog.Printf(&quot;Error shutting down meter provider: %v&quot;, err)\t&#125;&#125;()\n\n和 Tracer 类似，我们首先也得在 main 函数中调用 initMeterProvider() 函数来初始化 Meter，此时它会返回一个 sdkmetric.MeterProvider 对象。\nOpenTelemetry Go 的 SDK 中已经提供了对 go runtime 的自动埋点，我们只需要调用相关函数即可：\nerr := runtime.Start(runtime.WithMinimumReadMemStatsInterval(time.Second))if err != nil &#123;    log.Fatal(err)&#125;\n之后我们启动应用，在 Prometheus 中就可以看到  Go  应用上报的相关指标了。\n\nruntime_uptime_milliseconds_total  Go 的运行时指标\n\nPrometheus 中展示指标的 UI 能力有限，通常我们都是配合 grafana 进行展示的。\n手动上报指标当然除了 SDK 自动上报的指标之外，我们也可以类似于 trace 那样手动上报一些指标；\n比如我就想记录某个函数调用的次数。\nvar meter =  otel.Meter(&quot;test.io/k8s/combat&quot;)  apiCounter, err = meter.Int64Counter(      &quot;api.counter&quot;,      metric.WithDescription(&quot;Number of API calls.&quot;),      metric.WithUnit(&quot;&#123;call&#125;&quot;),  )  if err != nil &#123;      log.Err(err)  &#125;func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) &#123;      defer apiCounter.Add(ctx, 1)      return &amp;pb.HelloReply&#123;Message: fmt.Sprintf(&quot;hostname:%s, in:%s, md:%v&quot;, name, in.Name, md)&#125;, nil  &#125;\n\n只需要创建一个 Int64Counter 类型的指标，然后在需要埋点处调用它的函数 apiCounter.Add(ctx, 1) 即可。\n之后便可以在 Prometheus 中查到这个指标了。\n除此之外 OpenTelemetry 中的 metrics 定义和 Prometheus 也是类似的，还有以下几种类型：\n\nCounter：单调递增计数器，比如可以用来记录订单数、总的请求数。\nUpDownCounter：与 Counter 类似，只不过它可以递减。\nGauge：用于记录随时在变化的值，比如内存使用量、CPU 使用量等。\nHistogram：通常用于记录请求延迟、响应时间等。\n\n在 Java 中也提供有类似的 API 可以完成自定义指标：\nmessageInCounter = meter            .counterBuilder(MESSAGE_IN_COUNTER)            .setUnit(&quot;&#123;message&#125;&quot;)            .setDescription(&quot;The total number of messages received for this topic.&quot;)            .buildObserver();\n\n对于 Gauge 类型的数据用法如下，使用 buildWithCallback 回调函数上报数据，OpenTelemetry 会在框架层面每 30s 回调一次。\npublic static void registerObservers() &#123;          Meter meter = MetricsRegistration.getMeter();                meter.gaugeBuilder(&quot;pulsar_producer_num_msg_send&quot;)                  .setDescription(&quot;The number of messages published in the last interval&quot;)                  .ofLongs()                  .buildWithCallback(                          r -&gt; recordProducerMetrics(r, ProducerStats::getNumMsgsSent));    private static void recordProducerMetrics(ObservableLongMeasurement observableLongMeasurement, Function&lt;ProducerStats, Long&gt; getter) &#123;          for (Producer producer : CollectionHelper.PRODUCER_COLLECTION.list()) &#123;              ProducerStats stats = producer.getStats();              String topic = producer.getTopic();              if (topic.endsWith(RetryMessageUtil.RETRY_GROUP_TOPIC_SUFFIX)) &#123;                  continue;              &#125;        observableLongMeasurement.record(getter.apply(stats),                      Attributes.of(PRODUCER_NAME, producer.getProducerName(), TOPIC, topic));          &#125;&#125;\n更多具体用法可以参考官方文档链接：https://opentelemetry.io/docs/languages/java/instrumentation/#metrics\n如果我们不想将数据通过 collector 而是直接上报到 Prometheus 中，使用 OpenTelemetry 框架也是可以实现的。\n我们只需要配置下环境变量:\nexport OTEL_METRICS_EXPORTER=prometheus\n这样我们就可以访问 http://127.0.0.1:9464/metrics 获取到当前应用暴露出来的指标，此时就可以在 Prometheus 里配置好采集 job 来获取数据。\nscrape_configs:  - job_name: &quot;k8s-combat&quot;    # metrics_path defaults to &#x27;/metrics&#x27;    # scheme defaults to &#x27;http&#x27;.    static_configs:      - targets: [&quot;k8s-combat:9464&quot;]   \n\n这就是典型的 Pull 模型，而 OpenTelemetry 推荐使用的是 Push 模型，数据由 OpenTelemetry 进行采集然后推送到 Prometheus。\n这两种模式各有好处：\n\n\n\n\nPull模型\nPush 模型\n\n\n\n优点\n可以在一个集中的配置里管理所有的抓取端点，也可以为每一个应用单独配置抓取频次等数据。\n在 OpenTelemetry 的 collector中可以集中对指标做预处理之后再将过滤后的数据写入 Prometheus，更加的灵活。\n\n\n缺点\n1. 预处理指标比较麻烦，所有的数据是到了 Prometheus 后再经过relabel处理后再写入存储。2. 需要配置服务发现\n1. 额外需要维护一个类似于 collector 这样的指标网关的组件\n\n\n比如我们是用和 Prometheus 兼容的 VictoriaMetrics 采集了 istio 的相关指标，但里面的指标太多了，我们需要删除掉一部分。\n就需要在采集任务里编写规则：\napiVersion: operator.victoriametrics.com/v1beta1  kind: VMPodScrape  metadata:    name: isito-pod-scrape  spec:    podMetricsEndpoints:      - scheme: http        scrape_interval: &quot;30s&quot;        scrapeTimeout: &quot;30s&quot;        path: /stats/prometheus        metricRelabelConfigs:          - regex: ^envoy_.*|^url\\_\\_\\_\\_.*|istio_request_bytes_sum|istio_request_bytes_count|istio_response_bytes_sum|istio_request_bytes_sum|istio_request_duration_milliseconds_sum|istio_response_bytes_count|istio_request_duration_milliseconds_count|^ostrich_apigateway.*|istio_request_messages_total|istio_response_messages_total            action: drop_metrics    namespaceSelector:      any: true\n\n换成在 collector 中处理后，这些逻辑都可以全部移动到 collector 中集中处理。\n总结metrics 的使用相对于 trace 更简单一些，不需要理解复杂的 context、span 等概念，只需要搞清楚有哪几种 metrics 类型，分别应用在哪些不同的场景即可。\n参考链接：\n\nhttps://prometheus.io/docs/prometheus/latest/feature_flags/#otlp-receiver\nhttps://opentelemetry.io/docs/languages/java/instrumentation/#metrics\nhttps://opentelemetry.io/docs/languages/go/instrumentation/#metrics\n\n","categories":["OB","OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"日志与追踪的完美融合：OpenTelemetry MDC 实践指南","url":"/2024/09/05/ob/OpenTelemetry-client-log-mdc/","content":"前言在前面两篇实战文章中：\n\nOpenTelemetry 实战：从零实现分布式链路追踪\nOpenTelemetry 实战：从零实现应用指标监控\n\n覆盖了可观测中的指标追踪和 metrics 监控，下面理应开始第三部分：日志。\n但在开始日志之前还是要先将链路追踪和日志结合起来看看应用实际使用的实践。\n通常我们排查问题的方式是先查询异常日志，判断是否是当前系统的问题。\n如果不是，则在日志中捞出 trace_id 再到链路查询系统中查询链路，看看具体是哪个系统的问题，然后再做具体的排查。\n类似于这样：日志中会打印 trace_id 和 span_id。\n\n如果日志系统做的比较完善的话，还可以直接点击 trace_id 跳转到链路系统里直接查询链路信息。\n\n\n\nMDC这里的日志里关联 trace 信息的做法有个专有名词：MDC:(Mapped Diagnostic Context)。\n简单来说就是用于排查问题的上下文信息，通常是由键值对组成，类似于这样的数据：\n&#123;    &quot;timestamp&quot; : &quot;2024-08-05 17:27:31.097&quot;,    &quot;level&quot; : &quot;INFO&quot;,    &quot;thread&quot; : &quot;http-nio-9191-exec-1&quot;,    &quot;mdc&quot; : &#123;      &quot;trace_id&quot; : &quot;26242f945af80b044a60226af00211fb&quot;,      &quot;trace_flags&quot; : &quot;01&quot;,      &quot;span_id&quot; : &quot;3a7842b3e28ed5c8&quot;    &#125;,    &quot;logger&quot; : &quot;com.example.demo.DemoApplication&quot;,    &quot;message&quot; : &quot;request: name: \\&quot;1232\\&quot;\\n&quot;,    &quot;context&quot; : &quot;default&quot;  &#125;\n在 Java 中的 Log4j 和 Logback 都有提供对应的实现。\n如果我们使用了 OpenTelemetry 提供的 javaagent 再配合 logback 或者 Log4j 时就会自动具备打印 MDC 的能力：\njava -javaagent:/Users/chenjie/Downloads/blog-img/demo/opentelemetry-javaagent-2.4.0-SNAPSHOT.jar xx.jar\n\n比如我们只需要这样配置这样一个JSON 输出的 logback 即可：\n&lt;appender name=&quot;PROJECT_LOG&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;      &lt;file&gt;$&#123;PATH&#125;/demo.log&lt;/file&gt;        &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.FixedWindowRollingPolicy&quot;&gt;          &lt;fileNamePattern&gt;$&#123;PATH&#125;/demo_%i.log&lt;/fileNamePattern&gt;          &lt;maxIndex&gt;1&lt;/maxIndex&gt;      &lt;/rollingPolicy&gt;        &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt;          &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt;      &lt;/triggeringPolicy&gt;        &lt;layout class=&quot;ch.qos.logback.contrib.json.classic.JsonLayout&quot;&gt;          &lt;jsonFormatter                  class=&quot;ch.qos.logback.contrib.jackson.JacksonJsonFormatter&quot;&gt;              &lt;prettyPrint&gt;true&lt;/prettyPrint&gt;          &lt;/jsonFormatter&gt;          &lt;timestampFormat&gt;yyyy-MM-dd&#x27; &#x27;HH:mm:ss.SSS&lt;/timestampFormat&gt;      &lt;/layout&gt;    &lt;/appender&gt;    &lt;root level=&quot;INFO&quot;&gt;      &lt;appender-ref ref=&quot;STDOUT&quot;/&gt;      &lt;appender-ref ref=&quot;PROJECT_LOG&quot;/&gt;  &lt;/root&gt;\n\n\n就会在日志文件中输出 JSON 格式的日志，并且带上 MDC 的信息。\n自动 MDC 的原理我也比较好奇 OpenTelemetry 是如何自动写入 MDC 信息的，这里以 logback 为例。\n@Override  public ElementMatcher&lt;TypeDescription&gt; typeMatcher() &#123;    return implementsInterface(named(&quot;ch.qos.logback.classic.spi.ILoggingEvent&quot;));  &#125;    @Override  public void transform(TypeTransformer transformer) &#123;    transformer.applyAdviceToMethod(        isMethod()            .and(isPublic())            .and(namedOneOf(&quot;getMDCPropertyMap&quot;, &quot;getMdc&quot;))            .and(takesArguments(0)),        LoggingEventInstrumentation.class.getName() + &quot;$GetMdcAdvice&quot;);  &#125;\n\n会在调用 ch.qos.logback.classic.spi.ILoggingEvent.getMDCPropertyMap()/getMdc() 这两个函数中进行埋点。\n\n这些逻辑都是写在 javaagent 中的。\n\npublic Map&lt;String, String&gt; getMDCPropertyMap() &#123;      // populate mdcPropertyMap if null      if (mdcPropertyMap == null) &#123;          MDCAdapter mdc = MDC.getMDCAdapter();          if (mdc instanceof LogbackMDCAdapter)              mdcPropertyMap = ((LogbackMDCAdapter) mdc).getPropertyMap();          else              mdcPropertyMap = mdc.getCopyOfContextMap();      &#125;        // mdcPropertyMap still null, use emptyMap()      if (mdcPropertyMap == null)          mdcPropertyMap = Collections.emptyMap();        return mdcPropertyMap;  &#125;\n\n这个函数其实默认情况下会返回一个 logback 内置 MDC 的 map 数据（这里的数据我们可以自定义配置）。\n而这里要做的就是将 trace 的上下文信息写入这个 mdcPropertyMap 中。\n以下是 OpenTelemetry agent 中的源码：\nMap&lt;String, String&gt; spanContextData = new HashMap&lt;&gt;();    SpanContext spanContext = Java8BytecodeBridge.spanFromContext(context).getSpanContext();    if (spanContext.isValid()) &#123;    spanContextData.put(traceIdKey(), spanContext.getTraceId());    spanContextData.put(spanIdKey(), spanContext.getSpanId());    spanContextData.put(traceFlagsKey(), spanContext.getTraceFlags().asHex());  &#125;  spanContextData.putAll(ConfiguredResourceAttributesHolder.getResourceAttributes());    if (LogbackSingletons.addBaggage()) &#123;    Baggage baggage = Java8BytecodeBridge.baggageFromContext(context);      // using a lambda here does not play nicely with instrumentation bytecode process    // (Java 6 related errors are observed) so relying on for loop instead  for (Map.Entry&lt;String, BaggageEntry&gt; entry : baggage.asMap().entrySet()) &#123;      spanContextData.put(          // prefix all baggage values to avoid clashes with existing context          &quot;baggage.&quot; + entry.getKey(), entry.getValue().getValue());    &#125;&#125;    if (contextData == null) &#123;    contextData = spanContextData;  &#125; else &#123;    contextData = new UnionMap&lt;&gt;(contextData, spanContextData);  &#125;\n\n这就是核心的写入逻辑，从这个代码中也可以看出直接从上线文中获取的 span 的 context，而我们所需要的 trace_id/span_id  都是存放在 context 中的，只需要 get 出来然后写入进 map 中即可。\n从源码里还得知，只要我们开启 -Dotel.instrumentation.logback-mdc.add-baggage=true 配置还可以将 baggage 中的数据也写入到 MDC 中。\n而得易于 OpenTelemetry 中的 trace 是可以跨线程传输的，所以即便是我们在多线程里打印日志时 MDC 数据依然可以准确无误的传递。\nMDC 的原理public static final String MDC_ATTR_NAME = &quot;mdc&quot;;\n\n\n在 logback 的实现中是会调用刚才的 getMDCPropertyMap() 然后写入到一个 key 为 mdc 的 map 里，最终可以写入到文件或者控制台。\n这样整个原理就可以串起来了。\n自定义日志 数据提到可以自定义 MDC 数据其实也是有使用场景的，比如我们的业务系统经常有类似的需求，需要在日志中打印一些常用业务数据：\n\nuserId、userName\n客户端 IP等信息时\n\n此时我们就可以创建一个 Layout 类来继承 ch.qos.logback.contrib.json.classic.JsonLayout:\npublic class CustomJsonLayout extends JsonLayout &#123;    public CustomJsonLayout() &#123;    &#125;    protected void addCustomDataToJsonMap(Map&lt;String, Object&gt; map, ILoggingEvent event) &#123;        map.put(&quot;user_name&quot;, context.getProperty(&quot;userName&quot;));        map.put(&quot;user_id&quot;, context.getProperty(&quot;userId&quot;));        map.put(&quot;trace_id&quot;, TraceContext.traceId());    &#125;&#125;public class CustomJsonLayoutEncoder extends LayoutWrappingEncoder&lt;ILoggingEvent&gt; &#123;      public CustomJsonLayoutEncoder() &#123;      &#125;      public void start() &#123;          CustomJsonLayout jsonLayout = new CustomJsonLayout();          jsonLayout.setContext(this.context);          jsonLayout.setIncludeContextName(false);          jsonLayout.setAppendLineSeparator(true);          jsonLayout.setJsonFormatter(new JacksonJsonFormatter());          jsonLayout.start();          super.setCharset(StandardCharsets.UTF_8);          super.setLayout(jsonLayout);          super.start();      &#125;&#125;\n\n\n这里的 trace_id 是之前使用 skywalking 的时候由 skywalking 提供的函数：org.apache.skywalking.apm.toolkit.trace.TraceContext#traceId\n\n接着只需要在 logback.xml 中配置这个 CustomJsonLayoutEncoder 就可以按照我们自定义的数据输出日志了：\n&lt;appender name=&quot;PROJECT_LOG&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;      &lt;file&gt;$&#123;PATH&#125;/app.log&lt;/file&gt;        &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.FixedWindowRollingPolicy&quot;&gt;          &lt;fileNamePattern&gt;$&#123;PATH&#125;/app_%i.log&lt;/fileNamePattern&gt;          &lt;maxIndex&gt;1&lt;/maxIndex&gt;      &lt;/rollingPolicy&gt;        &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt;          &lt;maxFileSize&gt;100MB&lt;/maxFileSize&gt;      &lt;/triggeringPolicy&gt;        &lt;encoder class=&quot;xx.CustomJsonLayoutEncoder&quot;/&gt;  &lt;/appender&gt;&lt;root level=&quot;INFO&quot;&gt;      &lt;appender-ref ref=&quot;STDOUT&quot;/&gt;      &lt;appender-ref ref=&quot;PROJECT_LOG&quot;/&gt;  &lt;/root&gt;\n\n虽然这个功能也可以使用日志切面来打印，但还是没有直接在日志中输出更加方便，它可以直接和我们的日志关联在一起，只是多加了这几个字段而已。\nSpring Boot 使用OpenTelemetry 有给 springboot 应用提供一个 spring-boot-starter 包，用于在不使用  javaagent 的情况下也可以自动埋点。\n&lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.opentelemetry.instrumentation&lt;/groupId&gt;    &lt;artifactId&gt;opentelemetry-spring-boot-starter&lt;/artifactId&gt;    &lt;version&gt;OPENTELEMETRY_VERSION&lt;/version&gt;  &lt;/dependency&gt;&lt;/dependencies&gt;\n\n但在早期的版本中还不支持直接打印 MDC 日志：\n\n最新的版本已经支持\n\n即便已经支持默认输出 MDC 后，我们依然可以自定义的内容，比如我们想修改一下 key 的名称，由 trace_id 修改为 otel_trace_id 等。\n&lt;appender name=&quot;OTEL&quot; class=&quot;io.opentelemetry.instrumentation.logback.mdc.v1_0.OpenTelemetryAppender&quot;&gt;  &lt;traceIdKey&gt;otel_trace_id&lt;/traceIdKey&gt;  &lt;spanIdKey&gt;otel_span_id&lt;/spanIdKey&gt;  &lt;traceFlagsKey&gt;otel_trace_flags&lt;/traceFlagsKey&gt;&lt;/appender&gt;\n\n还是和之前类似，修改下 logback.xml 即可。\n他的实现逻辑其实和之前的 auto instrument 中的类似，只不过使用的 API 不同而已。\nauto instrument 是直接拦截代码逻辑修改 map 的返回值，而 OpenTelemetryAppender 是继承了 ch.qos.logback.core.UnsynchronizedAppenderBase 接口，从而获得了重写 MDC 的能力，但本质上都是一样的，没有太大区别。\n不过使用它的前提是我们需要引入以下一个依赖：\n&lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.opentelemetry.instrumentation&lt;/groupId&gt;    &lt;artifactId&gt;opentelemetry-logback-mdc-1.0&lt;/artifactId&gt;    &lt;version&gt;OPENTELEMETRY_VERSION&lt;/version&gt;  &lt;/dependency&gt;&lt;/dependencies&gt;\n\n如果不想修改 logback.yaml ，对于 springboot 来说还有更简单的方案，我们只需要使用以下配置即可自定义 MDC 数据：\nlogging.pattern.level = trace_id=%mdc&#123;trace_id&#125; span_id=%mdc&#123;span_id&#125; trace_flags=%mdc&#123;trace_flags&#125; %5p\n\n这里的 key 也可以自定义，只要占位符没有取错即可。\n\n使用这个的前提是需要加载  javaagent，因为这里的数据是 javaagent 里写进去的。\n\n总结以上就是关于 MDC 在 OpenTelemetry 中的使用，从使用和源码逻辑上都分析了一遍，希望对 MDC 和 OpenTelemetry 的理解更加深刻一些。\n关于 MDC 相关的概念与使用还是很有用的，是日常排查问题必不可少的一个工具。\n","categories":["OB","OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"OpenTelemetry 实战：从 0 到 1 编写一个 Instrumentation","url":"/2024/09/26/ob/OpenTelemetry-create-instrumentation/","content":"背景因为公司内部在使用 PowerJob 作为我们的分布式调度系统，同时又是使用 OpenTelemetry 作为可观测的底座，但目前 OpenTelemetry 还没有对 PowerJob 提供支持，目前社区只对同类型的 XXL-JOB 有支持。\n恰好公司内部也有一些开发同学有类似的需求：\n于是在这个背景下我便开始着手开发 PowerJob 的 instrumentation，最终的效果如下：\n\n从这个链路图中可以看到 grpc-consumer 提供了调度的入口函数，然后在内部发送了 Pulsar 消息，最终又调用了 grpc-provider 的 gRPC 接口。\n这样就可以把整个链路串起来，同时还能查看 PowerJob 调度的 JobId、以及调用参数等数据，这样排查问题时也更加直观。\n开发 Instrumentation 的前置知识在正式开发 Instrumentation 之前还需要了解一些前置知识点。\n\n这里我们以现有的  gRPC 和我编写的 PowerJob instrumentation 为例，可以看到 gRPC 的 instrumentation 中多了一个 library 的模块。\n这里就引申出了两种埋点方式：\n\nLibrary instrumentation\nJava agent instrumentation\n\n通常我们对一个框架或者一个库进行埋点时，首先需要找到它的埋点入口。\n以 grpc 为例，我们首先需要看他是否有提供扩展的 API 可以供我们埋点，恰好 grpc 是有提供客户端和服务端的拦截器的。\nio.grpc.ClientInterceptorio.grpc.ServerInterceptor\n\n我们便可以在这些拦截中加入埋点逻辑，比如客户端的埋点代码如下 io.opentelemetry.instrumentation.grpc.v1_6.TracingClientInterceptor ：\n\n这部分代码便是写在 grpc-1.6/library 模块下的。\n这样做有一个好处是：当我们的业务代码不想使用 javaagent 时还可以手动引入 grpc-1.6/library 包，然后使用 TracingClientInterceptor 拦截器也可以实现 trace 埋点的功能。\nimplementation(project(&quot;:instrumentation:grpc-1.6:library&quot;))\n之后 javaagent 这个模块也会引入 library ，然后直接使用它提供的 API 实现 agent 级别的埋点。\n而如果一些库或者中间件并没有提供这种扩展 API 时，我们就只能使用 agent 的方式在字节码层面上进行埋点，这样就不会限制框架了，理论上任何 Java 代码都可以埋点。\n所以总的来说一个库可能会没有 library instrumentation，但一定会有 agent instrumentation，我们可以根据当前框架的代码进行选择。\n\n而这里的 PowerJob 因为并没有提供扩展接口，所有只有 agent 的 instrumentation。\n\n找到埋点入口在开始编码之前我们需要对要埋点的库或者框架有一个清晰的理解，至少得知道它的核心逻辑在哪里。\n以 PowerJob 的调度执行逻辑为例：\npublic class TestBasicProcessor implements BasicProcessor &#123;    @Override    public ProcessResult process(TaskContext context) throws Exception &#123;        System.out.println(&quot;======== BasicProcessor#process ========&quot;);        System.out.println(&quot;TaskContext: &quot; + JsonUtils.toJSONString(context) + &quot;;time = &quot; + System.currentTimeMillis());        return new ProcessResult(true, System.currentTimeMillis() + &quot;success&quot;);    &#125;&#125;\n\n\n这是一个最简单的调度执行器的实现逻辑。\n\n从这里看出：如果我们想要在执行器中埋点，那最核心的就是这里的 process 函数。\n需要在 process 的执行前后拿到 context 数据，写入到 OpenTelemetry 中的 span 即可。\n\npublic class SimpleCustomizedHandler extends IJobHandler &#123;      @Override    public ReturnT&lt;String&gt; execute(String s) throws Exception &#123;      return new ReturnT&lt;&gt;(&quot;Hello World&quot;);    &#125;&#125;\n而在 xxl-job 中，它的核心逻辑就是这里的 execute 函数。\n选择合适的版本找到核心的埋点逻辑后还有一个很重要的工作要做：那就是选择你需要支持的版本。\n选择版本的原因是有可能框架或库在版本迭代过程中核心 API 发生了变化，比如：\n\n函数签名发生了改变\n包名也发生了改变\n\n以 xxl-job 为例，它在迭代过程中就发生了几次函数签名的修改，所以我们需要针对不同的版本做兼容处理：\n\n而我这里选择支持 PowerJob:4.0+ 的版本，因为社区在 4.0 之后做了大量重构，导致修改了包名，同时核心逻辑的函数签名也没发生过变化。\n\n\n4.0 之前的版本我就没做兼容了，感兴趣的朋友可以自行实现。\n\n逻辑实现首先第一步需要创建一个 InstrumentationModule:\n@AutoService(InstrumentationModule.class)  public class PowerJobInstrumentationModule extends InstrumentationModule &#123;    public PowerJobInstrumentationModule() &#123;      super(&quot;powerjob&quot;, &quot;powerjob-4.0&quot;);    &#125;    @Override    public List&lt;TypeInstrumentation&gt; typeInstrumentations() &#123;      return asList(new BasicProcessorInstrumentation());    &#125;&#125;\n\n\n\n这里的 @AutoService 注解，会在代码编译之后生成一份 SPI 文件。\n\n之后便是实现这里最核心的 BasicProcessorInstrumentation。\npublic class BasicProcessorInstrumentation implements TypeInstrumentation &#123;    @Override    public ElementMatcher&lt;TypeDescription&gt; typeMatcher() &#123;      return implementsInterface(named(&quot;tech.powerjob.worker.core.processor.sdk.BasicProcessor&quot;));    &#125;    @Override    public void transform(TypeTransformer transformer) &#123;      transformer.applyAdviceToMethod(          named(&quot;process&quot;).and(isPublic()).and(takesArguments(1)),          BasicProcessorInstrumentation.class.getName() + &quot;$ProcessAdvice&quot;);    &#125;&#125;  \n\n从它的代码也可以看出，这里主要是指定我们需要对哪个方法的哪个函数进行埋点，然后埋点之后的处理逻辑是在哪个类(ProcessAdvice)中实现的。\n之后便是 ProcessAdvice 的实现：\npublic static class ProcessAdvice &#123;      @SuppressWarnings(&quot;unused&quot;)    @Advice.OnMethodEnter(suppress = Throwable.class)    public static void onSchedule(        @Advice.This BasicProcessor handler,        @Advice.Argument(0) TaskContext taskContext,        @Advice.Local(&quot;otelRequest&quot;) PowerJobProcessRequest request,        @Advice.Local(&quot;otelContext&quot;) Context context,        @Advice.Local(&quot;otelScope&quot;) Scope scope) &#123;      Context parentContext = currentContext();      request = PowerJobProcessRequest.createRequest(taskContext.getJobId(), handler, &quot;process&quot;);      request.setInstanceParams(taskContext.getInstanceParams());      request.setJobParams(taskContext.getJobParams());      context = helper().startSpan(parentContext, request);      if (context == null) &#123;        return;      &#125;    scope = context.makeCurrent();    &#125;      @SuppressWarnings(&quot;unused&quot;)    @Advice.OnMethodExit(onThrowable = Throwable.class, suppress = Throwable.class)    public static void stopSpan(        @Advice.Return ProcessResult result,        @Advice.Thrown Throwable throwable,        @Advice.Local(&quot;otelRequest&quot;) PowerJobProcessRequest request,        @Advice.Local(&quot;otelContext&quot;) Context context,        @Advice.Local(&quot;otelScope&quot;) Scope scope) &#123;      helper().stopSpan(result, request, throwable, scope, context);    &#125;&#125;\n\n这里最主要的就是使用 OpenTelemetry 提供 SDK 在入口处调用 startSpan 开始一个 span，然后在函数退出时调用 stopSpan 函数。\n同时在执行前将一些请求信息存起来：\nrequest = PowerJobProcessRequest.createRequest(taskContext.getJobId(), handler, &quot;process&quot;);\n\n这样可以根据这些请求信息生成 span 的 attribute，也就是 jobId, jobParam 等数据。\nclass PowerJobExperimentalAttributeExtractor      implements AttributesExtractor&lt;PowerJobProcessRequest, Void&gt; &#123;      @Override    public void onStart(        AttributesBuilder attributes,        Context parentContext,        PowerJobProcessRequest powerJobProcessRequest) &#123;      attributes.put(POWERJOB_JOB_ID, powerJobProcessRequest.getJobId());      attributes.put(POWERJOB_JOB_PARAM, powerJobProcessRequest.getJobParams());      attributes.put(POWERJOB_JOB_INSTANCE_PARAM, powerJobProcessRequest.getInstanceParams());      attributes.put(POWERJOB_JOB_INSTANCE_TRPE, powerJobProcessRequest.getJobType());    &#125;\n\n比如这里的 jobId&#x2F; jobParams 数据都是从刚才写入的 PowerJobProcessRequest 中获取的。\nif (CAPTURE_EXPERIMENTAL_SPAN_ATTRIBUTES) &#123;    builder.addAttributesExtractor(        AttributesExtractor.constant(AttributeKey.stringKey(&quot;job.system&quot;), &quot;powerjob&quot;));    builder.addAttributesExtractor(new PowerJobExperimentalAttributeExtractor());  &#125;\n\n同时只需要将刚才的 PowerJobExperimentalAttributeExtractor 在初始化 Instrumenter 时进行配置，这样 OpenTelemetry 的 SDK 就会自动回调这个接口，从而获取到 Span 的 attribute。\n\nimport static net.bytebuddy.matcher.ElementMatchers.isPublic;  import static net.bytebuddy.matcher.ElementMatchers.named;  import static net.bytebuddy.matcher.ElementMatchers.takesArguments;import net.bytebuddy.asm.Advice;\n\n\n其实这里大部分的 API 都是 bytebuddy 提供的。\n\n不知道大家是否觉得眼熟，Instrumentation 的写法其实和 spring 的拦截器有异曲同工之妙：\nimport org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.ProceedingJoinPoint;@Aspectpublic class AroundExample &#123;\t@Around(&quot;execution(* com.xyz..service.*.*(..))&quot;)\tpublic Object doBasicProfiling(ProceedingJoinPoint pjp) throws Throwable &#123;\t\t// start stopwatch\t\tObject retVal = pjp.proceed();\t\t// stop stopwatch\t\treturn retVal;\t&#125;&#125;\n\n\n毕竟 Spring 的拦截器也是使用 bytebuddy 实现的。\n\n一些坑其实整个埋点过程非常简单，我们可以参考一些现有的 instrumentation 就可以很快实现逻辑；真正麻烦的时候在提交 PR 时需要通过 CI 校验。\n\n\n我这里大概提交了 8次才把  CI 全部跑通过。\n\n这里面有各种小坑，只有自己提交过才能感受得到，下面我就一一列举一些大家可能会碰到的问题。\n创建模块首先第一个是创建模块的时候记得使用 kotlin 作为 gradle 的 DSL。\n\nIDEA 这里默认选择的是 Groovy 作为 DSL；我当时没有注意，后面在项目构建过程中一直在报错，仔细核对后发现是 DSL 的问题，修改之后就能编译通过了。\n项目构建第二个是 module 的命名规则。\n\n我们需要遵守 v4_0_0 的规则，同时还得与 PowerJobInstrumentationModule 中定义的名称相同：\npublic PowerJobInstrumentationModule() &#123;    super(&quot;powerjob&quot;, &quot;powerjob-4.0&quot;);  &#125;\n\n比如如果我们的包名称是 powerjob.v1.1.0 ，那这里的名称也得是 &quot;powerjob-1.1.0&quot;\nMuzzle第三个是 Muzzle 校验，Muzzle 是为了保证 javaagent 在业务代码中使用时和运行时的依赖不发生冲突而定义的一个校验规则。\nmuzzle &#123;    pass &#123;      group.set(&quot;tech.powerjob&quot;)      module.set(&quot;powerjob-worker&quot;)      versions.set(&quot;[4.0.0,)&quot;)      assertInverse.set(true)      extraDependency(&quot;tech.powerjob:powerjob-official-processors:1.1.0&quot;)    &#125;  &#125;\n\n以我这个为例，它的含义是兼容 tech.powerjob:powerjob-worker:4.0.0+以上的版本。\nassertInverse.set(true): 的作用是与之相反的版本，也就是 4.0.0 以下的版本都不做支持，如果在这些版本中运行 javaagent 是不会生效的。\n\n因为这些低版本的 powerjob 不兼容我们的埋点代码。\n\nextraDependency：的作用是额外需要依赖的包，我这里额外使用了这个包里的一些类，如果不加上的话在做 Muzzle 校验时也会失败。\n单元测试最后便是单元测试了：\n@Testvoid testBasicProcessor() throws Exception &#123;  long jobId = 1;  String jobParam = &quot;abc&quot;;  TaskContext taskContext = genTaskContext(jobId, jobParam);  BasicProcessor testBasicProcessor = new TestBasicProcessor();  testBasicProcessor.process(taskContext);  testing.waitAndAssertTraces(      trace -&gt; &#123;        trace.hasSpansSatisfyingExactly(            span -&gt; &#123;              span.hasName(String.format(&quot;%s.process&quot;, TestBasicProcessor.class.getSimpleName()));              span.hasKind(SpanKind.INTERNAL);              span.hasStatus(StatusData.unset());              span.hasAttributesSatisfying(                  attributeAssertions(                      TestBasicProcessor.class.getName(), jobId, jobParam, BASIC_PROCESSOR));            &#125;);      &#125;);&#125;private static List&lt;AttributeAssertion&gt; attributeAssertions(    String codeNamespace, long jobId, String jobParam, String jobType) &#123;  List&lt;AttributeAssertion&gt; attributeAssertions =      new ArrayList&lt;&gt;(          asList(              equalTo(AttributeKey.stringKey(&quot;code.namespace&quot;), codeNamespace),              equalTo(AttributeKey.stringKey(&quot;code.function&quot;), &quot;process&quot;),              equalTo(AttributeKey.stringKey(&quot;job.system&quot;), &quot;powerjob&quot;),              equalTo(AttributeKey.longKey(&quot;scheduling.powerjob.job.id&quot;), jobId),              equalTo(AttributeKey.stringKey(&quot;scheduling.powerjob.job.type&quot;), jobType)));  if (!StringUtils.isNullOrEmpty(jobParam)) &#123;    attributeAssertions.add(        equalTo(AttributeKey.stringKey(&quot;scheduling.powerjob.job.param&quot;), jobParam));  &#125;  return attributeAssertions;&#125;\n\n测试的逻辑很简单，就是模拟一下核心逻辑的调用，然后断言是否存在我们预期的 Span，同时还得校验它的 attribute 是否符合我们的预期。\n这个单测当时也调了许久，因为 versions.set(&quot;[4.0.0,)&quot;) 这个配置，有一个 CI workflow 会校验最新版本的 powerjob 是否也能正常运行。\n\n比如它会拉取目前最新的依赖进行测试：\nimplementation(&quot;tech.powerjob:powerjob-worker:5.1.0&quot;)\n\n如果我们在单测中依赖了某些版本不存在的类，或者是函数签名发生过变化的函数用于测试，那这个 CI 就会执行失败。\n\n因为这里的构建日志非常多，同时还是并发测试的，如果我们想直接查看日志来定位问题会非常麻烦。\n当然社区也考虑到了，可以在 “Build scan” 这个步骤中查看 gradle 的构建日志。\n\n这里会直接输出具体是哪里构建出了问题，通过它我们就能很快定位到原因。\n我这里也是因为使用的某些帮助函数在最新的版本中发生了变化，为了测试通过，就不得不调整测试代码了。\n如果你发现必须得依赖这些类或者函数来配合测试，那就只有考虑分为多个不同的版本进行测试，类似于 xxl-job：\n\n总结以上就是整个 instrumentation 的编写过程，其中核心的埋点过程并不复杂，只要我们对需要埋点的库或框架比较熟悉，都可以实现埋点。\n真正麻烦的是需要通过社区复杂且严谨的 CI 流程，好在不管是哪一步的 CI 失败都可以查到具体的原因，有点类似于升级打怪，跟着错误信息走，最终都能验证通过。\n参考链接：\n\nhttps://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/CONTRIBUTING.md\nhttps://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/docs/contributing/writing-instrumentation.md\n\n","categories":["OB","OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"OpenTelemetry 深度定制：跨服务追踪的实战技巧","url":"/2024/06/26/ob/OpenTelemetry-custom-instrument/","content":"背景\n在上一篇《从 Dapper 到 OpenTelemetry：分布式追踪的演进之旅》中在最后提到在做一些 Trace 的定制开发。\n到现在差不多算是完成了，可以和大家分享一下。\n我们的需求是这样的：\n假设现在有三个服务：ServiceA、ServiceB、ServiceC\n\n\nServiceA 对外提供了一个 http 接口 request，在这个接口会调用 ServiceB 的 order 订单接口创建订单，同时 serviceB 调用 serviceC 的 pay 接口。\n整个调用关系如上图所示。\n默认情况下 span 中的 attribute 会记录当前 span 的一些信息，比如：这些都是当前一些当前 span 内置的信息，比如当前 gRPC 接口的一些基本数据：服务名、ip、端口等信息。\n但这里并没有上游的一些信息，虽然我们可以通过 Jaeger 的树状图得知上游是哪个应用调用过来的，但是一旦某个 span 下有多个子 span 的调用，就没办法很直观知道这个子 span 的上游是由谁发起的调用。\n比如如下这个链路：当一个调用链非常长，同时也非常复杂时，没办法第一时间知道某一个 span 的上游到底是谁发起的，需要手动一层层的去折叠，或者全靠眼睛去找。\n预期效果\n为此我们希望的效果是可以通过给每一个子 span 中加入两个 attribute，来标明它的父调用来源。\n比如在 serviceB 中的所有 span 中都会加上两个标签：来源是 serviceA，同时是 serviceA 的 request 接口发起的请求。\n而在 serviceC 中同样可以知道来源是 serviceB 的 Order 接口发起的调用。\n我启动了三个 demo 应用，分别是 create1，create2，create3.\ncreate1 中会提供一个 request 接口，在这里面调用 create2 的 create2 接口，create2 的接口里接着调用 create3 的 create3 接口。\ncreate1：\n@RequestMapping(&quot;/request&quot;)  public String request(@RequestParam String name) &#123;     HelloRequest request = HelloRequest.newBuilder()           .setName(name)           .build();     log.info(&quot;request: &#123;&#125;&quot;, request);     String message = myServiceStub.create2(request).getMessage();     Executors.newFixedThreadPool(1).execute(() -&gt; &#123;        myServiceStub.create2(request).getMessage();     &#125;);       return message;  &#125;\n\ncreate2:\n@Override  public void create2(HelloRequest request, StreamObserver&lt;HelloReply&gt; responseObserver) &#123;      HelloReply reply = HelloReply.newBuilder()              .setMessage(&quot;Create2 ==&gt; &quot; + request.getName())              .build();      log.info(&quot;Create2: &#123;&#125;&quot;, reply.getMessage());      myMethod(request.getName());      myServiceStub.create3(request);    responseObserver.onNext(reply);      responseObserver.onCompleted();  &#125;\n\ncreate3:\n@Override  public void create3(HelloRequest request, StreamObserver&lt;HelloReply&gt; responseObserver) &#123;      HelloReply reply = HelloReply.newBuilder()              .setMessage(&quot;Create3 ==&gt; &quot; + request.getName())              .build();      log.info(&quot;Create3: &#123;&#125;&quot;, reply.getMessage());      myMethod(request.getName());      responseObserver.onNext(reply);      responseObserver.onCompleted();  &#125;\n\njava -javaagent:opentelemetry-javaagent-2.4.0-SNAPSHOT.jar \\-Dotel.javaagent.extensions=otel-extensions-custom-context-1.0-SNAPSHOT.jar \\-Dotel.traces.exporter=otlp \\-Dotel.logs.exporter=none \\-Dotel.service.name=create2 \\-Dotel.exporter.otlp.protocol=grpc \\-Dotel.propagators=tracecontext,baggage,demo \\-Dotel.exporter.otlp.endpoint=http://127.0.0.1:5317 \\      -jar target/demo-0.0.1-SNAPSHOT.jar --spring.application.name=create2 --server.port=9191 --grpc.server.port=9292 --grpc.client.myService.address=static://127.0.0.1:9393\n只是每个应用都需要使用我这边单独打的 agent 包以及一个 extension(tel-extensions-custom-context-1.0-SNAPSHOT.jar) 才能生效。\n最终的效果如下：\nBaggage在讲具体的实现之前需要先了解几个 Trace 中的概念，在这里主要用到的是一个称为 Baggage 的对象。\n在之前的文章中其实提到过它的原理以及使用场景：从 Dapper 到 OpenTelemetry：分布式追踪的演进之旅\n\nBaggage 的中文翻译是：包裹📦；简单来说就是我们可以通过自定义 baggage 可以将我们想要的数据存放在其中，这样再整个 Trace 的任意一个 Span 中都可以读取到。\n@RequestMapping(&quot;/request&quot;)  public String request(@RequestParam String name) &#123;  \t// 写入    Baggage.current().toBuilder().            put(&quot;request.name&quot;, name).build()            .storeInContext(Context.current()).makeCurrent();&#125;         // 获取String value = Baggage.current().getEntryValue(&quot;request.name&quot;);  log.info(&quot;request.name: &#123;&#125;&quot;, value);\n\n理解了这个之后，我们要实现的将上游的信息传递到下游就可以通过这个组件实现了。\n只需要在上游创建 span 时将它自身数据写入到 Baggage 中，再到下游 span 取出来写入到 attribute 中即可。\nContextCustomizer这里的关键就是在哪里写入这个 Baggage，因为对第三方组件的 Instrumentation 的实现都是在 opentelemetry-java-instrumentation项目中。\n\njavaagent.jar 包也是通过该项目打包出来的。\n\n所以在该项目的 io.opentelemetry.instrumentation.api.instrumenter.Instrumenter#doStart 这个函数中我们发现一段逻辑：\n\n\n\n这个函数是在创建一个 span 的时候调用的，通常这个创建函数是在这些第三方库的拦截器中创建的。\n\n比如这是在 grpc 的拦截器中调用。\n// context customizers run before span start, so that they can have access to the parent span  // context, and so that their additions to the context will be visible to span processors  for (ContextCustomizer&lt;? super REQUEST&gt; contextCustomizer : contextCustomizers) &#123;    context = contextCustomizer.onStart(context, request, attributes);  &#125;\n\nContextCustomizer 是一个接口只提供了一个函数：\npublic interface ContextCustomizer&lt;REQUEST&gt; &#123;      /** Allows to customize the operation &#123;@link Context&#125;. */    Context onStart(Context parentContext, REQUEST request, Attributes startAttributes);  &#125;\n\n\nContext 是上下文信息，可以在自定义的 ContextCustomizer 继续往上下文中追加信息。\nREQUEST 是一个泛型：一般是当前第三方组件的请求信息：\n比如是 HTTP 时，这个 request 就是 HTTP 的请求信息。\n而如果是 gRPC ，则是 gRPC 的请求信息。\n其他的请求类型同理。\n\n\nstartAttributes 则是预先写入的一些属性，比如在上图中看到的一些 rpc.service/rpc.method等字段。\n\n// context customizers run before span start, so that they can have access to the parent span  // context, and so that their additions to the context will be visible to span processors\n\n从这个接口的调用注释可以看出：这个自定义的 context 会在 span 开始之前调用，所以在这里是可以访问到当前创建的 span 的父 context，同时在这里的 context 中新增的数据可以在 SpanProcessor 访问到。\nSpanProcessor而 SpanProcessor 又是一个非常的重要的组件，我们接着刚才的 contextCustomizer 处往后跟踪代码。\ncontext = contextCustomizer.onStart(context, request, attributes);\t---&gt;Span span = spanBuilder.setParent(context).startSpan();\t\t\t---&gt;io.opentelemetry.sdk.trace.SdkSpanBuilder#startSpan\t\t\t\t---&gt;io.opentelemetry.sdk.trace.SdkSpan#startSpan\t\t\t\t\t---&gt;spanProcessor.onStart(parentContext, span);\n\n可以看到 spanProcessor.onStart 这个函数会在 contextCustomizer 之后调用。\n\n/**   * SpanProcessor is the interface &#123;@link SdkTracer&#125; uses to allow synchronous hooks for when a   * &#123;@code Span&#125; is started or when a &#123;@code Span&#125; is ended.   */ //==========================================================/**   * Called when a &#123;@link io.opentelemetry.api.trace.Span&#125; is started, if the &#123;@link   * Span#isRecording()&#125; returns true.   * * &lt;p&gt;This method is called synchronously on the execution thread, should not throw or block the   * execution thread. * * @param parentContext the parent &#123;@code Context&#125; of the span that just started.   * @param span the &#123;@code Span&#125; that just started.   */void onStart(Context parentContext, ReadWriteSpan span);\n\n从注释中可以知道 SpanProcessor 是作为一个 span 的生命周期中的关键节点的 hook 函数。\n在这些函数中我们可以自定义一些 span 的数据，比如在 onStart 还可以往 span 中写入一些自定义的 attribute。\n这也是我们这次会用到的一个接口，我们的方案是：\n在 gRPC 构建 Instrument 时自定义一个 GrpcServerContextCustomizer ，在这个自定义的 ContextCustomizer 中写入一个 Baggage。\n然后在 io.opentelemetry.sdk.trace.SpanProcessor#onStart 接口中取出这个 Baggage 写入到当前 span 的 attribute 中。\n这样我们就可以看到之前提到的那些数据上游信息了。\n为 gRPC 添加上下文先来看看如何为 gRPC 添加 Baggage：\n我们先自定义一个 GrpcServerContextCustomizer 实现类：\npublic class GrpcServerContextCustomizer implements ContextCustomizer&lt;GrpcRequest&gt; &#123;    private final String currentServiceName;      private static final String PARENT_RPC_KEY = &quot;parent_rpc&quot;;    private static final String CURRENT_RPC_KEY = &quot;current_rpc&quot;;      private static final String CURRENT_HTTP_URL_PATH = &quot;current_http_url_path&quot;;      public GrpcServerContextCustomizer(String serviceName) &#123;      this.currentServiceName = serviceName;    &#125;    @Override    public Context onStart(Context parentContext, GrpcRequest grpcRequest,        Attributes startAttributeds) &#123;      BaggageBuilder builder = Baggage.fromContext(parentContext).toBuilder();        String currentRpc = Baggage.fromContext(parentContext).getEntryValue(CURRENT_RPC_KEY);      String fullMethodName = startAttributeds.get(AttributeKey.stringKey(&quot;rpc.method&quot;));      String rpcService = startAttributeds.get(AttributeKey.stringKey(&quot;rpc.service&quot;));      // call from grpc      String method = rpcService + &quot;:&quot; + fullMethodName;      String baggageInfo = getBaggageInfo(currentServiceName, method);        String httpUrlPath = Baggage.fromContext(parentContext).getEntryValue(CURRENT_HTTP_URL_PATH);      if (!StringUtils.isNullOrEmpty(httpUrlPath)) &#123;        // call from http        // currentRpc = currentRpc;  currentRpc = create1|GET:/request      // clear current_http_url_path      builder.put(CURRENT_HTTP_URL_PATH, &quot;&quot;);      &#125;      Baggage baggage = builder          .put(PARENT_RPC_KEY, currentRpc)          .put(CURRENT_RPC_KEY, baggageInfo)          .build();      return parentContext.with(baggage);      &#125;    private static String getBaggageInfo(String serviceName, String method) &#123;      if (StringUtils.isNullOrEmpty(serviceName)) &#123;        return &quot;&quot;;      &#125;    return serviceName + &quot;|&quot; + method;    &#125;  &#125;\n\n从这个代码中可以看出，我们需要先从上下文中获取 CURRENT_RPC_KEY ，从而得知当前的 span 是不是 root span。\n所以我们其实是把当前的 span 信息作为一个 PARENT_RPC_KEY 写入到 Baggage 中。\n这样在 SpanProcessor 中便可以直接取出 PARENT_RPC_KEY 作为上游的信息写入 span 的 attribute 中。\n@Override  public void onStart(Context parentContext, ReadWriteSpan span) &#123;    String parentRpc = Baggage.fromContext(parentContext).getEntryValue(&quot;parent_rpc&quot;);      if (!StringUtils.isNullOrEmpty(parentRpc)) &#123;          String[] split = parentRpc.split(&quot;\\\\|&quot;);          span.setAttribute(&quot;parent_rpc&quot;, parentRpc);          span.setAttribute(&quot;parent_service_name&quot;, split[0]);          span.setAttribute(&quot;parent_service_method&quot;, split[1]);     &#125;  &#125;\n\n\n需要注意的是，这里的 Baggage 需要使用 Baggage.fromContext(parentContext) 才能拿到刚才写入 Baggage 信息。\n\n之后我们找到构建 gRPCServerInstrumenterBuilder 的地方，写入我们刚才自定义的 GrpcServerContextCustomizer 即可。\n\n.addContextCustomizer(new GrpcServerContextCustomizer(serviceName))\n\n这里我们选择写入到是 serverInstrumenterBuilder 而不是clientInstrumenterBuilder，因为在服务端的入口就知道是从哪个接口进来的请求。\n为 spring boot 的 http 接口添加上下文如果只存在 gRPC 调用时只添加 gRPC 的上下文也够用了，但是我们也不排除由外部接口是通过 HTTP 访问进来的，然后再调用内部的 gRPC 接口；这也是非常常见的架构模式。\n所以我们还需要在 HTTP 中增加 ContextCustomizer 将自身的数据写入到 Baggage 中。\n好在 HttpServerRouteBuilder 自身是实现了 ContextCustomizer 接口的，我们只需要往里面写入 Baggage 数据即可。\npublic ContextCustomizer&lt;REQUEST&gt; build() &#123;    Set&lt;String&gt; knownMethods = new HashSet&lt;&gt;(this.knownMethods);    return (context, request, startAttributes) -&gt; &#123;      if (HttpRouteState.fromContextOrNull(context) != null) &#123;        return context;      &#125;    String method = getter.getHttpRequestMethod(request);      if (method == null || !knownMethods.contains(method)) &#123;        method = &quot;HTTP&quot;;      &#125;    String urlPath = getter.getUrlPath(request);      String methodPath = method + &quot;:&quot; + urlPath;        String currentRpc = Baggage.fromContext(context).getEntryValue(CURRENT_RPC_KEY);      String baggageInfo = getBaggageInfo(serviceName, methodPath);      Baggage baggage = Baggage.fromContext(context).toBuilder()          .put(PARENT_RPC_KEY, currentRpc)          .put(CURRENT_RPC_KEY, baggageInfo)          .put(CURRENT_HTTP_URL_PATH, methodPath)          .build();       return context.with(HttpRouteState.create(method, null, 0))          .with(baggage);    &#125;;&#125;\n\n这里新增了 CURRENT_HTTP_URL_PATH 用于标记当前的请求来源是 HTTP，在 grpc 的 ContextCustomizer 解析时会判断这个值是否为空。\nString httpUrlPath = Baggage.fromContext(parentContext).getEntryValue(CURRENT_HTTP_URL_PATH);  if (!StringUtils.isNullOrEmpty(httpUrlPath)) &#123;    // call from http    // currentRpc = currentRpc;  currentRpc = create1|GET:/request  // clear current_http_url_path  builder.put(CURRENT_HTTP_URL_PATH, &quot;&quot;);  &#125;\n\n\n\n这样就可以在 grpc 的下游接口拿到入口的 HTTP 接口数据了。\n\n当然也有可能是在 grpc 接口中调用 HTTP 的接口的场景，只是我们的业务中没有这种情况，所以就没有适配这类的场景。\n总结ContextCustomizer 接口没有提供对应的扩展，但是 SpanProcessor 是提供了扩展接口的。\n\n原本是想尽量别维护自己的 javaagent，但也好在 OpenTelemetry 是提供的接口，所以也并不会去修改原本的代码。\n\n所以我们还是需要创建一个 extensions 的项目在实现 SpanProcessor，这个在之前的 《实战：如何编写一个 OpenTelemetry Extensions》有详细讲到。\n所以最后的应用启动方式如下：\njava -javaagent:opentelemetry-javaagent-2.4.0-SNAPSHOT.jar \\-Dotel.javaagent.extensions=otel-extensions-custom-context-1.0-SNAPSHOT.jar \\\n\n需要使用我们手动打包的 javaagent 以及一个自定义扩展包。\n打包方式：\n./gradlew assemble\n\n\nopentelemetry-java-instrumentation 项目比较大，所以打包过程可能比较久。\n\n因为这其实是一些定制需求，所以就没提交到上游，感兴趣的可以自行合并代码测试。\n最后可以这个分支中查看到修改的部分：https://github.com/crossoverJie/opentelemetry-java-instrumentation/compare/main...add-grpc-context\n","categories":["OB"],"tags":["OpenTelemetry"]},{"title":"OpenTelemetry 实践指南：历史、架构与基本概念","url":"/2024/05/21/ob/OpenTelemetry-getstart/","content":"背景之前陆续写过一些和 OpenTelemetry 相关的文章：\n\n实战：如何优雅的从 Skywalking 切换到 OpenTelemetry\n实战：如何编写一个 OpenTelemetry Extensions\n从一个 JDK21+OpenTelemetry 不兼容的问题讲起\n\n这些内容的前提是最好有一些 OpenTelemetry 的背景知识，看起来就不会那么枯燥，为此这篇文章就来做一个入门科普，方便一些对 OpenTelemetry 不是那么熟的朋友快速掌握一些 OpenTelemetry 的基本概念。\n\n\n\n历史发展早在 OpenTelemetry 诞生之前可观测性这个概念就一直存在了，我记得我最早接触到这个概念是在 16 年当时的公司所使用的一个产品：pinpoint\n\n现如今这个项目依然比较活跃。\n\n依然还记得当时通过它可以直接看到项目调用的拓扑图，在时间坐标上框出高延迟的点就能列出这些请求，同时还能查看此时的运行日志。\n这样强大的功能对于一个刚工作一年的小白来说冲击力实属太大了一点。\n后来才了解到 pinpoint 属于 APM 这类产品，类似的产品还有：\n\nApache SkyWalking\n美团的 CAT 等\n\n他们都是可以用于性能分析和链路追踪的产品，到后来公司的运维层面也接入过 Zabbix、open-falcon 之类的产品：\n17之后全面切换到 spring boot 时，也用过社区提供的 spring-boot-admin 项目：\n这就是一个简单的可以监控 spring boot 应用的产品，用于展示 JVM 指标，或者自己也可以定义一些健康指标。\n\n再之后进入云原生体系后可观测性的技术栈稍有变化。\n\n日志使用 Sidecar 代理的方式通过 Agent 将数据写入 ElasticSearch 中。具体日志采集方式可以参考之前的文章：\n\n在 kubernetes 环境下如何采集日志\n\n而链路追踪则是使用的 skywalking，在 trace 这个领域 skywalking 还是非常受大家喜爱的。\n不过最近也从 skywalking 切换到了我们本文所讲到的 OpenTelemetry，具体可以看之前的文章：\n\n实战：如何优雅的从 Skywalking 切换到 OpenTelemetry\n\n指标采集使用的是自然也是 Prometheus 的那一套技术栈，只是 Prometheus 换为了与它完全兼容的 VictoriaMetric 目前是为了更省资源。\n客户端使用则是直接使用 Prometheus 的库进行指标暴露：\n&lt;dependency&gt;    &lt;groupId&gt;io.prometheus&lt;/groupId&gt;    &lt;artifactId&gt;prometheus-metrics-core&lt;/artifactId&gt;    &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;io.prometheus&lt;/groupId&gt;    &lt;artifactId&gt;prometheus-metrics-instrumentation-jvm&lt;/artifactId&gt;    &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;io.prometheus&lt;/groupId&gt;    &lt;artifactId&gt;prometheus-metrics-exporter-httpserver&lt;/artifactId&gt;    &lt;version&gt;1.0.0&lt;/version&gt;&lt;/dependency&gt;\n\n最终通过配置抓取策略，由 VictoriaMetrics 的 scrape 程序来抓取指标最终写入到它自己的存储中：\napiVersion: operator.victoriametrics.com/v1beta1  kind: VMPodScrape  metadata:    name: kubernetes-pod-scrape    namespace: monitoring  spec:    podMetricsEndpoints:      - scheme: http        scrape_interval: &quot;30s&quot;        path: /metrics        relabelConfigs:          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]            separator: ;            regex: &quot;true&quot;            replacement: $1            action: keep          # 端口相同          - action: keep_if_equal            source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_container_port_number ]          # 过滤INIT容器          - action: drop            source_labels: [ __meta_kubernetes_pod_container_init ]            regex: &quot;true&quot;          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]            separator: ;            regex: (.+)            target_label: __metrics_path__            replacement: $1            action: replace          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]            separator: ;            regex: ([^:]+)(?::\\d+)?;(\\d+)            target_label: __address__            replacement: $1:$2            action: replace          - separator: ;            regex: __meta_kubernetes_pod_label_(.+)            replacement: $1            action: labelmap          - source_labels: [__meta_kubernetes_namespace]            separator: ;            regex: (.*)            target_label: kubernetes_namespace            replacement: $1            action: replace          - source_labels: [__meta_kubernetes_pod_name]            separator: ;            regex: (.*)            target_label: kubernetes_pod_name            replacement: $1            action: replace        vm_scrape_params:          stream_parse: true    namespaceSelector:      any: true\n\n\n以上是 VM 提供的 CRD\n\nOpenTelemetry 诞生到此铺垫完成，不知道有没有发现在可观测性中关键的三个部分：日志、指标、trace 都是使用不同的开源产品，从而会导致技术栈较多，维护起来自然也是比较麻烦的。\n这么一个软件领域的核心能力自然需要提供一个完整方案的，将以上的不同技术栈都整合在一起，更加的方便开发者使用。\n在这之前也有两个社区想要做类似的事情：\n\nOpenTracing\nOpenCensus\n\n不过他们并没有统一整个可观测领域，直到 2019 年 CNCF 社区宣布成立 OpenTelemetry，并且将上述两个社区进行合并共同开发 OpenTelemetry。\n\n背靠 CNCF 云原生社区加上许多知名厂商的支持（Google、Amazon、Redhat 等），现在已经正式成为 CNCF 的顶级项目了。\n\nOpenTelemetry 架构介绍\n但我们打开 OpenTelemetry 社区的 GitHub 首页时，会看到有许多项目；第一反应应该是比较蒙的，下面我会着重介绍一些比较重要的项目。\n在开始之前还是先简单介绍下 OpenTelemetry 的一些基础组件和概念：\n整个 OpenTelemetry 系统其实可以简单分为三个部分：\n\n客户端\nOTel collector\n数据存储\n\n第一个客户端很好理解，也就是我们的业务应用；如果是 Java 应用只需要挂载一个 agent 就可以自动采集系统的指标、链路信息、日志等上传到 Collector 中。\n也就是上图的左边部分。\n之后就是非常关键的组件 collector，它可以通过 OTLP 协议接收刚才提到的客户端上传的数据，然后再内部进行处理，最终输出到后续的存储系统中。\nCollector\n\n上图是 collector 的架构图\n\n由于 OpenTelemetry 设计之初就是要做到厂商无关，所以它就得做出更高层级的设计。\n关键点就是这里的 Receiver 和 Exporter 都是模块化的设计，第三方开发者可以基于它的标准开发不同组件从而兼容不同的产品。\nReceiver：用于接收客户端上报的数据，不止是自己 agent 上报的数据，也可能会来自不同的厂商，比如 kubernetes、Kafka 等。\nExporter：同理，可以将 receiver 收到的数据进行处理之后输出到不同的组件中；比如 Kafka&#x2F;Pulsar&#x2F;Promethus&#x2F;Jaeger 等。\n比如我们可以使用 Nginx Receiver接收来着 Nginx 上报的数据。\n使用 MySQL Receiver接收来自 MySQL 的数据。\n当然通常我们使用最多的还是 OTLP Receiver，这是官方的 OTLP 协议的接收器，可以接受官方的一些指标，比如我们只使用了 Java Agent 进行数据上报时。https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver\n在这里是可以看到目前支持的所有第三方的 Receiver。\n\n\nOpenTelemetry 所支持的 Exporter 也很多，比如一些常见的存储：\n\nclickhouse exporter\nelasticsearch exporter\npulsar exporter\nprometheus exporter\notlp http exporter\n\nExporter 的使用场景很多：如果是指标相关的数据可以直接写入 Prometheus，如果是日志数据也可以直接写入 ElasticSearch。\n如果还有其他的特殊需求（删减属性等）则可以写入消息队列，自行处理完之后再发往 collector 进行后续的处理。\n可能你已经发现了，由于 collector 非常的灵活，所以我们可以像搭积木一样组装我们的 receiver 和 exporter，它会以我们配置的流水线的方式进行调用，这样我们就可以实现任意可定制的处理逻辑。\n而这些流水线的组装对于客户端来说都是透明的，也就是说 collector 的更改完全不会影响到业务；业务只需要按照 OTLP 的格式上报数据即可。\n在之前的从 Skywalking 切换到 OpenTelemetry 的文章中有人问为什么要切换到 OpenTelemetry？\n从这里也能看得出来，OpenTelemetry 的灵活度非常高，借助于 Exporter 可以任意的更换后端存储，或者增加&#x2F;删减一些不需要的指标数据等。\n\n当然我们也可以统一的在这里进行搜索，可以列出所有的第三方集成的组件：https://opentelemetry.io/ecosystem/registry/\n\nOpenTelemetry 项目介绍opentelemetry-java介绍完基本的概念后，我们可以看看  OTel 社区的一些主要开源项目。\n这里我们还是以刚才的那个架构图从作往右讲起，也就是主要分为客户端和 collector 端。\n目前官方支持的客户端语言已经非常齐全了，大部分的版本都已经是 Stable 稳定版，意味着可以进入生产环境。\n这里我们以 Java 客户端为例：其中我们重点关注下 opentelemetry-java 和 opentelemetry-java-instrumentation 这两个项目。\n我们用的最多的会是 opentelemetry-java-instrumentation，它会给我们提供一个 java agent 的 JAR 包：\njava -javaagent:path/to/opentelemetry-javaagent.jar \\     -jar myapp.jar\n我们只需要在 Java 应用中加上该  agent 就可以实现日志、指标、trace 的自动上报。\n而且它还实现了不同框架、库的指标采集与 trace。\n在这里可以查到支持的库与框架列表：\nhttps://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/docs/supported-libraries.md#libraries--frameworks\n\n总之几乎就是你能想到和不能想到的都支持了。\n\n而 opentelemetry-java 我们直接使用的几率会小一些，opentelemetry-java-instrumentation 本身也是基于它创建的，可以理解为是 Java 版本的核心基础库，一些社区支持的组件就可以移动到 instrumentation 这个库中。\n比如我在上篇文章：从一个 JDK21+OpenTelemetry 不兼容的问题讲起中涉及到的 HostResourceProvider 资源加载就是从 opentelemetry-java 中移动到了 opentelemetry-java-instrumentation。\n具体可以参考：https://github.com/open-telemetry/opentelemetry-java/issues/4701\ncollector\n之后就是 collector 的组件了，它同样的也有两个库：OpenTelemetry Collector 和 OpenTelemetry Collector Contrib\n其实通过他们的名字也可以看得出来，他们的作用与刚才的 Java 库类似：\n\nopentelemetry-collector：由官方社区维护，提供了一些核心能力；比如只包含了最基本的 otlp 的 receiver 和 exporter。\nopentelemetry-collector-contrib：包含了官方的 collector，同时更多的维护了社区提供的各种 receiver 和 exporter；就如上文提到的，一些社区组件（pulsar、MySQL、Kafka）等都维护在这个仓库。\n\n而我们生产使用时通常也是直接使用 opentelemetry-collector-contrib，毕竟它所支持的社区组件更多。\n总结因为 OpenTelemetry 想要解决的是整个可观测领域的所有需求，所以仓库非常多，社区也很开放，感兴趣的朋友可以直接参与贡献，这么多 repo 总有一个适合你的。\n后续会继续讲解如何安装以及配置我们的 OpenTelemetry。\n参考链接：\n\nhttps://github.com/pinpoint-apm/pinpoint\nhttps://github.com/codecentric/spring-boot-admin\nhttps://github.com/open-telemetry/opentelemetry-java\nhttps://github.com/open-telemetry/opentelemetry-java-instrumentation\nhttps://github.com/open-telemetry/opentelemetry-java/issues/4701\n\n#Blog #OpenTelemetry \n","categories":["OB"],"tags":["OpenTelemetry"]},{"title":"OpenTelemetry 实战：gRPC 监控的实现原理","url":"/2024/08/29/ob/OpenTelemetry-grpc-principle/","content":"前言\n最近在给 opentelemetry-java-instrumentation 提交了一个 PR，是关于给 gRPC 新增四个 metrics：\n\nrpc.client.request.size: 客户端请求包大小\nrpc.client.response.size：客户端收到的响应包大小\nrpc.server.request.size：服务端收到的请求包大小\nrpc.server.response.size：服务端响应的请求包大小\n\n这个 PR 的主要目的就是能够在指标监控中拿到 RPC 请求的包大小，而这里的关键就是如何才能拿到这些包的大小。\n\n\n首先支持的是 gRPC（目前在云原生领域使用的最多），其余的 RPC 理论上也是可以支持的：\n在实现的过程中我也比较好奇 OpenTelemetry 框架是如何给 gRPC 请求创建 span 调用链的，如下图所示：\n\n这是一个 gRPC 远程调用，java-demo 是 gRPC 的客户端，k8s-combat 是 gRPC 的服务端\n\n在开始之前我们可以根据 OpenTelemetry 的运行原理大概猜测下它的实现过程。\n首先我们应用可以创建这些链路信息的前提是：使用了 OpenTelemetry 提供的 javaagent，这个 agent 的原理是在运行时使用了 byte-buddy 增强了我们应用的字节码，在这些字节码中代理业务逻辑，从而可以在不影响业务的前提下增强我们的代码（只要就是创建 span、metrics 等数据）\n\nSpring 的一些代理逻辑也是这样实现的\n\ngRPC 增强原理而在工程实现上，我们最好是不能对业务代码进行增强，而是要找到这些框架提供的扩展接口。\n拿 gRPC 来说，我们可以使用它所提供的 io.grpc.ClientInterceptor 和 io.grpc.ServerInterceptor 接口来增强代码。\n打开 io.opentelemetry.instrumentation.grpc.v1_6.TracingClientInterceptor 类我们可以看到它就是实现了 io.grpc.ClientInterceptor：\n而其中最关键的就是要实现 io.grpc.ClientInterceptor#interceptCall 函数：\n@Override  public &lt;REQUEST, RESPONSE&gt; ClientCall&lt;REQUEST, RESPONSE&gt; interceptCall(      MethodDescriptor&lt;REQUEST, RESPONSE&gt; method, CallOptions callOptions, Channel next) &#123;    GrpcRequest request = new GrpcRequest(method, null, null, next.authority());    Context parentContext = Context.current();    if (!instrumenter.shouldStart(parentContext, request)) &#123;      return next.newCall(method, callOptions);    &#125;    Context context = instrumenter.start(parentContext, request);    ClientCall&lt;REQUEST, RESPONSE&gt; result;    try (Scope ignored = context.makeCurrent()) &#123;      try &#123;        // call other interceptors        result = next.newCall(method, callOptions);      &#125; catch (Throwable e) &#123;        instrumenter.end(context, request, Status.UNKNOWN, e);        throw e;      &#125;  &#125;    return new TracingClientCall&lt;&gt;(result, parentContext, context, request);  &#125;\n\n这个接口是 gRPC 提供的拦截器接口，对于 gRPC 客户端来说就是在发起真正的网络调用前后会执行的方法。\n所以在这个接口中我们就可以实现创建 span 获取包大小等逻辑。\n使用 byte-buddy 增强代码不过有一个问题是我们实现的 io.grpc.ClientInterceptor 类需要加入到拦截器中才可以使用：\nvar managedChannel = ManagedChannelBuilder.forAddress(host, port) .intercept(new TracingClientInterceptor()) // 加入拦截器.usePlaintext().build();\n\n但在 javaagent 中是没法给业务代码中加上这样的代码的。\n此时就需要 byte-buddy 登场了，它可以动态修改字节码从而实现类似于修改源码的效果。\n在 io.opentelemetry.javaagent.instrumentation.grpc.v1_6.GrpcClientBuilderBuildInstr umentation  类里可以看到 OpenTelemetry 是如何使用 byte-buddy 的。\n@Overridepublic ElementMatcher&lt;TypeDescription&gt; typeMatcher() &#123;  return extendsClass(named(&quot;io.grpc.ManagedChannelBuilder&quot;))      .and(declaresField(named(&quot;interceptors&quot;)));&#125;@Overridepublic void transform(TypeTransformer transformer) &#123;  transformer.applyAdviceToMethod(      isMethod().and(named(&quot;build&quot;)),      GrpcClientBuilderBuildInstrumentation.class.getName() + &quot;$AddInterceptorAdvice&quot;);&#125;@SuppressWarnings(&quot;unused&quot;)public static class AddInterceptorAdvice &#123;  @Advice.OnMethodEnter(suppress = Throwable.class)  public static void addInterceptor(      @Advice.This ManagedChannelBuilder&lt;?&gt; builder,      @Advice.FieldValue(&quot;interceptors&quot;) List&lt;ClientInterceptor&gt; interceptors) &#123;    VirtualField&lt;ManagedChannelBuilder&lt;?&gt;, Boolean&gt; instrumented =        VirtualField.find(ManagedChannelBuilder.class, Boolean.class);    if (!Boolean.TRUE.equals(instrumented.get(builder))) &#123;      interceptors.add(0, GrpcSingletons.CLIENT_INTERCEPTOR);      instrumented.set(builder, true);    &#125;  &#125;&#125;\n\n从这里的源码可以看出，使用了 byte-buddy 拦截了 io.grpc.ManagedChannelBuilder#intercept(java.util.List&lt;io.grpc.ClientInterceptor&gt;) 函数。\n\nio.opentelemetry.javaagent.extension.matcher.AgentElementMatchers#extendsClass&#x2F; isMethod 等函数都是 byte-buddy 库提供的函数。\n\n而这个函数正好就是我们需要在业务代码里加入拦截器的地方。\ninterceptors.add(0, GrpcSingletons.CLIENT_INTERCEPTOR);GrpcSingletons.CLIENT_INTERCEPTOR = new TracingClientInterceptor(clientInstrumenter, propagators);\n通过这行代码可以手动将 OpenTelemetry 里的 TracingClientInterceptor 加入到拦截器列表中，并且作为第一个拦截器。\n而这里的：\nextendsClass(named(&quot;io.grpc.ManagedChannelBuilder&quot;))        .and(declaresField(named(&quot;interceptors&quot;)))\n\n通过函数的名称也可以看出是为了找到 继承了io.grpc.ManagedChannelBuilder 类中存在成员变量 interceptors 的类。\ntransformer.applyAdviceToMethod(      isMethod().and(named(&quot;build&quot;)),      GrpcClientBuilderBuildInstrumentation.class.getName() + &quot;$AddInterceptorAdvice&quot;);\n然后在调用 build 函数后就会进入自定义的 AddInterceptorAdvice 类，从而就可以拦截到添加拦截器的逻辑，然后把自定义的拦截器加入其中。\n获取 span 的 attribute\n我们在 gRPC 的链路中还可以看到这个请求的具体属性，比如：\n\ngRPC 服务提供的 IP 端口。\n请求的响应码\n请求的 service 和 method\n线程等信息。\n这些信息在问题排查过程中都是至关重要的。\n\n\n\n可以看到这里新的 attribute 主要是分为了三类：\n\nnet.* 是网络相关的属性\nrpc.* 是和 grpc 相关的属性\nthread.* 是线程相关的属性\n\n所以理论上我们在设计 API 时最好可以将这些不同分组的属性解耦开，如果是 MQ 相关的可能还有一些 topic 等数据，所以各个属性之间是互不影响的。\n带着这个思路我们来看看 gRPC 这里是如何实现的。\nclientInstrumenterBuilder\t.setSpanStatusExtractor(GrpcSpanStatusExtractor.CLIENT)\t.addAttributesExtractors(additionalExtractors)        .addAttributesExtractor(RpcClientAttributesExtractor.create(rpcAttributesGetter))        .addAttributesExtractor(ServerAttributesExtractor.create(netClientAttributesGetter))        .addAttributesExtractor(NetworkAttributesExtractor.create(netClientAttributesGetter))\n\nOpenTelemetry 会提供一个 io.opentelemetry.instrumentation.api.instrumenter.InstrumenterBuilder#addAttributesExtractor构建器函数，用于存放自定义的属性解析器。\n从这里的源码可以看出分别传入了网络相关、RPC 相关的解析器；正好也就对应了图中的那些属性，也满足了我们刚才提到的解耦特性。\n而每一个自定义属性解析器都需要实现接口 io.opentelemetry.instrumentation.api.instrumenter.AttributesExtractor\npublic interface AttributesExtractor&lt;REQUEST, RESPONSE&gt; &#123;&#125;\n\n这里我们以 GrpcRpcAttributesGetter 为例。\nenum GrpcRpcAttributesGetter implements RpcAttributesGetter&lt;GrpcRequest&gt; &#123;  INSTANCE;  @Override  public String getSystem(GrpcRequest request) &#123;    return &quot;grpc&quot;;  &#125;  @Override  @Nullable  public String getService(GrpcRequest request) &#123;    String fullMethodName = request.getMethod().getFullMethodName();    int slashIndex = fullMethodName.lastIndexOf(&#x27;/&#x27;);    if (slashIndex == -1) &#123;      return null;    &#125;    return fullMethodName.substring(0, slashIndex);  &#125;\n\n可以看到 system 是写死的 grpc，也就是对于到页面上的 rpc.system 属性。\n而这里的 getService 函数则是拿来获取 rpc.service 属性的，可以看到它是通过 gRPC 的method 信息来获取 service 的。\n\npublic interface RpcAttributesGetter&lt;REQUEST&gt; &#123;      @Nullable    String getService(REQUEST request);&#125;\n而这里 REQUEST 其实是一个泛型，在 gRPC 里是 GrpcRequest，在其他 RPC 里这是对应的 RPC 的数据。\n这个 GrpcRequest 是在我们自定义的拦截器中创建并传递的。\n而我这里需要的请求包大小也是在拦截中获取到数据然后写入进 GrpcRequest。\n\nstatic &lt;T&gt; Long getBodySize(T message) &#123;    if (message instanceof MessageLite) &#123;      return (long) ((MessageLite) message).getSerializedSize();    &#125; else &#123;      // Message is not a protobuf message      return null;    &#125;&#125;\n\n这样就可以实现不同的 RPC 中获取自己的 attribute，同时每一组 attribute 也都是隔离的，互相解耦。\n自定义 metrics每个插件自定义 Metrics 的逻辑也是类似的，需要由框架层面提供 API 接口：\npublic InstrumenterBuilder&lt;REQUEST, RESPONSE&gt; addOperationMetrics(OperationMetrics factory) &#123;    operationMetrics.add(requireNonNull(factory, &quot;operationMetrics&quot;));    return this;  &#125;// 客户端的 metrics.addOperationMetrics(RpcClientMetrics.get());// 服务端的 metrics.addOperationMetrics(RpcServerMetrics.get());\n\n之后也会在框架层面回调这些自定义的 OperationMetrics:\n   if (operationListeners.length != 0) &#123;     // operation listeners run after span start, so that they have access to the current span     // for capturing exemplars     long startNanos = getNanos(startTime);     for (int i = 0; i &lt; operationListeners.length; i++) &#123;       context = operationListeners[i].onStart(context, attributes, startNanos);     &#125;   &#125;if (operationListeners.length != 0) &#123;    long endNanos = getNanos(endTime);    for (int i = operationListeners.length - 1; i &gt;= 0; i--) &#123;      operationListeners[i].onEnd(context, attributes, endNanos);    &#125;&#125;\n这其中最关键的就是两个函数 onStart 和 onEnd，分别会在当前这个 span 的开始和结束时进行回调。\n所以通常的做法是在 onStart 函数中初始化数据，然后在 onEnd 结束时统计结果，最终可以拿到 metrics 所需要的数据。\n以这个 rpc.client.duration 客户端的请求耗时指标为例：\n@Override  public Context onStart(Context context, Attributes startAttributes, long startNanos) &#123;    return context.with(        RPC_CLIENT_REQUEST_METRICS_STATE,        new AutoValue_RpcClientMetrics_State(startAttributes, startNanos));  &#125;@Override  public void onEnd(Context context, Attributes endAttributes, long endNanos) &#123;    State state = context.get(RPC_CLIENT_REQUEST_METRICS_STATE);\tAttributes attributes = state.startAttributes().toBuilder().putAll(endAttributes).build();  \tclientDurationHistogram.record(  \t    (endNanos - state.startTimeNanos()) / NANOS_PER_MS, attributes, context);&#125;\n\n在开始时记录下当前的时间，结束时获取当前时间和结束时间的差值正好就是这个 span 的执行时间，也就是 rpc client 的处理时间。\n在 OpenTelemetry 中绝大多数的请求时间都是这么记录的。\nGolang 增强而在 Golang 中因为没有 byte-buddy 这种魔法库的存在，不可以直接修改源码，所以通常的做法还是得硬编码才行。\n还是以 gRPC 为例，我们在创建 gRPC server 时就得指定一个 OpenTelemetry 提供的函数。\ns := grpc.NewServer(      grpc.StatsHandler(otelgrpc.NewServerHandler()),  )\n\n 在这个 SDK 中也会实现刚才在 Java 里类似的逻辑，限于篇幅具体逻辑就不细讲了。\n总结以上就是 gRPC 在 OpenTelemetry 中的具体实现，主要就是在找到需要增强框架是否有提供扩展的接口，如果有就直接使用该接口进行埋点。\n如果没有那就需要查看源码，找到核心逻辑，再使用 byte-buddy 进行埋点。\n\n比如 Pulsar 并没有在客户端提供一些扩展接口，只能找到它的核心函数进行埋点。\n而在具体埋点过程中 OpenTelemetry 提供了许多解耦的 API，方便我们实现埋点所需要的业务逻辑，也会在后续的文章继续分析 OpenTelemetry 的一些设计原理和核心 API 的使用。\n这部分 API 的设计我觉得是 OpenTelemetry 中最值得学习的地方。\n参考链接：\n\nhttps://bytebuddy.net/#/\nhttps://opentelemetry.io/docs/specs/semconv/rpc/rpc-metrics/#metric-rpcserverrequestsize\n\n","categories":["OB","OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"OpenTelemetry在企业内部应用所需要的技术栈","url":"/2024/09/15/ob/OpenTelemetry-enterprise/","content":"可观测性概念当一个软件或系统出于运行状态时，如果我们不对他加以观测，那它的运行状态对我们来说就是一个黑盒。\n\n如上图所示。\n\n我们只能通过业务的表象来判断它是否正常运行，无法在故障发生前进行预判，从而只能被动解决问题。\n\n\n这类问题在微服务时代体现的更加明显，即便是业务已经出现问题，在没有可观测性系统的前提下想要定位问题更是难上加难。\n好在可观测性这个概念由来已久，已经由一些业界大佬抽象出几个基本概念：\n\nLogs：离散的日志信息\nMetrics：聚合的指标\nTrace：请求基本的链路追踪\n\n结合这三个指标，我们排查问题的流程一般如下：\n首先根据 metrics 来判断是否有异常，这点可以通过在 Prometheus 的 AlertManager 配置一些核心的告警指标。\n比如当 CPU、内存使用率超过 80% 或者某个应用 Down 机后就发出告警。\ngroups:- name: AllInstances  rules:  - alert: InstanceDown    # Condition for alerting    expr: up == 0    for: 1m    # Annotation - additional informational labels to store more information    annotations:      title: &#x27;Instance &#123;&#123; $labels.instance &#125;&#125; down&#x27;      description: &#x27;&#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125; has been down for more than 1 minute.&#x27;    # Labels - additional labels to be attached to the alert    labels:      severity: &#x27;critical&#x27;\n\n这可以让我们尽早发现故障。\n之后我们可以通过链路信息找到发生故障的节点。\n然后通过这里的 trace_id 在应用中找到具体的日志：\nmdc.trace_id:4a686dedcdf4e95b1a83b36e62563a96\n再根据日志中的上下文确定具体的异常原因。\n这就是一个完整的排查问题的流程。\nOpenTelemetry 发展历史在 OpenTelemetry 开始之前还是先回顾下可观测性的发展历史，其中有几个重要时间点：\n\n2010 年 Google 发布了 Dapper 论文，给业界带来了实现分布式追踪的理论支持，之后的许多分布式链路追踪实现都有它的影子\nkubernetes 的发布奠定了后续云原生社区的基础\nJaeger 发布后成为了主流的链路存储系统\n2019 年 OpenTracing 和 OpenCensus 合并为 OpenTelemetry\n2021 年底 OpenTelemetry 发布第一个 GA release 版本\n\nOpenTelemetry 是什么？\n以前我们所接触到的类似于阿里的ARMS、美团的 CAT、Pinpoint 这类系统大多都有一个公司在背后进行驱动，与厂商绑定的非常紧密。\n而 OpenTelemetry 则相反，它主要由社区驱动，参与的公司众多；同时它定义和提供了一套可观测性的标准（包括 API、SDK、规范等数据）。\n使用它你可以灵活的选择和搭配任意的开源或商业产品来组成你的可观测性技术栈。\n\n因为社区非常活跃，所以当前也几乎支持主流的开发语言。\nOpenTelemetry 的架构OpenTelemetry 的架构主要分为三个部分：\n\n左侧的客户端 Agent，用于采集客户端的数据，通常就是我们的应用。\n中间的是 Collector-Service，用于接受客户端的数据、内部处理、导出数据到各种存储\n右侧的则是各种存储层，用于存储 Metrics、Logs、Traces 这些数据。\n\n我们基于官方推荐的技术架构选型了我们的技术栈：主要的区别就是使用 VictoriaMetrics 存储指标、StackRocks 存储 Trace，ElasticSearch 存储日志。\n\n只是目前我们的日志链路还没有完全切换到 OpenTelemetry 的链路，依然是在 Pod 中挂载了一个 sidecar，在这个 sidecar 中通过 filebeat 采集日志输出到 elasticsearch，后续也会逐步迁移。\n\n核心项目CollecotorOpenTelemetry 社区的项目众多，其中大部分都是各种语言的 SDK 和 API，其中最为关键的应该就是 opentelemetry-collector\n也就是刚才架构图中的中间部分，我们可以把它理解为类似 APIGateway 的角色，所有上报的 OTel 数据都得经过它的处理。\n\n主要由以下三部分组成：\n\nReceiver：用于接受客户端上报的数据\nProcess：内部的数据处理器\nExporter：将数据导出到不同的存储\n\n由于 OpenTelemetry 社区非常的活跃，所以这里支持的 Receiver、Processor 和 Exporter 类型非常多。\n\n其他核心项目我们以 Java 为例，对业务开发最重要的库就是 opentelemetry-java-instrumentation\n它可以打包一个 javaagent 给我们使用：\n# Java examplejava -javaagent:path/to/opentelemetry-javaagent.jar \\       -jar myapp.jar\n\n\n同时也支持了我们日常开发的绝大多数框架和中间件。\n\n支持的库与框架列表\n\n如果我们需要在应用中自定义打桩一些 Span、Metrics ，就还需要 opentelemetry-java 这个项目。\n它提供了具体的 SDK 可以方便的创建 Span 和 Metrics。\nTrace之后来看看 OpenTelemetry 中具体的三个维度的概念和应用，首先是 Trace。\n\nTrace 这个概念首先是 Google Dapper 论文中提到。\n如上图所示：一次用户请求经历了 4 次 PRC 调用，分别也属于不同的系统。\n每一次 RPC 调用就会产生一个 Span，将这些 span 串联起来就能形成一个调用链路。\n这个 Span 主要包含以下信息：\n\nSpanName\nParentID\nSpanID\n\n当我们将一个 Span 放大后会看到更加具体的信息：\n\nTraceId\nSpanName\nParentID\nSpanID\n开始时间\n结束时间在 Dapper 论文中使用 Annotations 来存放 span 的属性，当然也可以自定义存放一些数据，比如图中的 &quot;foo&quot;。\n\n\n在 OpenTelemetry 的 SDK 中称为  attribute，而在 Jaeger 的 UI 中又称为 tag，虽然叫法不同，但本质上是一个东西。\n\n最终就会形成上图中的树状结构的调用关系。\nSpan KindSpan 中还有一个非常重要的概念，就是 Span Kind，也就是 Span 的类型，这个类型可以在排查问题时很容易得知该服务的类型。\n按照官方的定义，Span 的类型分为：\n\nClient\nServer\nInternal\nProducer\nConsumer\n\n对于 RPC 的客户端和服务端自然就对应 Client 和 Server，而使用了消息队列的生产者消费者对应的就是 Produce 和 Consumer。\n除此之外发生在应用内部的一些关键 Span 的类型就是 Internal，比如我们需要对业务的某些关键函数生成 Span 时，此时的 Span 类型通常也都是 Internal。\n上下文传递\n在 Trace 中有一个关键技术问题需要被解决，也就是 Context 的上下文传递。\n这个特别是在分布式系统中必须要解决，我们可以简单把它理解为如何把上游生成的 trace_id 传递到下游，这样才能在追踪的链路追踪系统中串联起来。\n这个关键的技术名词在 OpenTelemetry 中称为：Context Propagation.\n在分布式系统中，数据都是通过网络传递的，所以这里的本质问题依然是如何将上下文数据序列化之后，在下游可以反序列化到 Context 中。\n聪明的小伙伴应该已经想到，我们可以将 trace_id 写入到跨进程调用的元数据中：\n\nhttp 可以存放在 http header 中\ngRPC 可以存放在 meta 中\nPulsar 可以存放在消息的 properties 中\n其余的中间件和框架也是同理\n\n然后在远程调用之前使用 Inject 将数据注入到这些元数据里，下游在接收到请求后再通过一个Extract 函数将元数据解析到 Context 中，这样 trace_id 就可以串联起来了。\n\n\n上图就是 Pulsar 和 gRPC 传递 trace_id 的过程，数据都是存放在元数据中的，这里的 traceparent 的值本质上就是 trace_id.\n\n具体的代码细节我会在下一篇继续分析。\n\nMetrics\nMetrics 相对于 Trace 来说则是要简单许多，OpenTelemetry 定义了许多命名规范和标准，这样大家在复用社区的一些监控模板时就要更加容易一些。\nMetrics Exemplars\nMetrics 还提供了一个 Exemplar 的功能，它的主要作用是可以将 Metrics 和 Trace 关联在一起，这样在通过 Metrics 发现问题时，就可以直接跳转到链路系统。\n因为 trace_id 可以通过 MDC 和日志关联，所以我们可以直接通过 Metrics 定位具体应用的日志，这样排查问题的效率将会非常高。\n扩展信息以上就是关于 OpenTelemetry 的整体架构，下面来扩展一些内容。\neBPFeBPF 是一个运行在 Linux 内核中的虚拟机，它提供一套特殊的指令集并允许我们在不重新编译内核、也不需要重启应用的情况下加载自定义的逻辑。\neBPF 技术具有三大特点：\n\n第一是无侵入，动态挂载，目标进程无需重启，而且因为是 Linux 内核提供功能，所以与语言无关，任何语言都可以支持。\n第二是高性能，eBPF 字节码会被 JIT 成机器码后执行，效率非常高；\n第三是更加安全，它会运行在自己的沙箱环境中，不会导致目标进程崩溃。\n\neBPF 虽然有很多优点，同时也有一些局限性，比如我想监控业务代码中的某个具体指标（订单创建数量），此时它就难以实现了，所以还得看我们的应用场景。更适合一些云平台，或者更偏向底层的应用。\n目前 eBPF 的应用场景还不够广泛，但假以时日一定会成为可观测领域的未来之星。\nSigNoz不知道大家发现没有，如果我们直接 OpenTelemetry 技术栈会需要为 Trace、Metrics、Logs 选择不同的存储，而且他们的查询界面也分散在不同的地方。\n那有没有一个统一的平台可以给我们提供完整的可观测体验呢？\n有这样的需求那就有对应的厂商实现了：\nSigNoz 就是这样的平台，它将 OpenTelemetry-collector 和数据存储全部整合在了一起，同时全面兼容  OpenTelemetry；可以说它就是基于 OpenTelemetry 构建的一个可观测产品。\n对于一些中小厂商，不想单独维护这些组件时是非常有用的。\nOpenObserve\nOpenObserve在 SigNoz 的基础上做的更加极致一些，它提供了一个统一的存储可以存放日志、Trace、Metrics 等数据。\n这样我们就可以只使用一个数据库存放所有的数据，同时它也提供了完整的 UI，并且也全面兼容 OpenTelemetry。\n这样对于运维来说会更加简单，只是可能带来的副作用就是需要与它完全绑定。\n总结以上就是 OpenTelemetry 在企业的应用，大家可以根据自己的情况选择自建 OTel 的技术栈，还是选择 SigNoz 和 OpenObserve 这类的标准化产品。\n","categories":["OB","OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"从 Dapper 到 OpenTelemetry：分布式追踪的演进之旅","url":"/2024/06/06/ob/OpenTelemetry-trace-concept/","content":"在之前写过两篇比较系统的关于 OpenTelemetry 的文章：\n\nOpenTelemetry 实践指南：历史、架构与基本概念\n实操 OpenTelemetry：通过 Demo 掌握微服务监控的艺术\n\n从基本概念到如何部署 demo 实战了解 OpenTelemetry，从那个 demo 中也可以得知整个 OpenTelemetry 体系的复杂性，包含了太多的组件和概念。\n为了能更清晰的了解每个关键组件的作用以及原理，我打算分为几期来讲解 OpenTelemetry 的三个核心组件：\n\nTrace\nMetrics\nLogs\n\n首先以 Trace 讲起。\n\nTrace开始之前还是先复习一下 Trace 的历史背景。\n如今现代的分布式追踪的起源源自于 Google 在 2010 年发布的一篇论文：\n\nDapper, a Large-Scale Distributed Systems Tracing Infrastructure\n\n在这篇论文中提出了分布式追踪的几个核心概念：\n\nTrace\nSpan\nSpan 的一些基础数据结构\n\n\n可视化追踪以及展示\n\n之后 Twitter 受到了 Dapper 的启发开源了现在我们熟知的 Zipkin，包含了存储和可视化 UI 展示我们的追踪链路。\nUber 也在 2015 年开源了 Jaeger 项目，它的功能和 Zipkin 类似，但目前我们用的较多的还是 Jaeger；现在已经成为 CNCF 的托管项目。\n之后陆续出现过 OpenTracing 和 OpenCensus 项目，他们都企图统一分布式追踪这一领域。\n直到 OpenTelemetry 的出现整合了以上两个项目，并且逐渐成为可观测领域的标准。\n\n更多历史背景可以参考之前的文章：OpenTelemetry 实践指南：历史、架构与基本概念\n\n\n\n这里我们结合 Dapper 论文中的资料进行分析，在这个调用中用户发起了一次请求，内部系统经历了 4 次 RPC 调用。\n从第二张图会看到一些关键信息：\n\nspanName\nparentId\nspanId\n\nparentId 很好理解，主要是定义调用的主次关系；要注意的是并行调用时 parentId 是同一个。\nspanId 在可以理解为每一个独立的操作，在这里就是一次 RPC 调用；同理一次数据库操作、消息的收发都是一个 span。\n\nspan 的更多内容在后文继续讲解。\n\nSpan当我们把某一个具体的 span 放大会看到更加详细的信息，其中最关键的如下：\n\ntraceId\nspanName\nspanId\nparentId\n开始时间\n结束时间\n\n由于一个完整的 trace 链路由 N 个 span 组成，所以这个链路必须得有一个唯一的 traceId 将这些 span 串联起来。这样才可以在可视化的时候更好的展示链路信息。\n以上的这些字段很容易理解，都是一些必须的信息。\n在 Dapper 论文中使用 Annotations 来存放 span 的属性，也就是刚才那些字段，当然也可以自定义存放一些数据，比如图中的 &quot;foo&quot;。\nOpenTelemetry 中的 SpanOpenTelemetry 的 trace 自然也是基于 Dapper 的，只是额外做了一些优化，比如在刚才那些字段的基础上新增了一些概念：\n&#123;  &quot;name&quot;: &quot;/v1/sys/health&quot;,  &quot;context&quot;: &#123;    &quot;trace_id&quot;: &quot;7bba9f33312b3dbb8b2c2c62bb7abe2d&quot;,    &quot;span_id&quot;: &quot;086e83747d0e381e&quot;  &#125;,  &quot;parent_id&quot;: &quot;&quot;,  &quot;start_time&quot;: &quot;2021-10-22 16:04:01.209458162 +0000 UTC&quot;,  &quot;end_time&quot;: &quot;2021-10-22 16:04:01.209514132 +0000 UTC&quot;,  &quot;status_code&quot;: &quot;STATUS_CODE_OK&quot;,  &quot;status_message&quot;: &quot;&quot;,  &quot;attributes&quot;: &#123;    &quot;net.transport&quot;: &quot;IP.TCP&quot;,    &quot;net.peer.ip&quot;: &quot;172.17.0.1&quot;,    &quot;net.peer.port&quot;: &quot;51820&quot;,    &quot;net.host.ip&quot;: &quot;10.177.2.152&quot;,    &quot;net.host.port&quot;: &quot;26040&quot;,    &quot;http.method&quot;: &quot;GET&quot;,    &quot;http.target&quot;: &quot;/v1/sys/health&quot;,    &quot;http.server_name&quot;: &quot;mortar-gateway&quot;,    &quot;http.route&quot;: &quot;/v1/sys/health&quot;,    &quot;http.user_agent&quot;: &quot;Consul Health Check&quot;,    &quot;http.scheme&quot;: &quot;http&quot;,    &quot;http.host&quot;: &quot;10.177.2.152:26040&quot;,    &quot;http.flavor&quot;: &quot;1.1&quot;  &#125;,  &quot;events&quot;: [    &#123;      &quot;name&quot;: &quot;&quot;,      &quot;message&quot;: &quot;OK&quot;,      &quot;timestamp&quot;: &quot;2021-10-22 16:04:01.209512872 +0000 UTC&quot;    &#125;  ]&#125;\n\n以这个 JSON 为例，新增了：\n\n Span Context\nSpan 的上下文，存放的都是不可变的数据，因为每个 Span 之间是存在关联关系的，这些关联关系都是存放在 context 中，主要就是 trace_id, span_id.\n\n\nAttributes: 可以理解为 Dapper 中的 Annotations，存放的是我们自定义的键值对，通常是由我们常用第三方开源 Instrumentation 内置的一些属性。\nSpan Events: Span 的一些关键事件。\n\n比如我们常用的 Redis 客户端 lettuce，它就会自己记录一些 Attributes。\n\n如果有多个 span 存在依赖关系：\n       [Span A]  ←←←(the root span)           |    +------+------+    |             |[Span B]      [Span C] ←←←(Span C is a `child` of Span A)    |             |[Span D]      +---+-------+              |           |          [Span E]    [Span F]\n\n大部分的可视化工具都是以时间线的方式进行展示：\n––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–&gt; time [Span A···················································]   [Span B··········································]      [Span D······································]    [Span C····················································]         [Span E·······]        [Span F··]\n\n这些和 Dapper 中描述的概念没有本质区别。\n\nSpan StatusSpan 还内置了一些 Status：\n\nUnset\nError\nOk\n\n默认情况下是 Unset，出现错误时则是 Error，一切正常时则是 Ok。\n通过可视化页面很容易得知某个 trace 中 span 的异常情况，点进去后可以看到具体的异常 span 以及它的错误日志。\nSpan Kind最后是 Span 的类型：\n\nClient\nServer\nInternal\nProducer\nConsumer\n\n\nClient 和 Server 非常好理解，比如我们有一个 gRPC 接口，调用方的 Span 是 client，而服务端的 Span 自然就是 Server。\nInternal 则是内部组件调用产生的 Span，这类 Span 相对会少一些。\nProducer 和 Consumer 一般指的是发起异步调用时的 Span，我们常见的就是往消息队列里生产和消费消息。\n通过这几种类型的 Span 也可以了解到什么情况下会创建 Span，通常是以下几种场景：\n\nRPC 调用\n数据库（Redis、MySQL、Mongo 等等）操作\n生产和消费消息\n有意义的内部调用\n\n通常在一个函数内部再调用其他的本地函数是不用创建 span 的，不然这个链路会非常的长。\nAnnotations当然也有一些特殊情况，比如我的某个内部函数非常重要，需要单独关心它的调用时长。\n此时我们就可以使用 Annotations 来单独创建自己的 Span。\n\n这个 Annotations 和 Dapper 中的不是同一个，只是 Java 中的注解。\n\n@Override  public void sayHello(HelloRequest request, StreamObserver&lt;HelloReply&gt; responseObserver) &#123;      Executors.newFixedThreadPool(1).execute(() -&gt; &#123;          myMethod(request.getName());      &#125;);            HelloReply reply = HelloReply.newBuilder()              .setMessage(&quot;Hello ==&gt; &quot; + request.getName())              .build();      responseObserver.onNext(reply);      responseObserver.onCompleted();  &#125;    @SneakyThrows  @WithSpan  public void myMethod(@SpanAttribute(&quot;request.name&quot;) String name) &#123;      TimeUnit.SECONDS.sleep(1);      log.info(&quot;myMethod:&#123;&#125;&quot;, name);  &#125;\n\n以这段代码为例，这是一个 gRPC 的服务端接口，在这个接口中调用了一个函数 myMethod，默认情况下并不会为它单独创建一个 Span。\n但如果我们想单独记录它，就可以使用 @WithSpan 这个注解，同时也可以使用  @SpanAttribute 来自定义 attribute。\n最终的效果如下：此时就会单独为这个函数创建一个 Span。\n\n需要单独引入一个依赖:\n\n&lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.opentelemetry.instrumentation&lt;/groupId&gt;    &lt;artifactId&gt;opentelemetry-instrumentation-annotations&lt;/artifactId&gt;    &lt;version&gt;2.3.0&lt;/version&gt;  &lt;/dependency&gt;&lt;/dependencies&gt;\nContext Propagation上下文传播也是 Trace 中非常重要的概念，刚才提到了每个 Span 都有自己不可变的上下文，那么后续的 Span 如何和上游的 Span 进行关联呢？\n这里有两种情况：\n\n同一进程\n垮进程\n\n同一进程同一个进程也分为两种情况：\n\n单线程\n多线程\n\n单线程的比较好处理，我们只需要把数据写入 ThreadLocal 中就可以做到线程隔离。\nprivate static final ThreadLocal&lt;Context&gt; THREAD_LOCAL_STORAGE = new ThreadLocal&lt;&gt;();@Override  @Nullable  public Context current() &#123;    return THREAD_LOCAL_STORAGE.get();  &#125;\n\n这点我们可以通过源码 io.opentelemetry.context.ThreadLocalContextStorage看到具体的实现过程。\n而如果是多线程时：\nExecutors.newFixedThreadPool(1).execute(() -&gt; &#123;      myMethod(request.getName());  &#125;);\n\n则需要对使用的线程池进行单独处理，将父线程中 threadlocal 中的数据拷贝出来进行传递，比如有阿里提供的 TransmittableThreadLocal，可以提供对线程池的支持。\n跨进程而如果是垮进程的场景，就需要将 context 的信息进行序列化传递。\n如果是 gRPC 调用会将信息存放到 metadata 中。\nHTTP 调用则是存放在 header 中。\n消息队列，比如 Pulsar 也可以将数据存放在消息中的 header 中进行传递。\n数据一旦跨进程传输成功后，就和单进程一样的处理方式了。\nBaggage\n有时候我们需要通过垮 Span 传递信息，比如如上图所示：我们需要在 serverB 中拿到 serverA 中收到的一个请求参数： http://127.0.0.1:8181/request\\?name\\=1232\n\n这个数据默认会作为 span 的 attribute ，但只会存在于第一个 span。\n如果我们想要在后续的 span 中也能拿到这个数据，甚至是垮进程也能获取到。\n那就需要使用 Baggage 这个对象了。\n它的使用也很简单：\n@RequestMapping(&quot;/request&quot;)  public String request(@RequestParam String name) &#123;  \t// 写入    Baggage.current().toBuilder().            put(&quot;request.name&quot;, name).build()            .storeInContext(Context.current()).makeCurrent();&#125;         // 获取String value = Baggage.current().getEntryValue(&quot;request.name&quot;);  log.info(&quot;request.name: &#123;&#125;&quot;, value);\n\n只要是属于同一个 trace 的调用就可以直接获取到数据。\n\ntraceId 也是垮 Span 传递的。\n\n而它的原理也是通过往 context 中写入数据实现的：\n@Immutable  class BaggageContextKey &#123;    static final ContextKey&lt;Baggage&gt; KEY = ContextKey.named(&quot;opentelemetry-baggage-key&quot;);      private BaggageContextKey() &#123;&#125;  &#125;\n\n而这个 context 是通过一个 entries 数据存储数据的，不管是在内部还是外部的跨进程调用，OpenTelemetry 都会将 context 通过 Context Propagation 传递出去。\n总结Trace 这部分的内容我觉得比 Metrics 和 Logs 更加复杂一些，毕竟多了一些数据结构；现在的内容也只是冰山一角，现在也在做 trace 的一些定制化开发，后续有新的进展会接着更新。\n参考链接：\n\nhttps://static.googleusercontent.com/media/research.google.com/zh-CN//archive/papers/dapper-2010-1.pdf\nhttps://opentelemetry.io/docs/languages/java/automatic/annotations/\nhttps://opentelemetry.io/docs/specs/otel/overview/#tracing-signal\nhttps://opentelemetry.io/docs/concepts/context-propagation/\nhttps://opentelemetry.io/docs/concepts/observability-primer/#distributed-traces\nhttps://tech.meituan.com/2023/04/20/traceid-google-dapper-mtrace.html\n\n","categories":["OB","OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"从 Prometheus 到 OpenTelemetry：指标监控的演进与实践","url":"/2024/06/13/ob/OpenTelemetry-metrics-concept/","content":"在上一篇：从 Dapper 到 OpenTelemetry：分布式追踪的演进之旅我们讲解了 Trace 的一些核心概念：\n\nTrace\nSpan\nContext\nBaggage 等\n\n这次我们来讲另一个话题 Metrics。\n\n\n背景关于 metrics 我最早接触相关概念的就是 prometheus，它是第二个加入 CNCF（云原生）社区的项目（第一个是 kubernetes），可见在云原生领域 Metrics 指标监控从诞生之初就是一个非常重要的组件。\n现实也确实如此，如今只要使用到了 kubernetes 相关的项目，对其监控就是必不可少的。\n当然也不止是云原生的项目才需要 Metrics 指标监控，我们任何一个业务都是需要的，不然我们的服务运行对开发运维来说都是一个黑盒，无法知道此时系统的运行情况，因此才需要我们的业务系统将一些关键运行指标暴露出来。\n\n业务数据：比如订单的增长率、销售金额等业务数据；同时还有应用自身的资源占用情况：\n\nQPS\nLatency\n内存\nCPU 等信息。\n\n 在使用 OpenTelemetry 之前，因为 prometheus 是这部分的绝对标准，所以我们通常都会使用 prometheus 的包来暴露这些指标：\n&lt;!-- The client --&gt;&lt;dependency&gt;  &lt;groupId&gt;io.prometheus&lt;/groupId&gt;  &lt;artifactId&gt;simpleclient&lt;/artifactId&gt;  &lt;version&gt;0.16.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Hotspot JVM metrics--&gt;&lt;dependency&gt;  &lt;groupId&gt;io.prometheus&lt;/groupId&gt;  &lt;artifactId&gt;simpleclient_hotspot&lt;/artifactId&gt;  &lt;version&gt;0.16.0&lt;/version&gt;&lt;/dependency&gt;\n\n暴露一个自定义的指标也很简单：\nimport io.prometheus.client.Counter;class YourClass &#123;  static final Counter requests = Counter.build()     .name(&quot;requests_total&quot;).help(&quot;Total requests.&quot;).register();  void processRequest() &#123;    requests.inc();    // Your code here.  &#125;&#125;\n\n\n这是暴露一个单调递增的指标，prometheus 还提供了其他几种指标类型：\n\n\nCounter\nGauge\nHistogram\n\n之后我们只需要在 prometheus 中配置一些抓取规则即可：\nscrape_configs:  - job_name: &#x27;springboot&#x27;    scrape_interval: 10s    static_configs:      - targets: [&#x27;localhost:8080&#x27;] # Spring Boot ip+port\n\n\n当然如果是运行在 kubernetes 环境，prometheus 也可以基于服务发现配置一些规则，自动抓取我们的 Pod 的数据，由于不是本文的重点就不过多介绍。\n\n基本组件在 OpenTelemetry 中自然也提供了 Metrics 这个组件，同时它也是完全兼容 Prometheus，所以我们理解和使用起来并不复杂。\nMeterProvider不同于 prometheus 客户端中直接提供了 Counter 就可以创建指标了，在 OpenTelemetry 中会提供一个 MeterProvider 的接口，使用这个接口可以获取 Meter，再使用 Meter 才可以创建 Counter、Gauge、Histogram 等数据。\n下面来看看具体如何使用，这里我以 Pulsar 源码的代码进行演示：\npublic InstrumentProvider(OpenTelemetry otel) &#123;      if (otel == null) &#123;          // By default, metrics are disabled, unless the OTel java agent is configured.          // This allows to enable metrics without any code change.        otel = GlobalOpenTelemetry.get();      &#125;    this.meter = otel.getMeterProvider()              .meterBuilder(&quot;org.apache.pulsar.client&quot;)              .setInstrumentationVersion(PulsarVersion.getVersion())              .build();  &#125;LongCounterBuilder builder = meter.counterBuilder(name)          .setDescription(description)          .setUnit(unit.toString());\n\n\nMeter ExporterMeter Exporter 则是一个 OpenTelemetry 独有的概念，与我们之前讲到的一样：OpenTelemetry 作为厂商无关的平台，允许我们将数据写入到任何兼容的产品里。\n所以我们在使用 Metrics 时需要指定一个 exporter：\n\n\n\nExporter 类型\n作用\n备注\n参数\n\n\n\nOTLP Exporter\n通过 OpenTelemetry Protocol（OTLP） 发送指标数据到 collect。\n默认生产环境中推荐使用，需要将数据发送到支持 OTLP 的后端，如 OpenTelemetry Collector。\n-Dotel.metrics.exporter&#x3D;otlp (default)\n\n\nConsole Exporter\n将指标数据打印到控制台的导出器。\n开发和调试，快速查看指标数据。\n-Dotel.metrics.exporter&#x3D;console\n\n\nPrometheus Exporter\n将指标数据以 Prometheus 抓取的格式暴露给 Prometheus 服务。\n与 Prometheus 集成，适用于需要 Prometheus 监控的场景，这个可以无缝和以往使用 prometheus 的场景兼容\n-Dotel.metrics.exporter&#x3D;prometheus\n\n\nMetric Instruments与 prometheus 类似，OpenTelemetry 也提供了以下几种指标类型：\n\nCounter：单调递增计数器，比如可以用来记录订单数、总的请求数。\nUpDownCounter：与 Counter 类似，只不过它可以递减。\nGauge：用于记录随时在变化的值，比如内存使用量、CPU 使用量等。\nHistogram：通常用于记录请求延迟、响应时间等。\n\n同时每个指标还有以下几个字段：\n\nName：名称，必填。\nKind：类型，必填。\nUnit：单位，可选。\nDescription：描述，可选。\n\nmessageInCounter = meter          .counterBuilder(MESSAGE_IN_COUNTER)          .setUnit(&quot;&#123;message&#125;&quot;)          .setDescription(&quot;The total number of messages received for this topic.&quot;)          .buildObserver();\n\n还是以 Pulsar 的为例，messageInCounter 是一个记录总的消息接收数量的 Counter 类型。\nsubscriptionCounter = meter          .upDownCounterBuilder(SUBSCRIPTION_COUNTER)          .setUnit(&quot;&#123;subscription&#125;&quot;)          .setDescription(&quot;The number of Pulsar subscriptions of the topic served by this broker.&quot;)          .buildObserver();\n这是记录一个订阅者数量的指标，类型是 UpDownCounter，也就是可以增加减少的指标。\nprivate static final List&lt;Double&gt; latencyHistogramBuckets =          Lists.newArrayList(.0005, .001, .0025, .005, .01, .025, .05, .1, .25, .5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0);DoubleHistogramBuilder builder = meter.histogramBuilder(&quot;pulsar.client.producer.message.send.duration&quot;)          .setDescription(&quot;Publish latency experienced by the application, includes client batching time&quot;)          .setUnit(Unit.Seconds.toString())          .setExplicitBucketBoundariesAdvice(latencyHistogramBuckets);\n\n这是一个记录 Pulsar producer 发送延迟的指标，类型是 Histogram。\nbacklogQuotaAge = meter          .gaugeBuilder(BACKLOG_QUOTA_AGE)          .ofLongs()          .setUnit(&quot;s&quot;)          .setDescription(&quot;The age of the oldest unacknowledged message (backlog).&quot;)          .buildObserver();\n\n这是一个记录最大 unack 也就是 backlog 时间的指标，类型是 Gauge。\n案例在之前的文章：实战：如何编写一个 OpenTelemetry Extensions中讲过如何开发一个 OpenTelemetry 的 extension，其实当时我就是开发了一个用于在 Pulsar 客户端中暴露指标的一个插件。\n\n不过目前 Pulsar 社区已经集成了该功能。\n\n其中的核心代码与上面讲到的类似：\npublic static void registerObservers() &#123;        Meter meter = MetricsRegistration.getMeter();            meter.gaugeBuilder(&quot;pulsar_producer_num_msg_send&quot;)                .setDescription(&quot;The number of messages published in the last interval&quot;)                .ofLongs()                .buildWithCallback(                        r -&gt; recordProducerMetrics(r, ProducerStats::getNumMsgsSent));private static void recordProducerMetrics(ObservableLongMeasurement observableLongMeasurement, Function&lt;ProducerStats, Long&gt; getter) &#123;        for (Producer producer : CollectionHelper.PRODUCER_COLLECTION.list()) &#123;            ProducerStats stats = producer.getStats();            String topic = producer.getTopic();            if (topic.endsWith(RetryMessageUtil.RETRY_GROUP_TOPIC_SUFFIX)) &#123;                continue;            &#125;        observableLongMeasurement.record(getter.apply(stats),                    Attributes.of(PRODUCER_NAME, producer.getProducerName(), TOPIC, topic));        &#125;&#125;\n\n只是这里使用了 buildWithCallback 回调函数，OpenTelemetry 会每隔 30s 调用一次这个函数，通常适用于 Gauge 类型的数据。\njava -javaagent:opentelemetry-javaagent.jar \\       -Dotel.javaagent.extensions=ext.jar  \\     -Dotel.metrics.exporter=prometheus \\     -Dotel.exporter.prometheus.port=18180 \\     -jar myapp.jar\n\n配合上 Prometheus 的两个启动参数就可以在本地 18180 中获取到指标数据：\ncurl http://127.0.0.1:18180/metrics\n\n当然也可以直接发往 OpenTelemetry-Collector 中，再由它发往 prometheus，只是这样需要额外在 collector 中配置一下：\nexporters:  debug: &#123;&#125;  otlphttp:    metrics_endpoint: http://promethus:8480/insert/0/opentelemetry/api/v1/pushservice:  pipelines:    metrics:      exporters:      - otlphttp      processors:      - k8sattributes      - batch      receivers:      - otlp\t\n\n\n这样我们就可以在 Grafana 中通过 prometheus 查询到数据了。\n有一点需要注意，如果我们自定义的指标最好是参考官方的语义和命名规范来定义这些指标名称。\n\n比如 OpenTelemetry 的规范中名称是用 . 来进行分隔的。\n\n切换为 OpenTelemetry 之后自然就不需要依赖 prometheus 的包，取而代之的是 OTel 的包：\n\ncompileOnly &#x27;io.opentelemetry:opentelemetry-sdk-extension-autoconfigure-spi:1.34.1&#x27;  compileOnly &#x27;io.opentelemetry.instrumentation:opentelemetry-instrumentation-api:1.32.0&#x27;\n总结相对来说 Metrics 的使用比 Trace 简单的多，同时 Metrics 其实也可以和 Trace 进行关联，也就是 Exemplars，限于篇幅就不在本文展开了，感兴趣的可以自行查阅。\n参考链接：\n\nhttps://github.com/apache/pulsar/blob/master/pulsar-client/src/main/java/org/apache/pulsar/client/impl/metrics/InstrumentProvider.java\nhttps://opentelemetry.io/docs/specs/semconv/general/metrics/\nhttps://opentelemetry.io/docs/specs/otel/metrics/data-model/#exemplars\n\n","categories":["OB","OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"如何给开源项目发起提案","url":"/2023/12/21/ob/Pulsar%20Proposal/","content":"背景前段时间在使用 Pulsar 的 admin API 时，发现其中的一个接口响应非常慢：\nadmin.topics().getPartitionedStats(topic);\n使用 curl 拿到的响应结果非常大，同时也非常耗时：\n具体的 issue 在这里：https://github.com/apache/pulsar/issues/21200\n\n\n后面经过分析，是因为某些 topic 的生产者和消费者非常多，导致这个查询 topic 统计的接口数据量非常大。\n但在我这个场景其实是不需要这些生产者和消费者信息的，现在就导致这个 topic 无法查看状态，所以就建议新增两个参数可以过滤这两个字段。\n流程因为涉及到新增 API 了，所以社区维护者就建议我起草一个提案试试：\n什么时候需要提案此时就涉及到什么情况下需要给社区发起一个提案的问题了。在官方的提案指南中有着详细的说明，简单来说就是：\n\n对任何模块新增了 API、或者是重大改动的新特性、监控指标、配置参数时都需要发起提案\n对应的如果只是对现有 bug 的修复、文档等一些可控的变更时，是不需要发起提案的，直接提交 PR 即可。\n\n提案步骤起草首先第一步就是根据官方模版起草一个提案：重点描述背景、目的、详细设计等。并发起一个 PR，如果不确定怎么写的话可以参考已经合并了的提案。\n邮件讨论之后则是将这个 PR 发送到开发组邮箱中，让社区成员参与讨论。\n这一步可能会比较耗时，提案内容可能会被反复修改。\n发起提案的一个重要目的是可以让社区成员进行讨论，评估是否需要这个提案或者是否有其他解决方法。\n发起投票经过讨论，如果提案获得通过后就可以发起投票了，至少需要有三个 binding 通过的投票后这个提案就通过了。\n\n虽然任何人都可以参与投票，但社区只会考虑 PMC 的投票建议；投票的时效性也只有 48h。\n\n\n48 小时候便可以发一个投票结果的邮件，如果达到通过条件便可以通知参与投票的 PMC 合并这个 PR 了。\n实现提案之后就是没啥好说的实现过程，因为通常我们是需要在提案里详细描述实现过程以及涉及到修改的地方。\n总结只要提案被 review 通过后实现起来就非常简单了，跟着提案里的流程实现就好了。\n\n这点非常类似于我们在企业中对某个业务做技术方案，如果大家都按照类似的流程严格审核方案，那实现起来是非常快的，而且可以尽量的减少事后扯皮。\n\n所以最后我的实现 PR 提交之后，都没有任何的修改意见，直接就合并了；也大大降低了审核人员的负担，提高整体效率。\n以上就是我第一次参与 Pulsar 社区的提案过程，我猜测其他社区的流程也是大差不差；其中重点就是异步沟通；大家都认可之后真的会比实时通信的效率高很多。\n具体的提案细节可以阅读官方指南 https://github.com/apache/pulsar/blob/master/pip/README.md\n#Blog #Pulsar \n","categories":["OB"],"tags":["Pulsar"]},{"title":"如何编写一个 Pulsar Broker Interceptor 插件","url":"/2023/12/11/ob/Pulsar-Broker-Interceptor/","content":"背景之前写过一篇文章 VictoriaLogs：一款超低占用的 ElasticSearch 替代方案讲到了我们使用 Victorialogs 来存储 Pulsar 消息队列的消息 trace 信息。\n\n而其中的关键的埋点信息是通过 Pulsar 的 BrokerInterceptor 实现的，后面就有朋友咨询这块代码是否开源，目前是没有开源的，不过借此机会可以聊聊如何实现一个 BrokerInterceptor 插件，当前还没有相关的介绍文档。\n\n\n其实当时我在找 BrokerInterceptor 的相关资料时就发现官方并没有提供对应的开发文档。\n只有一个 additional servlet的开发文档，而 BrokerInterceptor 只在 YouTube 上找到了一个社区分享的视频。\n虽说看视频可以跟着实现，但总归是没有文档方便。\n\n在这之前还是先讲讲 BrokerInterceptor 有什么用？\n其实从它所提供的接口就能看出，在消息到达 Broker 后的一些关键节点都提供了相关的接口，实现这些接口就能做很多事情了，比如我这里所需要的消息追踪。\n创建项目下面开始如何使用 BrokerInterceptor：首先是创建一个 Maven 项目，然后引入相关的依赖：\n&lt;dependency&gt;  &lt;groupId&gt;org.apache.pulsar&lt;/groupId&gt;  &lt;artifactId&gt;pulsar-broker&lt;/artifactId&gt;  &lt;version&gt;$&#123;pulsar.version&#125;&lt;/version&gt;  &lt;scope&gt;provided&lt;/scope&gt;  &lt;/dependency&gt;\n实现接口然后我们便可以实现 org.apache.pulsar.broker.intercept.BrokerInterceptor 来完成具体的业务了。\n在我们做消息追踪的场景下，我们实现了以下几个接口：\n\nmessageProduced\nmessageDispatched\nmessageAcked\n\n以 messageProduced 为例，需要解析出消息ID，然后拼接成一个字符串写入 Victorialogs 存储中，其余的两个埋点也是类似的。\n@Override  public void messageProduced(ServerCnx cnx, Producer producer, long startTimeNs, long ledgerId, long entryId,                              Topic.PublishContext publishContext) &#123;      String ns = getNs(producer.getTopic().getName());      if (!LogSender.checkNamespace(ns)) &#123;          return;      &#125;    String topic = producer.getTopic().getName();      String partition = getPartition(topic);      String msgId = String.format(&quot;%s:%s:%s&quot;, ledgerId, entryId, partition);      String s = new Event.Publish(msgId, producer.getClientAddress(), System.currentTimeMillis(),              producer.getProducerName(), topic).toString();      LogSender.send(s);  &#125;\n\n编写项目描述文件我们需要创建一个项目描述文件，路径如下：src/main/resources/META-INF/services/broker_interceptor.yml名字也是固定的，broker 会在启动的时候读取这个文件，其内容如下：\nname: interceptor-namedescription: descriptioninterceptorClass: com.xx.CustomInterceptor\n重点是填写自定义实现类的全限定名。\n配置打包插件&lt;build&gt;    &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt;    &lt;plugins&gt;      &lt;plugin&gt;        &lt;groupId&gt;org.apache.nifi&lt;/groupId&gt;        &lt;artifactId&gt;nifi-nar-maven-plugin&lt;/artifactId&gt;        &lt;version&gt;1.2.0&lt;/version&gt;        &lt;extensions&gt;true&lt;/extensions&gt;        &lt;configuration&gt;          &lt;finalName&gt;$&#123;project.artifactId&#125;-$&#123;project.version&#125;&lt;/finalName&gt;        &lt;/configuration&gt;        &lt;executions&gt;          &lt;execution&gt;            &lt;id&gt;default-nar&lt;/id&gt;            &lt;phase&gt;package&lt;/phase&gt;            &lt;goals&gt;              &lt;goal&gt;nar&lt;/goal&gt;            &lt;/goals&gt;          &lt;/execution&gt;        &lt;/executions&gt;      &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;\n由于 Broker 识别的是 nar 包，所以我们需要配置 nar 包插件，之后使用 mvn package 就会生成出 nar 包。\n配置 broker.conf我们还需要在 broker.conf 中配置：\nbrokerInterceptors: &quot;interceptor-name&quot;\n也就是刚才配置的插件名称。\n不过需要注意的是，如果你是使用 helm 安装的 pulsar，在 3.1 版本之前需要手动将brokerInterceptors 写入到 broker.conf 中。\nFROM apachepulsar/pulsar-all:3.0.1  COPY target/interceptor-1.0.1.nar /pulsar/interceptors/  RUN echo &quot;\\n&quot; &gt;&gt; /pulsar/conf/broker.conf  RUN echo &quot;brokerInterceptors=&quot; &gt;&gt; /pulsar/conf/broker.conf\n\n不然在最终容器中的 broker.conf 中是读取不到这个配置的，导致插件没有生效。\n\n我们是重新基于官方镜像打的一个包含自定义插件的镜像，最终使用这个镜像进行部署。\n\nhttps://github.com/apache/pulsar/pull/20719我在这个 PR 中已经将配置加入进去了，但得在 3.1 之后才能生效；也就是在 3.1 之前都得加上加上这行：\nRUN echo &quot;\\n&quot; &gt;&gt; /pulsar/conf/broker.conf  RUN echo &quot;brokerInterceptors=&quot; &gt;&gt; /pulsar/conf/broker.conf\n\n\n目前来看 Pulsar 的 BrokerInterceptor 应该使用不多，不然使用 helm 安装时是不可能生效的；而且官方文档也没用相关的描述。\n#Blog #Pulsar \n","categories":["OB"],"tags":["Pulsar"]},{"title":"Pulsar升级自动化：一键搞定集群升级与测试","url":"/2024/08/06/ob/Pulsar%20test%20framework/","content":"\n背景由于我在公司内部负责维护 Pulsar，需要时不时的升级 Pulsar 版本从而和社区保持一致。\n而每次升级过程都需要做相同的步骤：\n\n安装一个新版本的集群\n触发功能性测试\n触发性能测试\n查看监控是否正常\n应用有无异常日志\n流量是否正常\n各个组件的内存占用是否正常\n写入延迟是否正常\n\n\n\n\n\n命令行工具以上的流程步骤最好是全部一键完成，我们只需要人工检测下监控是否正常即可。\n于是我便写了一个命令行工具，执行流程如下：\npulsar-upgrade-cli -h                                                                                                  ok | at 10:33:18 A cli app for upgrading PulsarUsage:  pulsar-upgrade-cli [command]Available Commands:  completion  Generate the autocompletion script for the specified shell  help        Help about any command  install     install a target version  scale       scale statefulSet of the clusterFlags:      --burst-limit int                 client-side default throttling limit (default 100)      --debug                           enable verbose output  -h, --help                            help for pulsar-upgrade-cli      --kube-apiserver string           the address and the port for the Kubernetes API server      --kube-as-group stringArray       group to impersonate for the operation, this flag can be repeated to specify multiple groups.      --kube-as-user string             username to impersonate for the operation\n\n真实使用的 example 如下：\npulsar-upgrade-cli install \\                                                           --values ./charts/pulsar/values.yaml \\        --set namespace=pulsar-test \\        --set initialize=true \\        --debug \\        --test-case-schema=http \\        --test-case-host=127.0.0.1 \\        --test-case-port=9999 \\    pulsar-test ./charts/pulsar -n pulsar-test\n\n它的安装命令非常类似于 helm，也是直接使用 helm 的 value.yaml 进行安装；只是在安装成功后（等待所有的 Pod 都处于 Running 状态）会再触发 test-case 测试，也就是请求一个 endpoint。\n\n这个 endpoint 会在内部处理所有的功能测试和性能测试，具体细节就在后文分析。\n\n同时还提供了一个 scale（扩、缩容） 命令，可以用修改集群规模：\n# 缩容集群规模为0./pulsar-upgrade-cli scale --replicase 0 -n pulsar-test# 缩容为最小集群./pulsar-upgrade-cli scale --replicase 1 -n pulsar-test# 恢复为最满集群./pulsar-upgrade-cli scale --replicase 2 -n pulsar-test\n\n这个需求是因为我们的 Pulsar 测试集群部署在了一个 servless 的 kubernetes 集群里，它是按照使用量收费的，所以在我不需要的使用的时候可以通过这个命令将所有的副本数量修改为 0，从而减少使用成本。\n当只需要做简单的功能测试时便回将集群修改为最小集群，将副本数修改为只可以提供服务即可。\n而当需要做性能测试时就需要将集群修改为最高配置。\n这样可以避免每次都安装新集群，同时也可以有效的减少测试成本。\n实现原理require (      github.com/spf13/cobra v1.6.1      github.com/spf13/pflag v1.0.5       helm.sh/helm/v3 v3.10.2)\n这个命令行工具本质上是参考了 helm 的命令行实现的，所有主要也是依赖了 helm 和 cobra。\n下面以最主要的安装命令为例，核心的是以下的步骤：\n\n执行 helm 安装（这里是直接使用的 helm 的源码逻辑进行安装）\n等待所有的 Pod 成功运行\n触发 test-case 执行\n等待测试用例执行完毕\n检测是否需要卸载安装的集群\n\nfunc (e *installEvent) FinishInstall(cfg *action.Configuration, name string) error &#123;      bar.Increment()      bar.Finish()        clientSet, err := cfg.KubernetesClientSet()      if err != nil &#123;         return err      &#125;      ctx := context.Background()      ip, err := GetServiceExternalIp(ctx, clientSet, settings.Namespace(), fmt.Sprintf(&quot;%s-proxy&quot;, name))      if err != nil &#123;         return err      &#125;        token, err := GetPulsarProxyToken(ctx, clientSet, settings.Namespace(), fmt.Sprintf(&quot;%s-token-proxy-admin&quot;, name))      if err != nil &#123;         return err      &#125;      // trigger testcase      err = e.client.Trigger(context.Background(), ip, token)      return err  &#125;\n\n这里的 FinishInstall 需要获取到新安装的 Pulsar 集群的 proxy IP 地址和鉴权所使用的 token(GetServiceExternalIp()&#x2F;GetPulsarProxyToken())。\n将这两个参数传递给 test-case 才可以构建出 pulsar-client.\n这个命令的核心功能就是安装集群和触发测试，以及一些集群的基本运维能力。\n测试框架而关于这里的测试用例也有一些小伙伴咨询过，如何对 Pulsar 进行功能测试。\n其实 Pulsar 源码中已经包含了几乎所有我们会使用到的测试代码，理论上只要新版本的官方镜像已经推送了那就是跑了所有的单测，质量是可以保证的。\n那为什么还需要做功能测试呢？\n其实很很简单，Pulsar 这类基础组件官方都有提供基准测试，但我们想要用于生产环境依然需要自己做压测得出一份属于自己环境下的性能测试报告；\n根本目的是要看在自己的业务场景下是否可以满足（包括公司的软硬件，不同的业务代码）。\n所以这里的功能测试代码有一个很重要的前提就是：需要使用真实的业务代码进行测试。\n也就是业务在线上使用与 Pulsar 相关的代码需要参考功能测试里的代码实现，不然有些问题就无法在测试环节覆盖到。\n\n这里我就踩过坑，因为在功能测试里用的是官方的 example 代码进行测试的，自然是没有问题；但业务在实际使用时，使用到了一个 Schema 的场景，并没有在功能测试里覆盖到（官方的测试用例里也没有😂），就导致升级到某个版本后业务功能无法正常使用（虽然用法确实是有问题），但应该在我测试阶段就暴露出来。\n\n实现原理以上是一个集群的功能测试报告，这里我只有 8 个测试场景（结合实际业务使用），考虑到未来可能会有新的测试用例，所以在设计这个测试框架时就得考虑到扩展性。\nAbstractJobDefine job5 =          new FailoverConsumerTest(event, &quot;故障转移消费测试&quot;, pulsarClient, 20, admin);  CompletableFuture&lt;Void&gt; c5 = CompletableFuture.runAsync(job5::start, EXECUTOR);  AbstractJobDefine job6 = new SchemaTest(event,&quot;schema测试&quot;,pulsarClient,20,prestoService);  CompletableFuture&lt;Void&gt; c6 = CompletableFuture.runAsync(job6::start, EXECUTOR);  AbstractJobDefine job7 = new VlogsTest(event,&quot;vlogs test&quot;,pulsarClient,20, vlogsUrl);  CompletableFuture&lt;Void&gt; c7 = CompletableFuture.runAsync(job7::start, EXECUTOR);    CompletableFuture&lt;Void&gt; all = CompletableFuture.allOf(c1, c2, c3, c4, c5, c6, c7);  all.whenComplete((___, __) -&gt; &#123;      event.finishAll();      pulsarClient.closeAsync();      admin.close();  &#125;).get();\n\n对外提供的 trigger 接口就不贴代码了，重点就是在这里构建测试任务，然后等待他们全部执行完毕。\n@Datapublic abstract class AbstractJobDefine &#123;    private Event event;    private String jobName;    private PulsarClient pulsarClient;    private int timeout;    private PulsarAdmin admin;    public AbstractJobDefine(Event event, String jobName, PulsarClient pulsarClient, int timeout, PulsarAdmin admin) &#123;        this.event = event;        this.jobName = jobName;        this.pulsarClient = pulsarClient;        this.timeout = timeout;        this.admin = admin;    &#125;    public void start() &#123;        event.addJob();        try &#123;            CompletableFuture.runAsync(() -&gt; &#123;                StopWatch watch = new StopWatch();                try &#123;                    watch.start(jobName);                    run(pulsarClient, admin);                &#125; catch (Exception e) &#123;                    event.oneException(this, e);                &#125; finally &#123;                    watch.stop();                    event.finishOne(jobName, StrUtil.format(&quot;cost: &#123;&#125;s&quot;, watch.getTotalTimeSeconds()));                &#125;            &#125;, TestCase.EXECUTOR).get(timeout, TimeUnit.SECONDS);        &#125; catch (Exception e) &#123;            event.oneException(this, e);        &#125;    &#125;    /** run busy code     * @param pulsarClient pulsar client     * @param admin pulsar admin client     * @throws Exception e     */    public abstract void run(PulsarClient pulsarClient, PulsarAdmin admin) throws Exception;&#125;\n\n\n核心代码就是这个抽象的任务定义类，其中的 start 函数用于定义任务执行的模版：\n\n添加任务：具体实现是任务计数器+1\n开始计时\n执行抽血的 run 函数，具体实现交给子类\n异常时记录事件\n正常执行完毕后也记录事件\n\n下面来看一个普通用例的实现情况：\n就是重写了 run() 函数，然后在其中实现具体的测试用例，断言测试结果。\n这样当我们需要再添加用例的时候只需要再新增一个子类实现即可。\n同时还需要定义一个事件接口，用于处理一些关键的节点：\npublic interface Event &#123;        /**       * 新增一个任务       */      void addJob();        /** 获取运行中的任务数量       * @return 获取运行中的任务数量       */      TestCaseRuntimeResponse getRuntime();        /**       * 单个任务执行完毕       *       * @param jobName    任务名称       * @param finishCost 任务完成耗时       */      void finishOne(String jobName, String finishCost);        /**单个任务执行异常       * @param jobDefine 任务       * @param e 异常       */      void oneException(AbstractJobDefine jobDefine, Exception e);        /**       * 所有任务执行完毕       */      void finishAll();  &#125;\n\n其中 getRuntime 接口是用于在 cli 那边查询任务是否执行完毕的接口，只有任务执行完毕之后才能退出 cli。\n监控指标当这些任务运行完毕后我们需要重点查看应用客户端和 Pulsar broker 端是否有异常日志。\n同时还需要观察一些关键的监控面板：\n\n包含但不限于：\n\n消息吞吐量\nbroker 写入延迟\nBookkeeper 的写入、读取成功率，以及延迟。\n\n当然还有 zookeeper 的运行情况也需要监控，限于篇幅就不一一粘贴了。\n以上就是测试整个 Pulsar 集群的流程，当然还有一些需要优化的地方。\n比如使用命令行还是有些不便，后续可能会切换到网页上就可以操作。\n","categories":["OB"],"tags":["Pulsar"]},{"title":"从 Pulsar Client 的原理到它的监控面板","url":"/2023/08/03/ob/Pulsar-Client/","content":"\n#Blog #Pulsar \n背景前段时间业务团队偶尔会碰到一些 Pulsar 使用的问题，比如消息阻塞不消费了、生产者消息发送缓慢等各种问题。\n虽然我们有个监控页面可以根据 topic 维度查看他的发送状态，比如速率、流量、消费状态等信息。\n\n\n\n但也有几个问题：\n\n无法在应用维度查看他所依赖的所有  topic 的各种状态。\n监控的信息还不够，比如发送&#x2F;消费延迟、发送&#x2F;消费失败等数据。\n\n总之就是缺少一个全局的监控视角，通过这些指标可以很方便的分析出当时的运行情况。\n基于这个需求经过一段时间的折腾，现在已经上线使用几个月，目前比较稳定，效果图如下：\n现在就可以在每个应用的监控面板里看到自己使用了哪些 topic，分别的生产消费情况如何。\n核心流程要实现这些功能就得在应用的 metrics 中加入相关的监控信息，但官方的 Java client 是没有暴露出这些指标的。\n\n\n但 pulsar-client-go 是自带了这些指标的\n\n由于 SDK 不支持所以只能自己想办法实现了，为此其实有两种实现方案：\n\n魔改 Java client，在需要监控的地方手动埋点指标。\n由于我们使用了 SkyWalking，所以可以编写插件，以 agent 的方式获取数据、埋点指标。\n\n不过第一种方案有以下一些问题：\n\n需要自己维护一个代码分支，还需要定期和官方保持一致，难免会出现代码冲突。\n需要推动业务方进行依赖升级，线上有着几百个应用，推动起来时间太慢。\n\n第二种方案的好处就不言而喻了：\n\n升级无感知，只需要在我们的基础镜像中加上插件即可。\nJava client 的版本也更容易统一。\n\nClient 原理但其实不管是哪种方案我们都得熟悉 Java Client 的实现原理，才能知道哪些数据是我们需要重点关注的，可以帮助我们更好的定位问题。\n\n\n本文重点不在于此，具体代码就不仔细分析了。\n\n从上图可以看出，如果我们想要监控消费是否存在阻塞的情况，这几个内部队列是需要重点监控的，一旦他们出现堆积，那就会出现消费阻塞。\n其实这些数据都可以通过\norg.apache.pulsar.client.api.ProducerStatsorg.apache.pulsar.client.api.ConsumerStats\n这两个接口获取到生产者和消费者的大部分指标，只是这里还有一个小插曲。\n那就是在获取消费者队列大小的时候，获取到的数据一直为空。\n最终经过源码排查，原来是我们大量使用的 messageListener 在获取队列大小时有 bug，导致获取到的数据一直都为 0.\n相关的 issue 和 PR 可以在这两个链接查看，问题原因和修复过程都有具体描述：https://github.com/apache/pulsar/issues/20076https://github.com/apache/pulsar/pull/20245\n\n但这个修复得在新版本才能使用，就导致我们现在的监控页面一直显示为空。\n\n开发 SkyWalking 插件然后就是开发一个 SkyWalking 的插件了，其实直接使用 SW 开发插件是上手 Java-Agent 比较快的方式。\nSW 的 SDK 封装了许多 agent 原生接口，使得开发起来非常容易；当然缺点也有，就是得集成整个 SW 的 agent。\n这里我简单介绍下这个插件的运行流程：\n\n在创建和删除 consumer 的时候维护 consumerPool\n启动一个定时任务，定期从这些 consumer 中获取指标数据。\n\n\n当消费多分区 topic 时，为了能唯一标志一个 consumer，所以给每个消费者都加了一个 hashcode 的 label。\n\n因为我们所有的 Java 技术栈都是使用的 Prometheus 的包来生成 metrics ，所以该插件也是使用该包生成的数据。\n&lt;dependency&gt;    &lt;groupId&gt;io.prometheus&lt;/groupId&gt;    &lt;artifactId&gt;simpleclient&lt;/artifactId&gt;    &lt;version&gt;0.12.0&lt;/version&gt;    &lt;scope&gt;provided&lt;/scope&gt;  &lt;/dependency&gt;\n\n为了兼容一些特殊 Java 应用没有该包时会启动报错，所以在初始化插件的时候需要检测当前 classpath 下是否存在该依赖。\n\n这些功能 SW 已经封装好了，对我们来说也是开箱即用。\n\n其实 SW 插件自己也是支持 metrics 的，由于我们只是使用了它的 trace 功能，所以这里就没有使用它的 API。\n\n关于开发一个 SW 插件的流程也比较简单，可以参考官方文档或者是一些现成的插件源码。https://skywalking.apache.org/docs/skywalking-java/next/en/setup/service-agent/java-agent/java-plugin-development-guide/\n总结有了这个监控面板后，对于 Pulsar 客户端内部的一些运行情况就不再是黑盒了，还可以基于此做一些报警，比如消费堆积、发送延迟过大等。\n当然仅仅只有这个面板依然是不够的，后续我们又开发了可以通过 messageId 查询它的整个生命周期，包括：\n\n生产者、消费者信息\n消息生产时间\n推送时间\nack 时间等\n\n\n同时借助与 Pulsar-SQL 的能力，还能以列表的形式展示当前 topic 的消息列表。当然在实现这两个功能的同时也踩了不少坑，提了几个 PR ，后面在抽时间做具体的分享。\n","categories":["Pulsar"],"tags":["Metrics"]},{"title":"请注意，你的 Pulsar 集群可能有删除数据的风险","url":"/2024/01/09/ob/Pulsar-Delete-Topic/","content":"在上一篇 Pulsar3.0新功能介绍中提到，在升级到 3.0 的过程中碰到一个致命的问题，就是升级之后 topic 被删除了。\n正好最近社区也补充了相关细节，本次也接着这个机会再次复盘一下，毕竟这是一个非常致命的 Bug。\n\n\n\n现象先来回顾下当时的情况：升级当晚没有出现啥问题，各个流量指标、生产者、消费者数量都是在正常范围内波动。\n\n事后才知道，因为只是删除了很少一部分的 topic，所以从监控中反应不出来。\n\n早上上班后陆续有部分业务反馈应用连不上 topic，提示 topic nof found.\norg.springframework.beans.factory.BeanCreationException: Error creating bean with name &#x27;Producer&#x27;: Invocation of init method failed; nested exception is org.apache.pulsar.client.api.PulsarClientException$TopicDoesNotExistException: Topic Not Found.\n\n因为只是部分应用在反馈，所以起初怀疑是 broker 升级之后导致老版本的 pulsar-client 存在兼容性问题。\n所以我就拿了平时测试用的 topic 再配合多个老版本的 sdk 进行测试，发现没有问题。\n\n直到这一步还好，至少证明是小范故障。\n\n因为提示的是 topic 不存在，所以就准备查一下 topic 的元数据是否正常。\n查询后发现元数据是存在的。\n之后我便想看看提示了 topic 不存在的 topic 的归属，然后再看看那个 broker 中是否有异常日志。\n发现查看归属的接口也是提示 topic 不存在，此时我便怀疑是 topic 的负载出现了问题，导致这些 topic 没有绑定到具体的 broker。\n于是便重启了 broker，结果依然没有解决问题。之后我们查询了 topic 的 internal state 发现元数据中会少一个分区。\n紧急恢复我们尝试将这个分区数恢复后，发现这个 topic 就可以正常连接了。\n于是再挑选了几个异常的 topic 发现都是同样的问题，恢复分区数之后也可以正常连接了。\n所以我写了一个工具遍历了所有的 topic，检测分区数是否正常，不正常时便修复。\nvoid checkPartition() &#123;      String namespace = &quot;tenant/ns&quot;;      List&lt;String&gt; topicList = pulsarAdmin.topics().getPartitionedTopicList(namespace);      for (String topic : topicList) &#123;          PartitionedTopicStats stats = pulsarAdmin.topics().getPartitionedStats(topic, true);          int partitions = stats.getMetadata().partitions;          int size = stats.getPartitions().size();          if (partitions != size) &#123;              log.info(&quot;topic=&#123;&#125;,partitions=&#123;&#125;,size=&#123;&#125;&quot;, topic, partitions, size);              pulsarAdmin.topics().updatePartitionedTopic(topic, partitions);          &#125;    \t&#125;&#125;\n\n排查修复好所有 topic 之后便开始排查根因，因为看到的是元数据不一致所以怀疑是 zk 里的数据和 broker 内存中的数据不同导致的这个问题。\n但我们查看了 zookeeper 中的数据发现一切又是正常的，所以只能转变思路。\n之后我们通过有问题的 topic 在日志中找到了一个关键日志：以及具体的堆栈。\n此时具体的原因已经很明显了，元数据这些自然是没问题；根本原因是 topic 被删除了，但被删除的 topic 只是某个分区，所以我们在查询 internalState 时才发发现少一个 topic。\n通过这个删除日志定位到具体的删除代码：\norg.apache.pulsar.broker.service.persistent.PersistentTopic#checkReplication\n原来是这里的  configuredClusters 值为空才导致的 topic 调用了 deleteForcefully()被删除。\n而这个值是从 topic 的 Policy 中获取的。\n复现问题通过上图中的堆栈跟踪，怀疑是重启  broker 导致的 topic unload ，同时 broker 又在构建 topic 导致了对 topicPolicy 的读写。\n最终导致 topicPolicy 为空。\n只要写个单测可以复现这个问题就好办了：\n@Testpublic void testCreateTopicAndUpdatePolicyConcurrent() throws Exception &#123;    final int topicNum = 100;    final int partition = 10;    // (1) Init topic    admin.namespaces().createNamespace(&quot;public/retention&quot;);    final String topicName = &quot;persistent://public/retention/policy_with_broker_restart&quot;;    for (int i = 0; i &lt; topicNum; i++) &#123;        final String shadowTopicNames = topicName + &quot;_&quot; + i;        admin.topics().createPartitionedTopic(shadowTopicNames, partition);    &#125;    // (2) Set Policy    for (int i = 90; i &lt; 100; i++) &#123;        final String shadowTopicNames = topicName + &quot;_&quot; + i;        CompletableFuture.runAsync(() -&gt; &#123;            while (true) &#123;                PublishRate publishRate = new PublishRate();                publishRate.publishThrottlingRateInMsg = 100;                try &#123;                    admin.topicPolicies().setPublishRate(shadowTopicNames, publishRate);                &#125; catch (PulsarAdminException e) &#123;                &#125;            &#125;        &#125;);    &#125;    for (int i = 90; i &lt; 100; i++) &#123;        final String shadowTopicNames = topicName + &quot;_&quot; + i;        CompletableFuture.runAsync(() -&gt; &#123;            while (true) &#123;                try &#123;                    admin.lookups().lookupPartitionedTopic(shadowTopicNames);                &#125; catch (Exception e) &#123;                &#125;            &#125;        &#125;);    &#125;    admin.namespaces().unload(&quot;public/retention&quot;);    admin.namespaces().unload(&quot;public/retention&quot;);    admin.namespaces().unload(&quot;public/retention&quot;);    Thread.sleep(1000* 5);    for (int i = 0; i &lt; topicNum; i++) &#123;        final String shadowTopicNames = topicName + &quot;_&quot; + i;        log.info(&quot;check topic: &#123;&#125;&quot;, shadowTopicNames);        PartitionedTopicStats partitionedStats = admin.topics().getPartitionedStats(shadowTopicNames, true);        Assert.assertEquals(partitionedStats.getPartitions().size(), partition);    &#125;&#125;\n\n同时还得查询元数据有耗时才能复现：\n\n只能手动 sleep 模拟这个耗时\n\n具体也可以参考这个 issuehttps://github.com/apache/pulsar/issues/21653#issuecomment-1842962452\n此时就会发现有 topic 被删除了，而且是随机删除的，因为出现并发的几率本身也是随机的。\n\n这里画了一个流程图就比较清晰了，在 broker 重启的时候会有两个线程同时topicPolicy 进行操作。\n在 thread3 读取 topicPolicy 进行判断时，thread2 可能还没有把数据准备好，所以就导致了 topic 被删除。\n修复既然知道了问题原因就好修复了，我们只需要把 thread3 和 thread2 修改为串行执行就好了。\n\n这也是处理并发最简单高效的方法，就是直接避免并发；加锁、队列啥的虽然也可以解决，但代码复杂度也高了很多，所以能不并发就尽量不要并发。\n\n但要把这个修复推送到社区上游主分支最好是要加上单测，这样即便是后续有其他的改动也能保证这个 bug 不会再次出现。\n之后在社区大佬的帮助下完善了单测，最终合并了这个修复。\n\n再次证明写单测往往比代码更复杂，也更花费时间。\n\nPR：https://github.com/apache/pulsar/pull/21704\n使用修复镜像因为社区合并代码再发版的周期较长，而我们又急于修复该问题；不然都不敢重启 broker，因为每重启一次都可能会导致不知道哪个 topic 就被删除了。\n所以我们自己在本地构建了一个修复的镜像，准备在线上进行替换。\n此时坑又来了，我们满怀信心的替换了一个镜像再观察日志发现居然还有删除的日志😱。\n冷静下来一分析，原来是当前替换进行的 broker 没有问题了，但它处理的 topic 被转移到了其他 broker 中，而其他的 broker 并没有替换为我们最新的镜像。\n所以导致 topic 在其他 broker 中依然被删除了。\n\n除非我们停机，将所有的镜像都替换之后再一起重启。\n\n但这样的成本太高了，最好是可以平滑发布。\n最终我们想到一个办法，使用 arthas 去关闭了一个 broker 的一个选项，之后就不会执行出现 bug 的那段代码了。\n\ncurl -O https://arthas.aliyun.com/arthas-boot.jar &amp;&amp; java -jar arthas-boot.jar 1 -c &quot;vmtool -x 3 --action getInstances --className org.apache.pulsar.broker.ServiceConfiguration  --express &#x27;instances[0].setTopicLevelPoliciesEnabled(false)&#x27;&quot;\n\n我也将操作方法贴到了对于 issue 的评论区。https://github.com/apache/pulsar/issues/21653#issuecomment-1857548997如果不幸碰到了这个 bug，可以参考修复。\n总结删除的这些 topic 的同时它的订阅者也被删除了，所以我们还需要修复订阅者：\nString topicName = &quot;persistent://tenant/ns/topicName&quot;;  pulsarTopicService.createSubscription(topicName, &quot;subName&quot;, MessageId.latest);\n之所以说这个 bug 非常致命，是因为这样会导致 topic 的数据丢失，同时这些 topic 上的数据也会被删除。\n\n后续 https://github.com/apache/pulsar/pull/21704#issuecomment-1878315926社区也补充了一些场景。\n\n\n其实场景 2 更容易出现复现，毕竟更容易出现并发；也就是我们碰到的场景\n\n说来也奇怪，结合社区的 issue 和其他大佬的反馈，这个问题只有我们碰到了，估计也是这个问题的触发条件也比较苛刻：\n\n开启 systemTopic/topicLevelPolices  systemTopicEnabled: “true”  topicLevelPoliciesEnabled: “true”\n设置足够多的 topicPolicies\n重启 broker\n重启过程中从 zk 中获取数据出现耗时\n\n\n\n符合以上条件的集群就需要注意了。\n其实这个问题在这个 PR 就已经引入了https://github.com/apache/pulsar/pull/11021\n所以已经存在蛮久了，后续我们也将检测元数据作为升级流程之一了，确保升级后数据依然是完整的。\n相关的 issue 和 PR：https://github.com/apache/pulsar/issues/21653https://github.com/apache/pulsar/pull/21704\n#Blog #Pulsar \n","categories":["OB"],"tags":["Pulsar"]},{"title":"升级到 Pulsar3.0 后深入了解 JWT 鉴权","url":"/2023/11/19/ob/Pulsar-JWT/","content":"\n背景最近在测试将 Pulsar 2.11.2 升级到 3.0.1的过程中碰到一个鉴权问题，正好借着这个问题充分了解下 Pulsar 的鉴权机制是如何运转的。\n\nPulsar 支持 Namespace/Topic 级别的鉴权，在生产环境中往往会使用 topic 级别的鉴权，从而防止消息泄露或者其他因为权限管控不严格而导致的问题。\n\n我们会在创建 topic 的时候为 topic 绑定一个应用，这样就只能由这个应用发送消息，其他的应用尝试发送消息的时候会遇到 401 鉴权的异常。\n\n同理，对于订阅者也可以关联指定的应用，从而使得只有规定的应用可以消费消息。\n\n鉴权流程以上的两个功能本质上都是通过 Pulsar 的 admin-API 实现的。\n这里关键的就是 role，在我们的场景下通常是一个应用的 AppId，只要是一个和项目唯一绑定的 ID 即可。\n这只是授权的一步，整个鉴权流程图如下：\n详细步骤生成公私钥bin/pulsar tokens create-key-pair --output-private-key my-private.key --output-public-key my-public.key\n将公钥分发到 broker 的节点上，鉴权的时候 broker 会使用公钥进行验证。\n而私钥通常是管理员单独保存起来用于在后续的步骤为客户端生成 token\n使用私钥生成 token之后我们便可以使用这个私钥生成 token 了：\nbin/pulsar tokens create --private-key file:///path/to/my-private.key \\            --subject 123456eyJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJhZG1pbiJ9\n\n其中的 subject 和本文长提到的 role 相等\n\n使用 subject 授权只是单纯生成了 token 其实并没有什么作用，还得将 subject(role) 与 topic 进行授权绑定。\n也就是上图的这个步骤。\n\n这里创建的 admin 客户端也得使用一个 superRole 角色的 token 才有权限进行授权。 superRole 使用在  broker.conf 中进行配置。\n\n客户端使用 token 接入 brokerPulsarClient client = PulsarClient.builder()    .serviceUrl(&quot;pulsar://broker.example.com:6650/&quot;)    .authentication(AuthenticationFactory.token(&quot;eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJKb2UifQ.ipevRNuRP6HflG8cFKnmUPtypruRC4fb1DWtoLL62SY&quot;)）    .build();\n使用刚才私钥生成的 token 接入 broker 才能生产或者消费数据。\noriginalPrincipal cannot be a proxy role这些流程正常都没啥问题，但直到我升级了 Pulsar3.0 后客户端直接就连不上了。\n在 broker 中看到了 WARN 的警告日志：\ncannot specify originalPrincipal when connecting without valid proxy role\n之后在 3.0 的升级日志中看到相关的 Issue。\n从这个 PR 相关的代码和变更的文档可以得知：\n升级到 3.0 之后风险校验等级提高了，proxyRole 这个字段需要在 broker 中进行指定（之前的版本不需要强制填写）。\n因为我们使用了 Proxy 组件，所有的请求都需要从 proxy 中转一次，这个 proxyRole 是为了告诉 broker：只有使用了 proxyRole 作为 token 的 Proxy 才能访问 broker，这样保证了 broker 的安全。\nsuperUserRoles: broker-admin,admin,proxy-admin proxyRoles: proxy-admin\n以上是我的配置，我的 Proxy 配置的也是 proxy-admin 这个 token，所以理论上是没有问题的，但依然鉴权失败了，查看 broker 的日志后拿到以下日志：\nIllegal combination of role [proxy-admin] and originalPrincipal [proxy-admin]: originalPrincipal cannot be a proxy role.\n排查了许久依然没有太多头绪，所以我提了相关的 issue:https://github.com/apache/pulsar/issues/21583之后我咨询了 Pulsar 的 PMC @Technoboy  在他的提示下发现我在测试的时候使用的是 proxy-admin，正好和 proxyRoles 相等。阅读源码和这个 PR 的 comment 之后得知：也就是说客户端不能使用和 proxyRole 相同的角色进行连接，这个角色应当也只能给 Proxy 使用，这样的安全性才会高。\n所以这个 Comment 还在讨论这是一个 breaking change? 还是一个增强补丁。因为合并这个 PR 后对没有使用 proxyRole 的客户端将无法连接，同时也可能出现我这种 proxyRole 就是客户端使用的角色，这种情况也会鉴权失败。\n所以我换了一个 superRole 角色就可以了，比如换成了 admin。\n\n但其实即便是放到我们的生产系统，只要配置了 proxyRole 也不会有问题，因为我们应用所使用的 role 都是不这里的 superUserRole，全部都是使用 AppId 生成的。\n\ntoken 不一致但也有一个疑惑，我在换为存放在 configmap 中的 admin token 之前(测试环境使用的是 helm 安装集群，所以这些 token 都是存放在 configmap 中的)，\n为了验证是否只要非 proxyRole 的 superRole 都可以使用，我就自己使用了私钥重新生成了一个 admin 的 token。\nbin/pulsar tokens create --private-key file:///pulsar/private/private.key --subject admin\n这样生成的 token 也是可以使用的，但是我将 token 复制出来之后却发现 helm 生成的 token 与我用 pulsar 命令行生成的 token 并不相同。\n为了搞清楚为什么 token 不同但鉴权依然可以通过的原因，之后我将 token decode之后知道了原因：原来是 Header 不同从而导致最终的 token 不同，helm 生成的 token 中多了一个 typ 字段。\n\n之后我检查了 helm 安装的流程，发现原来 helm 的脚本中使用的并不是 Java 的命令行工具：\n$&#123;PULSARCTL_BIN&#125; token create -a RS256 --private-key-file $&#123;privatekeytmpfile&#125; --subject $&#123;role&#125; 2&amp;&gt; $&#123;tokentmpfile&#125;\n\n这个 PULSARCTL_BIN 是一个由 Go 写的命令行工具，我查看了其中的源码，才知道 Go 的 JWT 工具会自带一个 header。https://github.com/streamnative/pulsarctl\n而 Java 是没有这个逻辑的，但也只是加了 header，payload 的值都是相同的。这样也就解释了为什么 token 不同但确依然能使用的原因。\n#Blog #Pulsar \n","categories":["OB"],"tags":["Pulsar"]},{"title":"使用 SQL 的方式查询消息队列数据以及踩坑指南","url":"/2023/08/30/ob/Pulsar-SQL/","content":"\n背景为了让业务团队可以更好的跟踪自己消息的生产和消费状态，需要一个类似于表格视图的消息列表，用户可以直观的看到发送的消息；同时点击详情后也能查到消息的整个轨迹。\n\n 消息列表\n\n\n\n\n点击详情后查看轨迹\n\n原理介绍由于 Pulsar 并没有关系型数据库中表的概念，所有的数据都是存储在 Bookkeeper 中，为了模拟使用 SQL 查询的效果 Pulsar 提供了 Presto (现在已经更名为 Trino)的插件。\n\nTrino 是一个分布式的 SQL 查询引擎，它也提供了插件能力，如果我们想通过 SQL 从自定义数据源查询数据时，基于它的 SPI 编写一个插件是很方便的。\n\n这样便可以类似于查询数据库一样查询 Pulsar 数据：\n\nPulsar 插件的运行流程如上图所示：\n\n启动的时候通过 Pulsar-Admin 接口获取一些元数据，比如 Scheme，topic 分区信息等。\n然后会创建一个只读的 Bookkeeper 客户端，用于获取数据。\n之后根据 SQL 条件过滤数据即可。\n\n相关代码：\n使用 Pulsar-SQL\n使用起来也很简单，官方提供了两个命令：\n\nsql-worker: 会启动一个 trino 服务端同时运行了 Pulsar 插件\nsql: 就是一个 SQL 命令行终端。\n\n遇到的问题自己在本地运行的时候自然是没问题，可是一旦想在生产运行，同时如果你的 Pulsar 集群是运行再 k8s 环境中时就会碰到一些问题。\n无法使用现有 Trino 集群首先第一个问题是如果生产环境已经有了一个 Trino 集群想要复用的时候就会碰到问题，常规流程是将 Pulsar 的插件复制到 Trino 的 Plugin 目录，然后重启 Trino 后就能使用该插件。\n当然社区也是支持这么做的：但是当我将 Pulsar-plugin 复制到 Trino 中运行的时候却失败了，整体的流程可以参考这个 issue：https://github.com/apache/pulsar/discussions/20941\n简单来说 Trino 的官方镜像和 pulsar-plugin 并不能兼容，这个问题直接影响到我们是否可以在生产环境使用它。\n但是手动编译出来的 Trino 服务和插件是兼容的，可以直接运行。\n\n因此我只能在本地编译出 Trino 服务端和 pulsar-plugin 然后打包成一个镜像来运行了，当然这样的坏处就是无法利用到我们现有的 Trino 集群，又得重新部署一个了。\n\n流程也比较麻烦：\n\n首先是本地编译 Pulsar-SQL 模块\n将生成物复制到当前目录\n执行 make docker 打出 docker 镜像并上传到私服\n再执行 kubectl 将 trino 部署到 k8s 环境中\n\n整个流程做下来加上和社区的沟通，更加确定这个功能应该是很少有人在生产环境使用的，毕竟第一个坑就很麻烦，更别提后续的问题了😂。\nPresto 插件不支持 AuthToken第二个问题也是个深坑，当我把 Trino 部署好查询数据的时候直接抛了一个调用 pulsar-admin  接口连接超时的异常。\n结果排查了半天发现原来是 pulsar-plugin 里没有提供 JWT 的验证方式，而我们的 Pulsar 集群恰好是打开了 JWT 验证的。\n为此我只能先在本地修复了这个问题，同时也提交了 PR，预计会在下一个大版本合并吧：https://github.com/apache/pulsar/pull/20860\n新创建的 topic 查询失败第二个问题是当查询一个新创建的 topic 时，客户端会直接 block，相关的复现流程在这里：https://github.com/apache/pulsar/issues/20910\n\n这个问题还好，不是很致命，是我在本地测试的时候无意间发现的。\n本地我已经修复了，后面也提交了一个 PR，目前还在讨论中：https://github.com/apache/pulsar/pull/20911\n查询消息会丢失最后一条这个问题也不是很严重，数据量少的时候会发现，就是在指定了消息发送时间的查询条件时，最后一条消息会被过滤掉，相关 issue 在这里：https://github.com/apache/pulsar/issues/20919这个我只是定位到了原因，但不太清楚 为什么要这么做(-1)，影响也不是很大，就放在这里搁置了。\nSchema 不兼容最后发现的一个问题是我们线上某些 topic 查询数据的时候会抛出 Not a record: &quot;string&quot;的异常，但只是部分 topic，也排查了很久，整个源码中没有任何一个地方有这个异常。\nhttps://github.com/apache/pulsar/issues/20945\n\n根本原因是生产者生成的 schema 有问题，类型已经是 JSON 了，但是 schema 却是 string，这样导致 pulsar-plugin  在反序列化 schema 的时候抛出了异常，由于是 pb 反序列化抛出的异常，所以源码中都搜索不到。\n\n没有问题的 topic 使用了正确的 schema\n\n后续我也在本地修复了这个问题，当抛出异常后就将 schema 降级为基本类型进行解析。\n不过本质问题还是客户端使用有误，如果对 schema 理解不准确的话还是建议使用 byte[] 吧，这样至少兼容性不会有问题。相关 PR：https://github.com/apache/pulsar/pull/20955\n总结Pulsar-SQL 是一个非常有用的功能，只是我们使用过程中确实发现了一些问题，大部分都已经修复了；希望对后续使用该功能的朋友有所帮助。#Pulsar \n","categories":["Pulsar"],"tags":["SQL"]},{"title":"分布式系统如何做负载均衡","url":"/2024/07/15/ob/Pulsar-loadbalance/","content":"背景Pulsar 有提供一个查询 Broker 负载的接口：\n    /**     * Get load for this broker.     *     * @return     * @throws PulsarAdminException     */LoadManagerReport getLoadReport() throws PulsarAdminException;public interface LoadManagerReport extends ServiceLookupData &#123;        ResourceUsage getCpu();        ResourceUsage getMemory();        ResourceUsage getDirectMemory();        ResourceUsage getBandwidthIn();        ResourceUsage getBandwidthOut();&#125;\n\n可以返回一些 broker 的负载数据，比如 CPU、内存、流量之类的数据。\n\n\n\n我目前碰到的问题是目前会遇到部分节点的负债不平衡，导致资源占用不均衡，所以想要手动查询所有节点的负载数据，然后人工进行负载。\n\n理论上这些数据是在运行时实时计算的数据，如果对于单机的倒还好说，每次请求这个接口直接实时计算一次就可以了。\n但对于集群的服务来说会有多个节点，目前 Pulsar 提供的这个接口只能查询指定节点的负载数据，也就是说每次得传入目标节点的 IP 和端口。\n\n所以我的预期是可以提供一个查询所有节点负载的接口，已经提了 issue，最近准备写 Purpose 把这个需求解决了。\n实现这个需求的方案有两种：\n\n拿到所有 broker 也就是服务节点信息，依次遍历调用接口，然后自己组装信息。\n从 zookeeper 中获取负载信息。\n\n理论上第二种更好，第一种实现虽然更简单，但每次都发起一次 http 请求，多少有些浪费。\n第二种方案直接从源头获取负载信息，只需要请求一次就可以了。\n而正好社区提供了一个命令行工具可以直接打印所有的 broker 负载数据：\npulsar-perf monitor-brokers --connect-string &lt;zookeeper host:port&gt;\n\n\n分布式系统常用组件提供的命令行工具其实就是直接从 zookeeper 中查询的数据。\n在分布式系统中需要一个集中的组件来管理各种数据，比如：\n\n可以利用该组件来选举 leader 节点\n使用该组件来做分布式锁\n为分布式系统同步数据\n统一的存放和读取某些数据\n\n可以提供该功能的组件其实也不少：\n\nzookeeper\netcd\noxia\n\nZookeeper 是老牌的分布式协调组件，可以做 leader 选举、配置中心、分布式锁、服务注册与发现等功能。\n在许多中间件和系统中都有应用，比如：\n\nApache Pulsar 中作为协调中心\nKafka 中也有类似的作用。\n在 Dubbo 中作为服务注册发现组件。\n\n\netcd 的功能与 zookeeper 类似，可以用作服务注册发现，也可以作为 Key Value 键值对存储系统；在 kubernetes 中扮演了巨大作用，经历了各种考验，稳定性已经非常可靠了。\n\nOxia 则是 StreamNative 开发的一个用于替换 Zookeeper 的中间件，功能也与 Zookeeper 类似；目前已经可以在 Pulsar 中替换 Zookeeper，只是还没有大规模的使用。\nPulsar 中的应用下面以 Pulsar 为例（使用 zookeeper），看看在这类大型分布式系统中是如何处理负载均衡的。\n再开始之前先明确下负载均衡大体上会做哪些事情。\n\n首先上报自己节点的负载数据\nLeader 节点需要定时收集所有节点的负载数据。\n这些负载数据中包括：\nCPU、堆内存、堆外内存等通用数据的使用量\n流出、流入流量\n一些系统特有的数据，比如在 Pulsar 中就是：\n每个 broker 中的 topic、consumer、producer、bundle 等数据。\n\n\n\n\n\n\n再由 leader 节点读取到这些数据后选择负载较高的节点，将数据迁移到负载较低的节点。\n\n以上就是一个完整的负载均衡的流程，下面我们依次看看在 Pulsar 中是如何实现这些逻辑的。\n在 Pulsar 中提供了多种负载均衡策略，以下是加载负载均衡器的逻辑：\nstatic LoadManager create(final PulsarService pulsar) &#123;      try &#123;          final ServiceConfiguration conf = pulsar.getConfiguration();          // Assume there is a constructor with one argument of PulsarService.          final Object loadManagerInstance = Reflections.createInstance(conf.getLoadManagerClassName(),                  Thread.currentThread().getContextClassLoader());          if (loadManagerInstance instanceof LoadManager) &#123;              final LoadManager casted = (LoadManager) loadManagerInstance;              casted.initialize(pulsar);              return casted;          &#125; else if (loadManagerInstance instanceof ModularLoadManager) &#123;              final LoadManager casted = new ModularLoadManagerWrapper((ModularLoadManager) loadManagerInstance);              casted.initialize(pulsar);              return casted;          &#125;      &#125; catch (Exception e) &#123;          LOG.warn(&quot;Error when trying to create load manager: &quot;, e);      &#125;      // If we failed to create a load manager, default to SimpleLoadManagerImpl.      return new SimpleLoadManagerImpl(pulsar);  &#125;\n\n默认使用的是 ModularLoadManagerImpl， 如果出现异常那就会使用 SimpleLoadManagerImpl 作为兜底。\n他们两个的区别是 ModularLoadManagerImpl 的功能更全，可以做更为细致的负载策略。\n接下来以默认的 ModularLoadManagerImpl 为例讲解上述的流程。\n上报负载数据在负载均衡器启动的时候就会收集节点数据然后进行上报：\n   public void start() throws PulsarServerException &#123;     try &#123;         String brokerId = pulsar.getBrokerId();         brokerZnodePath = LoadManager.LOADBALANCE_BROKERS_ROOT + &quot;/&quot; + brokerId;         // 收集本地负载数据         updateLocalBrokerData();// 上报 zookeeper         brokerDataLock = brokersData.acquireLock(brokerZnodePath, localData).join();     &#125; catch (Exception e) &#123;         log.error(&quot;Unable to acquire lock for broker: [&#123;&#125;]&quot;, brokerZnodePath, e);         throw new PulsarServerException(e);     &#125; &#125;\n\n\n\n首先获取到当前 broker 的 Id 然后拼接一个 zookeeper 节点的路径，将生成的 localData 上传到 zookeeper 中。\n// 存放 broker 的节点信息ls /loadbalance/brokers[broker-1:8080, broker-2:8080]// 根据节点信息查询负载数据get /loadbalance/brokers/broker-1:8080\n上报的数据：\n&#123;&quot;webServiceUrl&quot;:&quot;http://broker-1:8080&quot;,&quot;pulsarServiceUrl&quot;:&quot;pulsar://broker-1:6650&quot;,&quot;persistentTopicsEnabled&quot;:true,&quot;nonPersistentTopicsEnabled&quot;:true,&quot;cpu&quot;:&#123;&quot;usage&quot;:7.311714728372232,&quot;limit&quot;:800.0&#125;,&quot;memory&quot;:&#123;&quot;usage&quot;:124.0,&quot;limit&quot;:2096.0&#125;,&quot;directMemory&quot;:&#123;&quot;usage&quot;:36.0,&quot;limit&quot;:256.0&#125;,&quot;bandwidthIn&quot;:&#123;&quot;usage&quot;:0.8324254085661579,&quot;limit&quot;:1.0E7&#125;,&quot;bandwidthOut&quot;:&#123;&quot;usage&quot;:0.7155446715644209,&quot;limit&quot;:1.0E7&#125;,&quot;msgThroughputIn&quot;:0.0,&quot;msgThroughputOut&quot;:0.0,&quot;msgRateIn&quot;:0.0,&quot;msgRateOut&quot;:0.0,&quot;lastUpdate&quot;:1690979816792,&quot;lastStats&quot;:&#123;&quot;my-tenant/my-namespace/0x4ccccccb_0x66666664&quot;:&#123;&quot;msgRateIn&quot;:0.0,&quot;msgThroughputIn&quot;:0.0,&quot;msgRateOut&quot;:0.0,&quot;msgThroughputOut&quot;:0.0,&quot;consumerCount&quot;:2,&quot;producerCount&quot;:0,&quot;topics&quot;:1,&quot;cacheSize&quot;:0&#125;&#125;,&quot;numTopics&quot;:1,&quot;numBundles&quot;:1,&quot;numConsumers&quot;:2,&quot;numProducers&quot;:0,&quot;bundles&quot;:[&quot;my-tenant/my-namespace/0x4ccccccb_0x66666664&quot;],&quot;lastBundleGains&quot;:[],&quot;lastBundleLosses&quot;:[],&quot;brokerVersionString&quot;:&quot;3.1.0-SNAPSHOT&quot;,&quot;protocols&quot;:&#123;&#125;,&quot;advertisedListeners&quot;:&#123;&quot;internal&quot;:&#123;&quot;brokerServiceUrl&quot;:&quot;pulsar://broker-1:6650&quot;&#125;&#125;,&quot;loadManagerClassName&quot;:&quot;org.apache.pulsar.broker.loadbalance.impl.ModularLoadManagerImpl&quot;,&quot;startTimestamp&quot;:1690940955211,&quot;maxResourceUsage&quot;:0.140625,&quot;loadReportType&quot;:&quot;LocalBrokerData&quot;&#125;\n\n\n采集数据public static SystemResourceUsage getSystemResourceUsage(final BrokerHostUsage brokerHostUsage) &#123;      SystemResourceUsage systemResourceUsage = brokerHostUsage.getBrokerHostUsage();        // Override System memory usage and limit with JVM heap usage and limit      double maxHeapMemoryInBytes = Runtime.getRuntime().maxMemory();      double memoryUsageInBytes = Runtime.getRuntime().totalMemory() - Runtime.getRuntime().freeMemory();      double memoryUsage = memoryUsageInBytes / MIBI;      double memoryLimit = maxHeapMemoryInBytes / MIBI;      systemResourceUsage.setMemory(new ResourceUsage(memoryUsage, memoryLimit));        // Collect JVM direct memory      systemResourceUsage.setDirectMemory(new ResourceUsage((double) (getJvmDirectMemoryUsed() / MIBI),              (double) (DirectMemoryUtils.jvmMaxDirectMemory() / MIBI)));        return systemResourceUsage;  &#125;\n\n会在运行时获取一些 JVM 和 堆外内存的数据。\n收集所有节点数据作为 leader 节点还需要收集所有节点的负载数据，然后根据一些规则选择将负载较高的节点移动到负债较低的节点中。\nprivate void updateAllBrokerData() &#123; // 从 zookeeper 中获取所有节点    final Set&lt;String&gt; activeBrokers = getAvailableBrokers();    final Map&lt;String, BrokerData&gt; brokerDataMap = loadData.getBrokerData();    for (String broker : activeBrokers) &#123;        try &#123;            String key = String.format(&quot;%s/%s&quot;, LoadManager.LOADBALANCE_BROKERS_ROOT, broker);            // 依次读取各个节点的负载数据            Optional&lt;LocalBrokerData&gt; localData = brokersData.readLock(key).get();            if (!localData.isPresent()) &#123;                brokerDataMap.remove(broker);                log.info(&quot;[&#123;&#125;] Broker load report is not present&quot;, broker);                continue;            &#125;            if (brokerDataMap.containsKey(broker)) &#123;                // Replace previous local broker data.                brokerDataMap.get(broker).setLocalData(localData.get());            &#125; else &#123;                // Initialize BrokerData object for previously unseen                // brokers.                // 将数据写入到本地缓存                brokerDataMap.put(broker, new BrokerData(localData.get()));            &#125;        &#125; catch (Exception e) &#123;            log.warn(&quot;Error reading broker data from cache for broker - [&#123;&#125;], [&#123;&#125;]&quot;, broker, e.getMessage());        &#125;    &#125;    // Remove obsolete brokers.    for (final String broker : brokerDataMap.keySet()) &#123;        if (!activeBrokers.contains(broker)) &#123;            brokerDataMap.remove(broker);        &#125;    &#125;&#125;\n\n会从 zookeeper 的节点中获取到所有的 broker 列表（broker 会在启动时将自身的信息注册到 zookeeper 中。）\n然后依次读取各自节点的负载数据，也就是在负载均衡器启动的时候上报的数据。\n筛选出所有 broker 中需要 unload 的 bundle在 Pulsar 中 topic 是最核心的概念，而为了方便管理大量 topic，提出了一个 Bundle 的概念； Bundle 是一批 topic 的集合，管理 Bundle 自然会比 topic 更佳容易。\n所以在 Pulsar 中做负载均衡最主要的就是将负载较高节点中的 bundle 转移到低负载的 broker 中。\nprivate void updateAllBrokerData() &#123;    final Set&lt;String&gt; activeBrokers = getAvailableBrokers();    final Map&lt;String, BrokerData&gt; brokerDataMap = loadData.getBrokerData();    for (String broker : activeBrokers) &#123;        try &#123;            String key = String.format(&quot;%s/%s&quot;, LoadManager.LOADBALANCE_BROKERS_ROOT, broker);            Optional&lt;LocalBrokerData&gt; localData = brokersData.readLock(key).get();            if (!localData.isPresent()) &#123;                brokerDataMap.remove(broker);                log.info(&quot;[&#123;&#125;] Broker load report is not present&quot;, broker);                continue;            &#125;            if (brokerDataMap.containsKey(broker)) &#123;                // Replace previous local broker data.                brokerDataMap.get(broker).setLocalData(localData.get());            &#125; else &#123;                // Initialize BrokerData object for previously unseen                // brokers.                brokerDataMap.put(broker, new BrokerData(localData.get()));            &#125;        &#125; catch (Exception e) &#123;            log.warn(&quot;Error reading broker data from cache for broker - [&#123;&#125;], [&#123;&#125;]&quot;, broker, e.getMessage());        &#125;    &#125;    // Remove obsolete brokers.    for (final String broker : brokerDataMap.keySet()) &#123;        if (!activeBrokers.contains(broker)) &#123;            brokerDataMap.remove(broker);        &#125;    &#125;&#125;\n\n负载均衡器在启动的时候就会查询所有节点的数据，然后写入到 brokerDataMap 中。\n同时也会注册相关的 zookeeper 事件，当注册的节点发生变化时（一般是新增或者删减了 broker 节点）就会更新内存中缓存的负载数据。\n之后 leader 节点会定期调用 org.apache.pulsar.broker.loadbalance.impl.ModularLoadManagerImpl#doLoadShedding 函数查询哪些数据需要卸载，然后进行重新负载。\nfinal Multimap&lt;String, String&gt; bundlesToUnload = loadSheddingStrategy.findBundlesForUnloading(loadData, conf);\n最核心的就是调用这个 findBundlesForUnloading 函数，会返回需要卸载 bundle 集合，最终会遍历这个集合调用 admin API 进行卸载和重平衡。\n而这个函数会有多种实现，本质上就是根据传入的各个节点的负载数据，然后根据自定义的规则返回一批需要卸载的数据。\n以默认的 org.apache.pulsar.broker.loadbalance.impl.ThresholdShedder 规则为例：\n它是根据带宽、内存、流量等各个指标的权重算出每个节点的负载值，之后为整个集群计算出一个平均负载值。\n以上图为例：超过 ShedBundles 的数据就需要被卸载掉，然后转移到低负载的节点中。\n所以最左边节点和超出的 bundle 部分就需要被返回。\n具体的计算逻辑如下：\nprivate void filterAndSelectBundle(LoadData loadData, Map&lt;String, Long&gt; recentlyUnloadedBundles, String broker,                                   LocalBrokerData localData, double minimumThroughputToOffload) &#123;    MutableDouble trafficMarkedToOffload = new MutableDouble(0);    MutableBoolean atLeastOneBundleSelected = new MutableBoolean(false);    loadData.getBundleDataForLoadShedding().entrySet().stream()            .map((e) -&gt; &#123;                String bundle = e.getKey();                BundleData bundleData = e.getValue();                TimeAverageMessageData shortTermData = bundleData.getShortTermData();                double throughput = shortTermData.getMsgThroughputIn() + shortTermData.getMsgThroughputOut();                return Pair.of(bundle, throughput);            &#125;).filter(e -&gt;                    !recentlyUnloadedBundles.containsKey(e.getLeft())            ).filter(e -&gt;                    localData.getBundles().contains(e.getLeft())            ).sorted((e1, e2) -&gt;                    Double.compare(e2.getRight(), e1.getRight())            ).forEach(e -&gt; &#123;                if (trafficMarkedToOffload.doubleValue() &lt; minimumThroughputToOffload                        || atLeastOneBundleSelected.isFalse()) &#123;                    selectedBundlesCache.put(broker, e.getLeft());                    trafficMarkedToOffload.add(e.getRight());                    atLeastOneBundleSelected.setTrue();                &#125;            &#125;);&#125;\n\n从代码里看的出来就是在一个备选集合中根据各种阈值和判断条件筛选出需要卸载的 bundle。\n\n而 SimpleLoadManagerImpl 的实现如下：\nsynchronized (currentLoadReports) &#123;\tfor (Map.Entry&lt;ResourceUnit, LoadReport&gt; entry : currentLoadReports.entrySet()) &#123;\t\tResourceUnit overloadedRU = entry.getKey();\t\tLoadReport lr = entry.getValue();\t\t// 所有数据做一个简单的筛选，超过阈值的数据需要被 unload\t\tif (isAboveLoadLevel(lr.getSystemResourceUsage(), overloadThreshold)) &#123;\t\t\tResourceType bottleneckResourceType = lr.getBottleneckResourceType();\t\t\tMap&lt;String, NamespaceBundleStats&gt; bundleStats = lr.getSortedBundleStats(bottleneckResourceType);\t\t\tif (bundleStats == null) &#123;\t\t\t\tlog.warn(&quot;Null bundle stats for bundle &#123;&#125;&quot;, lr.getName());\t\t\t\tcontinue;\t\t\t&#125;\n\n就是很简单的通过将判断节点的负载是否超过了阈值 isAboveLoadLevel，然后做一个简单的排序就返回了。\n从这里也看得出来 SimpleLoadManagerImpl 和 ModularLoadManager 的区别，SimpleLoadManagerImpl 更简单，并没有提供多个 doLoadShedding 的筛选实现。\n总结总的来说对于无状态的服务来说，理论上我们只需要做好负载算法即可（轮训、一致性哈希、低负载优先等）就可以很好的平衡各个节点之间的负载。\n而对于有状态的服务来说，负载均衡就是将负载较高节点中的数据转移到负载低的节点中。\n其中的关键就是需要存储各个节点的负载数据（业界常用的是存储到 zookeeper 中），然后再由一个 leader 节点从这些节点中根据某种负载算法选择出负载较高的节点以及负载较低的节点，最终把数据迁移过去即可。\n","categories":["OB","Pulsar"],"tags":["Pulsar"]},{"title":"Pulsar3.0新功能介绍","url":"/2024/01/03/ob/Pulsar3.0-new-feature/","content":"\n在上一篇文章 Pulsar3.0 升级指北讲了关于升级 Pulsar 集群的关键步骤与灾难恢复，本次主要分享一些 Pulsar3.0 的新功能与可能带来的一些问题。\n升级后所遇到的问题先来个欲扬先抑，聊聊升级后所碰到的问题吧。\n其中有两个问题我们感知比较明显，特别是第一个。\n\n\ntopic被删除我们在上个月某天凌晨从 2.11.2 升级到 3.0.1 之后，进行了上一篇文章中所提到的功能性测试，发现没什么问题，觉得一切都还挺顺利的，半个小时搞定后就下班了。\n结果哪知道第二天是被电话叫醒的，有部分业务反馈业务重启之后就无法连接到 Pulsar 了。\n最终定位是 topic 被删除了。\n\n其中的细节还蛮多的，修复过程也是一波三折，后面我会单独写一篇文章来详细梳理这个过程。\n\n在这个 issue 和 PR 中有详细的描述：https://github.com/apache/pulsar/issues/21653https://github.com/apache/pulsar/pull/21704\n感兴趣的朋友也可以先看看。\n监控指标丢失第二个问题不是那么严重，是升级后发现  bookkeeper 的一些监控指标丢失了，比如这里的写入延迟：我也定位了蛮久，但不管是官方的 docker 镜像还是源码编译都无法复现这个问题。\n最终丢失的指标有这些：\n\nbookkeeper_server_ADD_ENTRY_REQUEST\nbookkeeper_server_ADD_ENTRY_BLOCKED\nbookkeeper_server_READ_ENTRY_BLOCKED\nbookie_journal_JOURNAL_CB_QUEUE_SIZE\nbookie_read_cache_hits_count\nbookie_read_cache_misses_count\nbookie_DELETED_LEDGER_COUNT\nbookie_MAJOR_COMPACTION_COUNT\n\n详细内容可以参考这个 issue：https://github.com/apache/pulsar/issues/21766\n新特性讲完了遇到的 bug，再来看看带来的新特性，重点介绍我们用得上的特性。\n支持低负载均衡\n当我们升级或者是重启 broker 的时候，全部重启成功后其实会发现最后重启的那个 broker 是没有流量的。\n这个原理和优化在之前写过的 Pulsar负载均衡原理及优化 其实有详细介绍。\n本次 3.0 终于将那个优化发版了，之后只要我们配置 lowerBoundarySheddingEnabled: true 就能开启这个低负载均衡的一个特性，使得低负载的 broker 依然有流量进入。\n跳过空洞消息Pulsar 可能会因为消息消费异常导致游标出现空洞，从而导致磁盘得不到释放；\n所以我们有一个定时任务，会定期扫描积压消息的 topic 判断是否存在空洞消息，如果存在便可以在管理台使用 skipMessage API 跳过空洞消息，从而释放磁盘。\n但在 3.0 之前这个跳过 API 存在 bug，只要跳过的数量超过 8 时，实际跳过的数量就会小于 8.\n具体 issue 和修复过程在这里：https://github.com/apache/pulsar/issues/20262https://github.com/apache/pulsar/pull/20326\n总之这个问题在 3.0 之后也是修复了，有类似需求的朋友也可以使用。\n新的负载均衡器同时也支持了一个新的负载均衡器，解决了以下问题：\n\n以前的负载均衡大量依赖 zk，当 topic 数量增多时对扩展性带来问题。\n新的负载均衡器使用 non-persistent 来存储负载信息，就不再依赖 zk 。\n\n\n以前的负载均衡器需要依赖 leader broker 进行重定向到具体的 broker，其实这些重定向并无意义，徒增了系统开销。\n新的负载均衡器使用了 SystemTopic 来存放 topic 的所有权信息，这样每个 broker 都可以拿到数据，从而不再需要从 leader broker 重定向了。\n\n\n\n更多完整信息可以参考这个 PIP: PIP-192: New Pulsar Broker Load Balancer\n支持大规模延迟消息第二个重大特性是支持大规模延迟消息，相信是有不少企业选择 Pulsar 也是因为他原生就支持延迟消息。\n我们也是大量在业务中使用延迟消息，以往的延迟消息有着以下一些问题：\n\n内存开销过大，延迟消息的索引都是保存在内存中，即便是可以分布在多个 broker 中分散存储，但消耗依然较大\n重点优化了索引的内存占有量。\n\n\n重启 broker 时会消耗大量时候重建索引\n支持了索引快照，最大限度的降低了构建索引的资源消耗。\n\n\n\n待优化功能监控面板优化最后即便是升级到了 3.0 依然还有一些待优化的功能，在之前的 从 Pulsar Client 的原理到它的监控面板中有提到给客户端加了一些监控埋点信息。\n最终使用下来发现还缺一个 ack 耗时的一个面板，其实日常碰到最多的问题就是突然不能消费了（或者消费过慢）。\n这时如果有这样的耗时面板，首先就可以定位出是否是消费者本身的问题。\n目前还在开发中，大概类似于这样的数据。\n总结Pulsar3.0 是 Pulsar 的第一个 LTS 版本，推荐尽快升级可以获得长期支持。但只要是软件就会有 bug，即便是 LTS 版本，所以大家日常使用碰到 Bug 建议多向社区反馈，一起推动 Pulsar 的进步。\n#Blog #Pulsar \n","categories":["OB"],"tags":["Pulsar"]},{"title":"Pulsar3.0 升级指北","url":"/2023/12/24/ob/Pulsar3.0-upgrade/","content":"\nPulsar3.0 介绍Pulsar3.0 是 Pulsar 社区推出的第一个 LTS 长期支持版本。\n\n如图所示，LTS 版本会最长支持到 36 个月，而 Feature 版本最多只有六个月；类似于我们使用的 JDK11,17,21 都是可以长期使用的；所以也推荐大家都升级到 LTS 版本。\n\n作为首个 LTS 版本，3.0 自然也是自带了许多新特性，这个会在后续介绍。\n\n\n\n升级指南先来看看升级指南：在官方的兼容表中会发现：不推荐跨版本升级。\n也就是说如果你现在还在使用的是 2.10.x，那么推荐是先升级到 2.11.x 然后再升级到 3.0.x.\n而且根据我们的使用经验来看，首个版本是不保险的，即便是 LTS 版本；所以不推荐直接升级到 3.0.0，而是更推荐 3.0.1+，这个小版本会修复 3.0 所带来的一些 bug。\n先讲一下我们的升级流程，大家可以用做参考。\n升级前准备根据我们的使用场景，为了以防万一，首先需要将我们的插件依赖升级到对应的版本。其实简单来说就是更新下依赖，然后再重新打包，在后续的流程进行测试。\n预热镜像之后是预热镜像，我们使用 harbor 搭建了自己的 docker 镜像仓库，这样在升级重启镜像的时候可以更快的从内网拉取镜像。\n\n毕竟一个 pulsar-all 的镜像也不小，尽量的缩短启动时间。\n\n预热的过程也很简单：\ndocker pull apachepulsar/pulsar-all:3.0.1docker tag apachepulsar/pulsar-all:3.0.1 harbor-private.xx.com/pulsar/pulsar-all:3.0.1docker image push harbor-private.xx.com/pulsar/pulsar-all:3.0.1\n\n之后升级的时候就可以使用私服的镜像了。\n功能测试我这边有写了一个 cli 可以帮我快速创建或升级一个集群，然后触发我所编写的功能测试。\n./pulsar-upgrade-cli upgrade pulsar-test ./charts/pulsar --version x.x.x -f charts/pulsar/values.yaml -n pulsar-test\n\n这个 cli 很简单，一共就做三件事：\n\n使用 helm 接口升级集群\n等待所有的 Pod 都升级成功\n触发功能测试\n\n之后的效果如下：\n主要就是覆盖了我们的使用场景，都跑通过之后才会走后续的流程。\n运行监控\n之后会启动一个 200 左右的并发生产和消费数据，模拟线上的使用情况，会一直让这个任务跑着，大概一晚上就可以了，第二天通过监控查看：\n\n应用有无异常日志\n流量是否正常\n各个组件的内存占用\n写入延迟等信息\n\n升级步骤组件的升级步骤这里参考了官方指南：https://pulsar.apache.org/docs/3.1.x/administration-upgrade/#upgrade-zookeeper-optional\n\n升级ZK\n关闭auto recovery\n升级Bookkeeper\n升级Broker\n升级Proxy\n开启auto recovery\n\n只要一步步按照这个流程走，问题不大，哪一步出现问题后需要及时回滚，回滚流程参考下面的回滚部分。\n同时在升级过程中需要一直查看 broker 的 error 日志，如果有明显的不符合预期的日志一定要注意。\n\n在升级  bookkeeper 的时候，broker 可能会出现 bk 连接失败的异常，这个可以不用在意。\n\n线上验证都升级完后就是线上业务验证环节了：\n\n 查看监控面板，是否有明显的流量、内存、延迟的异常指标。 ✅ 2023-12-24\n topic 元数据完整性验证：这个是因为我们这次升级出了一个 topic 被删除的 bug，所以需要重点验证下；这部分会在下次详细分析。 ✅ 2023-12-24\n 查看业务消息收发有无异常 ✅ 2023-12-24\n 链路查询是否正常，我们有一个消息链路查询的页面，主要是使用 Pulsar-SQL 和 broker-interceptor 实现的。 ✅ 2023-12-24\n\n异常回滚当出现异常的时候需要立即回滚，这里的异常一般就是消息收发异常，客户端掉线等。\n经过我的测试 3.0.x 的存储和之前的版本是兼容的，所以 bookkeeper 都能降级其他的组件就没啥可担心的了。\n需要降级时直接将所有组件降级为上一个版本即可。\n灾难恢复因为是从 2.x 升级到 3.x 也是涉及到了跨大版本，所以也准备了灾难恢复的方案。\n\n比如极端情况下升级失败，所有数据丢失的情况。\n\n整个灾难恢复的主要目的就是恢复后的集群对外提供的域名不发生变化，同时所有的客户端可以自动重连上来，也就是最坏的情况下所有的数据丢了可以接受，但不能影响业务正常使用。\n所以我们的流程如下：\n备份 topic@SneakyThrows  @Test  void backup()&#123;      List&lt;String&gt; topicList = pulsarAdmin.topics().getPartitionedTopicList(&quot;tenant/namespace&quot;);      log.info(&quot;topic size=&#123;&#125;&quot;,topicList.size());      // create a custom thread pool      CopyOnWriteArrayList&lt;TopicMeta&gt; dataList = new CopyOnWriteArrayList&lt;&gt;();      ExecutorService customThreadPool = Executors.newFixedThreadPool(10);      for (String topicName : topicList) &#123;          customThreadPool.execute(()-&gt; &#123;              PartitionedTopicMetadata metadata;              try &#123;                  metadata = pulsarAdmin.topics().getPartitionedTopicMetadata(topicName);                  TopicMeta topicMeta = new TopicMeta();                    // backup topic                  topicMeta.setName(topicName);                  topicMeta.setPartition(metadata.partitions);                    // backup permission                  Map&lt;String, Set&lt;AuthAction&gt;&gt; permissions = pulsarAdmin.topics().getPermissions(topicName);                  topicMeta.setPermissions(permissions);                    // back sub                  List&lt;String&gt; subscriptions = new ArrayList&lt;&gt;();                  PartitionedTopicStats topicStats = pulsarAdmin.topics().getPartitionedStats(topicName, true);                  topicStats.getSubscriptions().forEach((k,v)-&gt; subscriptions.add(k));                  topicMeta.setSubscriptions(subscriptions);                    dataList.add(topicMeta);              &#125; catch (PulsarAdminException e) &#123;                  throw new RuntimeException(e);              &#125;        &#125;);    &#125;      customThreadPool.shutdown();      while (!customThreadPool.isTerminated()) &#123;      &#125;      log.info(&quot;&#123;&#125;&quot;,dataList.size());      log.info(&quot;&#123;&#125;&quot;,JSONUtil.toJsonStr(dataList));  &#125;// TopicMetaData@Data  public class TopicMeta &#123;      private String name;      private int partition;      Map&lt;String, Set&lt;AuthAction&gt;&gt; permissions;      List&lt;String&gt; subscriptions = new ArrayList&lt;&gt;();  &#125;\n第一步是备份 topic：\n\ntopic 主要是名称和分区数量\n备份权限\n备份 topic 的订阅者\n\n公私钥备份因为我们客户端使用了 JWT 验证，所有为了使得恢复的 Pulsar 集群可以让客户端无缝切换到新集群，因此必须得使用相同的公私钥。\n这个其实比较简单，我们使用的是 helm 安装的集群，所以只需要备份好 Secret 即可。\napiVersion: v1  data:    PRIVATEKEY: XXX    PUBLICKEY: XXX kind: Secret  metadata:    name: pulsar-token-asymmetric-key    namespace: pulsar  type: Opaque  # 还有几个 superUser 的 Secret\n数据恢复创建新集群首先使用 helm 重新创建一个新集群：\n./scripts/pulsar/prepare_helm_release.sh -n pulsar -k pulsarhelm install \\    --values charts/pulsar/values.yaml \\    --set namespace=pulsar\\      --set initialize=true \\      pulsar ./charts/pulsar -n pulsar\n\n恢复公私钥直接使用刚才备份的公私钥覆盖到新集群即可。\n恢复namespace进入 toolset pod 创建需要使用的 tenant/namespace\nk exec -it pulsar-toolset-0 -n pulsar bashbin/pulsar-admin tenants create tenantbin/pulsar-admin namespaces create tenant/namespace\n\n元数据恢复之后便是最重要的元数据恢复了：\n@SneakyThrows  @Test  void restore() &#123;      PulsarAdmin pulsarAdmin = PulsarAdmin.builder().serviceHttpUrl(&quot;http://url:8080&quot;)              .authentication(AuthenticationFactory.token(token))              .build();      Path filePath = Path.of(&quot;restore-ns.json&quot;);      String fileContent = Files.readString(filePath);      List&lt;TopicMeta&gt; topicMetaList = JSON.parseArray(fileContent, TopicMeta.class);      ExecutorService customThreadPool = Executors.newFixedThreadPool(50);      for (TopicMeta topicMeta : topicMetaList) &#123;          customThreadPool.execute(() -&gt; &#123;              // Create topic              try &#123;                  pulsarAdmin.topics().createPartitionedTopic(topicMeta.getName(), topicMeta.getPartition());              &#125; catch (PulsarAdminException e) &#123;                  log.error(&quot;Create topic error&quot;);              &#125;              // Create sub              for (String subscription : topicMeta.getSubscriptions()) &#123;                  try &#123;                      pulsarAdmin.topics().createSubscription(topicMeta.getName(), subscription, MessageId.latest);                  &#125; catch (PulsarAdminException e) &#123;                      log.error(&quot;createSubscription error&quot;);                  &#125;            &#125;              // Grant permission              topicMeta.getPermissions().forEach((role, authActions) -&gt; &#123;                  permission(pulsarAdmin, topicMeta.getName(), role, authActions);              &#125;);              log.info(&quot;topic:&#123;&#125; restore success&quot;, topicMeta.getName());              &#125;);    &#125;      customThreadPool.shutdown();      while (!customThreadPool.isTerminated()) &#123;      &#125;    log.info(&quot;restore success&quot;);  &#125;private synchronized void permission(PulsarAdmin pulsarAdmin, String topic, String role, Set&lt;AuthAction&gt; authActions) &#123;      try &#123;          pulsarAdmin.topics().grantPermission(topic, role, authActions);      &#125; catch (PulsarAdminException e) &#123;          log.error(&quot;grantPermission error&quot;, e);      &#125;  &#125;\n流程和备份类似：\n\n创建分区 topic\n创建订阅者\n授权角色信息\n\n因为授权接口限制了并发调用，所有需要加锁，导致整个恢复的流程就会比较慢。\n8000 topic 的 namespace 大概恢复时间为 40min 左右。\n之后依次恢复其他 namespace 即可。\n恢复 policeadmin.namespaces().setNamespaceMessageTTL(&quot;tenant/namespace&quot;, 3600 * 6);admin.namespaces().setBacklogQuota(&quot;tenant/namespace&quot;, BacklogQuota)\n\n如果之前的集群有设置 TTL 或者是 backlogQuota 时都需要手动恢复。\n总结以上就是整个升级和灾难恢复的流程，当然灾难恢复希望大家不要碰到。\n我会在下一篇详细介绍 Pulsar 3.0 的新功能以及所碰到的一些坑。\n","categories":["Pulsar","OB"],"tags":["Pulsar"]},{"title":"StarRocks 物化视图刷新流程和原理","url":"/2024/11/18/ob/StarRocks-MV-refresh-Principle/","content":"前段时间给 StarRocks 的物化视图新增了一个特性，那也是我第一次接触 StarRocks，因为完全不熟悉这个数据库，所以很多东西都是从头开始了解概念。\n为了能顺利的新增这个特性（具体内容可以见后文），我需要把整个物化视图的流程串联一遍，于是便有了这篇文章。\n在开始之前简单了解下物化视图的基本概念：\n\n简单来说，视图和 MySQL 这类传统数据库的概念类似，也是用于解决大量消耗性能的 SQL 的，可以提前将这些数据查询好然后放在一张单独的表中，这样再查询的时候性能消耗就比较低了。\n\n\n刷新条件为了保证视图数据的实时性，还需要在数据发生变化的时候能够及时刷新视图里的数据，目前有这几个地方会触发视图刷新：\n\n手动刷新视图，使用 REFRESH MATERIALIZED VIEW order_mv; 语句\n将视图设置为 active 状态：ALTER MATERIALIZED VIEW order_mv ACTIVE;\n基表数据发生变化时触发刷新。\n\n\n\ntruncate 基表时触发刷新：truncate table trunc_db.t1; \ndrop partition 时触发：ALTER TABLE &lt;tbl_name&gt; DROP PARTITION(S) p0, p1 [, ...];\n\n这里的 truncate table  和 drop partition 目前的版本还存在 bug：当基表和物化视图不在一个数据库时不会触发自动刷新，目前已经修复了。\n\n\nhttps://github.com/StarRocks/starrocks/pull/52618\nhttps://github.com/StarRocks/starrocks/pull/52295\n\n刷新流程\n如图所示，当触发一次刷新之后主要就是需要计算出需要刷新的分区。\n第一次触发刷新的时候是不会带上周期（比如时间范围），然后根据过滤计算出来的周期，默认情况下只会使用第一个周期（我们可以通过 partition_refresh_number 参数来调整单次刷新的分区数量）。\n\n然后如果还有其余的周期，会将这些周期重新触发一次刷新任务（会带上刚才剩余的周期数据），这样进行递归执行。\n\n通过日志会看到返回的分区数据。\n新增优化参数我们在使用物化视图的时候，碰到一个场景：\nCREATE TABLE IF NOT EXISTS test.par_tbl1(    datekey DATETIME,    k1      INT,    item_id STRING,    v2      INT)PRIMARY KEY (`datekey`,`k1`) PARTITION BY date_trunc(&#x27;day&#x27;, `datekey`); CREATE TABLE IF NOT EXISTS test.par_tbl2(    datekey DATETIME,    k1      INT,    item_id STRING,    v2      INT)PRIMARY KEY (`datekey`,`k1`) PARTITION BY date_trunc(&#x27;day&#x27;, `datekey`); CREATE TABLE IF NOT EXISTS test.par_tbl3(    datekey DATETIME,    k1      INT,    item_id STRING,    v2      INT) PRIMARY KEY (`datekey`,`k1`);\n\n但我们有三张基表，其中 1 和 2 都是分区表，但是 3 是非分区表。\n此时基于他们新建了一个物化视图：\nCREATEMATERIALIZED VIEW test.mv_testREFRESH ASYNCPARTITION BY a_timePROPERTIES (&quot;excluded_trigger_tables&quot; = &quot;par_tbl3&quot;)ASselect date_trunc(&quot;day&quot;, a.datekey) as a_time, date_trunc(&quot;day&quot;, b.datekey) as b_time,date_trunc(&quot;day&quot;, c.datekey) as c_timefrom test.par_tbl1 a         left join test.par_tbl2 b on a.datekey = b.datekey and a.k1 = b.k1         left join test.par_tbl3 c on a.k1 = c.k1;\n\n\n\n当我同时更新了分区表和非分区表的数据时：\nUPDATE `par_tbl1` SET `v2` = 2 WHERE `datekey` = &#x27;2024-08-05 01:00:00&#x27; AND `k1` = 3;UPDATE `par_tbl3` SET `item_id` = &#x27;3&#x27; WHERE `datekey` = &#x27;2024-10-01 01:00:00&#x27; AND `k1` = 3;\n\n预期的结果是只有 par_tbl1 表里修改的数据会被同步到视图（&quot;excluded_trigger_tables&quot; = &quot;par_tbl3&quot;已经被设置为不会触发视图刷新），但实际情况是 par_tbl1 和 par_tbl2 表里所有的数据都会被刷新到物化视图中。\n我们可以使用这个 SQL 查询无刷视图任务的运行状态：\nSELECT * FROM information_schema.task_runs order by create_time desc;\n\n这样就会造成资源损耗，如果这两张基表的数据非常大，本次刷新会非常耗时。\n所以我们的需求是在这样的场景下也只刷新修改的数据。\n因此我们在新建物化视图的时候新增了一个参数：\nCREATEMATERIALIZED VIEW test.mv_testREFRESH ASYNCPARTITION BY a_timePROPERTIES (&quot;excluded_trigger_tables&quot; = &quot;par_tbl3&quot;,&quot;excluded_refresh_tables&quot;=&quot;par_tbl3&quot;)ASselect date_trunc(&quot;day&quot;, a.datekey) as a_time, date_trunc(&quot;day&quot;, b.datekey) as b_time,date_trunc(&quot;day&quot;, c.datekey) as c_timefrom test.par_tbl1 a         left join test.par_tbl2 b on a.datekey = b.datekey and a.k1 = b.k1         left join test.par_tbl3 c on a.k1 = c.k1;\n\n这样当在刷新数据的时候，会判断 excluded_refresh_tables 配置的表是否有发生数据变化，如果有的话则不能将当前计算出来的分区（1,2 两张表的全量数据）全部刷新，而是继续求一个交集，只计算基表发生变化的数据。\n这样就可以避免 par_tbl1、par_tbl2 的数据全量刷新，而只刷新修改的数据。\n这样的场景通常是在关联的基表中有一张字典表，通常数据量不大，所以也不需要分区的场景。\n这样在创建物化视图的时候就可以使用这两个参数 excluded_trigger_tables，excluded_refresh_tables 将它排除掉了。\n\n整体的刷新逻辑并不复杂，主要就是几个不同的刷新入口以及刷新过程中计算分区的逻辑。\n参考链接：\n\nhttps://docs.starrocks.io/zh/docs/using_starrocks/async_mv/Materialized_view/#%E7%90%86%E8%A7%A3-starrocks-%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE\nhttps://docs.starrocks.io/zh/docs/using_starrocks/async_mv/use_cases/data_modeling_with_materialized_views/#%E5%88%86%E5%8C%BA%E5%BB%BA%E6%A8%A1\nhttps://github.com/StarRocks/starrocks/pull/52295\nhttps://github.com/StarRocks/starrocks/pull/52618\n\n","categories":["StarRocks"],"tags":["StarRocks"]},{"title":"如何在本地打包 StarRocks 发行版","url":"/2025/05/12/ob/StarRocks-build-in-local/","content":"最近我们在使用 StarRocks 的时候碰到了一些小问题：\n\n重启物化视图的时候会导致视图全量刷新，大量消耗资源。  - 修复 PR：https://github.com/StarRocks/starrocks/pull/57371\nexcluded_refresh_tables 参数与 MV 不在一个数据库的时候，无法生效。\n修复 PR：https://github.com/StarRocks/starrocks/pull/58752\n\n\n\n而提交的 PR 是有发布流程的，通常需要间隔一段时间才会发布版本，但是我们线上又等着用这些修复，没办法就只有在本地打包了。\n好在社区已经考虑到这种场景了，专门为我们提供了打包的镜像。\n\n\n\nFE 是 Java 开发的，本地构建还比较容易，而 BE 是基于 cpp 开发的，构建环境比较复杂，在统一的 docker 镜像里构建会省去不少环境搭建流程。\n\n我们先要拉取对应的打包镜像：\nstarrocks/dev-env-ubuntu:3.3.9\n\n根据自己的版本号拉取即可，比如我这里使用的是 3.3.9 的版本。\n然后需要根据我使用的 tag 拉取一个我们自己的开发分支，在这个分支上将修复的代码手动合并进来。\n然后便可以开始打包了。\ngit clone git@github.com:StarRocks/starrocks.git /xx/starrocksdocker run -it -v /xx/starrocks/.m2:/root/.m2 \\ -v /xx/starrocks:/root/starrocks \\ --name 3.3.9 -d starrocks/dev-env-ubuntu:3.3.9docker exec -it 3.3.9 bashcd /root/starrocks/./build.sh --fe --clean\n\n我们需要将宿主机的代码磁盘挂载到镜像里，这样镜像就会使用我们的源码进行编译构建。\n最终会在 /xx/starrocks/output 目录生成我们的目标文件。\n\n替换目标镜像既然 fe 的各种 jar 包都已经构建出来了，那就可以基于这些 jar 包手动打出 fe 的 image 了。\n我们可以参考官方例子，使用 fe-ubuntu.Dockerfile 来构建 FE 的镜像。\nDOCKER_BUILDKIT=1 docker build --build-arg ARTIFACT_SOURCE=local --build-arg LOCAL_REPO_PATH=. -f fe-ubuntu.Dockerfile -t fe-ubuntu:main ../../..\n\n除此之外还有更简单的方式，也是更加稳妥的方法。\n我们可以直接使用官方的镜像作为基础镜像，只替换其中核心的 starrocks-fe.jar 。\n\n这个 jar 包会在编译的时候构建出来\n\n因为 starrocks-fe.jar 也是通过同样的镜像打包出来的，所以运行起来不会出现兼容性问题（同样的 jdk 版本），而且也能保证原有的镜像没有修改。\nFROM starrocks/fe-ubuntu:3.3.9COPY starrocks-fe.jar /opt/starrocks/fe/lib/\n\ndocker build -t fe-ubuntu:3.3.9-fix-&#123;branch&#125; .\n\n这样我们就可以放心的替换线上的镜像了。\n参考链接：\n\nhttps://docs.starrocks.io/zh/docs/developers/build-starrocks/Build_in_docker&#x2F; \nhttps://github.com/StarRocks/starrocks/blob/759a838ae15b91056233f180aedc88da67a84937/docker/dockerfiles/fe/README.md#L15\n\n","categories":["StarRocks"],"tags":["StarRocks"]},{"title":"StarRocks 物化视图创建与刷新全流程解析","url":"/2025/06/27/ob/StarRocks-create-sync/","content":"最近在为 StarRocks 的物化视图增加多表达式支持的能力，于是便把物化视图（MV）的创建刷新流程完成的捋了一遍。\n之前也写过一篇：StarRocks 物化视图刷新流程和原理，主要分析了刷新的流程，以及刷新的条件。\n这次从头开始，从 MV 的创建开始来看看 StarRocks 是如何管理物化视图的。\n创建物化视图CREATEMATERIALIZED VIEW mv_test99REFRESH ASYNC EVERY(INTERVAL 60 MINUTE)PARTITION BY p_timePROPERTIES (&quot;partition_refresh_number&quot; = &quot;1&quot;)ASselect date_trunc(&quot;day&quot;, a.datekey) as p_time, sum(a.v1) as valuefrom par_tbl1 agroup by p_time, a.item_id\n\n\n\n创建物化视图的时候首先会进入这个函数：com.starrocks.sql.analyzer.MaterializedViewAnalyzer.MaterializedViewAnalyzerVisitor#visitCreateMaterializedViewStatement\n\n\n其实就是将我们的创建语句结构化为一个 CreateMaterializedViewStatement 对象，这个过程是使用 ANTLR 实现的。\n\n这个函数负责对创建物化视图的 SQL 语句进行语义分析、和基本的校验。\n比如：\n\n分区表达式是否正确\n基表、数据库这些的格是否正确\n\n\n\n校验分区分区表达式的各种信息。\n\n然后会进入函数：com.starrocks.server.LocalMetastore#createMaterializedView()\n这个函数的主要作用如下：\n\n检查数据库和物化视图是否存在。\n\n初始化物化视图的基本信息：\n\n获取物化视图的列定义（schema）\n验证列定义的合法性\n初始化物化视图的属性（如分区信息）。\n\n\n处理刷新策略：\n\n根据刷新类型（如 ASYNC、SYNC、MANUAL 或 INCREMENTAL）设置刷新方案。\n对于异步刷新，设置刷新间隔、开始时间等，并进行参数校验。\n\n\n创建物化视图对象：\n\n根据运行模式（存算分离和存算一体）创建不同类型的物化视图对象\n设置物化视图的索引、排序键、注释、基础表信息等。\n\n\n处理分区逻辑：\n\n如果物化视图是非分区的，创建单一分区并设置相关属性。\n如果是分区的，解析分区表达式并生成分区映射关系\n\n\n绑定存储卷：\n\n如果物化视图是云原生类型，绑定存储卷。\n\n\n\n序列化关键数据对于一些核心数据，比如分区表达式、原始的创建 SQL 等，需要再重启的时候可以再次加载到内存里供后续使用时；\n就需要将这些数据序列化到元数据里。\n这些数据定期保存在 fe/meta 目录中。\n我们需要序列化的字段需要使用 @SerializedName注解。\n@SerializedName(value = &quot;partitionExprMaps&quot;)  private Map&lt;ExpressionSerializedObject, ExpressionSerializedObject&gt; serializedPartitionExprMaps;\n\n同时在 com.starrocks.catalog.MaterializedView#gsonPreProcess/gsonPostProcess 这两个函数中将数据序列化和反序列化。\n元数据的同步与加载当 StarRocks 的 FE 集群部署时，会由 leader 的 FE 启动一个 checkpoint 线程，定时扫描当前的元数据是否需要生成一个 image.$&#123;JournalId&#125; 的文件。\n\n\n其实就是判断当前日志数量是否达到上限（默认是 5w）生成一次。\n\n具体的流程如下：\n\n更多元数据同步和加载流程可以查看我之前的文章：深入理解 StarRocks 的元数据管理\n刷新物化视图创建完成后会立即触发一次 MV 的刷新逻辑。\n同步分区刷新 MV 的时候有一个很重要的步骤：同步 MV 和基表的分区。\n\n这个步骤在每次刷新的时候都会做，只是如果基表分区和 MV 相比没有变化的话就会跳过。\n\n这里我们以常用的 Range 分区为例，核心的函数为：com.starrocks.scheduler.mv.MVPCTRefreshRangePartitioner#syncAddOrDropPartitions\n它的主要作用是同步物化视图的分区，添加、删除分区来保持 MV 的分区与基础表的分区一致；核心流程：\n\n计算分区差异：根据指定的分区范围，计算物化视图与基础表之间的分区差异。\n同步分区：\n删除旧分区：删除物化视图中与基础表不再匹配的分区。\n添加新分区：根据计算出的差异，添加新的分区到物化视图。\n\n\n\n\n分区同步完成之后就可以计算需要刷新的分区了：\n以上内容再结合之前的两篇文章：\n\nStarRocks 物化视图刷新流程和原理\n深入理解 StarRocks 的元数据管理\n\n就可以将整个物化视图的创建与刷新的核心流程掌握了。\n#StarRocks #Blog \n","categories":["OB"],"tags":["StarRocks"]},{"title":"StarRocks 开发环境搭建踩坑指北","url":"/2024/10/09/ob/StarRocks-dev-env-build/","content":"背景最近这段时间在处理一个 StarRocks 的关于物化视图优化的一个问题，在此之前其实我也没有接触过 StarRocks 这类主要处理数据分析的数据库，就更别提在这上面做优化了。\n在解决问题之前我先花了一两天时间熟悉了一下 StarRocks 的一些概念和使用方法，然后又花了一些时间搭建环境然后复现了该问题。\n之后便开始阅读源码，大概知道了相关代码的执行流程，但即便是反复阅读了多次代码也没有找到具体出现问题的地方。\n所以便考虑在本地 Debug 源码，最终调试半天之后知道了问题所以，也做了相关修改，给社区提交了 PR，目前还在推进过程中。\n\n\n环境搭建这里比较麻烦的是如何在本地 debug 代码。根据官方的架构图会发现 StarRocks 主要分为两个部分：\n\nFE：也就是常说的前端部分，主要负责元数据管理和构建执行计划。\nBE：后端存储部分，执行查询计划并存储数据。\n\n其中 FE 是 Java 写的，而存储的 BE 则是 C++ 写的，我这次需要修改的是 FE 前端的部分，所以本篇文章主要讨论的是 FE 相关的内容。\n好在社区已经有关于如何编译和构建源码的教程，这里我列举一些重点，FE 首先需要安装以下一些工具：\n\nThrift\nProtobuf\nPython3\nJDK8+\n\nbrew install alberttwong/thrift/thrift@0.13$ thrift -version  Thrift version 0.13.0brew install protobuf\n\n以上默认是在  Mac 平台上安装的流程，所以全程使用 brew 最方便了，如果是其他平台也是同理，只要安装好这些工具即可。\n紧接着便是编译 FE，我们需要先下载源码，然后进入 FE 的目录：\ngit clone https://github.com/StarRocks/starrocks.gitcd femvn install -DskipTests\n\n然后直接使用 maven 编译安装即可。\n这里需要注意⚠️，因为编译过程中需要使用 Python3 来执行一些构建任务，新版本的 Mac 都是内置 Python3 的，但如果是老版本的 Mac 内置的则是 Python2。\n这时就需要我们将 Python3 的命令手动在构建任务里指定一下：\n\n比如我这里的 Python3  命令为 python3\n我们需要在 fe/fe-core/pom.xml 目录里修改下 Python 的命令名称：\n修改之后再 mvn install 编译一次，如果一切顺利的话便会编译成功。\n搭建本地集群启动 FE我的最终目的是可以在本地 IDEA 中启动 FE 然后再配合启动一个 BE，这样就可以在 IDEA 中调试 FE 的源码了。\n在启动 FE 之前还需要创建一些目录：\ncp -r conf fe/confcp -r bin fe/bincp -r webroot fe/webrootcd fe  mkdir log  mkdir meta\n\n主要就是要在 FE 的目录下创建配置文件、执行脚本、日志、元数据等目录。\n接着便可以打开 com.starrocks.StarRocksFE 类在 IDEA 中运行了，在启动之前还需要配置一下环境变量：\n# 修改为自己的目录export PID_DIR=/Users/smith/Code/starrocks/fe/binexport STARROCKS_HOME=/Users/smith/Code/starrocks/feexport LOG_DIR=/Users/smith/Code/starrocks/fe/log\n\n同时需要配置下 fe.conf 中的 priority_networks 网络配置：\npriority_networks = 10.10.10.0/24\n这个 IP 得是宿主机的 IP，后续我们使用 docker 启动 BE 的时候也需要用到。\n\n如果启动失败，可以在日志目录下查看日志：\n2024-09-16 21:21:59.942+08:00 ERROR (main|1) [NodeMgr.getCheckedSelfHostPort():642] edit_log_port 9010 is already in use. will exit.\n\n碰到这个异常：提示端口被占用，那可以尝试关闭代理之后再试试。\n启动成功后我们便可以使用 MySQL 兼容的客户端进行连接了，这里我使用的是 tableplus:\n然后我们使用以下 sql  可以查询 fe 的节点状态：\nSHOW PROC &#x27;/frontends&#x27;;\n\n看到类似的输出则代表启动成功了。\n启动 BE之后我们便可以使用 Docker 来启动 BE 了，之所以用 docker 启动，是因为 BE 是 C++ 编写的，想要在 Mac 上运行比较麻烦，最好是得有一台 Ubuntu22 的虚拟机。\n如果我们不需要调试 BE 的话，只使用 docker 启动是再合适不过了。\ndocker run -p 9060:9060 -p 8040:8040 -p 9050:9050 -p 8060:8060 -p 9070:9070 -itd --rm --name be -e &quot;TZ=Asia/Shanghai&quot; starrocks/be-ubuntu\n\n我们需要将 FE 需要连接 BE 的端口暴露出来，启动成功后该镜像并不会直接启动 BE，我们需要进入容器手动启动。\ndocker exec -it be bash\n\n在启动之前我们依然需要修改下 be.conf 中的 priority_networks 配置：\n修改为和 fe.conf 中相同的配置。\n之后使用以下命令启动 be:\nbin/start_be.sh --daemon\n\n启动日志我们可以在 logs 目录中查看。\n绑定 FE 和 BE接下来还有最后一步就是将 FE 和 BE 绑定在一起。\n我们在 fe 中执行以下 sql：\nALTER SYSTEM ADD BACKEND &quot;127.0.0.1:9050&quot;;\n\n手动添加一个节点，之后再使用：\nSHOW PROC &#x27;/backends&#x27;;\n\n可以查询到 BE 的节点状态：\n\n如果出现以下结果代表连接成功，这样我们就可以创建数据库和表了。\n总结这部分内容（本地 FE 联结 docker 里的 FE）官方文档并没有提及，也是我踩了不少坑、同时还咨询了一些大佬才全部调试成功。\n还有一点需要注意的事：如果我们网络环境发生了变化，比如从家里的 Wi-Fi 切换到了公司的，需要手动删除下 FE/meta 下的所有文件再次启动，BE 则是需要重启一下容器。\n参考链接：\n\nhttps://docs.starrocks.io/zh/docs/developers/development-environment/IDEA/\nhttps://docs.starrocks.io/zh/docs/deployment/deploy_manually/#%E7%AC%AC%E5%9B%9B%E6%AD%A5%E6%90%AD%E5%BB%BA%E9%9B%86%E7%BE%A4\n\n","categories":["StarRocks"],"tags":["StarRocks"]},{"title":"StarRocks 开发环境搭建踩坑指北之存算分离篇","url":"/2025/09/18/ob/StarRocks-dev-shard-data-build/","content":"前段时间碰到一个 StarRocks 物化视图的 bug: https://github.com/StarRocks/starrocks/issues/55301\n但是这个问题只能在存算分离的场景下才能复现，为了找到问题原因我便尝试在本地搭建一个可以 Debug 的存算分离版本。\n之前也分享过在本地 Debug StarRocks，不过那是存算一体的版本，而存算分离稍微要复杂一些。\n\n这里提到的本地 Debug 主要是指可以调试 FE，而 CN&#x2F;BE 则是运行在容器环境，避免本地打包和构建运行环境。\n\n\n\n\n当前 StarRocks 以下的存算分离部署方式，在本地推荐直接使用 MinIO 部署。\n\n启动 MinIO首先第一步启动 MinIO:\ndocker run -d --rm --name minio \\  -e MINIO_ROOT_USER=miniouser \\  -e MINIO_ROOT_PASSWORD=miniopassword \\  -p 9001:9001 \\  -p 9000:9000 \\  --entrypoint sh \\  minio/minio:latest \\  -c &#x27;mkdir -p /minio_data/starrocks &amp;&amp; minio server /minio_data --console-address &quot;:9001&quot;&#x27;\n\n进入 MinIO 容器设置 access token:\ndocker exec -it minio shmc alias set myminio http://10.0.9.20:9000 miniouser miniopassword; mc admin user svcacct add --access-key AAAAAAAAAAAAAAAAAAAA --secret-key BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB myminio miniouser\n\n启动 cn:docker run -p 9060:9060 -p 8040:8040 -p 9050:9050 -p 8060:8060 -p 9070:9070 -itd --rm --name cn -e &quot;TZ=Asia/Shanghai&quot; starrocks/cn-ubuntu:3.4-latest\n\n修改 cn.conf :\ncd cn/config/echo &quot;priority_networks = 10.0.9.20/24&quot; &gt;&gt; cn.properties\n\n 使用脚本手动启动 cn:\nbin/start_cn.sh --daemon\n\n使用以下配置在本地 IDEA 中启动 FE:\nLOG_DIR = $&#123;STARROCKS_HOME&#125;/log    DATE = &quot;$(date +%Y%m%d-%H%M%S)&quot;    sys_log_level = INFO    http_port = 8030  rpc_port = 9020  query_port = 9030  edit_log_port = 9010  mysql_service_nio_enabled = true    run_mode = shared_data  cloud_native_storage_type = S3  aws_s3_endpoint = 10.0.9.20:9000  # set the path in MinIO  aws_s3_path = starrocks  # credentials for MinIO object read/write  # 这里的 key 为刚才设置的 access tokenaws_s3_access_key = AAAAAAAAAAAAAAAAAAAA  aws_s3_secret_key = BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB  aws_s3_use_instance_profile = false  aws_s3_use_aws_sdk_default_behavior = false  # Set this to false if you do not want default  # storage created in the object storage using  # the details provided above  enable_load_volume_from_conf = true  # 本机 IP，需要与 cn 中的配置对齐priority_networks = 10.0.9.20/24\n\n启动 FE 之前最好先删除 meta/. 下的所有元数据文件然后再启动。\n添加 CN 节点FE 启动成功之后连接上 FE，然后手动添加 CN 节点。\nALTER SYSTEM ADD COMPUTE NODE &quot;127.0.0.1:9050&quot;;show compute nodes;\n\n然后就可以创建存算分离的表了。\nCREATE TABLE IF NOT EXISTS par_tbl1(    datekey DATETIME,    k1      INT,    item_id STRING,    v2      INT)PRIMARY KEY (`datekey`,`k1`) PARTITION BY date_trunc(&#x27;day&#x27;, `datekey`) PROPERTIES (&quot;compression&quot; = &quot;LZ4&quot;,&quot;datacache.enable&quot; = &quot;true&quot;,&quot;enable_async_write_back&quot; = &quot;false&quot;,&quot;enable_persistent_index&quot; = &quot;true&quot;,&quot;persistent_index_type&quot; = &quot;LOCAL&quot;,&quot;replication_num&quot; = &quot;1&quot;,&quot;storage_volume&quot; = &quot;builtin_storage_volume&quot;);\n\n\n最终其实是参考官方提供的 docker-compose 的编排文件进行部署的：https://raw.githubusercontent.com/StarRocks/demo/master/documentation-samples/quickstart/docker-compose.yml\n\n如果只是想在本地搭建一个存算分离的版本，可以直接使用这个 docker compose.\n\n其中有两个坑需要注意：\n创建表超时建表出现超时，提示需要配置时间:\nadmin set frontend config(&quot;tablet_create_timeout_second&quot;=&quot;50&quot;)\n\n配置也不能解决问题，依然会超时，可以看看本地是否有开启代理，尝试关闭代理试试看。\nunknown compression type(0) backend [id&#x3D;10002]不支持的压缩类型：这个问题我在使用 main 分支的 FE 与最新的 starrocks/cn-ubuntu:3.4-latest 的镜像会触发，当我把 FE 降低到具体到 tag 分支，比如 3.3.9 的时候就可以了。\n具体原因就没有细究了，如果要本地 debug 使用最新的 tag 也能满足调试的需求。\n参考链接：\n\nhttps://github.com/StarRocks/starrocks/issues/55301\nhttps://docs.starrocks.io/zh/docs/deployment/shared_data/minio/\n\n","categories":["OB","StarRocks"],"tags":["StarRocks"]},{"title":"深入理解 StarRocks 的元数据管理","url":"/2024/11/11/ob/StarRocks-meta/","content":"背景最近在排查 starrocks 线上的一个告警日志：\n\n每隔一段时间都会打印 base-table 也就是物化视图的基表被删除了，但其实表还在，也没人去删除；我们就怀疑是否真的表被删除了（可能是 bug）。\n与此同时还有物化视图 inactive 的日志，也怀疑如果视图是 inactive 之后会导致业务使用有问题。\n为了确认这个日志是否对使用影响，就得需要搞清楚它出现的原因；于是我就着手从日志打印的地方开始排查。\n\n\n问题排查从这个代码可以看出，是在查询表的信息的时候没有查到，从而导致日志打印 base-table 被 dropped 了。\n而我查询了几天的 drop table 的日志，依然没有找到可能是程序 bug 导致被删除的痕迹。\n\n好在 starrocks 的日志打印非常详细，包含了线程名称、类+方法名称，还有具体的代码函数，很容易就定位日志输出的地方。\n\n元数据只是为何会调用到这里还需要阅读源码从而找到原因，在开始之前需要先了解一下 starrocks 元数据的一些基本概念。\n\n其实在这篇文章：StarRocks 元数据管理及 FE 高可用机制中已经有全面的介绍，只是这篇文章有点早了，和现在最新的代码不太匹配。\n\n在 StarRocks 元数据中会保存 Database、Table 等信息。\n这些数据定期保存在 fe/meta 目录中。\nStarRocks 对元数据的每一次操作（增删改查数据库、表、物化视图）都会生成 editLog 的操作日志。\n\n\n新建数据库、修改表名称等\n\n当 StarRocks 的 FE 集群部署时，会由 leader 的 FE 启动一个 checkpoint 线程，定时扫描当前的元数据是否需要生成一个 image.$&#123;JournalId&#125; 的文件。\n\n\n其实就是判断当前日志数量是否达到上限（默认是 5w）生成一次。\n\n具体的流程如下：\n\n判断当前是否需要将日志生成 image\n加载当前 image 里的元数据到内存\n从 bdb 中读取最新的 Journal，然后进行重放（replay）：其实就是更新刚才加载到内存中的元数据。\n基于内存中的元数据重新生成一份 image 文件\n删除历史的 image 文件\n将生成的 image 文件名称通知 FE 的 follower 节点，让他们下载到本地，从而可以实现 image 同步。\n\n\n\n通知 follower 下载 image。\n\n元数据同步流程完整的流程图如下图：\n在这个流程图有一个关键 loadImage 流程：\n他会读取 image 这个文件里的数据，然后反序列化后加载到内存里，主要就是恢复数据库和表。\n还会对每个表调用一次 onReload() 函数，而这个函数会只 MV(MATERIALIZED VIEWS) 生效。\n这个函数正好就是在文初提到的这个函数 com.starrocks.catalog.MaterializedView#onReloadImpl：\n从他的实现来看就是判断视图所依赖的基表是否存在，如果有一个不存在就会将当前基表置为 inactive。\n如果碰到视图的基表也是视图，那就递归再 reload 一次。\n复现问题既然知晓了这个加载流程，再结合源码应该不难看出这里的问题所在了。\n从这里的加载数据库可以看出端倪，如果我的视图和基表不在同一个数据库里，此时先加载视图是不是就会出现问题？\n加载视图的时候会判断基表是否存在，而此时基表所在的数据库还没加载到内存里，自然就会查询不到从而出现那个日志。\n我之前一直在本地模拟，因为都是在同一个数据库里的基表和视图，所以一直不能复现。\n只要将基表和视图分开在不同的数据库中，让视图先于数据库前加载就会触发这个日志。\n修复问题要修复这个问题也很简单，只要等到所有的数据库都表都加载完毕后再去 reload 物化视图就可以了。\n当我回到 main 分支准备着手修改时，发现这个问题已经被修复了：https://github.com/StarRocks/starrocks/pull/51002\n\n修复过程也很简单，就是 reload 时跳过了 MV，等到所有的数据都加载完之后会在 com.starrocks.server.GlobalStateMgr#postLoadImage 手动加载 MV。\n\n这个 PR 修复的问题也是我一开始提到的，会打印许多令人误解的日志。\n到这里就可以解释文章开头的那个问题了：打印的这个 base-table 被删除的日志对业务来说没有影响，只是一个 bug 导致出现了这个日志。\n额外提一句，这个日志也比较迷，没有打印数据库名称，如果有数据库名称的话可能会更快定位到这个问题。\n参考文章：\n\nhttps://xie.infoq.cn/article/6f2f9f56916f0eb2fdb6b001a\nhttps://github.com/StarRocks/starrocks/pull/51002\n\n","categories":["StarRocks"],"tags":["StarRocks"]},{"title":"StarRocks 如何在本地搭建存算分离集群","url":"/2025/09/18/ob/StarRocks-shard-data-cluster/","content":"之前写过一篇 StarRocks 开发环境搭建踩坑指北之存算分离篇讲解如何在本地搭建一个可以 debug 的存算分离版本。\n但最近在本地调试一个场景，需要 CN 节点是以集群的方式启动，我还是按照老方法通过 docker 启动 CN，然后 export 端口的方式让 FE 进行绑定。\n比如用以下两个命令可以启动两个 CN 节点。\ndocker run -p 9060:9060 -p 8040:8040 -p 9050:9050 -p 8060:8060 -p 9070:9070 -itd --rm --name cn -e &quot;TZ=Asia/Shanghai&quot; starrocks/cn-ubuntu:3.5.2\n\ndocker run -p 9061:9060 -p 8041:8040 -p 9051:9050 -p 8061:8060 -p 9071:9070 -itd --rm --name cn2 -e &quot;TZ=Asia/Shanghai&quot; starrocks/cn-ubuntu:3.5.2\n\n\n\n然后按照之前的方式在 FE 中手动绑定这两个节点：\nALTER SYSTEM ADD COMPUTE NODE &quot;127.0.0.1:9050&quot;;  ALTER SYSTEM ADD COMPUTE NODE &quot;127.0.0.1:9051&quot;;  show compute nodes;\n\n\n此时会出现新增的第二个节点的状态有问题，比如 metrics 取不到，workerId 是-1（-1 代表节点创建失败了，默认值是 -1)\n经过 debug 发现是在添加节点的时候，由于生成的 workerIpPort 与上一个节点相同（127.0.0.1:9060) 从而导致这个节点被跳过了。\n也就是说我这两个 CN 节点不能是相同的 IP（用不同的端口来区分）。\n解决这个问题有以下几个办法：\n\n再找一个台机器来跑 CN2 节点\n启动一个虚拟机来跑 CN2 节点\n使用 docker compose 来启动 CN 集群，会在集群内自动分配不同的 IP\n利用 Docker Bridge 创建一个虚拟网络，由他来分配 IP\n\n第一种方案直接 Pass 了，我手上没有多余的设备。\n第二种方案倒是可以直接用 OrbStack 启动一个 VM，但是还不如后面的 docker 来的轻量，此外还需要我安装运行环境，也 pass 了。\n第三种方案看似可行，但也比较繁琐，由于 CN 给 docker compose 管理了，FE 要和 CN 网络打通也得在 docker compose 里运行，这样我 Debug 就不方便了，更别提如果需要频繁修改源码的情况。\n\n甚至每次修改代码后都得重新打包上传镜像，以及开启 remote debug，非常麻烦。\n\n这么看来就第四种方案最为合适了。\n使用 Docker Bridge 网络我们可以使用 Docker Bridge 创建一个虚拟网络，使用这个虚拟网络启动的镜像会自动分配自定义范围的 IP；同时本地启动的 FE 也能直接访问。\ndocker network create --subnet=172.18.0.0/16 --gateway=172.18.0.1 my_custom_net\n首先用 docker 创建一个 network。\n\n--subnet=172.18.0.0/16: 定义网络的 IP 地址范围。这里我们使用了 172.18.x.x 这个私有网段。\n--gateway=172.18.0.1: 指定这个网络的网关地址。\n\n之后我们就可以使用这个虚拟网络来启动容器了。\ndocker run --ip 172.18.0.20 --net my_custom_net -p 9060:9060 -p 8040:8040 -p 9050:9050 -p 8060:8060 -p 9070:9070 -itd --rm --name cn -e &quot;TZ=Asia/Shanghai&quot; starrocks/cn-ubuntu:3.5.2docker run --ip 172.18.0.30 --net my_custom_net -p 9061:9060 -p 8041:8040 -p 9051:9050 -p 8061:8060 -p 9071:9070 -itd --rm --name cn2 -e &quot;TZ=Asia/Shanghai&quot; starrocks/cn-ubuntu:3.5.2\n\n这样这两个容器就会被分配不同的 IP，并且网络和宿主机也是互通的。\n需要注意的是这里的子网尽量选择 172.16.0.0 到 172.31.255.255 这个 IP 段，192.168.0.0 到 192.168.255.255 这个范围段很有可能家里或公司的路由器占用了。\n而这里的网关 --gateway=172.18.0.1地址也需要在我们自定义的 IP 范围里。\n同时我们也不需要在这两个容器内为 CN 指定 priority_networks 参数了。\n同理 minio 也得使用这个虚拟网络启动：\ndocker run -d --rm --name minio \\  --ip 172.18.0.10 \\  --net my_custom_net \\  -e MINIO_ROOT_USER=miniouser \\  -e MINIO_ROOT_PASSWORD=miniopassword \\  -p 9001:9001 \\  -p 9000:9000 \\  --entrypoint sh \\  minio/minio:latest \\  -c &#x27;mkdir -p /minio_data/starrocks &amp;&amp; minio server /minio_data --console-address &quot;:9001&quot;&#x27;\n\n设置 token 的时候也要指定对应的 IP:\nmc alias set myminio http://172.18.0.10:9000 miniouser miniopassword; mc admin user svcacct add --access-key AAAAAAAAAAAAAAAAAAAA --secret-key BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB myminio miniouser\n\n当 CN 和 minio 都启动之后，我们在 FE 里手动绑定这两个 CN 节点:\nALTER SYSTEM ADD COMPUTE NODE &quot;172.18.0.20:9050&quot;;ALTER SYSTEM ADD COMPUTE NODE &quot;172.18.0.30:9050&quot;\n\n这样这两个节点就可以绑定成功了。\n#Blog \n","categories":["OB","StarRocks"],"tags":["StarRocks"]},{"title":"VictoriaLogs：一款超低占用的 ElasticSearch 替代方案","url":"/2023/08/23/ob/VictoriaLogs-Intro/","content":"\n背景前段时间我们想实现 Pulsar 消息的追踪流程，追踪实现的效果图如下：\n实现其实比较简单，其中最重要的就是如何存储消息。\n\n消息的读取我们是通过 Pulsar 自带的 BrokerInterceptor 实现的，对这个感兴趣的朋友后面会单独做一个分享。\n\n\n\n根据这里的显示内容我们大概需要存储这些信息：\n\n客户端地址\n消息发布时间\n分发消费者、订阅者名称\nACK 消费者、订阅者名称\n消息 ID最终捋了下：\n\n都以两个 consumer 计算：一条消息占用内存：140+ 535*2 + 536*2 =2282byte存储三天：TPS * 86400 * 3&#x3D;TPS*259200 条总存储：2282*TPS*259200≈ 百GB\n根据我们的 TPS 计算，三天的大概会使用到 上百 G 的存储，这样首先就排除了 Redis 这种内存型数据库。\n同样的换成 MySQL 存储也不划算，因为其实这些数据并不算那么重要。\n做了几个技术选型都不太满意，不是资源开销太大就是没有相关的运维经验。\n后面在领导的提醒下，我们使用的 VictoriaMetrics 开源了一个 VictoriaLogs，虽然当时的版本还是 0.1.0，使用过他们家 Metrics 的应该都会比较信任他们的技术能力，所以就调研了一下。\n具体的信息可以查看官方文档：https://docs.victoriametrics.com/VictoriaLogs/\n\n简单来说就是它也是一个日志存储数据库，并且有着极低的资源占有率，相对于 ElasticSearch 来说内存、磁盘、CPU 都是几十倍的下降率。\n\n通过官方的压测对比图会发现确实在各方面对 ES 都是碾压。\n官方宣传的第一反应是不能全信，于是我自己压测了一下，果然 CPU 内存 磁盘的占用都是极低的。\n\n 同时也发现运维部署确实简单，直接一个 helm install 就搞定，就是一个二进制文件，不会依赖第二个组件。\n\n按照刚才同样的数据存储三天，只需要不到 6G 的磁盘空间，我们生产环境已经平稳运行一段时间了。因为我们是批量写入数据的，所以在最高峰 20K 的 TPS 下 CPU 使用不到 0.1 核，内存使用最高 120M，这点确实是对 ES 碾压了。\n磁盘占用也是非常少。\n这些有点得归功于它有些的压缩、编解码算法，以及 Golang  带来的相对于 Java 的极低资源占用。\n还存在的问题如果一切都这么完美的话那 VictoriaLogs  确实也太变态了， 自然他也有一些不太完美的地方。\n分词功能有限首先第一个是分词功能有限，只能做简单的搜索，无法做到类似于 ES 的各种分词，插件当然也别想了。\n不支持集群当前版本不支持集群部署，也就是无法横向扩展了；不过幸好他的的单机性能已经非常强了。\n这也是目前阶段部署简单的原因。\n过期时间无法混用VictoriaLogs 支持为数据配置过期时间自动删除，有点类似于 Redis，它会在后台启动一个协程定期判断数据是否过期，但只能对所有数据统一设置。\n比如我想在 VictoriaLogs 中存放两种不同类型的数据，同时他们的过期删除时间也不相同；比如一个是三天删除，一个是三月后删除。\n这样的需求目前是无法实现的，只能部署两个 VictoriaLogs.\n默认无法查询所有字段\n由于 VictoriaLogs  可以存储非结构化数据，默认情况下只能查询内置的三个字段，我们自定义的字段目前没法自动查询，需要我们手动指定。\n这个倒不是致命问题，只是使用起来稍微麻烦一些；社区也有一些反馈，相信不久就会优化该功能。\n\nhttps://github.com/VictoriaMetrics/VictoriaMetrics/issues/4780\nhttps://github.com/VictoriaMetrics/VictoriaMetrics/issues/4513\n\n没有官方 SDK\n这也是个有了更好的一个功能，目前只能根据 REST API 自己编写。\n总结当前我们只用来存储 Pulsar 链路追踪数据，目前看来非常稳定，各方面资源占用极少；所以后续我们会陆续讲一些日志类型的数据迁移过来，比如审计日志啥的。\n之后再逐步完善功能后，甚至可以将所有应用存放在 ElasticSeach 中的日志也迁移过来，这样确实能省下不少资源。\n总得来说 VictoriaLogs  资源占用极少，如果只是拿来存储日志相关的数据，没有很强的分词需求那它将非常合适。\n截止到目前最新版也才 0.3.0 还有很大的进步空间，有类似需求的可以持续关注。#Blog #Vlogs #CloudNative \n","categories":["VictoriaLogs"],"tags":["CloudNative"]},{"title":"顶级开源社区都能吵起来？","url":"/2024/03/20/ob/about-opensource-argument/","content":"起因因为订阅了 Pulsar 的开发者邮件，前段时间看到一封标题为《(Apache committer criteria) [ANNOUNCE] New Committer: Asaf Mesika》的邮件。\n乍一看以为是欢迎 Asaf Mesika 成为 Committer，但仔细一看不太对劲，这内容也太多了，以往的欢迎都是简单的 Congratulations! 作为回复，这篇内容明显有点多了，于是便仔细看了下。\n\n\n\n\n争论大概的意思是这封邮件的作者 Kalwit 对成为 Committer 的标准产生了疑问：\n\n他觉得本次提名成为 Committer 的大部分贡献都是一些文档相关的内容，还有少部分是与监控相关的提案。他们团队使用 Pulsar 有一段时间了，但目前还未发现稳定的 Pulsar 版本；大部分的 Review 都是来自同一公司（streamnative）。看起来是整个 Pulsar 项目由某一家公司控制了，他们当初选择从 Kafka 切换到 Pulsar 就是因为 Kafka 由 Confluent 控制，才选择一个更加开放的社区。\n\n这样的一封有着“讨伐”意味的邮件一经发出，自然是一石激起千层浪，社区里很多成员都发表了回复。\n这里我挑选了几个代表性的回复：大概意思就是 Pulsar 是一个开放性项目，任何人都可以参加，每两周也有 Zoom 会议，也是每个人都可以参加。\n在远程的社区异步沟通过程中，很有可能你的请求没有得到及时的响应，这很正常。\nPulsar 是由社区开发负责维护的，没有公司对此负责，因此没有得到响应时是没有公司可以责怪的；需要大家一起来解决问题，并不一定是需要 PMC（项目管理委员会成员）还是 committer才能提出意见，任何人都可以发表自己的看法。\n但这个过程中大家的身份都是志愿者，需要大家自发的去做这些事情。\n后续 Kalwit 又继续回复了一些邮件，总体内容就是对社区治理存在疑惑；特别是担心社区背后由某一家公司作为主导，从而导致社区和公司的利益进行绑定。\n当然社区的观点依然是，Pulsar 社区不受某一具体公司掌控，并举了具体数据：在 41 个 PMC 成员中，只有 9 位是 StreamNative 的员工，41 位 committer 中有 13 位是 StreamNative 的员工。\n\n其实以我目前在社区的观感，确实是 streamnative 公司社区维护者更加活跃，其他的一些 committer 可能由于工作变动啥的很少再贡献项目了。\n\n提案被否Kalwit 举了一些例子认为这些 PIP 提案没有获得通过，但是 SN 团队提出的提案大部分都能通过。\n我觉得这确实是一种客观现象，但可能更多的原因并不是 SN 公司想要主导 Pulsar 社区的进展，而是他们在社区之外（不管是线上还是线下）进行过额外的沟通，也许在提交草案之前就已经达成了初步一致了，所以在提案审核阶段只需要做一些具体的调整就很容易被通过。\n我自己也提过一些提案，大概提交了三个只有一个通过了；我个人的感受是这个过程中响应时间确实不可控（毕竟是异步沟通），但并不会存在某个团队想要控制哪些提案可以通过，哪些提案不行的这种说法。\n都是在就事论事的讨论事情，而且不通过的话也会由相关的回复和建议，确实大部分情况也是我考虑不周。\n我也看过 asafm 的贡献，其中关于 Pulsar 集成 OpenTelemetry 的提案确实是下了功夫的（一万多字的内容），从头讲解了 OpenTelemetry 的概念，以及 Pulsar 需要做哪些事情来集成。\n个人感受厂商绑定看完之后我个人的感受是 Kalwit 或者他的团队在参与社区的时候应该是进展不顺利，一些提案或者改动没有得到支持，但看到 SN 公司提交的内容更快得到响应，所以得出了以上的结论：Pulsar 社区由 SN 公司进行了主导。\n他的顾虑也不是没有道理，就像他说的 Kafka 社区由 confluence 公司主导，类似的还有 Dubbo 社区由阿里主导、Golang 由 google 主导。\n但项目如果加入了 Apache 那他原本的公司其实已经失去了对项目的所有权，只是刚开始的一些 PMC&#x2F;committer 大部分会是这个公司的员工，毕竟他们是项目的发起者，也更加熟悉整个系统。\n如果社区发展的健康，后续应该会补充一些其他开发者，这些开发者不受雇于之前发起的项目的公司，甚至是以个人身份加入；只有这样社区就会更加多样化，出现“一言堂”的几率就会大大降低。\n我觉得造成这种现象的原因和一开始该项目是由某一个特定公司发起有有很大的原因，比如 Dubbo、Golang，所以他们公司在社区的声浪更大，自己公司的需求优先级也会更高，毕竟会有来自同一公司的更多的人来审核这些需求。\n虽说如前面邮件里回复的：社区是由志愿者自愿维护的，但不可否认的是在这些做开源项目商业化的公司内有一批人就在专门维护社区工作。\n他们会把自己商业化过程中遇到的一些问题，或者是新的 feature 也提交给社区，但这里的区别是他们是拿工资的，积极性肯定要比在社区用爱发电的开发者更积极。\n这样就会导致社区中最活跃的那批人大概率是靠社区养活自己的人，但这也不是什么坏事；如果你个人或者公司强依赖于某一个开源项目，那也可以想办法多做贡献，成为 committer，这样在一些需要投票的环节也能有一席之位。\n厂商无关当然也有对应的不是由某一个厂商发起的项目，比如我最近参与较多的 OpenTelemetry 社区。\n按照官方说法有着 1000 多位独立的开发者，代表了超过 180 家公司，在维护者的列表中也可以看到大多数都是来自于不同的公司：\n所以自然也就没有某一厂商主导的说法，所以想要避免这类事情再次发生，最好的方法还是吸纳更多的开发者加入，只有社区成员丰富起来社区才好良性发展。\n参考链接：\n\nhttps://lists.apache.org/thread/gzx4j9q0xdtcvrfvvq72t9tm2rt9h3r7\nhttps://github.com/apache/pulsar/pull/21080\nhttps://opentelemetry.io/blog/2024/opentelemetry-announced-support-for-profiling/\n\n","categories":["OB"],"tags":["OpenSource"]},{"title":"时隔五年 9K star 的 IM 项目发布 v2.0.0 了","url":"/2024/11/04/ob/cim-2.0.0/","content":"最近业余时间花了小三个月重构了 cim，也将版本和升级到了 v2.0.0，合并了十几个 PR 同时也新增了几位开发者。\n\n\n其中有两位也是咱们星球里的小伙伴🎉\n\n\n介绍上次发版还是在五年前了：\n因为确实已经很久没有更新了，在开始之前还是先介绍 cim 是什么。\n这里有一张简单的使用图片：同时以前也有录过相关的视频：\n通过 cim 这个名字和视频可以看出，它具备 IM 即时通讯的基本功能，同时基于它可以实现：\n\n即时通讯\n消息推送\nIOT 消息平台\n\n现在要在本地运行简单许多了，前提是有 docker 就可以了。\ndocker run --rm --name zookeeper -d -p 2181:2181 zookeeper:3.9.2docker run --rm --name redis -d -p 6379:6379 redis:7.4.0git clone https://github.com/crossoverJie/cim.gitcd cimmvn clean package -DskipTests=truecd cim-server &amp;&amp; cim-client &amp;&amp; cim-forward-routemvn clean package spring-boot:repackage -DskipTests=true\n\n架构cim 的架构图如下：主要分为三个部分：\n\nClient 基本交互功能\n消息收发\n消息查询\n延迟消息\n\n\nRoute 提供了消息路由以及相关的管理功能\nAPI 转发\n消息推送\n会话管理\n可观测性\n\n\nServer 主要就提供长链接能力，以及真正的消息推送\n\n同时还有元数据中心（支持扩展实现）、消息存储等组件；\n不管是客户端、route、server 都是支持集群：\n\nroute 由于是无状态，可以任意扩展\nserver 通过注册中心也支持集群部署，当发生宕机或者是扩容时，客户端会通过心跳和重连机制保证可用性。\n\n所以整个架构不存在单点，同时比较简单清晰的，大部分组件都支持可扩展。\n流程\n为了更方便理解，花了一个流程图。\n\nserver 在启动之后会先在元数据中心注册\n同时 route 会订阅元数据中的 server 信息\n客户端登陆时会调用 route 获取一个 server 的节点信息\n然后发起登陆请求。\n成功之后会保持长链接。\n\n\n客户端向发送消息时会调用 route 接口来发起消息\nroute 根据长链接关系选择 server 进行消息推送\n\n\n\nv2.0.0接下来介绍下本次 v2.0.0 有哪些重大变更，毕竟是修改了大的版本号。\n这里列举一些重大的改动：\n\n首先是支持了元数据中心，解耦了 zookeeper，也支持自定义实现。\n支持了集成测试，可以保证提交的 PR 对现有功能的影响降到最低，代码质量有一定保证；review 代码时更加放心。\n单独抽离了 client-sdk，代码耦合性更好且更易维护。\n服务之间调用的 RPC 完成了重构\n支持了动态 URL\n泛型数据解析\n\n\n还有社区小伙伴贡献的一些 bug 修复、RpcProxyManager 的 IOC 支持等特性。\n\n总结更多的部署和使用可以参考项目首页的 README，有详细的介绍。\ncim 目前还需要优化的地方非常多；接下来的重点是实现 ACK，同时会完善一下通讯协议。\ntodo 列表我也添加了很多，所以非常推荐感兴趣的朋友可以先看看 todo 列表，说不定就有你感兴趣的可以参与一下。\n","categories":["IM"],"tags":["IM"]},{"title":"IM系统重构到 SDK 设计的最佳实践","url":"/2024/10/13/ob/cim-client-sdk/","content":"\nSDK 设计\n在之前提到了 cim 在做集成测试的时候遇到的问题，需要提供一个 SDK 来解决，于是我花了一些时间编写了 SDK，同时也将 cim-client 重构了。\n\n\n重构后的代码长这个样子：\n@Beanpublic Client buildClient(@Qualifier(&quot;callBackThreadPool&quot;) ThreadPoolExecutor callbackThreadPool,                          Event event) &#123;    OkHttpClient okHttpClient = new OkHttpClient.Builder().connectTimeout(3, TimeUnit.SECONDS)            .readTimeout(3, TimeUnit.SECONDS)            .writeTimeout(3, TimeUnit.SECONDS)            .retryOnConnectionFailure(true).build();    return Client.builder()            .auth(ClientConfigurationData.Auth.builder()                    .userName(appConfiguration.getUserName())                    .userId(appConfiguration.getUserId())                    .build())            .routeUrl(appConfiguration.getRouteUrl())            .loginRetryCount(appConfiguration.getReconnectCount())            .event(event)            .reconnectCheck(client -&gt; !shutDownSign.checkStatus())            .okHttpClient(okHttpClient)            .messageListener(new MsgCallBackListener(msgLogger))            .callbackThreadPool(callbackThreadPool)            .build();&#125;\n\n\n配合 springboot 使用时只需要创建一个 Client 即可，这个 Client 里维护了核心的：\n\n长链接创建、状态维护\n心跳检测\n超时、网络异常重连等\n\n同时也提供了简易的 API 可以直接收发消息：\n这样在集成到业务代码中时会更方便。\n以前的代码耦合度非常高，同时因为基础代码是 18 年写的，现在真的没有眼看了；\n重构的过程中使用一些 Java8+ 的一些语法糖精简了许多代码，各个模块间的组织关系也重新梳理，现在会更易维护了。\n比如由于创建客户端需要许多可选参数，于是就提供了 Builder 模式的创建选项：\npublic interface ClientBuilder &#123;        Client build();      ClientBuilder auth(ClientConfigurationData.Auth auth);      ClientBuilder routeUrl(String routeUrl);      ClientBuilder loginRetryCount(int loginRetryCount);      ClientBuilder event(Event event);      ClientBuilder reconnectCheck(ReconnectCheck reconnectCheck);      ClientBuilder okHttpClient(OkHttpClient okHttpClient);      ClientBuilder messageListener(MessageListener messageListener);      ClientBuilder callbackThreadPool(ThreadPoolExecutor callbackThreadPool);  &#125;\n\n\n以上部分 API 的设计借鉴了 Pulsar。\n\nProxy 优化除此之外还优化了请求代理，这个 Proxy 主要是用于方便在各个服务中发起 rest 调用，我这里为了轻量也没有使用 Dubbo、SpringCloud 这类服务框架。\n但如果都硬编码 http client 去请求时会有许多重复冗余的代码，比如创建连接、请求参数、响应解析、异常处理等。\n于是在之前的版本中就提供了一个 ProxyManager 的基本实现：\n@Override  public List&lt;OnlineUsersResVO.DataBodyBean&gt; onlineUsers() throws Exception&#123;      RouteApi routeApi = new ProxyManager&lt;&gt;(RouteApi.class, routeUrl, okHttpClient).getInstance();        Response response = null;      OnlineUsersResVO onlineUsersResVO = null;      try &#123;          response = (Response) routeApi.onlineUser();          String json = response.body().string() ;          onlineUsersResVO = JSON.parseObject(json, OnlineUsersResVO.class);        &#125;catch (Exception e)&#123;          log.error(&quot;exception&quot;,e);      &#125;finally &#123;          response.body().close();      &#125;      return onlineUsersResVO.getDataBody();  &#125;\n\n虽然提供了一些连接管理和参数封装等基础功能，但只实现了一半。\n从上面的代码也可以看出序列化都得自己实现，这些代码完全是冗余的。\n经过重构后以上的代码可以精简到如下：\n// 声明接口@Request(method = Request.GET)  BaseResponse&lt;Set&lt;CIMUserInfo&gt;&gt; onlineUser() throws Exception;// 初始化routeApi = RpcProxyManager.create(RouteApi.class, routeUrl, okHttpClient);public Set&lt;CIMUserInfo&gt; onlineUser() throws Exception &#123;      BaseResponse&lt;Set&lt;CIMUserInfo&gt;&gt; onlineUsersResVO = routeApi.onlineUser();      return onlineUsersResVO.getDataBody();  &#125;\n\n这个调整之后就非常类似于 Dubbo gRPC 这类 RPC 框架的使用，只需要把接口定义好，就和调用本地函数一样的简单。\n为了方便后续可能调用一些外部系统，在此基础上还支持了指定多种请求 method、指定 URL 、返回结果嵌套泛型等。\n@Request(url = &quot;sample-request?author=beeceptor&quot;)  EchoGeneric&lt;EchoResponse.HeadersDTO&gt; echoGeneric(EchoRequest message);@Test  public void testGeneric() &#123;      OkHttpClient client = new OkHttpClient();      String url = &quot;http://echo.free.beeceptor.com&quot;;      Echo echo = RpcProxyManager.create(Echo.class, url, client);      EchoRequest request = new EchoRequest();      request.setName(&quot;crossoverJie&quot;);      request.setAge(18);      request.setCity(&quot;shenzhen&quot;);      // 支持泛型解析    EchoGeneric&lt;EchoResponse.HeadersDTO&gt; response = echo.echoGeneric(request);      Assertions.assertEquals(response.getHeaders().getHost(), &quot;echo.free.beeceptor.com&quot;);  &#125;\n\n\n支持动态 URL 调用\n还有一个 todo：希望可以将 ProxyManager 交给 Spring 去管理，之前是在每次调用的地方都会创建一个 Proxy 对象，完全没有必要，代码也很冗余。\n但有网友在实现过程中发现，有个场景的请求地址是动态的，如果是交给 Spring 管理为单例后是没法修改 URL 地址的，因为这个地址是在创建对象的时候初始化的。\n所以我就在这里新增了一个动态 URL 的特性：\nEchoResponse echoTarget(EchoRequest message, @DynamicUrl(useMethodEndpoint = false) String url);Echo echo = RpcProxyManager.create(Echo.class, client);String url = &quot;http://echo.free.beeceptor.com/sample-request?author=beeceptor&quot;;EchoResponse response = echo.echoTarget(request, url);\n\n在声明接口的时候使用 @DynamicUrl 的方法参数注解，告诉代理这个参数是 URL。这样就可以允许在创建  Proxy 对象的时候不指定 URL，而是在实际调用时候再传入具体的 URL，更方便创建单例了。\n集成测试优化同时还优化了集成测试，支持了 server 的集群版测试。\nhttps://github.com/crossoverJie/cim/blob/4c149f8bda78718e3ecae2c5759aa9732eff9132/cim-client-sdk/src/test/java/com/crossoverjie/cim/client/sdk/ClientTest.java#L210\n@Test  public void testReconnect() throws Exception &#123;      super.startTwoServer();      super.startRoute();        String routeUrl = &quot;http://localhost:8083&quot;;      String cj = &quot;cj&quot;;      String zs = &quot;zs&quot;;      Long cjId = super.registerAccount(cj);      Long zsId = super.registerAccount(zs);      var auth1 = ClientConfigurationData.Auth.builder()              .userName(cj)              .userId(cjId)              .build();      var auth2 = ClientConfigurationData.Auth.builder()              .userName(zs)              .userId(zsId)              .build();        @Cleanup      Client client1 = Client.builder()              .auth(auth1)              .routeUrl(routeUrl)              .build();      TimeUnit.SECONDS.sleep(3);      ClientState.State state = client1.getState();      Awaitility.await().atMost(10, TimeUnit.SECONDS)              .untilAsserted(() -&gt; Assertions.assertEquals(ClientState.State.Ready, state));          AtomicReference&lt;String&gt; client2Receive = new AtomicReference&lt;&gt;();      @Cleanup      Client client2 = Client.builder()              .auth(auth2)              .routeUrl(routeUrl)              .messageListener((client, message) -&gt; client2Receive.set(message))              .build();      TimeUnit.SECONDS.sleep(3);      ClientState.State state2 = client2.getState();      Awaitility.await().atMost(10, TimeUnit.SECONDS)              .untilAsserted(() -&gt; Assertions.assertEquals(ClientState.State.Ready, state2));        Optional&lt;CIMServerResVO&gt; serverInfo2 = client2.getServerInfo();      Assertions.assertTrue(serverInfo2.isPresent());      System.out.println(&quot;client2 serverInfo = &quot; + serverInfo2.get());        // send msg      String msg = &quot;hello&quot;;      client1.sendGroup(msg);      Awaitility.await()              .untilAsserted(() -&gt; Assertions.assertEquals(String.format(&quot;cj:%s&quot;, msg), client2Receive.get()));      client2Receive.set(&quot;&quot;);          System.out.println(&quot;ready to restart server&quot;);      TimeUnit.SECONDS.sleep(3);      Optional&lt;CIMServerResVO&gt; serverInfo = client1.getServerInfo();      Assertions.assertTrue(serverInfo.isPresent());      System.out.println(&quot;server info = &quot; + serverInfo.get());        super.stopServer(serverInfo.get().getCimServerPort());      System.out.println(&quot;stop server success! &quot; + serverInfo.get());          // Waiting server stopped, and client reconnect.      TimeUnit.SECONDS.sleep(30);      System.out.println(&quot;reconnect state: &quot; + client1.getState());      Awaitility.await().atMost(15, TimeUnit.SECONDS)              .untilAsserted(() -&gt; Assertions.assertEquals(ClientState.State.Ready, state));      serverInfo = client1.getServerInfo();      Assertions.assertTrue(serverInfo.isPresent());      System.out.println(&quot;client1 reconnect server info = &quot; + serverInfo.get());        // Send message again.      log.info(&quot;send message again, client2Receive = &#123;&#125;&quot;, client2Receive.get());      client1.sendGroup(msg);      Awaitility.await()              .untilAsserted(() -&gt; Assertions.assertEquals(String.format(&quot;cj:%s&quot;, msg), client2Receive.get()));      super.stopTwoServer();  &#125;\n\n比如在这里编写了一个客户端重连的单测，代码有点长，但它的主要流程如下：\n\n启动两个 Server：Server1，Server2\n启动 Route\n在启动两个 Client 发送消息\n校验消息发送是否成功\n\n\n停止 Client1 连接的 Server\n等待 Client 自动重连到另一个 Server\n再次发送消息\n校验消息发送是否成功\n\n\n\n这样就可以验证在服务端 Server 宕机后整个服务是否可用，消息收发是否正常。\npublic void startTwoServer() &#123;      if (!zooKeeperContainer.isRunning())&#123;          zooKeeperContainer.start();      &#125;    zookeeperAddr = String.format(&quot;%s:%d&quot;, zooKeeperContainer.getHost(), zooKeeperContainer.getMappedPort(ZooKeeperContainer.DEFAULT_CLIENT_PORT));      SpringApplication server = new SpringApplication(CIMServerApplication.class);      String[] args1 = new String[]&#123;              &quot;--cim.server.port=11211&quot;,              &quot;--server.port=8081&quot;,              &quot;--app.zk.addr=&quot; + zookeeperAddr,      &#125;;    ConfigurableApplicationContext run1 = server.run(args1);      runMap.put(Integer.parseInt(&quot;11211&quot;), run1);          SpringApplication server2 = new SpringApplication(CIMServerApplication.class);      String[] args2 = new String[]&#123;              &quot;--cim.server.port=11212&quot;,              &quot;--server.port=8082&quot;,              &quot;--app.zk.addr=&quot; + zookeeperAddr,      &#125;;    ConfigurableApplicationContext run2 = server2.run(args2);      runMap.put(Integer.parseInt(&quot;11212&quot;), run2);  &#125;public void stopServer(Integer port) &#123;      runMap.get(port).close();      runMap.remove(port);  &#125;\n这里的启动两个 Server 就是创建了两个 Server 应用，然后保存好端口和应用之间的映射关系。\n这样就可以根据客户端连接的 Server 信息指定停止哪一个 Server，更方便做测试。\n这次重启 cim 的维护后会尽量维护下去，即便更新时间慢一点。\n后续还会加上消息 ack、离线消息等之前呼声很高的功能，感兴趣的完全可以一起参与。\n源码地址：https://github.com/crossoverJie/cim\n","categories":["cim"],"tags":["cim"]},{"title":"我的 CodeReview 实战经验","url":"/2025/05/21/ob/codereview-practice/","content":"背景Code Review 是大家日常开发过程中很常见的流程，当然也不排除一些团队为了快速上线，只要功能测试没问题就直接省去了 Code Review。\n我个人觉得再忙的团队  Code Review 还是很有必要的（甚至可以事后再 Review），好处很多：\n\n跳出个人开发的思维误区，更容易发现问题\n增进团队交流，提高整体的技术氛围\n团队水平检测器，不管是审核者还是被审核的，review 几次后大概就知道是什么水平了\n\n通常 Code Review 有两种场景，一种是公司内部，还有就是开源社区。\n\n\n开源社区先说开源社区，最近也在做 cim 项目里做 Review，同时也在 Pulsar、OpenTelemetry、StarRocks 这些项目里做过 Reviewer。\n以下是一些我参与 Code Review 的一些经验：\n先提 issue在提交 PR 进行 Code Review 之前最好先提交一个 issue 和社区讨论下，你的这个改动社区是否接受。\n我见过一些事前没有提前沟通，然后提交了一个很复杂的 PR，会导致维护者很难 Review，同时也会打击参与者的积极性。\n所以强烈建议一些复杂的修改一定先要提前和社区沟通，除非这是一些十拿九稳的问题。\n个人 CI一些大型项目往往都有完善的 CI 流程来保证代码质量，通常都有以下的校验：\n\n各种测试流程（单元测试、集成测试）\n代码 Code Style 检测\n安全、依赖检测等\n\n如果一个 PR 连 CI 都没跑过，其实也没有提前 Review 的必要了，所以在提 PR 之前都建议先在自己的 repo 里将主要的 CI 都跑过再提交 PR。\n这个在 Pulsar 的官方贡献流程里也有单独提到。\n\n同时在 PR 模板里也有提到，建议先在自己的 fork 的 repo 里完成 CI 之后再提交到 upstream。\n\n这个其实也很简单，我们只要给自己的 repo 提交一个 PR，然后在 repo 设置中开启 Action，之后就会触发 CI 了。\n\n如果自己的 PR 还需要频繁的提交修改，那建议可以先修改为  draft，这样可以提醒维护者稍后再做 Review。\n同时也不建议提交一个过大的 PR，尽量控制在 500 行改动以内，这样才方便 Review。\nReview 代码\nGithub 有提供代码对比页面，但也只是简单的代码高亮，没法像 IDE 这样提供函数跳转等功能。\n\n所以对于 Reviewer 来说，最好是在本地 IDE 中添加 PR 的 repo，这样就可以直接切换到 PR 的分支，然后再本地跟代码，也更好调试。\n有相关的修改建议可以直接在 github 页面上进行评论，这样两者结合起来 Review，效率会更高。\nReview 代码其实不比写代码轻松，所以对免费帮你做 Review 的要多保持一些瑞思拜。\nAI Review现在 Github 已经支持 copilot 自动 Review 了，它可以帮我们总结变更，同时对一些参加的错误提供修改建议。\n使用它还是可以帮我们省不少事情，推荐开启。\n企业内部在企业内部做 Code Review 流程上要简单许多，毕竟沟通成本要低一些，往往都是达成一致之后才会开始开发，所以重点就是 Review 的过程了。\n既然是在公司内部，那就要发挥线下沟通的优势了；当然在开始前还是建议在内部的代码工具里比如说 gitlab 中提交一个 MR，先让参会人员都提前看看大概修改了哪些内容，最好是提前在 gitlab 中评论，带着问题开会讨论。\n实际 Review 过程应该尽量关注业务逻辑与设计，而不是代码风格、格式等细枝末节的问题。\n提出修改意见的时候也要对事不对人，我见过好几次在 Review 现场吵起来的场景，就是代入了一些主观情绪，被 Review 的觉得自己能力被质疑，从而产生了一些冲突。\nCode Review 做得好的话整个团队都会一起进步，对个人来说参与一些优质开源项目的 Code Review 也会学到很多东西。\n","categories":["OB"],"tags":["OpenSource"]},{"title":"手把手教你为开源项目贡献代码","url":"/2024/01/25/ob/create-a-plugin-for-cprobe/","content":"背景前段时间无意间看到一篇公众号 招贤令：一起来搞一个新开源项目，作者介绍他想要做一个开源项目：cprobe 用于整合目前市面上散落在各地的 Exporter，统一进行管理。\n比如我们常用的 blackbox_exporter/mysqld_exporter 等。\n\n以往的每一个 Exporter 都需要单独部署运维。\n\n\n同时又完全兼容 Prometheus 生态，也可以复用现有的监控面板。\n恰好这段时间我也在公司从事可观测性相关的业务，发现这确实是一个痛点。\n于是便一直在关注这个项目，同时也做了些贡献；因为该项目的核心是用于整合 exporter，所以为其编写插件也是非常重要的贡献了。\n编写插件整个项目执行流程图如下：\n可以看到编写插件最核心的便是自定义插件解析自定义的配置文件、抓取指标的逻辑。\n比如我们需要在配置中指定抓取目标的域名、抓取规则等。\n这里  cprobe 已经抽象出了两个接口，我们只需要做对应的实现即可。\ntype Plugin interface &#123;      // ParseConfig is used to parse config      ParseConfig(baseDir string, bs []byte) (any, error)      // Scrape is used to scrape metrics, cfg need to be cast specific cfg      Scrape(ctx context.Context, target string, cfg any, ss *types.Samples) error  &#125;\n\n下面就以我之前编写的 Consul 为例。\n# Allows any Consul server (non-leader) to service a read.  allow_stale = true    # === CA  # File path to a PEM-encoded certificate authority used to validate the authenticity of a server certificate.  ca_file = &quot;/etc/consul.d/consul-agent-ca.pem&quot;    # File path to a PEM-encoded certificate used with the private key to verify the exporter&#x27;s authenticity.  cert_file = &quot;/etc/consul.d/consul-agent.pem&quot;    # Generate a health summary for each service instance. Needs n+1 queries to collect all information.  health_summary = true    # File path to a PEM-encoded private key used with the certificate to verify the exporter&#x27;s authenticity  key_file = &quot;/etc/consul.d/consul-agent-key.pem&quot;    # Disable TLS host verification.  insecure = false\n\n这里每个插件的配置都不相同，所以我们需要将配置解析到具体的结构体中。\nfunc (*Consul) ParseConfig(baseDir string, bs []byte) (any, error) &#123;      var c Config      err := toml.Unmarshal(bs, &amp;c)      if err != nil &#123;         return nil, err      &#125;        if c.Timeout == 0 &#123;         c.Timeout = time.Millisecond * 500      &#125;      return &amp;c, nil  &#125;\n解析配置文件没啥好说的，根据自己的逻辑实现即可，可能会配置一些默认值而已。\n\n下面是核心的抓取逻辑，本质上就是使用对应插件的 Client 获取一些核心指标封装为 Prometheus 的 Metric，然后由 cprobe 写入到远端的 Prometheus 中(或者是兼容 Prometheus 的数据库中)。\n// Create clientconfig.HttpClient.Timeout = opts.Timeout  config.HttpClient.Transport = transport    client, err := consul_api.NewClient(config)  if err != nil &#123;      return nil, err  &#125;    var requestLimitChan chan struct&#123;&#125;  if opts.RequestLimit &gt; 0 &#123;      requestLimitChan = make(chan struct&#123;&#125;, opts.RequestLimit)  &#125;\n\n所有的指标数据都是通过对应的客户端获取。\n如果是迁移一个存在的  export 到 cprobe 中时，这些抓取代码我们都可以直接复制对应 repo 中的代码。\n比如我就是参考的：https://github.com/prometheus/consul_exporter\n除非我们是重新写一个插件，不然对于一些流行的库或者是中间件都已经有对应的 exporter 了。\n具体的列表可以参考这里：https://prometheus.io/docs/instrumenting/exporters/\n\n之后便需要在对应的插件目录(./conf.d)创建我们的配置文件：\n为了方便测试，可以在启动 cprobe 时添加 -no-writer 让指标打印在控制台，从而方便调试。\n总结之前就有人问我有没有毕竟好上手的开源项目，这不就来了吗？\n正好目前项目创建时间不长，代码和功能也比较简单，同时还有可观察系统大佬带队，确实是一个非常适合新手参与的开源项目。\n项目地址：\nhttps://github.com/cprobe/cprobe\n私货\n最后夹带一点私货：前两天帮一个读者朋友做了一次付费的技术咨询（主要是关于 Pulsar 相关的），也是我第一次做付费内容，这种拿人钱财替人消灾难道就是知识付费的味道吗😂？\n\n所以我就趁热打铁在朋友圈发了个广告，没想到又有个朋友找我做关于职场相关咨询，最后能帮助到对方自己也很开心。\n其实经常也有人通过社媒、邮件等渠道找我帮忙看问题，一些简单的我通常也会抽时间回复。\n但后面这位朋友也提到，如果我不是付费，他也不好意思来找我聊这些内容，毕竟涉及到一些隐私，同时也需要占用双方 1～2 小时的时间。\n这样明码标价的方式确实也能更方便的沟通，同时也能减轻对方的心里负担，直接从白嫖转为付费大佬。\n铺垫了这么多，主要目的是想进行一个小范围的尝试，如果对以下内容感兴趣的朋友欢迎加我微信私聊：\n\n包括但不限于技术、职场、开源等我有经验的行业都可以聊。\n\n\n反馈不错的话也需要可以作为我的长期副业做下去。#Blog \n","categories":["OB"],"tags":["OpenSource","开源"]},{"title":"Github commit 签名+合并 Commit","url":"/2023/09/18/ob/git-tips-rebase/","content":"\n背景前段时间给 VictoriaLogs 提交了一个 PR：https://github.com/VictoriaMetrics/VictoriaMetrics/pull/4934\n本来一切都很顺利，只等合并了，但在临门一脚的时候社区维护人员问我可否给 git commit 加上签名。\n\n于是我就默默的调试到了凌晨四点😭\n\n\n\n以前我也没怎么注意过这个选项，经过 Google 后发现 Idea 在提交的时候可以自行设置。\n当我勾选了这个提交新的代码后，依然被告知没有正确的签名，这时我才发现理解错误了。\n为 GitHub 的提交签名结合这位社区大佬给的文档，他所需要的是每次提交的代码都是有签名的，类似于这样：\n如果我们想要 GitHub 现实 Verified 这个标签，那就需要对 commit 或者是打的 tag 进行签名。\n而签名的方式有三种：GPG, SSH, S/MIME，这里我以 GPG 签名为例，整体流程如下：\n\n先在https://www.gnupg.org/download/这里下载安装 GPG 的命令行程序。\ngpg --full-generate-key\n\n使用这个命令生成 key，之后会根据提示录入一些信息，包含你的 ID 和邮箱，建议都和 GitHub 的 ID 邮箱保持一致即可，然后一路回车完事。\n之后可以使用这个命令查看刚才创建的 Key：\ngpg --list-secret-keys --keyid-format=long------------------------------------sec   4096R/3AA5C34371567BD2 2016-03-10 [expires: 2017-03-10]uid                          Hubot &lt;hubot@example.com&gt;ssb   4096R/4BB6D45482678BE3 2016-03-10\n\n我们需要将 3AA5C34371567BD2 这个 Key 的 ID 字符串复制，之后执行：\ngpg --armor --export 3AA5C34371567BD2# Prints the GPG key ID, in ASCII armor format\n\n此时会打印出公钥，我们将\n-----BEGIN PGP PUBLIC KEY BLOCK----------END PGP PUBLIC KEY BLOCK-----\n这些数据复制到 GitHub 的个人设置页面：\n此时还没完，如果我们直接提交代码的也不会有 Verified 的标签。\n\n我们还需要打开 git 的 config 设置：\ngit config commit.gpgsign true# 全局打开git config --global commit.gpgsign truegit commit -S -m &quot;YOUR_COMMIT_MESSAGE&quot;git push\n\n这样提交的 Commit 就会打上验证的标签了。\n\n-S 的效果和在 idea 中选中 Sign-off 的效果一样。\n\n官方文档也有详细的步骤：https://docs.github.com/en/authentication/managing-commit-signature-verification/about-commit-signature-verification\nSquash 合并提交不过在我这个 PR 的背景下还有一个步骤没有完成，就是我之前提交的 Commit 都没要验证，我需要将他们都合并为一个验证的 Commit 然后在强制推送上去，这样整个 git log 看起来才足够简洁。\n最终效果如下，只有一个 Commit 存在。\n这时候就得需要 git rebase 出马了。\n以刚才测试的这两个提交为例，我需要将他们合并为一个提交。\n我们先使用这个命令：\ngit rebase -i HEAD~Ngit rebase -i HEAD~2\nN 就是我们需要合并几个提交，在我这里就是 2.\n我们需要将除了第一个 commit 之外的都修改为 s，也就是下面注释里的 squash 的简写（压缩的意思）。\n这是一个 vim 的交互编辑模式，编辑完成之后保存退出。\n\n不会还有程序员不知道如何保存 vim 退出吧🐕。\n\n保存后又会弹出一个编辑页面，让我们填写这次压缩之后的提交记录，默认会帮我生成好，当然你也可以全部删掉后重写。\n\n我这里就直接使用它生成好的就可以了，依然还是保存退出。\n最后再强行推送到我所在的分支即可：\ngit push origin test-rebase -f\n\n在这个分支的提交页面也只会看到刚才强行推送的记录了，刚才的两个提交已经合并为这一个了。\n\n将修改提交到其他分支有时候线上出现问题需要马上修复的时候，我会不下意识的直接就开始改了，等真的提交代码被拒的时候才发现是在主分支上。\n我觉得有类似需求的场景还不少，这时候就需要将当前分支的修改提交到一个新的分支上，总不能 revert 之后重新再写吧。\n所以通常我的流程是这样的：\n# 新建一个分支git branch newbranch# 将当前分支的修改临时保存到暂缓区，同时回滚当前分支。git stash# 切换到新的分支git checkout newbranch# 从暂缓区中取出刚才的修改git stash pop\n\n这样之前分支的修改就会同步到新的分支上了，借着便在新的分支上继续开发了。\n总结借着这个机会也了解了 rebase 的骚操作挺多的，不过我平时用的最多的还是 merge，这个倒没有好坏之分，只要同组的开发者都达成一致即可。\n#Blog #Github #Git\n","categories":["git"],"tags":["rebase"]},{"title":"【译】五个我最近在 Go 里学到的小技巧","url":"/2024/07/02/ob/go-5-tips/","content":"原文链接：https:&#x2F;&#x2F;medium.com&#x2F;@andreiboar&#x2F;5-small-tips-i-recently-learned-in-go-cf52d50cf129\n让编译器计算数组数量我们在 Go 通常很少使用数组 arrays，一般使用切片 Slice 来代替；\n但是当你需要使用的时候，如果你对需要指定数量大小感到很烦时可以使用 [...] 让编译器自动帮我们计算数组大小：\npackage main    import &quot;fmt&quot;    func main() &#123;  arr := [3]int&#123;1, 2, 3&#125;  sameArr := [...]int&#123;1, 2, 3&#125; // Use ... instead of 3    // Arrays are equivalent  fmt.Println(arr)  fmt.Println(sameArr)  &#125;\n\n\n\n使用 go run . 替换 go run main.go每当我用 Go 写第一行代码时，我都习惯于开始写 main.go:\npackage mainimport &quot;fmt&quot;func main() &#123;    sayHello()&#125;func sayHello() &#123;    fmt.Println(&quot;Hello!&quot;)&#125;  \n\n但是当 main.go 变得越来越大时，我喜欢把一些结构体移动到新的文件里，还是在 main 这个包中。\nmain.go:\npackage main    func main() &#123;  \tsayHello()  &#125;\n\nsay_hello.go:\npackage main    import &quot;fmt&quot;    func sayHello() &#123;  \tfmt.Println(&quot;Hello!&quot;)  &#125;\n\n此时使用 go run main.go 将会得到以下的错误：\n# command-line-arguments  ./main.go:4:2: undefined: sayHello\n\n此时可以使用 go run . 来解决这个问题。\n使用下划线让你的数字变得更易读你知道可以使用下划线使得你的长数字更易读吗？\npackage mainimport &quot;fmt&quot;func main() &#123;    number := 10000000    better := 10_000_000    fmt.Println(number == better)&#125; \n\n可以在同一个包下有不同的测试包在 Go 中我通常认为一个目录下只能有一个包，但也不是完全正确的。\n假设你有一个包名为：yourpackage 此时你可以还可以在同一个目录下创建一个名为 yourpackage_test 的包，同时在这个包里编写你的测试代码。\n这样做的好处是，那些没有被 exporter 的函数在 yourpackage_test 包下是不能直接访问的，确保测试的是被暴露的函数。\n多次传递相同参数的简单方法在使用字符串格式化函数时，我总是觉得必须重复一个多次使用的参数很烦人：\npackage mainimport &quot;fmt&quot;func main() &#123;    name := &quot;Bob&quot;    fmt.Printf(&quot;My name is %s. Yes, you heard that right: %s\\n&quot;, name, name)&#125; \n还好还有更简便的方法，这样只需要传递一次参数：\npackage main    import &quot;fmt&quot;    func main() &#123;  \tname := &quot;Bob&quot;  \tfmt.Printf(&quot;My name is %[1]s. Yes, you heard that right: %[1]s\\n&quot;, name)  &#125;\n\n在这个 Twitter 里发现的：\n希望你今天学到了一些新东西，最近有没有发现一些你从来不知道的 Golang 小技巧？\n","categories":["翻译","Go"],"tags":["Go"]},{"title":"关于 Golang 的错误处理的讨论可以大结局了","url":"/2025/06/05/ob/go-error-future/","content":"  原文链接：[ On | No ] syntactic support for error handling\n\n关于 Go 语言最有争论的就是错误处理：\nx, err := call()if err != nil &#123;        // handle err&#125;\n\n\nif err != nil 类似于这样的代码非常多，淹没了其余真正有用的代码。这通常发生在进行大量API调用的代码中，其中错误处理很普遍，只是简单地返回错误，有些最终的代码看起来像这样：\nfunc printSum(a, b string) error &#123;    x, err := strconv.Atoi(a)    if err != nil &#123;        return err    &#125;    y, err := strconv.Atoi(b)    if err != nil &#123;        return err    &#125;    fmt.Println(&quot;result:&quot;, x + y)    return nil&#125;\n\n\n\n在这个函数的十行代码中，只有四行看起来是有实际的作用。其余六行看起来甚至会影响主要的逻辑。所以关于错误处理的抱怨多年来一直位居我们年度用户调查的榜首也就不足为奇了。（有一段时间，缺乏泛型支持超过了对错误处理的抱怨，但现在 Go 已经支持泛型了，错误处理又回到了榜首。）\nGo团队认真对待社区反馈，因此多年来我们一直在尝试为这个问题找到解决方案，并听取 Go 社区的意见。\nGo 团队的第一次明确尝试可以追溯到 2018 年，当时Russ Cox正式提到了这个问题，作为我们当时称为 Go2 努力的一部分。他基于 Marcel van Lohuizen 的草案设计概述了一个可能的解决方案。该设计基于check和handle机制，相当全面。草案包括对替代解决方案的详细分析，包括与其他语言采用的方法的比较。如果您想知道您的特定错误处理想法之前是否被考虑过，请阅读这份文档！\n// printSum implementation using the proposed check/handle mechanism.func printSum(a, b string) error &#123;    handle err &#123; return err &#125;    x := check strconv.Atoi(a)    y := check strconv.Atoi(b)    fmt.Println(&quot;result:&quot;, x + y)    return nil&#125;\ncheck和handle方法被认为过于复杂，大约一年后，在2019年，我们推出了更加简化的、现在臭名昭著的try提案。它基于 check 和 handle 的思想，但 check 伪关键字变成了try内置函数，handle部分被省略了。为了探索try内置函数的影响，我们编写了一个简单的工具（tryhard），使用try重写现有的错误处理代码。这个提案被激烈争论，在GitHub问题上接近900条评论。\n// printSum implementation using the proposed try mechanism.func printSum(a, b string) error &#123;    // use a defer statement to augment errors before returning    x := try(strconv.Atoi(a))    y := try(strconv.Atoi(b))    fmt.Println(&quot;result:&quot;, x + y)    return nil&#125;\n\n然而，try通过在出错时从封闭函数返回来影响控制流，并且可能从深度嵌套的表达式中这样做，从而隐藏了这种控制流。这使得该提案对许多人来说难以接受，尽管在这个提案上投入了大量精力，我们还是决定放弃这项工作。回顾起来，引入一个新关键字可能会更好，这是我们现在可以做的事情，因为我们通过go.mod文件和特定文件的指令对语言版本有细粒度的控制。将try的使用限制在赋值和语句中可能会缓解一些其他的担忧。Jimmy Frasche的最近提案基本上回到了原始的check和handle设计，并解决了该设计的一些缺点，正朝着这个方向发展。\ntry提案的反响导致了大量的反思，包括Russ Cox的一系列博客文章：“关于Go提案流程的思考”。其中一个结论是，我们可能通过提出一个几乎完全成熟的提案，给社区反馈留下很少的空间，以及一个”具有威胁性”的实现时间表，从而降低了获得更好结果的机会。根据“Go提案流程：大型变更”：”回顾起来，try是一个足够大的变更，我们发布的新设计应该是第二版草案设计，而不是带有实现时间表的提案”。但不管在这种情况下可能存在的流程和沟通失败，用户对该提案有着非常强烈地抵触情绪。\n当时我们没有更好的解决方案，几年来都没有为错误处理追求语法变更。不过，社区中的许多人受到了启发，我们收到了源源不断的错误处理提案，其中许多非常相似，有些有趣，有些难以理解，有些不可行。为了跟踪不断扩大的提案，一年后，Ian Lance Taylor 创建了一个总体问题，总结了改进错误处理的提议变更的当前状态。创建了一个Go Wiki来收集相关的反馈、讨论和文章。\n关于错误处理冗长性的抱怨持续存在（参见2024年上半年Go开发者调查结果），因此，在Go团队内部提案经过一系列日益完善之后，Ian Lance Taylor 在2024年发布了“使用?减少错误处理样板代码”。这次的想法是借鉴Rust中实现的构造，特别是?操作符。希望通过依靠使用既定符号的现有机制，并考虑我们多年来学到的东西，我们应该能够最终取得一些进展。在一小批用户调研中，向开发者展示使用 ? 的 Go 代码时，绝大多数参与者正确猜出了代码的含义，这进一步说服我们再试一次。为了能够看到变化的影响，Ian 编写了一个工具，将普通 Go 代码转换为使用提议的新语法的代码，我们还在编译器中对该功能进行了原型设计。\n// printSum implementation using the proposed &quot;?&quot; statements.func printSum(a, b string) error &#123;    x := strconv.Atoi(a) ?    y := strconv.Atoi(b) ?    fmt.Println(&quot;result:&quot;, x + y)    return nil&#125;\n不幸的是，与其他错误处理想法一样，这个新提案也很快被评论淹没，许多人建议进行微调，通常基于个人偏好。Ian关闭了提案，并将内容移到了讨论区，以促进对话并收集进一步的反馈。一个稍作修改的版本得到了稍微积极一些的接受，但广泛的支持仍然难以达成一致。\n经过这么多年的尝试，Go团队提出了三个完整的提案，社区提出了数百个提案，其中大多数是各类提案的变体，所有这些都未能获得足够（更不用说压倒性）的支持，我们现在面临的问题是：如何继续？我们是否应该继续？\n我们认为不应该。\n更准确地说，我们应该停止尝试解决_语法问题_，至少在可预见的未来是这样。提案流程为这个决定提供了理由：\n\n提案流程的目标是及时就结果达成普遍共识。如果提案审查无法在问题跟踪器上的问题讨论中确定普遍共识，通常的结果是提案被拒绝。\n\n没有一个错误处理提案达到任何接近共识的程度，所以它们都被拒绝了。即使是 Google 的 Go 团队最资深的成员也不一致同意目前最佳的方案（也许在某个时候会改变）。但是没有具体的共识，我们就无法合理地向前推进。\n有支持现状的有效证据： \n\n如果 Go 早期就为错误处理引入了特定的语法糖，今天几乎没有人会争论它。但我们已经走过了15年，机会已经过去了，Go 有一种完全合适的错误处理方式，即使有时看起来可能很冗长。\n\n从另一个角度看，假设我们今天找到了完美的解决方案。将其纳入语言只会导致从一个不满意的用户群体（支持变更的）转移到另一个（喜欢现状的）。当我们决定向语言添加泛型时，我们处于类似的情况，尽管有一个重要的区别是：今天没有人被迫使用泛型，好的泛型库的编写使得用户可以基本忽略它们是不是泛型，这要归功于类型推断。相反，如果向语言添加新的错误处理语法构造，几乎每个人都需要开始使用它，以免他们的代码变得不符合最新的范式。\n\n不添加额外的语法符合 Go 的设计规则之一：不提供多种做同一件事的方式。在” foot traffic “的领域有这个规则的例外：赋值就是一个例子。具有讽刺意味的是，在短变量声明（:=）中重新声明变量的能力是为了解决因错误处理而产生的问题而引入的：没有重新声明，错误检查序列需要为每个检查使用不同名称的err变量（或额外的单独变量声明）。当时更好的解决方案可能是为错误处理提供更多的语法支持。那样的话，可能就不需要重新声明规则了，没有它各种相关的复杂性也就不存在了。\n\n回到实际的错误处理代码，如果错误得到处理，冗长性就会被淡化。良好的错误处理通常需要向错误添加额外信息。例如，用户调查中的一个反复出现的评论是关于缺少与错误相关的堆栈信息。这可以通过生成并返回增强错误的支持函数来解决。在这个例子中，模板代码的相对数量要小得多：\n\n\nfunc printSum(a, b string) error &#123;    x, err := strconv.Atoi(a)    if err != nil &#123;        return fmt.Errorf(&quot;invalid integer: %q&quot;, a)    &#125;    y, err := strconv.Atoi(b)    if err != nil &#123;        return fmt.Errorf(&quot;invalid integer: %q&quot;, b)    &#125;    fmt.Println(&quot;result:&quot;, x + y)    return nil&#125;\n\n新的标准库功能也可以帮助减少错误处理样板代码，这与Rob Pike 2015年的博客文章“错误就是值”的观点非常相似。例如在某些情况下，cmp.Or可用于一次处理一系列错误：\n\nfunc printSum(a, b string) error &#123;    x, err1 := strconv.Atoi(a)    y, err2 := strconv.Atoi(b)    if err := cmp.Or(err1, err2); err != nil &#123;        return err    &#125;    fmt.Println(&quot;result:&quot;, x+y)    return nil&#125;\n\n\n\n\n编写、阅读和调试代码都是完全不同的工作。编写重复的错误检查可能很乏味，但今天的 IDE 提供了强大的、甚至是 LLM 辅助的代码补全。编写基本的错误检查对这些工具来说很简单。在阅读代码时冗长性最明显，但工具在这里也可能有所帮助；例如，有 Go 语言设置的 IDE 可以提供一个切换开关来隐藏错误处理代码。\n\n在调试错误处理代码时，能够快速添加println或有一个专门的行位置来在调试器中设置断点会很有帮助。当已经有专门的if语句时，这很容易。但如果所有错误处理逻辑都隐藏在check、try或?后面，代码可能必须首先更改为普通的if语句，这会使调试复杂化，甚至可能引入一些错误。\n\n还有实际的考虑：想出一个新的错误处理语法想法很容易；因此社区提出了大量的提案。想出一个经得起审查的好解决方案：就不那么容易了。正确设计语言变更并实际实现它需要协调一致的努力。真正的成本仍然在后面：所有需要更改的代码、需要更新的文档、需要调整的工具。综合考虑，语法变更非常昂贵，Go 团队相对较小，还有很多其他优先事项要处理。\n\n最后一点，我们中的一些人最近有机会参加Google Cloud Next 2025，Go团队在那里有一个展位，我们还举办了一个小型的Go聚会。我们有机会询问的每一位Go用户都坚决认为我们不应该为了更好的错误处理而改变语言。许多人提到，当刚从另一种具有错误处理支持的语言转过来时，Go中缺乏特定的错误处理支持最为明显。随着人们使用的时间越来越久，这个问题变得不那么重要了。这当然不是一个足够大的代表性人群，但它是我们在 GitHub上 看到的不同人群。\n\n\n当然，也有支持变更的理由：\n\n缺乏更好的错误处理支持仍然是我们用户调查中最大的抱怨。如果Go团队真的认真对待用户反馈，我们最终应该为此做些什么。（尽管似乎也没有压倒性的支持语言变更。）\n\n也许单一地关注减少字符数不是一个正确的方向。更好的方法可能是使用关键字使默认错误处理高度可见，同时也要删除模板代码（err != nil）。这种方法可能使读者（代码审查者）更容易看到错误被处理了，而不需要”看多次”，从而提高代码质量和安全性。这将使我们回到check和handle的起点。\n\n我们真的不知道现在的冗长问题在多大程度上是错误检查直接导致的。\n\n\n尽管如此，迄今为止没有任何解决错误处理的尝试获得足够的支持。如果我们诚实地评估我们所处的位置，我们只能承认我们既没有对问题的共同理解，也不是都同意首先存在问题。考虑到这一点，我们做出以下符合当下的决定：\n_在可预见的未来，Go团队将停止为错误处理追求语法语言变更。我们还将关闭所有主要涉及错误处理语法的开放和即将提交的提案，不再进一步跟进。\n社区在探索、讨论和辩论这些问题上投入了巨大的努力。虽然这可能没有导致错误处理语法的任何变化，但这些努力已经为 Go 语言和我们的流程带来了许多其他改进。也许，在未来的某个时候，关于错误处理会出现更清晰的图景。在那之前，我们期待着将这种令人难以置信的热情集中在新的机会上，让Go对每个人都变得更好。\n总结一下\n问题背景：Go的错误处理一直被认为过于冗长，多年来一直是用户调查中的首先被抱怨的。\n\n历次尝试：\n\n2018年的 check 和 handle 机制\n2019年的 try 提案\n2024年的 ? 操作符提案\n\n\n最终决定：经过多年尝试和数百个提案，Go团队决定在可预见的未来停止追求错误处理的语法变更，主要原因包括：\n\n没有达成共识\n现有方式虽然冗长但足够好\n改变会造成社区分裂\n工具和库可以帮助缓解问题\n\n\n未来方向：团队将关注其他改进Go语言的机会，而不是继续在错误处理语法上投入精力。\n\n\n由于  Go 长期没有错误处理的解决方案，导致这个问题被拖了很久，从而每个开发者也都有自己的使用习惯，越多人参与讨论就越难以达成一致。\n","categories":["OB"]},{"title":"Go 语言史诗级更新-循环Bug修复","url":"/2023/09/24/ob/go-loop-fix/","content":"\n背景前两天 Golang 的官方博客更新了一篇文章：Fixing For Loops in Go 1.22\n看这个标题的就是修复了 Go 循环的 bug，这真的是史诗级的更新；我身边接触到的大部分 Go 开发者都犯过这样的错误，包括我自己，所以前两年我也写过类似的博客：简单的 for 循环也会踩的坑\n\n\n先来简单回顾下使用使用 for 循环会碰到的问题：\nlist := []*Demo&#123;&#123;&quot;a&quot;&#125;, &#123;&quot;b&quot;&#125;&#125;  for _, v := range list &#123;  \tgo func() &#123;  \t\tfmt.Println(&quot;name=&quot;+v.Name)  \t&#125;()  &#125;    type Demo struct &#123;  \tName string  &#125;\n\n预期的结果应该是打印 a,b，但实际打印的却是b,b。\n\nLet’s Encrypt: CAA Rechecking bug类似的问题连 mozilla 团队也没能幸免，所以也确实是一个非常常见的问题，这样的写法符合大部分的开发者的直觉，毕竟其他语言这么使用也没有问题。\n当然在现阶段要解决也很简单，要么就是在使用之前先复制一次，或者使用闭包传参：\n// 复制 list := []*Demo&#123;&#123;&quot;a&quot;&#125;, &#123;&quot;b&quot;&#125;&#125;   for _, v := range list &#123;    temp:=v    go func() &#123;     fmt.Println(&quot;name=&quot;+temp.Name)    &#125;()   &#125;// 闭包 list := []*Demo&#123;&#123;&quot;a&quot;&#125;, &#123;&quot;b&quot;&#125;&#125;   for _, v := range list &#123;    go func(temp *Demo) &#123;     fmt.Println(&quot;name=&quot;+temp.Name)    &#125;(v)   &#125;\n\n\n还好官方也意识到了这个问题：所以在 1.22 中我们可以不用再写这个     v:=v这个多余的复制语句了，也不会出现上面的问题。\n我们在 1.21 中可以使用环境变量预览这个特性:\n❯ GOEXPERIMENT=loopvar go testname=bname=a\n在 1.22 发布后建议大家都可以升级了，将这种恶心的 bug 扼杀在摇篮里。\n1.22 后带来了一个好消息是今后少了一道面试题，坏消息是又新增了一个 1.22 版本带来了哪些变化的面试题😂\n更多详情可以参看官方播客：https://go.dev/blog/loopvar-preview\n","categories":["Golang"],"tags":["loop"]},{"title":"从 Helm 到 Operator：Kubernetes应用管理的进化","url":"/2024/07/08/ob/how-operator-working/","content":"🧰Helm 的作用在开始前需要先对 kubernetes  Operator 有个简单的认识。\n以前我们在编写部署一些简单 Deployment 的时候只需要自己编写一个 yaml 文件然后 kubectl apply 即可。\n\n\napiVersion: apps/v1  kind: Deployment  metadata:    labels:      app: k8s-combat    name: k8s-combat  spec:    replicas: 1    selector:      matchLabels:        app: k8s-combat    template:      metadata:        labels:          app: k8s-combat      spec:        containers:          - name: k8s-combat            image: crossoverjie/k8s-combat:v1            imagePullPolicy: Always            resources:              limits:                cpu: &quot;1&quot;                memory: 300Mi              requests:                cpu: &quot;0.1&quot;                memory: 30Mi\n\nkubectl apply -f deployment.yaml\n\n这对于一些并不复杂的项目来说完全够用了，但组件一多就比较麻烦了。\n这里以 Apache Pulsar 为例：它的核心组件有:\n\nBroker\nProxy\nZookeeper\nBookkeeper\nPrometheus(可选)\nGrafana(可选)等组件，每个组件的启动还有这依赖关系。\n必须需要等 Zookeeper 和 Bookkeeper 启动之后才能将流量放进来。\n\n\n\n此时如果还继续使用 yaml 文件一个个部署就会非常繁琐，好在社区有提供 Helm 一键安装程序，使用它我们只需要在一个统一的 yaml 里简单的配置一些组件，就可以由 helm 来部署整个复杂的 Pulsar 系统。\ncomponents:    # zookeeper    zookeeper: true    # bookkeeper    bookkeeper: true    # bookkeeper - autorecovery    autorecovery: true    # broker    broker: true    # functions    functions: false    # proxy    proxy: true    # toolset    toolset: true    # pulsar manager    pulsar_manager: false  monitoring:    # monitoring - prometheus    prometheus: true    # monitoring - grafana    grafana: true    # monitoring - node_exporter    node_exporter: true    # alerting - alert-manager    alert_manager: false\n\n比如在 helm 的 yaml 中我们可以选择使用哪些 components，以及是否启用监控组件。\n最后直接使用这个文件进行安装：\nhelm install pulsar apache/pulsar \\\t--values charts/pulsar/values.yaml \\\t--set namespace=pulsar \\    --set initialize=true\n\n它就会自动生成各个组件的 yaml 文件，然后统一执行。\n所以 helm 的本质上和 kubectl apply yaml 一样的，只是我们在定义 value.yaml 时帮我们处理了许多不需要用户低频修改的参数。\n我们可以使用 helm 将要执行的 yaml 输出后人工审核\nhelm install pulsar apache/pulsar --dry-run --debug &gt; debug.yaml\n\n\n🤔Operator 是什么💔Helm 的痛点Helm 虽然可以帮我们部署或者升级一个大型应用，但他却没法帮我们运维这个应用。\n举个例子：比如我希望当 Pulsar Broker 的流量或者内存达到某个阈值后就指定扩容 Broker，闲时再自动回收。\n或者某个 Bookkeeper 的磁盘使用率达到阈值后可以自动扩容磁盘，这些仅仅使用 Helm 时都是无法实现的。\n以上这些需求我们目前也是通过监控系统发出报警，然后再由人工处理。\n其中最大的痛点就是进行升级：\n\n升级ZK\n关闭auto recovery\n升级Bookkeeper\n升级Broker\n升级Proxy\n开启auto recovery\n\n因为每次升级是有先后顺序的，需要依次观察每个组件运行是否正常才能往后操作。\n如果有 Operator 理性情况下下我们只需要更新一下镜像版本，它就可以自动执行以上的所有步骤最后将集群升级完毕。\n所以相对于 Helm 来说 Operator 是可以站在一个更高的视角俯视整个应用系统，它能发现系统哪个地方需要它从而直接修复。\n💎CRD(Custom Resource Definitions)而提到 Operator 那就不得不提到 CRD(Custom Resource Definitions)翻译过来就是自定义资源。\n这是 kubernetes 提供的一个 API 扩展机制，类似于内置的 Deployment/StatefulSet/Services 资源，CRD 是一种自定义的资源。\n这里以我们常用的 prometheus-operator 和 VictoriaMetrics-operator 为例：\nPrometheus：\n\n**Prometheus**：用于定义 Prometheus 的 Deployment\n**Alertmanager**：用于定义 Alertmanager\n**ScrapeConfig**：用于定会抓取规则\n\napiVersion: monitoring.coreos.com/v1alpha1kind: ScrapeConfigmetadata:  name: static-config  namespace: my-namespace  labels:    prometheus: system-monitoring-prometheusspec:  staticConfigs:    - labels:        job: prometheus      targets:        - prometheus.demo.do.prometheus.io:9090\n\n使用时的一个很大区别就是资源的 kind: ScrapeConfig 为自定义的类型。\nVictoriaMetrics 的 CRD：\n\nVMPodScrape：Pod 的抓取规则\nVMCluster：配置 VM 集群\nVMAlert：配置 VM 的告警规则\n等等\n\n# vmcluster.yamlapiVersion: operator.victoriametrics.com/v1beta1kind: VMClustermetadata:  name: demospec:  retentionPeriod: &quot;1&quot;  replicationFactor: 2  vmstorage:    replicaCount: 2    storageDataPath: &quot;/vm-data&quot;    storage:      volumeClaimTemplate:        spec:          resources:            requests:              storage: &quot;10Gi&quot;    resources:      limits:        cpu: &quot;1&quot;        memory: &quot;1Gi&quot;  vmselect:    replicaCount: 2    cacheMountPath: &quot;/select-cache&quot;    storage:      volumeClaimTemplate:        spec:          resources:            requests:              storage: &quot;1Gi&quot;    resources:      limits:        cpu: &quot;1&quot;        memory: &quot;1Gi&quot;      requests:        cpu: &quot;0.5&quot;        memory: &quot;500Mi&quot;  vminsert:    replicaCount: 2\n\n以上是用于创建一个 VM 集群的 CRD 资源，应用之后就会自动创建一个集群。\nOperator 原理Operator 通常是运行在 kubernetes API server 的 webhook 之上，简单来说就是在一些内置资源的关键节点 API-server 会调用我们注册的一个 webhook，在这个 webhook 中我们根据我们的 CRD 做一些自定义的操作。\n理论上我们可以使用任何语言都可以写 Operator，只需要能处理 api-server 的回调即可。\n只是 Go 语言有很多成熟的工具，比如常用的 kubebuilder 和 operator-sdk.\n他们内置了许多命令行工具，可以帮我们节省需要工作量。\n这里以 operator-sdk 为例：\n$ operator-sdk create webhook --group cache --version v1alpha1 --kind Memcached --defaulting --programmatic-validation\n\n会直接帮我们创建好一个标准的 operator 项目:\n├── Dockerfile├── Makefile├── PROJECT├── api│   └── v1alpha1│       ├── memcached_webhook.go│       ├── webhook_suite_test.go├── config│   ├── certmanager│   │   ├── certificate.yaml│   │   ├── kustomization.yaml│   │   └── kustomizeconfig.yaml│   ├── default│   │   ├── manager_webhook_patch.yaml│   │   └── webhookcainjection_patch.yaml│   └── webhook│       ├── kustomization.yaml│       ├── kustomizeconfig.yaml│       └── service.yaml├── go.mod├── go.sum└── main.go\n\n其中 Makefile 中包含了开发过程中常用的工具链（包括根据声明的结构体自动生成 CRD 资源、部署k8s 环境测试等等）、Dockerfile 等等。\n这样我们就只需要专注于开发业务逻辑即可。\n因为我前段时间给 https://github.com/open-telemetry/opentelemetry-operator 贡献过两个 feature，所以就以这个 Operator 为例：\n它有一个 CRD: kind: Instrumentation，在这个 CRD 中可以将 OpenTelemetry 的 agent 注入到应用中。\napiVersion: opentelemetry.io/v1alpha1  kind: Instrumentation  metadata:    name: instrumentation-test-order  namespace: test  spec:    env:      - name: OTEL_SERVICE_NAME        value: order  selector:      matchLabels:        app: order    java:      image: autoinstrumentation-java:2.4.0-release      extensions:        - image: autoinstrumentation-java:2.4.0-release          dir: /extensions        env:        - name: OTEL_RESOURCE_ATTRIBUTES          value: service.name=order        - name: OTEL_INSTRUMENTATION_MESSAGING_EXPERIMENTAL_RECEIVE_TELEMETRY_ENABLED          value: &quot;true&quot;        - name: OTEL_TRACES_EXPORTER          value: otlp        - name: OTEL_METRICS_EXPORTER          value: otlp        - name: OTEL_LOGS_EXPORTER          value: none        - name: OTEL_EXPORTER_OTLP_ENDPOINT          value: http://open-telemetry-opentelemetry-collector.otel.svc.cluster.local:4317        - name: OTEL_EXPORTER_OTLP_COMPRESSION          value: gzip        - name: OTEL_EXPERIMENTAL_EXPORTER_OTLP_RETRY_ENABLED          value: &quot;true&quot;\n\n它的运行规则是当我们的 Pod 在启动过程中会判断 Pod 的注解中是否开启了注入 OpenTelemetry 的配置。\n如果开启则会将我们在 CRD 中自定义的镜像里的 javaagent 复制到业务容器中，同时会将下面的那些环境变量也一起加入的业务容器中。\n要达到这样的效果就需要我们注册一个回调 endpoint。\nmgr.GetWebhookServer().Register(&quot;/mutate-v1-pod&quot;, &amp;webhook.Admission&#123;      Handler: podmutation.NewWebhookHandler(cfg, ctrl.Log.WithName(&quot;pod-webhook&quot;), decoder, mgr.GetClient(),         []podmutation.PodMutator&#123;            sidecar.NewMutator(logger, cfg, mgr.GetClient()),            instrumentation.NewMutator(logger, mgr.GetClient(), mgr.GetEventRecorderFor(&quot;opentelemetry-operator&quot;), cfg),         &#125;),&#125;)\n\n当 Pod 创建或有新的变更请求时就会回调我们的接口。\nfunc (pm *instPodMutator) Mutate(ctx context.Context, ns corev1.Namespace, pod corev1.Pod) (corev1.Pod, error) &#123;      logger := pm.Logger.WithValues(&quot;namespace&quot;, pod.Namespace, &quot;name&quot;, pod.Name)    &#125;\n\n在这个接口中我们就可以拿到 Pod 的信息，然后再获取 CRD Instrumentation 做我们的业务逻辑。\nvar otelInsts v1alpha1.InstrumentationList  if err := pm.Client.List(ctx, &amp;otelInsts, client.InNamespace(ns.Name)); err != nil &#123;      return nil, err  &#125;// 从 CRD 中将数据复制到业务容器中。pod.Spec.InitContainers = append(pod.Spec.InitContainers, corev1.Container&#123;\tName:      javaInitContainerName,\tImage:     javaSpec.Image,\tCommand:   []string&#123;&quot;cp&quot;, &quot;/javaagent.jar&quot;, javaInstrMountPath + &quot;/javaagent.jar&quot;&#125;,\tResources: javaSpec.Resources,\tVolumeMounts: []corev1.VolumeMount&#123;&#123;\t\tName:      javaVolumeName,\t\tMountPath: javaInstrMountPath,\t&#125;&#125;,&#125;)for i, extension := range javaSpec.Extensions &#123;\tpod.Spec.InitContainers = append(pod.Spec.InitContainers, corev1.Container&#123;\t\tName:      initContainerName + fmt.Sprintf(&quot;-extension-%d&quot;, i),\t\tImage:     extension.Image,\t\tCommand:   []string&#123;&quot;cp&quot;, &quot;-r&quot;, extension.Dir + &quot;/.&quot;, javaInstrMountPath + &quot;/extensions&quot;&#125;,\t\tResources: javaSpec.Resources,\t\tVolumeMounts: []corev1.VolumeMount&#123;&#123;\t\t\tName:      javaVolumeName,\t\t\tMountPath: javaInstrMountPath,\t\t&#125;&#125;,\t&#125;)&#125;\n\n\n不过需要注意的是想要在测试环境中测试 operator 是需要安装一个 cert-manage，这样 webhook 才能正常的回调。\n\n要使得 CRD 生效，我们还得先将 CRD 安装进 kubernetes 集群中，不过这些 operator-sdk 这类根据已经考虑周到了。\n我们只需要定义好 CRD 的结构体：\n然后使用 Makefile 中的工具 make bundle 就会自动将结构体转换为 CRD。\n参考链接：\n\nhttps://github.com/VictoriaMetrics/operator\nhttps://github.com/prometheus-operator/prometheus-operator\n\n","categories":["OB","kubernetes","kubernetes"],"tags":["kubernetes","Operator"]},{"title":"如何找到并快速上手一个开源项目","url":"/2024/07/01/ob/how-to-involve-OpenSource/","content":"以前有写过两篇文章来简单聊过如何做开源的事情，最近我自己组了一个社区里面也有不少朋友对开源感兴趣，于是我便根据自己的经验系统的梳理了一些关于开源的事情。\n\n新手如何快速参与开源项目\n手把手教你为开源项目贡献代码\n\n\n有兴趣的可以先看看之前这两篇。\n\n\n🔎如何找到自己感兴趣的开源项目首先第一步先想清楚自己搞开源的目的是什么：\n\n参考社区大佬的代码，提升技术\n丰富个人履历，提高面试通过率\n更功利一点就是想成为某个项目的 Committer&#x2F;PMC\n\n\n单纯喜欢分享，热爱开源，认可开源改变世界💪。\n\n我认为前面三种都是一个目的，提升自己获得后续的好处；最后一种则是妥妥的纯热爱。\n以我个人来说，我两者都沾一点；我相信大部分人都是前面三类的目的，到这里我可能要先浇点冷水。\n\n往往一个开源项目从你熟悉它开始到提第一个 PR 然后到合并中间经历的时间可能是大大超出你的预期的。\n\n特别是越大型越专业的项目（我相信你也是想加入这类有一定知名度的项目）。\n因为开源社区大部分都是执行异步沟通，与即时通讯的快速反馈不同，甚至还有不少 reviewer 处于不同的时区。\n所以一开始就想做好心理预期，不要指望着我给某个项目提交一个很牛逼的功能，然后他们快速 review 合并，然后给你 commit 权限。\n而且有不少开源项目是由某一个公司主导的，比如（Pulsar、Golang、Kafka），他们可能对于外部社区来的新手并不那么上心，一个 PR 晾在那里几个月没人理都是很正常的。\n所以我建议一开始选择的项目有以下几个筛选标准：\n\n尽量是自己日常在用，熟悉的项目。\n最近有在及时更新维护的项目。\n对社区新人的接纳程度是否足够包容。\n这点可以在 Github 里查找标签为 help want/contribution welcome 的 issue 或者是 PR。\n查看这些 issue&#x2F; PR 最近的活跃时间，贡献者是否为新人。\n往往一个包容度较高的项目以上信息都是很活跃的。\n\n\n项目主要维护者是否来着不同的公司，是否足够活跃。\n\n\n推荐几个我认为比较符合我刚才提到的条件的项目：\n\nhttps://github.com/open-telemetry/opentelemetry-java-instrumentation/issues/7195\nhttps://github.com/apache/pulsar-client-go/issues?q=is%3Aopen+label%3Atype%2Ffeature+sort%3Aupdated-desc\nhttps://github.com/apache/hertzbeat/\n\n🖐如何快速上手一个开源项目如果找到了自己想贡献的项目，如果自己还不太熟悉的话，那就可以尝试以下步骤来快速上手它。\n✅单元测试首先第一个就是单元测试，单元测试是一个非常不错的方式来上手一个新的开源项目，但重点不是去看现有的单测，而是自己去写✍️。\n写过单元测试的小伙伴就知道，如果要达到 90% 以上的覆盖率时需要对自己写的每一行代码都得了解，甚至在写的过程中会发现部分代码是不是没有必要，从而再帮助自己梳理一遍业务。\n所以写单测确实是快速熟悉某个项目的方法，但这针对于一些逻辑简单的项目；对于一些业务复杂的项目建议还是快速跑通官方推荐一个功能。\n🌟以 Pulsar 为例以 Apache Pulsar为例，那就先跑一个消息的生产者和消费者 demo；跑通了之后再尝试看看它客户端已有的单测代码，然后尝试改一些断言，此时就会发现预期值为什么会这么定义。https://github.com/apache/pulsar/blob/631b13ad23d7e48c6e82d38f97c23d129062cb7c/pulsar-broker/src/test/java/org/apache/pulsar/client/impl/BrokerClientIntegrationTest.java#L1077\n\n比如这里的一个 consumer 取消订阅两次时候就会抛出异常，此时我们就可以根据异常的地方找到源码里对连接状态的判断条件。\n就可以得知：当客户端取消订阅时会修改连接状态。\n💓HertzBeat下面以 Apache HertzBeat为例来看看当时我是如何贡献单元测试的。\n通过官方的架构图可以得知 HertzBeat 是通过一个 collector 去直连目标采集数据的。\n比如通过 Redis 的客户端去获取监控数据，然后再存放到自己的时序数据库中进行展示。\n所以这个采集的过程就是比较核心的逻辑，我们可以看看他的接口定义。\n一共就三个接口，分别是：\n\ncollect采集接口：在 Metrics 中定义了采集的目标信息（地址、端口等）\n采集完后的数据写入到 Builder 供后续的写入存储\n\n\npreCheck：提前做一些参数校验\nsupportProtocol：返回定义的协议类型，通过这个类型找到对应采集器\n\n\n然后就交由不同的实现类去采集不同的指标。\n这里我以 RedisCommonCollectImpl为例，主要的单测逻辑就是模拟 Redis 客户端的返回数据，然后在 Collect 的代码里查看不同的处理逻辑，其实就是要覆盖各种分支以及异常的情况。\n最后再断言采集到的数据与预期是否匹配即可，贴一段核心逻辑：\n至于应该返回什么预期结果，有些 collector 可能会在代码注释里写清楚，但这个 Redis 没有写。\n不过也有办法，我们可以把代码在本地跑起来之后进入管理台查看内置的监控模版。\n这里是用于定义会监控哪些字段的地方，这样我们就可以在代码预先生成好预期返回值了。\n\n具体的单测代码请看这里：https://github.com/apache/hertzbeat/blob/master/collector/src/test/java/org/apache/hertzbeat/collector/collect/redis/RedisClusterCollectImplTest.java#L46\n📝总结参与一个成熟社区的开源有一点一定要记住，就是要仔细阅读贡献者文档。\n里面往往会写清楚如何构建代码、代码规范、提交规范等信息，这些都捋清楚后提交的 PR 才更容易被社区接受。\n后面会继续更新集成测试与 e2e 测试等内容。\n","categories":["OB"]},{"title":"如何监控 Nginx","url":"/2024/07/23/ob/how-to-monitoring-nginx/","content":"前段时间接到一个需求，希望可以监控 Nginx 的运行状态。\n我们都知道 Nginx 作为一个流行的 Web 服务器提供了多种能力，包括反向代理、负载均衡；也支持了许多协议，包括：\n\ngRPC\nhttp\nWebSocket 等作为一个流量入口的中间件，对其的监控就显得至关重要了。\n\n\n\n市面上也有一些现成的产品可以监控 Nginx，比如知名的监控服务商 datadog 也提供了 Nginx 的监控。\n\n但是我这是一个内网服务，并不能使用这些外部的云厂商，所有就只能在内部搭建 Nginx 的监控服务了。\n不过 Nginx 默认情况下并没有提供 /metrics 的 endpoint，但好在它提供了一个额外的模块：stub_status 可以用于获取监控数据。\nserver &#123;  listen 80;  server_name _;  location /status &#123;    stub_status on;    access_log off;  &#125;  location / &#123;      root /usr/share/nginx/html;      index index.html index.htm;  &#125;&#125;\n\n这样访问 http://127.0.0.1:80/status 就可以拿到一些基本的运行数据。\n但这个格式明显不是 Prometheus 所支持的 metrics 格式，无法直接将数据采集到 Prometheus 中然后通过 Grafana 进行查看。\n所以还得需要一个中间层来将这些数据转换为 Prometheus 可以接收的 metrics 数据。\nnginx-prometheus-exporter好在社区已经提供了类似的工具：nginx-prometheus-exporter 它读取刚才 status endpoint 所暴露的数据，然后转换为 Prometheus 格式，并对外提供了一个 /metrics 的 endpoint 供 Prometheus 来采集。\n转换数据我们在启动这个 nginx-exporter 时需要传入刚才 Nginx 暴露的 /status endpoint。\ndocker run -p 9113:9113 nginx/nginx-prometheus-exporter:1.1.0 --nginx.scrape-uri=http://&lt;nginx&gt;:8080/stub_status\n\nconst templateMetrics string = `Active connections: %dserver accepts handled requests%d %d %dReading: %d Writing: %d Waiting: %d`// 读取 Nginx status 数据body, err := io.ReadAll(resp.Body)if err != nil &#123;\treturn nil, fmt.Errorf(&quot;failed to read the response body: %w&quot;, err)&#125;r := bytes.NewReader(body)stats, err := parseStubStats(r)if err != nil &#123;\treturn nil, fmt.Errorf(&quot;failed to parse response body %q: %w&quot;, string(body), err)&#125;// 解析 Nginx status 数据func parseStubStats(r io.Reader) (*StubStats, error) &#123;\tvar s StubStats\tif _, err := fmt.Fscanf(r, templateMetrics,\t\t&amp;s.Connections.Active,\t\t&amp;s.Connections.Accepted,\t\t&amp;s.Connections.Handled,\t\t&amp;s.Requests,\t\t&amp;s.Connections.Reading,\t\t&amp;s.Connections.Writing,\t\t&amp;s.Connections.Waiting); err != nil &#123;\t\treturn nil, fmt.Errorf(&quot;failed to scan template metrics: %w&quot;, err)\t&#125;\treturn &amp;s, nil&#125;\t\n\n\n最后会把刚才解析到的数据生成 metrics：\nch &lt;- prometheus.MustNewConstMetric(c.metrics[&quot;connections_active&quot;],      prometheus.GaugeValue, float64(stats.Connections.Active))  ch &lt;- prometheus.MustNewConstMetric(c.metrics[&quot;connections_accepted&quot;],      prometheus.CounterValue, float64(stats.Connections.Accepted))  ch &lt;- prometheus.MustNewConstMetric(c.metrics[&quot;connections_handled&quot;],      prometheus.CounterValue, float64(stats.Connections.Handled))  ch &lt;- prometheus.MustNewConstMetric(c.metrics[&quot;connections_reading&quot;],      prometheus.GaugeValue, float64(stats.Connections.Reading))  ch &lt;- prometheus.MustNewConstMetric(c.metrics[&quot;connections_writing&quot;],      prometheus.GaugeValue, float64(stats.Connections.Writing))  ch &lt;- prometheus.MustNewConstMetric(c.metrics[&quot;connections_waiting&quot;],      prometheus.GaugeValue, float64(stats.Connections.Waiting))  ch &lt;- prometheus.MustNewConstMetric(c.metrics[&quot;http_requests_total&quot;],      prometheus.CounterValue, float64(stats.Requests))\n\n这些 metrics 是一开始就定义好的：\n// NewNginxCollector creates an NginxCollector.func NewNginxCollector(nginxClient *client.NginxClient, namespace string, constLabels map[string]string, logger log.Logger) *NginxCollector &#123;\treturn &amp;NginxCollector&#123;\t\tnginxClient: nginxClient,\t\tlogger:      logger,\t\tmetrics: map[string]*prometheus.Desc&#123;\t\t\t&quot;connections_active&quot;:   newGlobalMetric(namespace, &quot;connections_active&quot;, &quot;Active client connections&quot;, constLabels),\t\t\t&quot;connections_accepted&quot;: newGlobalMetric(namespace, &quot;connections_accepted&quot;, &quot;Accepted client connections&quot;, constLabels),\t\t\t&quot;connections_handled&quot;:  newGlobalMetric(namespace, &quot;connections_handled&quot;, &quot;Handled client connections&quot;, constLabels),\t\t\t&quot;connections_reading&quot;:  newGlobalMetric(namespace, &quot;connections_reading&quot;, &quot;Connections where NGINX is reading the request header&quot;, constLabels),\t\t\t&quot;connections_writing&quot;:  newGlobalMetric(namespace, &quot;connections_writing&quot;, &quot;Connections where NGINX is writing the response back to the client&quot;, constLabels),\t\t\t&quot;connections_waiting&quot;:  newGlobalMetric(namespace, &quot;connections_waiting&quot;, &quot;Idle client connections&quot;, constLabels),\t\t\t&quot;http_requests_total&quot;:  newGlobalMetric(namespace, &quot;http_requests_total&quot;, &quot;Total http requests&quot;, constLabels),\t\t&#125;,\t\tupMetric: newUpMetric(namespace, constLabels),\t&#125;&#125;\n\n而这个函数是在 exporter 启动时候会调用：\n&quot;github.com/prometheus/client_golang/prometheus&quot;prometheus.MustRegister(collector.NewNginxCollector(ossClient, &quot;nginx&quot;, labels, logger))\n\n使用的是 prometheus 包提供的注册函数，将我们刚才自定义的获取 metrics 的逻辑注册进去，这样当我们在 Prometheus 中配置好采集任务之后就可以定期扫描 /status 的数据然后转换为 Prometheus 指标返回。\nglobal:  scrape_interval: 10sscrape_configs:  - job_name: nginx-exportor    static_configs:    - targets: [&#x27;127.0.0.1:9113&#x27;]\n\n这样就可以将 nginx status 的数据定期采集到 Prometheus 中了，最后使用社区提供的 grafana 面板便可以可视化的查看这些监控数据：\nNginx Plus同时这个 nginx-exporter 还支持 Nginx Plus(这是 Nginx 的商用增强版)，它的实现原理类似，只是它支持的指标更多一些而已。\ntype NginxPlusCollector struct &#123;      upMetric                       prometheus.Gauge      logger                         log.Logger      cacheZoneMetrics               map[string]*prometheus.Desc      workerMetrics                  map[string]*prometheus.Desc      nginxClient                    *plusclient.NginxClient      streamServerZoneMetrics        map[string]*prometheus.Desc      streamZoneSyncMetrics          map[string]*prometheus.Desc      streamUpstreamMetrics          map[string]*prometheus.Desc      streamUpstreamServerMetrics    map[string]*prometheus.Desc      locationZoneMetrics            map[string]*prometheus.Desc      resolverMetrics                map[string]*prometheus.Desc      limitRequestMetrics            map[string]*prometheus.Desc      limitConnectionMetrics         map[string]*prometheus.Desc      streamLimitConnectionMetrics   map[string]*prometheus.Desc      upstreamServerMetrics          map[string]*prometheus.Desc      upstreamMetrics                map[string]*prometheus.Desc      streamUpstreamServerPeerLabels map[string][]string      serverZoneMetrics              map[string]*prometheus.Desc      upstreamServerLabels           map[string][]string      streamUpstreamServerLabels     map[string][]string      serverZoneLabels               map[string][]string      streamServerZoneLabels         map[string][]string      upstreamServerPeerLabels       map[string][]string      workerLabels                   map[string][]string      cacheZoneLabels                map[string][]string      totalMetrics                   map[string]*prometheus.Desc      variableLabelNames             VariableLabelNames      variableLabelsMutex            sync.RWMutex      mutex                          sync.Mutex  &#125;\n\n\n\nPrometheus 社区中提供不少这类 exporter：\n这些 exporter 要解决的问题都是类似的，对于一些没有暴露 /metrics 的中间件通过他们提供的客户端直连，然后将获取到的数据转换为 Prometheus 所支持的格式。\n\n需要单独的 exporter 支持的中间件大部分都是一些老牌产品，在设计之初就没有考虑可观测性的需求，现在一些新的中间件几乎都原生支持 metrics，这种产品只需要在 Prometheus 中配置采集任务即可。\n\nCprobe不知道大家发现没有，社区中提供的 exporter 还是挺多的，但如果我们都需要在自己的生产环境将这些 exporter 部署起来多少会有些繁琐：\n\n不同的 exporter 需要的参数可能不同\n暴露的端口可能不同\n配置文件难以统一管理\n\n在这个背景下社区有大佬发起了一个 cprobe 项目，这是一个大而全的项目，可以将散落在各处的 exporter 都整合在一起。\n并且统一抽象了接入方式，使得所有的插件都可以用类似的配置书写方式来维护这些插件。\n目前已经支持以下一些常用的中间件：\n\n这里的 Nginx 就是本次监控的需求贡献的，因为还需要监控这里支持的一些其他中间件，所以最终也是使用 cprobe 来部署监控。\n整合 Nginx exporter 到 Cprobe 中下面来看看如何将社区中已经存在的 Nginx exporter 整合到  cprobe 中：\n在开始之前我们先要抽象出这个插件需要哪些配置？\n这个其实很好解决，我们直接看看需要实现的 exporter 中提供了哪些参数，这里以 Nginx 的为例：\n排除掉一些我们不需要的，比如端口、日志级别、endpoint等配置之外，就只需要一些关于 SSL 的配置，所以最终我们需要的配置文件如下：\nnginx_plus = false  # Path to the PEM encoded CA certificate file used to validate the servers SSL certificate.  ssl_ca_cert = &#x27;&#x27;  # Path to the PEM encoded client certificate file to use when connecting to the server.  ssl_client_cert = &#x27;&#x27;  # Path to the PEM encoded client certificate key file to use when connecting to the server.  ssl_client_key = &#x27;&#x27;  # Perform SSL certificate verification.  ssl_verify = false  timeout = &#x27;5s&#x27;\n\n然后将这个 toml 里的配置转换为一个 struct。\n在 cprobe 中有一个核心的接口：\ntype Plugin interface &#123;\t// ParseConfig is used to parse config\tParseConfig(baseDir string, bs []byte) (any, error)\t// Scrape is used to scrape metrics, cfg need to be cast specific cfg\tScrape(ctx context.Context, target string, cfg any, ss *types.Samples) error&#125;\n\nParseConfig 用于将刚才的配置文件流格式化为插件所需要的配置。\nScrape 函数则是由 cprobe 定时调用的函数，会传入抓取的目标地址，每个插件将抓到的数据写入 *types.Samples 中即可。\ncprobe 会将 *types.Samples 的数据发送到 remote 的 Prometheus 中。\n接下来看看 Nginx 插件的实现：\ntype Config struct &#123;\tNginxPlus     bool          `toml:&quot;nginx_plus&quot;`\tSSLCACert     string        `toml:&quot;ssl_ca_cert&quot;`\tSSLClientCert string        `toml:&quot;ssl_client_cert&quot;`\tSSLClientKey  string        `toml:&quot;ssl_client_key&quot;`\tSSLVerify     bool          `toml:&quot;ssl_verify&quot;`\tTimeout       time.Duration `toml:&quot;timeout&quot;`&#125;func (n *Nginx) ParseConfig(baseDir string, bs []byte) (any, error) &#123;\tvar c Config\terr := toml.Unmarshal(bs, &amp;c)\tif err != nil &#123;\t\treturn nil, err\t&#125;\tif c.Timeout == 0 &#123;\t\tc.Timeout = time.Millisecond * 500\t&#125;\treturn &amp;c, nil&#125;\n\nParseConfig 很简单，就是将配置文件转换为 struct。\n抓取函数 Scrape 也很简单：\ncollect, err := registerCollector(transport, target, nil, conf)  if err != nil &#123;      return err  &#125;    ch := make(chan prometheus.Metric)  go func() &#123;      collect.Collect(ch)      close(ch)  &#125;()\n\n就是构建之前在 nginx exporter 中的 prometheus.Collector，其实代码大部分也是从那边复制过来的。所以其实迁移一个 exporter 到 cprobe 中非常简单，只需要：\n\n定义好需要的配置。\n去掉不需要的代码，比如日志、端口之类的。\n适配好刚才那两个核心函数 ParseConfig/Scrape 即可。\n\n但这样也有些小问题，现有的一些 exporter 还在迭代，那边更新的版本需要有人及时同步过来。\n除非有一天 cprobe 可以作为一个标准，版本更新都在 cprobe 这边完成，这样就真的是做大做强了。\n不过这些依旧是适配老一代的中间件产品，逐步都会适配现代的可观测体系，这些 exporter 也会逐渐走下历史舞台。\n参考链接：\n\nhttps://prometheus.io/docs/instrumenting/exporters/\nhttps://github.com/nginxinc/nginx-prometheus-exporter\n\n","categories":["OB"],"tags":["Nginx","Monitor"]},{"title":"实战：如何编写一个 OpenTelemetry Extensions","url":"/2024/04/15/ob/how-to-write-otel-extensions/","content":"前言前段时间我们从 SkyWalking 切换到了 OpenTelemetry ，与此同时之前使用 SkyWalking 编写的插件也得转移到 OpenTelemetry 体系下。\n我也写了相关介绍文章：实战：如何优雅的从 SkyWalking 切换到 OpenTelemetry\n好在 OpenTelemetry 社区也提供了 Extensions 的扩展开发，我们可以不用去修改社区发行版：opentelemetry-javaagent.jar 的源码也可以扩展其中的能力。\n比如可以：\n\n修改一些 trace，某些 span 不想记录等。\n新增 metrics\n\n\n\n这次我准备编写的插件也是和 metrics 有关的，因为 pulsar 的 Java sdk 中并没有暴露客户端的一些监控指标，所以我需要在插件中拦截到一些关键函数，然后执行暴露出指标。\n截止到本文编写的时候， Pulsar 社区也已经将 Java-client 集成了 OpenTelemetry，后续正式发版后我这个插件也可以光荣退休了。\n\n由于 OpenTelemetry 社区还处于高速发展阶段，我在中文社区没有找到类似的参考文章（甚至英文社区也没有，只有一些 example 代码，或者是只有去社区成熟插件里去参考代码）\n其中也踩了不少坑，所以觉得非常有必要分享出来帮助大家减少遇到同类问题的机会。\n开发流程OpenTelemetry extension 的写法其实和 skywalking 相似，都是用的 bytebuddy这个字节码增强库，只是在一些 API 上有一些区别。\n创建项目首先需要创建一个 Java 项目，这里我直接参考了官方的示例，使用了 gradle 进行管理（理论上 maven 也是可以的，只是要找到在 gradle 使用的 maven 插件）。\n这里贴一下简化版的 build.gradle 文件：\nplugins &#123;    id &#x27;java&#x27;    id &quot;com.github.johnrengelman.shadow&quot; version &quot;8.1.1&quot;    id &quot;com.diffplug.spotless&quot; version &quot;6.24.0&quot;&#125;group = &#x27;com.xx.otel.extensions&#x27;version = &#x27;1.0.0&#x27;ext &#123;    versions = [            // this line is managed by .github/scripts/update-sdk-version.sh            opentelemetrySdk           : &quot;1.34.1&quot;,            // these lines are managed by .github/scripts/update-version.sh            opentelemetryJavaagent     : &quot;2.1.0-SNAPSHOT&quot;,            opentelemetryJavaagentAlpha: &quot;2.1.0-alpha-SNAPSHOT&quot;,            junit                      : &quot;5.10.1&quot;    ]    deps = [    // 自动生成服务发现 service 文件            autoservice: dependencies.create(group: &#x27;com.google.auto.service&#x27;, name: &#x27;auto-service&#x27;, version: &#x27;1.1.1&#x27;)    ]&#125;repositories &#123;    mavenLocal()    maven &#123; url &quot;https://maven.aliyun.com/repository/public&quot; &#125;    mavenCentral()&#125;configurations &#123;    otel&#125;dependencies &#123;    implementation(platform(&quot;io.opentelemetry:opentelemetry-bom:$&#123;versions.opentelemetrySdk&#125;&quot;))    /*    Interfaces and SPIs that we implement. We use `compileOnly` dependency because during    runtime all necessary classes are provided by javaagent itself.     */    compileOnly &#x27;io.opentelemetry:opentelemetry-sdk-extension-autoconfigure-spi:1.34.1&#x27;    compileOnly &#x27;io.opentelemetry.instrumentation:opentelemetry-instrumentation-api:1.32.0&#x27;    compileOnly &#x27;io.opentelemetry.javaagent:opentelemetry-javaagent-extension-api:1.32.0-alpha&#x27;    //Provides @AutoService annotation that makes registration of our SPI implementations much easier    compileOnly deps.autoservice    annotationProcessor deps.autoservice    // https://mvnrepository.com/artifact/org.apache.pulsar/pulsar-client    compileOnly &#x27;org.apache.pulsar:pulsar-client:2.8.0&#x27;&#125;test &#123;    useJUnitPlatform()&#125;\n\n然后便是要创建  javaagent 的一个核心类：\n@AutoService(InstrumentationModule.class)  public class PulsarInstrumentationModule extends InstrumentationModule &#123;    public PulsarInstrumentationModule() &#123;        super(&quot;pulsar-client-metrics&quot;, &quot;pulsar-client-metrics-2.8.0&quot;);    &#125;\t&#125;\n\n在这个类中定义我们插件的名称，同时使用 @AutoService 注解可以在打包的时候帮我们在 META-INF/services/目录下生成 SPI 服务发现的文件：\n\n这是一个 Google 的插件，本质是插件是使用 SPI 的方式进行开发的。\n\n关于 SPI 以前也写过一篇文章，不熟的朋友可以用作参考：\n\nJava SPI 的原理与应用\n\n创建 Instrumentation之后就需要创建自己的 Instrumentation，这里可以把它理解为自己的拦截器，需要配置对哪个类的哪个函数进行拦截：\npublic class ProducerCreateImplInstrumentation implements TypeInstrumentation &#123;    @Override    public ElementMatcher&lt;TypeDescription&gt; typeMatcher() &#123;        return named(&quot;org.apache.pulsar.client.impl.ProducerBuilderImpl&quot;);    &#125;    @Override    public void transform(TypeTransformer transformer) &#123;        transformer.applyAdviceToMethod(                isMethod()                        .and(named(&quot;createAsync&quot;)),                ProducerCreateImplInstrumentation.class.getName() + &quot;$ProducerCreateImplConstructorAdvice&quot;);    &#125;\n\n比如这就是对 ProducerBuilderImpl 类的 createAsync 创建函数进行拦截，拦截之后的逻辑写在了 ProducerCreateImplConstructorAdvice 类中。\n值得注意的是对一些继承和实现类的拦截方式是不相同的：\n@Override  public ElementMatcher&lt;TypeDescription&gt; typeMatcher() &#123;      return extendsClass(named(ENHANCE_CLASS));      // return implementsInterface(named(ENHANCE_CLASS));&#125;\n\n从这两个函数名称就能看出，分别是针对继承和实现类进行拦截的。\n\n这里的 API 比 SkyWalking 的更易读一些。\n\n之后需要把我们自定义的 Instrumentation 注册到刚才的 PulsarInstrumentationModule 类中：\n@Overridepublic List&lt;TypeInstrumentation&gt; typeInstrumentations() &#123;    return Arrays.asList(            new ProducerCreateImplInstrumentation(),            new ProducerCloseImplInstrumentation(),            );&#125;\n有多个的话也都得进行注册。\n编写切面代码之后便是编写我们自定义的切面逻辑了，也就是刚才自定义的 ProducerCreateImplConstructorAdvice 类：\npublic static class ProducerCreateImplConstructorAdvice &#123;    @Advice.OnMethodEnter(suppress = Throwable.class)    public static void onEnter() &#123;        // inert your code        MetricsRegistration.registerProducer();    &#125;    @Advice.OnMethodExit(suppress = Throwable.class)    public static void after(            @Advice.Return CompletableFuture&lt;Producer&gt; completableFuture) &#123;        try &#123;            Producer producer = completableFuture.get();            CollectionHelper.PRODUCER_COLLECTION.addObject(producer);        &#125; catch (Throwable e) &#123;            System.err.println(e.getMessage());        &#125;    &#125;&#125;\n\n可以看得出来其实就是两个核心的注解：\n\n@Advice.OnMethodEnter 切面函数调用之前\n@Advice.OnMethodExit 切面函数调用之后\n\n还可以在 @Advice.OnMethodExit的函数中使用 @Advice.Return获得函数调用的返回值。\n当然也可以使用 @Advice.This 来获取切面的调用对象。\n编写自定义 metrics因为我这个插件的主要目的是暴露一些自定义的 metrics，所以需要使用到 io.opentelemetry.api.metrics 这个包：\n这里以 Producer 生产者为例，整体流程如下：\n\n创建生产者的时候将生产者对象存储起来\nOpenTelemetry 框架会每隔一段时间回调一个自定义的函数\n在这个函数中遍历所有的 producer 获取它的监控指标，然后暴露出去。\n\n注册函数：\npublic static void registerObservers() &#123;      Meter meter = MetricsRegistration.getMeter();        meter.gaugeBuilder(&quot;pulsar_producer_num_msg_send&quot;)              .setDescription(&quot;The number of messages published in the last interval&quot;)              .ofLongs()              .buildWithCallback(                      r -&gt; recordProducerMetrics(r, ProducerStats::getNumMsgsSent));\n\n\nprivate static void recordProducerMetrics(ObservableLongMeasurement observableLongMeasurement, Function&lt;ProducerStats, Long&gt; getter) &#123;      for (Producer producer : CollectionHelper.PRODUCER_COLLECTION.list()) &#123;          ProducerStats stats = producer.getStats();          String topic = producer.getTopic();          if (topic.endsWith(RetryMessageUtil.RETRY_GROUP_TOPIC_SUFFIX)) &#123;              continue;          &#125;        observableLongMeasurement.record(getter.apply(stats),                  Attributes.of(PRODUCER_NAME, producer.getProducerName(), TOPIC, topic));      &#125;&#125;\n\n回调函数，在这个函数中遍历所有的生产者，然后读取它的监控指标。\n这样就完成了一个自定义指标的暴露，使用的时候只需要加载这个插件即可：\njava -javaagent:opentelemetry-javaagent.jar \\     -Dotel.javaagent.extensions=ext.jar     -jar myapp.jar\n\n-Dotel.javaagent.extensions=/extensions当然也可以指定一个目录，该目录下所有的 jar 都会被作为 extensions 被加入进来。\n打包使用 ./gradlew build 打包，之后可以在build/libs/目录下找到生成物。\n当然也可以将 extension 直接打包到 opentelemetry-javaagent.jar中，这样就可以不用指定 -Dotel.javaagent.extensions参数了。\n具体可以在 gradle 中加入以下 task：\ntask extendedAgent(type: Jar) &#123;  dependsOn(configurations.otel)  archiveFileName = &quot;opentelemetry-javaagent.jar&quot;  from zipTree(configurations.otel.singleFile)  from(tasks.shadowJar.archiveFile) &#123;    into &quot;extensions&quot;  &#125;  //Preserve MANIFEST.MF file from the upstream javaagent  doFirst &#123;    manifest.from(      zipTree(configurations.otel.singleFile).matching &#123;        include &#x27;META-INF/MANIFEST.MF&#x27;      &#125;.singleFile    )  &#125;&#125;\n\n具体可以参考这里的配置：https://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/examples/extension/build.gradle#L125\n踩坑看起来这个开发过程挺简单的，但其中的坑还是不少。\nNoClassDefFoundError首先第一个就是我在调试过程中出现 NoClassDefFoundError 的异常。\n但我把打包好的 extension 解压后明明是可以看到这个类的。\n\n排查一段时间后没啥头绪，我就从头仔细阅读了开发文档：\n发现我们需要重写 getAdditionalHelperClassNames函数，用于将我们外部的一些工具类加入到应用的 class loader 中，不然在应用在运行的时候就会报 NoClassDefFoundError 的错误。\n因为是字节码增强的关系，所以很多日常开发觉得很常见的地方都不行了，比如：\n\n如果切面类是一个内部类的时候，必须使用静态函数\n只能包含静态函数\n不能包含任何字段，常量。\n不能使用任何外部类，如果要使用就得使用 getAdditionalHelperClassNames 额外加入到 class loader 中（这一条就是我遇到的问题）\n所有的函数必须使用 @Advice 注解\n\n以上的内容其实在文档中都有写：\n所以还是得仔细阅读文档。\n缺少异常日志其实上述的异常刚开始都没有打印出来，只有一个现象就是程序没有正常运行。\n因为没有日志也不知道如何排查，也怀疑是不是运行过程中报错了，所以就尝试把@Advice 注解的函数全部 try catch ，果然打印了上述的异常日志。\n\n之后我注意到了注解的这个参数，原来在默认情况下是不会打印任何日志的，需要手动打开。\n比如这样：@Advice.OnMethodExit(suppress = Throwable.class)\n调试日志最后就是调试功能了，因为我这个插件的是把指标发送到 OpenTelemetry-collector ，再由它发往 VictoriaMetrics/Prometheus；由于整个链路比较长，我想看到最终生成的指标是否正常的干扰条件太多了。\n好在 OpenTelemetry 提供了多种 metrics.exporter 的输出方式：\n\n-Dotel.metrics.exporter&#x3D;otlp (default)，默认通过 otlp 协议输出到 collector 中。\n-Dotel.metrics.exporter&#x3D;logging，以 stdout 的方式输出到控制台，主要用于调试\n-Dotel.metrics.exporter&#x3D;logging-otlp\n-Dotel.metrics.exporter&#x3D;prometheus，以 Prometheus 的方式输出，还可以配置端口，这样也可以让 Prometheus 进行远程采集，同样的也可以在本地调试。\n\n采用哪种方式可以根据环境情况自行选择。\nOpentelemetry-operator 配置 extension最近在使用 opentelemetry-operator注入 agent 的时候发现 operator 目前并不支持配置 extension，所以在社区也提交了一个草案，下周会尝试提交一个 PR 来新增这个特性。\n\n这个需求我在 issue 列表中找到了好几个，时间也挺久远了，不太确定为什么社区还为实现。\n\n目前 operator 只支持在自定义镜像中配置 javaagent.jar，无法配置 extension：\n\n这个原理在之前的文章中有提到。\n\napiVersion: opentelemetry.io/v1alpha1kind: Instrumentationmetadata:  name: my-instrumentationspec:  java:    image: your-customized-auto-instrumentation-image:java\n\n我的目的是可以在自定义镜像中把 extension 也复制进去，类似于这样：\nFROM busyboxADD open-telemetry/opentelemetry-javaagent.jar /javaagent.jar# Copy extensions to specify a path.ADD open-telemetry/ext-1.0.0.jar /ext-1.0.0.jarRUN chmod -R go+r /javaagent.jarRUN chmod -R go+r /ext-1.0.0.jar\n\n然后在 CRD 中配置这个 extension 的路径：\napiVersion: opentelemetry.io/v1alpha1kind: Instrumentationmetadata:  name: my-instrumentationspec:  java:    image: custom-image:1.0.0    extensions: /ext-1.0.0.jar    env:    # If extension.jar already exists in the container, you can only specify a specific path with this environment variable.      - name: OTEL_EXTENSIONS_DIR        value: /custom-dir\n\n这样 operator 在拿到 extension 的路径时，就可以在环境变量中加入 -Dotel.javaagent.extensions=$&#123;java.extensions&#125; 参数，从而实现自定义 extension 的目的。\n总结整个过程其实并不复杂，只是由于目前用的人还不算多，所以也很少有人写教程或者文章，相信用不了多久就会慢慢普及。\n这里有一些官方的 example可以参考。\n参考链接：\n\nhttps://github.com/apache/pulsar/pull/22178\nhttps://opentelemetry.io/docs/languages/java/automatic/extensions/\nhttps://github.com/open-telemetry/opentelemetry-java-instrumentation/tree/main/examples/extension#extensions-examples\nhttps://github.com/open-telemetry/opentelemetry-operator/issues/1758#issuecomment-1982159356\n\n","categories":["OB"],"tags":["OpenTelemetry"]},{"title":"Istio 安装过程中遇到的坑","url":"/2024/12/25/ob/istio-install-problem/","content":"安装 Istio最近这段时间一直在做服务网格（Istio）相关的工作，背景是我们准备自建 Istio，首先第一件事情就是要安装。\n我这里直接使用官网推荐的 istioctl 进行安装：\n$ cat &lt;&lt;EOF &gt; ./my-config.yamlapiVersion: install.istio.io/v1alpha1  kind: IstioOperator  metadata:    namespace: istio-1-18-5  spec:    profile: minimal    revision: istio-1-18-5    meshConfig:      accessLogFile: /dev/stdoutEOF$ istioctl install -f my-config.yaml -n istio-1-18-5\n\n这里我使用的 profile 是 minimal，它只会安装核心的控制面，具体差异见下图：\n\n\n输出以下内容时代表安装成功：\nThis will install the Istio 1.18.5 minimal profile with [&quot;Istio core&quot; &quot;Istiod&quot;] components into the cluster. Proceed? (y/N) y✔ Istio core installed                                                                                                                                   ✔ Istiod installed                                                                         ✔ Installation complete  \n\n之后我们便可以在指定的 namespace 下查询到控制面的 Pod：\nk get pod -n istio-1-18-5NAME                                   READY   STATUS    RESTARTS   AGEistiod-istio-1-18-5-6cb9898585-64jtg   1/1     Running   0          22h\n\n然后只需要将需要注入 sidecar 的 namespace 中开启相关的配置即可，比如我这里将 test 这个 namespace 开启 sidecar 注入：\napiVersion: v1kind: Namespacemetadata:  labels:    istio.io/rev: istio-1-18-5    kubernetes.io/metadata.name: test\n\n最主要的就是加上 istio.io/rev: istio-1-18-5 的标签，标签的值就是我们在安装 istio 时指定的值：revision: istio-1-18-5。\n此时只要我们在这个 namespace 下部署一个 Pod 就会为这个 Pod 挂载一个 sidecar。\n\n更新配置的坑\n默认情况下 Istio 会将应用 Pod 的暴露出来的 metrics 和 sidecar 的指标合并在一起，然后暴露为 :15020/stats/prometheus 这个 endpoint。\n而我们自己在 Pod 上定义的注解则是被覆盖掉了：\n但我们是将应用和 sidecar 的指标分开采集的，所以我们不需要这个自动合并。\n\n\n会单独配置 15090 端口的采集任务\n\n所以我需要将这个功能关闭，安装文档的说明只需要在控制面中将 enablePrometheusMerge 修改为 false 即可。\n安装好 Istio 控制面之后会创建一个 IstioOperator 的 CRD 资源：\nk get IstioOperator -ANAMESPACE      NAME                           REVISION       STATUS   AGEistio-1-18-5   installed-state-istio-1-18-5   istio-1-18-5            27h\n\n所有控制面的配置都可以在这里面修改，所以我想当然的在这里加入了 enablePrometheusMerge: false 的配置。\n\n加上之后我重启了 Pod 发现依然还是 Istio 的注解：\n\n也就是说这个配置并没有生效，即便是我把控制面也重启了也没有效果。\n按照原理来说，这些配置应该是控制面下发给数据面的，大胆猜测下也就是控制面没有拿到最新的配置。\n但是我卸载控制面，再安装的时候就指定这个配置确是生效的，也就是说配置没问题，只是我在安装完成后再修改就没法同步。\n之后我在 stackoverflow 上找到了类似的问题：\n简单来说安装好 istio 之后我们也可以继续使用 istioctl install -f xx.yaml 进行更新。\n原理后来我仔细看了下 istioctl 这个命令的 help 文档，发现其实已经在描述里写清楚了：甚至还有个别名就叫 apply 这就和 kubectl apply 的命令非常类似了，也更容易理解了，任何的修改只需要 apply 执行一次就可以了。\n不过我也在好奇，既然创建的是一个 IstioOperator 的 CRD，理论上是需要一个 Operator 来读取这里的数据然后再创建一个控制面，同步配置之类的操作。\n但当我安装好 Istio 之后并没看到有一个 Operator 的 Pod 在运行，所以就比较好奇 install 这个命令是如何实现配置同步的。\n经过对 istioctl 的 debug 找到了具体的原因：\n\n在 istioctl install -f xx.yaml 执行之后会直接解析 xx.yaml 里的 IstioOperator 生成所有的 manifest 资源，在这个过程中也会生成一个 ConfigMap，所有的配置都是存放在其中的。\n所以其实我手动修改这个 ConfigMap 也可以动态更新控制面的配置，之前我只是修改了 CRD，我以为还有一个 Operator 来监听这里的变化然后同步数据；实际上并不存在这个逻辑，而是直接应用的 manifest。\n参考链接：\n\nhttps://istio.io/v1.18/docs/setup/install/istioctl\nhttps://istio.io/latest/docs/ops/integrations/prometheus/#option-1-metrics-merging\nhttps://stackoverflow.com/questions/70076326/how-to-update-istio-configuration-after-installation\n\n","categories":["Istio","k8s"],"tags":["Istio"]},{"title":"从一个 JDK21+OpenTelemetry 不兼容的问题讲起","url":"/2024/05/13/ob/jdk21+springboot+OTel+SPI/","content":"背景前段时间公司领导让我排查一个关于在 JDK21 环境中使用 Spring Boot 配合一个 JDK18 新增的一个 SPI(java.net.spi.InetAddressResolverProvider) 不生效的问题。\n但这个不生效的前置条件有点多：\n\nJDK 的版本得在 18+\nSpringBoot3.x\n还在额外再配合使用 -javaagent:opentelemetry-javaagent.jar 使用，也就是 OpenTelemetry 提供的 agent。\n\n才会导致自定义的 InetAddressResolverProvider 无法正常工作。\n\n\n\n在复现这个问题之前先简单介绍下 java.net.spi.InetAddressResolverProvider 这个 SPI；它是在 JDK18 之后才提供的，在这之前我们使用 InetAddress 的内置解析器来解析主机名和 IP 地址，但这个解析器之前是不可以自定义的。\n在某些场景下会不太方便，比如我们需要请求 order.service 这个域名时希望可以请求到某一个具体 IP 地址上，我们可以自己配置 host ，或者使用服务发现机制来实现。\n但现在通过 InetAddressResolverProvider 就可以定义在请求这个域名的时候返回一个我们预期的 IP 地址。\n同时由于它是一个 SPI，所以我们只需要编写一个第三方包，任何项目依赖它之后在发起网络请求时都会按照我们预期的 IP 进行请求。\n复现要使用它也很简单，主要是两个类：\n\nInetAddressResolverProvider：这是一个抽象类，我们可以继承它之后重写它的 get 函数返回一个 InetAddressResolver 对象\nInetAddressResolver：一个接口，主要提供了两个函数；一个用于传入域名返回 IP 地址，另一个反之：传入 IP 地址返回域名。\n\npublic class MyAddressResolverProvider extends InetAddressResolverProvider &#123;    @Override    public InetAddressResolver get(Configuration configuration) &#123;        return new MyAddressResolver();    &#125;    @Override    public String name() &#123;        return &quot;MyAddressResolverProvider Internet Address Resolver Provider&quot;;    &#125;&#125;public class MyAddressResolver implements InetAddressResolver &#123;    public MyAddressResolver() &#123;        System.out.println(&quot;=====MyAddressResolver&quot;);    &#125;    @Override    public Stream&lt;InetAddress&gt; lookupByName(String host, LookupPolicy lookupPolicy)            throws UnknownHostException &#123;        if (host.equals(&quot;fedora&quot;)) &#123;            return Stream.of(InetAddress.getByAddress(new byte[] &#123;127, 127, 10, 1&#125;));        &#125;        return Stream.of(InetAddress.getByAddress(new byte[] &#123;127, 0, 0, 1&#125;));    &#125;    @Override    public String lookupByAddress(byte[] addr) &#123;        System.out.println(&quot;++++++&quot; + addr[0] + &quot; &quot; + addr[1] + &quot; &quot; + addr[2] + &quot; &quot; + addr[3]);        return  &quot;fedora&quot;;    &#125;&#125;---```javaaddresses = InetAddress.getAllByName(&quot;fedora&quot;);// output: 127 127 10 1\n这里我简单实现了一个对域名 fedora 的解析，会直接返回 127.127.10.1。\n如果使用 IP 地址进行查询时：\nInetAddress byAddress = InetAddress.getByAddress(new byte[]&#123;127, 127, 10, 1&#125;);System.out.println(&quot;+++++&quot; + byAddress.getHostName());// output: fedora\n\n当然要要使得这个 SPI 生效的前提条件是我们需要新建一个文件：META-INF/services/java.net.spi.InetAddressResolverProvider里面的内容是我们自定义类的全限定名称：\ncom.example.demo.MyAddressResolverProvider\n\n这样一个完整的 SPI 就实现完成了。\n\n正常情况下我们将应用打包为一个 jar 之后运行：\njava -jar target/demo-0.0.1-SNAPSHOT.jar\n是可以看到输出结果是符合预期的。\n一旦我们使用配合上 spring boot 打包之后，也就是加上以下的依赖：\n&lt;parent&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;    &lt;version&gt;3.2.3&lt;/version&gt;    &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;  &lt;/parent&gt;&lt;build&gt;    &lt;plugins&gt;     &lt;plugin&gt;      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;      &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;     &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;\n再次执行其实也没啥问题，也能按照预期输出结果。\n但我们加上 OpenTelemetry 的 agent 时：\njava  -javaagent:opentelemetry-javaagent.jar \\      -jar target/demo-0.0.1-SNAPSHOT.jar\n\n就会发现在执行解析的时候抛出了 java.net.UnknownHostException异常。\n从结果来看就是没有进入我们自定义的解析器。\nSPI 原理在讲排查过程之前还是要先预习下关于 Java SPI 的原理以及应用场景。\n以前写过一个 http 框架 cicada，其中有一个可拔插 IOC 容器的功能：\n\n就是可以自定义实现自己的 IOC 容器，将自己实现的 IOC 容器打包为一个第三方包加入到依赖中，cicada 框架就会自动使用自定义的 IOC 实现。\n\n要实现这个功能本质上就是要定义一个接口，然后根据依赖的不同实现创建接口的实例对象。\npublic interface CicadaBeanFactory &#123;    /**     * Register into bean Factory     * @param object     */    void register(Object object);    /**     * Get bean from bean Factory     * @param name     * @return     * @throws Exception     */    Object getBean(String name) throws Exception;    /**     * get bean by class type     * @param clazz     * @param &lt;T&gt;     * @return bean     * @throws Exception     */    &lt;T&gt; T getBean(Class&lt;T&gt; clazz) throws Exception;    /**     * release all beans     */    void releaseBean() ;&#125;\n\n获取具体的示例代码时就只需要使用 JDK 内置的 ServiceLoader 进行加载即可：\npublic static CicadaBeanFactory getCicadaBeanFactory() &#123;      ServiceLoader&lt;CicadaBeanFactory&gt; cicadaBeanFactories = ServiceLoader.load(CicadaBeanFactory.class);      if (cicadaBeanFactories.iterator().hasNext())&#123;          return cicadaBeanFactories.iterator().next() ;      &#125;      return new CicadaDefaultBean();  &#125;\n\n代码也非常的简洁，和刚才提到的 InetAddressResolverProvider 一样我们需要新增一个 META-INF/services/top.crossoverjie.cicada.base.bean.CicadaBeanFactory 文件来配置我们的类名称。\nprivate boolean hasNextService() &#123;    if (nextName != null) &#123;        return true;    &#125;    if (configs == null) &#123;        try &#123;        \t// PREFIX = META-INF/services/            String fullName = PREFIX + service.getName();            if (loader == null)                configs = ClassLoader.getSystemResources(fullName);            else                configs = loader.getResources(fullName);        &#125; catch (IOException x) &#123;            fail(service, &quot;Error locating configuration files&quot;, x);        &#125;    &#125;    while ((pending == null) || !pending.hasNext()) &#123;        if (!configs.hasMoreElements()) &#123;            return false;        &#125;        pending = parse(service, configs.nextElement());    &#125;    nextName = pending.next();    return true;&#125;\n\n在 ServiceLoader 类中会会去查找 META-INF/services 的文件，然后解析其中的内容从而反射生成对应的接口对象。\n这里还有一个关键是通常我们的代码都会打包为一个 JAR 包，类加载器需要加载这个  JAR 包，同时需要在这个 JAR 包里找到我们之前定义的那个 spi 文件，如果这里查不到文件那就认为没有定义 SPI。\n这个是本次问题的重点，会在后文分析原因的时候用到。\n排查因为问题就出现在是否使用 opentelemetry-javaagent.jar 上，所以我需要知道在使用了 agent 之后有什么区别。\n从刚才的对 SPI 的原理分析，加上 agent 出现异常，说明理论上就是没有读取到我们配置的文件: java.net.spi.InetAddressResolverProvider。\n于是我便开始 debug，在 ServiceLoader 加载 jar 包的时候是可以看到具体使用的是什么 classLoader 。\n这是不配置 agent 的时候使用的 classLoader：使用这个 loader 是可以通过文件路径在 jar 包中查找到我们配置的文件。\n而配置上 agent 之后使用的 classLoader:却是一个 JarLoader，这样是无法加载到在 springboot 格式下的配置文件的，至于为什么加载不到，那就要提一下 maven 打包后的文件目录和 spring boot 打包后的文件目录的区别了。\n这里我截图了同样的一份代码不同的打包方式：上面的是传统 maven，下图是 spring boot；其实主要的区别就是在 pom 中使用了一个构建插件：\n&lt;build&gt;    &lt;plugins&gt;     &lt;plugin&gt;      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;      &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;     &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/build&gt;\n\n\n或者使用 spring-boot 命令再次打包的效果也是一样的。\n\n会发现 spring boot 打包后会多出一层 BOOT-INF 的文件夹，然后会在 MANIFIST.MF 文件中定义 Main-Class 和 Start-Class.\n\n通过上面的 debug 其实会发现 JarLoader 只能在加载 maven 打包后的文件，也就是说无法识别 BOOT-INF 这个目录。\n正常情况下 spring boot 中会有一个额外的 java.nio.file.spi.FileSystemProvider 实现:通过这个类的实现可以直接从 JAR 包中加载资源，比如我们自定义的 SPI 资源等。\n初步判断使用 opentelemetry-javaagent.jar的 agent 之后，它的类加载器优先于了 spring boot ，从而导致后续的加载失败。\n远程 debug这里穿插几个 debug 小技巧，其中一个是远程 debug，因为这里我是需要调试 javaagent，正常情况下是无法直接 debug 的。\n所以我们可以使用以下命令启动应用：\njava -agentlib:jdwp=&quot;transport=dt_socket,server=y,suspend=y,address=5000&quot; -javaagent:opentelemetry-javaagent.jar \\      -jar target/demo-0.0.1-SNAPSHOT.jar\n\n然后在 idea 中配置一个 remote 启动。\n\n注意这里的端口得和命令行中的保持一致。\n\n当应用启动之后便可以在 idea 中启动这个 remote 了，这样便可以正常 debug 了。\n条件断点第二个是条件断点也非常有用，有时候我们需要调试一个公共函数，调用的地方非常多。\n而我们只需要关心某一类行为的调用，此时就可以对这个函数中的变量进行判断，当他们满足某些条件时再进入断点，这样可以极大的提高我们的调试效率：\n配置也很简单，只需要在断点上右键就可以编辑条件了。\n社区咨询虽然我根据现象初步可以猜测下原因，但依然不确定如何调整才能解决这个问题，于是便去社区提了一个 issue。\n最后在社区大佬的帮助下发现我们需要禁用掉 OpenTelemetry agent 中的一个 resource 就可以了。\n这个 resource 是由 agent 触发的，它优先于 spring boot 之前进行 SPI 的加载。目的是为了给 metric 和 trace 新增两个属性：\n加载的核心代码在这里，只要禁用掉之后就不会再加载了。\n禁用前：\n禁用后：\n当我们禁用掉之后就不会存在这两个属性了，不过我们目前并没有使用这两个属性，所以为了使得 SPI 生效就只有先禁用掉了，后续再看看社区还有没有其他的方案。\n想要复现 debug 的可以在这里尝试：https://github.com/crossoverJie/demo\n参考连接：\n\nhttps://github.com/TogetherOS/cicada\nhttps://docs.spring.io/spring-boot/docs/current/maven-plugin/reference/htmlsingle/#packaging.repackage-goal\nhttps://github.com/open-telemetry/opentelemetry-java-instrumentation/issues/10921\nhttps://github.com/open-telemetry/opentelemetry-java-instrumentation/blob/main/instrumentation/resources/library/README.md#host\n\n","categories":["OB","OpenTelemetry"],"tags":["OpenTelemetry"]},{"title":"k8s 入门到实战--部署应用到 k8s","url":"/2023/08/31/ob/k8s-0-start/","content":"\n背景最近这这段时间更新了一些 k8s 相关的博客和视频，也收到了一些反馈；大概分为这几类：\n\n公司已经经历过服务化改造了，但还未接触过云原生。\n公司部分应用进行了云原生改造，但大部分工作是由基础架构和运维部门推动的，自己只是作为开发并不了解其中的细节，甚至 k8s 也接触不到。\n还处于比较传统的以虚拟机部署的传统运维为主。\n\n其中以第二种占大多数，虽然公司进行了云原生改造，但似乎和纯业务研发同学来说没有太大关系，自己工作也没有什么变化。\n恰好我之前正好从业务研发的角度转换到了基础架构部门，两个角色我都接触过，也帮助过一些业务研发了解公司的云原生架构；\n为此所以我想系统性的带大家以研发的角度对 k8s 进行实践。\n因为 k8s 部分功能其实是偏运维的，对研发来说优先级并不太高；所以我不太会涉及一些 k8s 运维的知识点，比如安装、组件等模块；主要以我们日常开发会使用到的组件讲起。\n\n\n计划入门\n部署应用到 k8s\n跨服务调用\n集群外部访问\n\n进阶\n如何使用配置\n服务网格实战\n\n运维你的应用\n应用探针\n滚动更新与回滚\n优雅采集日志\n应用可观测性\n指标可视化\n\n\n\nk8s 部署常见中间件\nhelm 一键部署\n编写 Operator 自动化应用生命周期\n\n这里我整理了一下目录，每个章节都有博客+视频配合观看，大家可以按照喜好选择。\n因为还涉及到了视频，所以只能争取一周两更，在两个月内全部更新完毕。\n根据我自己的经验，以上内容都掌握的话对 k8s 的掌握会更进一步。\n部署应用到 k8s首先从第一章【部署应用到 k8s】开始，我会用 Go 写一个简单的 Web 应用，然后打包为一个 Docker 镜像，之后部署到 k8s 中，并完成其中的接口调用。\n编写应用func main() &#123;     http.HandleFunc(&quot;/ping&quot;, func(w http.ResponseWriter, r *http.Request) &#123;        log.Println(&quot;ping&quot;)        fmt.Fprint(w, &quot;pong&quot;)     &#125;)       http.ListenAndServe(&quot;:8081&quot;, nil)  &#125;\n\n 应用非常简单就是提供了一个 ping  接口，然后返回了一个 pong.\nDockerfile# 第一阶段：编译 Go 程序  FROM golang:1.19 AS dependencies  ENV GOPROXY=https://goproxy.cn,direct  WORKDIR /go/src/app  COPY go.mod .  #COPY ../../go.sum .  RUN --mount=type=ssh go mod download    # 第二阶段：构建可执行文件  FROM golang:1.19 AS builder  WORKDIR /go/src/app  COPY . .  #COPY --from=dependencies /go/pkg /go/pkg  RUN go build    # 第三阶段：部署  FROM debian:stable-slim  RUN apt-get update &amp;&amp; apt-get install -y curl  COPY --from=builder /go/src/app/k8s-combat /go/bin/k8s-combat  ENV PATH=&quot;/go/bin:$&#123;PATH&#125;&quot;    # 启动 Go 程序  CMD [&quot;k8s-combat&quot;]\n\n 之后编写了一个 dockerfile 用于构建 docker 镜像。\ndocker:     @echo &quot;Docker Build...&quot;     docker build . -t crossoverjie/k8s-combat:v1 &amp;&amp; docker image push crossoverjie/k8s-combat:v1\n\n使用 make docker  会在本地构建镜像并上传到 dockerhub\n编写 deployment下一步便是整个过程中最重要的环节了，也是唯一和 k8s 打交道的地方，那就是编写 deployment。\n \n\n在之前的视频《一分钟了解 k8s》中讲过常见的组件：\n其中我们最常见的就是 deployment，通常用于部署无状态应用；现在还不太需要了解其他的组件，先看看 deployment 如何编写：\napiVersion: apps/v1  kind: Deployment  metadata:    labels:      app: k8s-combat    name: k8s-combat  spec:    replicas: 1    selector:      matchLabels:        app: k8s-combat    template:      metadata:        labels:          app: k8s-combat      spec:        containers:          - name: k8s-combat            image: crossoverjie/k8s-combat:v1            imagePullPolicy: Always            resources:              limits:                cpu: &quot;1&quot;                memory: 300Mi              requests:                cpu: &quot;0.1&quot;                memory: 30Mi\n\n开头两行的 apiVersion  和 kind 可以暂时不要关注，就理解为 deployment 的固定写法即可。\nmetadata：顾名思义就是定义元数据的地方，告诉 Pod 我们这个 deployment 叫什么名字，这里定义为：k8s-combat\n中间的：\nmetadata:    labels:      app: k8s-combat\n\n也很容易理解，就是给这个 deployment 打上标签，通常是将这个标签和其他的组件进行关联使用才有意义，不然就只是一个标签而已。\n\n标签是键值对的格式，key, value 都可以自定义。\n\n而这里的  app: k8s-combat 便是和下面的 spec 下的 selector 选择器匹配，表明都使用  app: k8s-combat  进行关联。\n而 template 中所定义的标签也是为了让选择器和 template 中的定义的 Pod 进行关联。\n\nPod 是 k8s 中相同功能容器的分组，一个 Pod 可以绑定多个容器，这里就只有我们应用容器一个了；后续在讲到 istio 和日志采集时便可以看到其他的容器。\n\ntemplate 中定义的内容就很容易理解了，指定了我们的容器拉取地址，以及所占用的资源(cpu/ memory)。\nreplicas: 1：表示只部署一个副本，也就是只有一个节点的意思。\n部署应用之后我们使用命令:\nkubectl apply -f deployment/deployment.yaml\n\n\n生产环境中往往会使用云厂商所提供的 k8s 环境，我们本地可以使用 https://minikube.sigs.k8s.io/docs/start/ minikube 来模拟。\n\n就会应用这个 deployment 同时将容器部署到 k8s 中，之后使用:\nkubectl get pod\n\n 在后台 k8s 会根据我们填写的资源选择一个合适的节点，将当前这个 Pod 部署过去。\n\n 就会列出我们刚才部署的 Pod:\n❯ kubectl get podNAME                                READY   STATUS    RESTARTS   AGEk8s-combat-57f794c59b-7k58n         1/1     Running   0          17h\n\n我们使用命令：\nkubectl exec -it k8s-combat-57f794c59b-7k58n  bash\n就会进入我们的容器，这个和使用 docker 类似。\n之后执行 curl 命令便可以访问我们的接口了：\nroot@k8s-combat-57f794c59b-7k58n:/# curl http://127.0.0.1:8081/pingpongroot@k8s-combat-57f794c59b-7k58n:/#\n\n这时候我们再开一个终端执行：\n❯ kubectl logs -f k8s-combat-57f794c59b-7k58n2023/09/03 09:28:07 ping\n便可以打印容器中的日志，当然前提是应用的日志是写入到了标准输出中。\n总结以上就是这一章节的主要内容，重点就是将我们应用程序员打包为 docker 镜像后上传到镜像仓库，再配置好 deployment 由 k8s 进行调度运行。\n下一章主要会涉及服务内部的调用，感兴趣的朋友可以先关注起来。\n相关的源码和 yaml 资源文件都存在这里：https://github.com/crossoverJie/k8s-combat\n#Blog #K8s \n","categories":["k8s"],"tags":["Go"]},{"title":"k8s入门到实战-使用Ingress","url":"/2023/09/15/ob/k8s-Ingress/","content":"\n背景前两章中我们将应用部署到了 k8s 中，同时不同的服务之间也可以通过 service 进行调用，现在还有一个步骤就是将我们的应用暴露到公网，并提供域名的访问。\n这一步类似于我们以前配置 Nginx 和绑定域名，提供这个能力的服务在 k8s 中称为 Ingress。\n通过这个描述其实也能看出 Ingress 是偏运维的工作，但也不妨碍我们作为研发去了解这部分的内容；了解整个系统是如何运转的也是研发应该掌握的技能。\n\n安装 Ingress 控制器在正式使用 Ingress 之前需要给 k8s 安装一个 Ingress 控制器，我们这里安装官方提供的 Ingress-nginx 控制器。\n当然还有社区或者企业提供的各种控制器：\n有两种安装方式: helm 或者是直接 apply 一个资源文件。\n关于 helm 我们会在后面的章节单独讲解。\n这里就直接使用资源文件安装即可，我已经上传到 GitHub 可以在这里访问：https://github.com/crossoverJie/k8s-combat/blob/main/deployment/ingress-nginx.yaml\n其实这个文件也是直接从官方提供的复制过来的，也可以直接使用这个路径进行安装：\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml\n\n\nyaml 文件的内容是一样的。\n\n不过要注意安装之后可能容器状态一直处于 Pending 状态，查看容器的事件时会发现镜像拉取失败。\nk describe pod ingress-nginx-controller-7cdfb9988c-lbcst -n ingress-nginx\n\n\ndescribe 是一个用于查看 k8s 对象详细信息的命令。\n\n在刚才那份 yaml 文件中可以看到有几个镜像需要拉取，我们可以先在本地手动拉取镜像：\ndocker pull registry.k8s.io/ingress-nginx/controller:v1.8.2\n\n如果依然无法拉取，可以尝试配置几个国内镜像源镜像拉取：\n\n\n我这里使用的 docker-desktop 自带的 k8s，推荐读者朋友也使用这个工具。\n\n创建 Ingress使用刚才的 yaml 安装成功之后会在 ingress-nginx 命名空间下创建一个 Pod，通过 get 命令查看状态为 Running 即为安装成功。\n$ k get pod -n ingress-nginxNAME                            READY   STATUS    RESTARTS      AGEingress-nginx-controller-7cdf   1/1     Running   2 (35h ago)   3d\n\n\nNamespace 也是 k8s 内置的一个对象，可以简单理解为对资源进行分组管理，我们通常可以使用它来区分各个不同的环境，比如 dev&#x2F;test&#x2F;prod 等，不同命名空间下的资源不会互相干扰，且相互独立。\n\n之后便可以创建 Ingress 资源了：\napiVersion: networking.k8s.io/v1  kind: Ingress  metadata:    name: k8s-combat-ingress  spec:    ingressClassName: nginx    rules:      - host: www.service1.io        http:          paths:            - backend:                service:                  name: k8s-combat-service                  port:                    number: 8081              path: /              pathType: Prefix      - host: www.service2.io        http:          paths:            - backend:                service:                  name: k8s-combat-service-2                  port:                    number: 8081              path: /              pathType: Prefix\n\n看这个内容也很容易理解，创建了一个 Ingress 的对象，其中的重点就是这里的规则是如何定义的。\n\n在 k8s 中今后还会接触到各种不同的 Kind\n\n这里的 ingressClassName: nginx   也是在刚开始安装的控制器里定义的名字，由这个资源定义。\napiVersion: networking.k8s.io/v1  kind: IngressClass  metadata:    labels:      app.kubernetes.io/component: controller      app.kubernetes.io/instance: ingress-nginx      app.kubernetes.io/name: ingress-nginx      app.kubernetes.io/part-of: ingress-nginx      app.kubernetes.io/version: 1.8.2    name: nginx\n\n咱们这个规则很简单，就是将两个不同的域名路由到两个不同的 service。\n\n这里为了方便测试又创建了一个 k8s-combat-service-2 的 service，和 k8s-combat-service 是一样的，只是改了个名字而已。\n\n测试也是为了方便测试，我在应用镜像中新增了一个接口，用于返回当前 Pod 的 hostname。\nhttp.HandleFunc(&quot;/&quot;, func(w http.ResponseWriter, r *http.Request) &#123;     name, _ := os.Hostname()     fmt.Fprint(w, name)  &#125;)\n\n\n由于我实际并没有 www.service1.io/www.service2.io 这两个域名，所以只能在本地配置 host 进行模拟。\n10.0.0.37 www.service1.io10.0.0.37 www.service2.io\n\n\n我测试所使用的 k8s 部署在我家里一台限制的 Mac 上，所以这里的 IP 它的地址。\n\n当我们反复请求两次这个接口，会拿到两个不同的 hostname，也就是将我们的请求轮训负载到了这两个 service 所代理的两个 Pod 中。\n❯ curl http://www.service1.io/k8s-combat-service-79c5579587-b6nlj%❯ curl http://www.service1.io/k8s-combat-service-79c5579587-bk7nw%❯ curl http://www.service2.io/k8s-combat-service-2-7bbf56b4d9-dkj9b%❯ curl http://www.service2.io/k8s-combat-service-2-7bbf56b4d9-t5l4g\n\n我们也可以直接使用 describe 查看我们的 ingress 定义以及路由规则：\n$ k describe ingress k8s-combat-ingressName:             k8s-combat-ingressLabels:           &lt;none&gt;Namespace:        defaultAddress:          localhostIngress Class:    nginxDefault backend:  &lt;default&gt;Rules:  Host             Path  Backends  ----             ----  --------  www.service1.io                   /   k8s-combat-service:8081 (10.1.0.65:8081,10.1.0.67:8081)  www.service2.io                   /   k8s-combat-service-2:8081 (10.1.0.63:8081,10.1.0.64:8081)Annotations:       &lt;none&gt;Events:            &lt;none&gt;\n\n如果我们手动新增一个域名解析：\n10.0.0.37 www.service3.io❯ curl http://www.service3.io/&lt;html&gt;&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;\n会直接 404，这是因为没有找到这个域名的规则。\n访问原理整个的请求路径如上图所示，其实我们的 Ingress 本质上也是一个 service（所以它也可以启动多个副本来进行负载），只是他的类型是 LoadBalancer，通常这种类型的 service 会由云厂商绑定一个外部 IP，这样就可以通过这个外部 IP 访问 Ingress 了。\n\n而我们应用的 service 是 ClusterIP，只能在应用内部访问\n\n\n通过 service 的信息也可以看到，我们 ingress 的 service 绑定的外部 IP 是 localhost（本地的原因）\n总结Ingress 通常是充当网关的作用，后续我们在使用 Istio 时，也可以使用 Istio 所提供的控制器来替换掉 Ingress-nginx，可以更方便的管理内外网流量。\n本文的所有源码在这里可以访问：https://github.com/crossoverJie/k8s-combat\n","categories":["k8s"],"tags":["Ingress"]},{"title":"k8s-服务网格实战-入门Istio","url":"/2023/10/31/ob/k8s-Istio01/","content":"\n背景终于进入大家都比较感兴趣的服务网格系列了，在前面已经讲解了：\n\n如何部署应用到 kubernetes\n服务之间如何调用\n如何通过域名访问我们的服务\n如何使用 kubernetes 自带的配置 ConfigMap\n\n基本上已经够我们开发一般规模的 web 应用了；但在企业中往往有着复杂的应用调用关系，应用与应用之间的请求也需要进行管理。比如常见的限流、降级、trace、监控、负载均衡等功能。\n在我们使用 kubernetes 之前往往都是由微服务框架来解决这些问题，比如 Dubbo、SpringCloud 都有对应的功能。\n但当我们上了 kubernetes 之后这些事情就应该交给一个专门的云原生组件来解决，也就是本次会讲到的 Istio，它是目前使用最为广泛的服务网格解决方案。\n\n官方对于 Istio 的解释比较简洁，落到具体的功能点也就是刚才提到的：\n\n限流降级\n路由转发、负载均衡\n入口网关、TLS安全认证\n灰度发布等\n\n\n再结合官方的架构图可知：Istio 分为控制面 control plane 和数据面 data plane。\n控制面可以理解为 Istio 自身的管理功能：\n\n比如服务注册发现\n管理配置数据面所需要的网络规则等\n\n而数据面可以简单的把他理解为由 Envoy 代理的我们的业务应用，我们应用中所有的流量进出都会经过 Envoy 代理。\n所以它可以实现负载均衡、熔断保护、认证授权等功能。\n安装首先安装 Istio 命令行工具\n\n这里的前提是有一个 kubernetes 运行环境\n\nLinux 使用：\ncurl -L https://istio.io/downloadIstio | sh -\n\nMac 可以使用 brew：\nbrew install istioctl\n\n其他环境可以下载 Istio 后配置环境变量：\nexport PATH=$PWD/bin:$PATH\n\n之后我们可以使用 install 命令安装控制面。\n\n这里默认使用的是 kubectl 所配置的 kubernetes 集群\n\nistioctl install --set profile=demo -y\n这个的 profile 还有以下不同的值，为了演示我们使用 demo 即可。\n使用# 开启 default 命名空间自动注入$ k label namespace default istio-injection=enabled$ k describe ns defaultName:         defaultLabels:       istio-injection=enabled              kubernetes.io/metadata.name=defaultAnnotations:  &lt;none&gt;Status:       ActiveNo resource quota.No LimitRange resource.\n之后我们为 namespace 打上 label，使得 Istio 控制面知道哪个 namespace 下的 Pod 会自动注入 sidecar。\n这里我们为 default 这个命名空间打开自动注入 sidecar，然后在这里部署我们之前使用到的 deployment-istio.yaml\n$ k apply -f deployment/deployment-istio.yaml$ k get podNAME                                  READY   STATUS    RESTARTSk8s-combat-service-5bfd78856f-8zjjf   2/2     Running   0          k8s-combat-service-5bfd78856f-mblqd   2/2     Running   0          k8s-combat-service-5bfd78856f-wlc8z   2/2     Running   0       \n此时会看到每个Pod 有两个 container（其中一个就是 istio-proxy sidecar），也就是之前做 gRPC 负载均衡测试时的代码。\n还是进行负载均衡测试，效果是一样的，说明 Istio 起作用了。\n此时我们再观察 sidecar 的日志时，会看到刚才我们所发出和接受到的流量：\n$ k logs -f k8s-combat-service-5bfd78856f-wlc8z -c istio-proxy[2023-10-31T14:52:14.279Z] &quot;POST /helloworld.Greeter/SayHello HTTP/2&quot; 200 - via_upstream - &quot;-&quot; 12 61 14 9 &quot;-&quot; &quot;grpc-go/1.58.3&quot; &quot;6d293d32-af96-9f87-a8e4-6665632f7236&quot; &quot;k8s-combat-service:50051&quot; &quot;172.17.0.9:50051&quot; inbound|50051|| 127.0.0.6:42051 172.17.0.9:50051 172.17.0.9:40804 outbound_.50051_._.k8s-combat-service.default.svc.cluster.local default[2023-10-31T14:52:14.246Z] &quot;POST /helloworld.Greeter/SayHello HTTP/2&quot; 200 - via_upstream - &quot;-&quot; 12 61 58 39 &quot;-&quot; &quot;grpc-go/1.58.3&quot; &quot;6d293d32-af96-9f87-a8e4-6665632f7236&quot; &quot;k8s-combat-service:50051&quot; &quot;172.17.0.9:50051&quot; outbound|50051||k8s-combat-service.default.svc.cluster.local 172.17.0.9:40804 10.101.204.13:50051 172.17.0.9:54012 - default[2023-10-31T14:52:15.659Z] &quot;POST /helloworld.Greeter/SayHello HTTP/2&quot; 200 - via_upstream - &quot;-&quot; 12 61 35 34 &quot;-&quot; &quot;grpc-go/1.58.3&quot; &quot;ed8ab4f2-384d-98da-81b7-d4466eaf0207&quot; &quot;k8s-combat-service:50051&quot; &quot;172.17.0.10:50051&quot; outbound|50051||k8s-combat-service.default.svc.cluster.local 172.17.0.9:39800 10.101.204.13:50051 172.17.0.9:54012 - default[2023-10-31T14:52:16.524Z] &quot;POST /helloworld.Greeter/SayHello HTTP/2&quot; 200 - via_upstream - &quot;-&quot; 12 61 28 26 &quot;-&quot; &quot;grpc-go/1.58.3&quot; &quot;67a22028-dfb3-92ca-aa23-573660b30dd4&quot; &quot;k8s-combat-service:50051&quot; &quot;172.17.0.8:50051&quot; outbound|50051||k8s-combat-service.default.svc.cluster.local 172.17.0.9:44580 10.101.204.13:50051 172.17.0.9:54012 - default[2023-10-31T14:52:16.680Z] &quot;POST /helloworld.Greeter/SayHello HTTP/2&quot; 200 - via_upstream - &quot;-&quot; 12 61 2 2 &quot;-&quot; &quot;grpc-go/1.58.3&quot; &quot;b4761d9f-7e4c-9f2c-b06f-64a028faa5bc&quot; &quot;k8s-combat-service:50051&quot; &quot;172.17.0.10:50051&quot; outbound|50051||k8s-combat-service.default.svc.cluster.local 172.17.0.9:39800 10.101.204.13:50051 172.17.0.9:54012 - default\n\n总结本期的内容比较简单，主要和安装配置相关，下一期更新如何配置内部服务调用的超时、限流等功能。\n其实目前大部分操作都是偏运维的，即便是后续的超时配置等功能都只是编写 yaml 资源。\n但在生产使用时，我们会给开发者提供一个管理台的可视化页面，可供他们自己灵活配置这些原本需要在 yaml 中配置的功能。\n其实各大云平台厂商都有提供类似的能力，比如阿里云的 EDAS 等。\n本文的所有源码在这里可以访问：https://github.com/crossoverJie/k8s-combat\n#Blog #Istio \n","categories":["k8s"],"tags":["Istio"]},{"title":"k8s-服务网格实战-配置 Mesh（灰度发布）","url":"/2023/11/07/ob/k8s-Istio02/","content":"\n在上一篇 k8s-服务网格实战-入门Istio中分享了如何安装部署 Istio，同时可以利用 Istio 实现 gRPC 的负载均衡。\n\n今天我们更进一步，深入了解使用 Istio 的功能。从 Istio 的流量模型中可以看出：Istio 支持管理集群的出入口请求（gateway），同时也支持管理集群内的 mesh 流量，也就是集群内服务之间的请求。\n本次先讲解集群内部的请求，配合实现以下两个功能：\n\n灰度发布（对指定的请求分别路由到不同的 service 中）\n配置 service 的请求权重\n\n灰度发布在开始之前会部署两个 deployment 和一个 service，同时这两个 deployment 所关联的 Pod 分别对应了两个不同的 label，由于在灰度的时候进行分组。\n使用这个 yaml 会部署所需要的 deployment 和 service。\nkubectl apply -f https://raw.githubusercontent.com/crossoverJie/k8s-combat/main/deployment/istio-mesh.yaml \n\n\n首先设想下什么情况下我们需要灰度发布，一般是某个重大功能的测试，只对部分进入内测的用户开放入口。\n假设我们做的是一个 App，我们可以对拿到了内测包用户的所有请求头中加入一个版本号。\n比如 version=200 表示新版本，version=100 表示老版本。同时在服务端会将这个版本号打印出来，用于区分请求是否进入了预期的 Pod。\n// Client version := r.URL.Query().Get(&quot;version&quot;)  name := &quot;world&quot;  ctx, cancel := context.WithTimeout(context.Background(), time.Second)  md := metadata.New(map[string]string&#123;      &quot;version&quot;: version,  &#125;)  ctx = metadata.NewOutgoingContext(ctx, md)  defer cancel()  g, err := c.SayHello(ctx, &amp;pb.HelloRequest&#123;Name: name&#125;)// Serverfunc (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) &#123;      md, ok := metadata.FromIncomingContext(ctx)      var version string      if ok &#123;         version = md.Get(&quot;version&quot;)[0]      &#125;    log.Printf(&quot;Received: %v, version: %s&quot;, in.GetName(), version)      name, _ := os.Hostname()      return &amp;pb.HelloReply&#123;Message: fmt.Sprintf(&quot;hostname:%s, in:%s, version:%s&quot;, name, in.Name, version)&#125;, nil  &#125;\n\n对 service 分组进行灰度测试时往往需要新增部署一个灰度服务，这里我们称为 v2（也就是上图中的 Pod2）。\n同时需要将 v1 和 v2 分组：\napiVersion: networking.istio.io/v1alpha3  kind: DestinationRule  metadata:    name: k8s-combat-service-ds  spec:    host: k8s-combat-service-istio-mesh    subsets:      - name: v1        labels:          app: k8s-combat-service-v1      - name: v2        labels:          app: k8s-combat-service-v2\n这里我们使用 Istio 的 DestinationRule 定义 subset，也就是将我们的 service 下的 Pod 分为 v1&#x2F;v2。\n\n使用 标签 app 进行分组\n\n注意这里的 host: k8s-combat-service-istio-mesh 通常配置的是 service 名称。\napiVersion: v1  kind: Service  metadata:    name: k8s-combat-service-istio-mesh  spec:    selector:      appId: &quot;12345&quot;    type: ClusterIP    ports:      - port: 8081        targetPort: 8081        name: app      - name: grpc        port: 50051        targetPort: 50051\n也就是这里 service 的名称，同时也支持配置为 host: k8s-combat-service-istio-mesh.default.svc.cluster.local，如果使用的简写Istio 会根据当前指定的 namespace 进行解析。\n\nIstio 更推荐使用全限定名替代我们这里的简写，从而避免误操作。\n\n当然我们也可以在 DestinationRule 中配置负载均衡的策略，这里我们先略过：\napiVersion: networking.istio.io/v1alpha3  kind: DestinationRule  metadata:    name: k8s-combat-service-ds  spec:    host: k8s-combat-service-istio-mesh   trafficPolicy:    loadBalancer:      simple: ROUND_ROBIN  \n\n\n这样我们就定义好了两个分组：\n\nv1：app: k8s-combat-service-v1\nv2：app: k8s-combat-service-v2\n\n之后就可以配置路由规则将流量分别指定到两个不同的组中，这里我们使用 VirtualService 进行配置。\napiVersion: networking.istio.io/v1alpha3  kind: VirtualService  metadata:    name: k8s-combat-service-vs  spec:    gateways:      - mesh    hosts:      - k8s-combat-service-istio-mesh # match this hosthttp:    - name: v1      match:        - headers:            version:              exact: &#x27;100&#x27;      route:        - destination:            host: k8s-combat-service-istio-mesh            subset: v1    - name: v2      match:        - headers:            version:              exact: &#x27;200&#x27;      route:        - destination:            host: k8s-combat-service-istio-mesh            subset: v2    - name: default      route:        - destination:            host: k8s-combat-service-istio-mesh            subset: v1\n\n这个规则很简单，会检测 http 协议的 header 中的 version 字段值，如果为 100 这路由到 subset=v1 这个分组的 Pod 中，同理为 200 时则路由到 subset=v2 这个分组的 Pod 中。\n当没有匹配到 header 时则进入默认的 subset:v1\n\n gRPC 也是基于 http 协议，它的 metadata 也就对应了 http 协议中的 header。\n\nheader&#x3D;100Greeting: hostname:k8s-combat-service-v1-5b998dc8c8-hkb72, in:world, version:100istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=100&quot;Greeting: hostname:k8s-combat-service-v1-5b998dc8c8-hkb72, in:world, version:100istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=100&quot;Greeting: hostname:k8s-combat-service-v1-5b998dc8c8-hkb72, in:world, version:100istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=100&quot;Greeting: hostname:k8s-combat-service-v1-5b998dc8c8-hkb72, in:world, version:100istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=100&quot;\n\nheader&#x3D;200Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;\n\n根据以上的上面的测试请求来看，只要我们请求头里带上指定的 version 就会被路由到指定的 Pod 中。\n利用这个特性我们就可以在灰度验证的时候单独发一个灰度版本的 Deployment，同时配合客户端指定版本就可以实现灰度功能了。\n配置权重同样基于 VirtualService 我们还可以对不同的 subset 分组进行权重配置。\napiVersion: networking.istio.io/v1alpha3  kind: VirtualService  metadata:    name: k8s-combat-service-vs  spec:    gateways:      - mesh    hosts:      - k8s-combat-service-istio-mesh # match this host    http:      - match:          - uri:              exact: /helloworld.Greeter/SayHello        route:          - destination:              host: k8s-combat-service-istio-mesh              subset: v1            weight: 10          - destination:              host: k8s-combat-service-istio-mesh              subset: v2            weight: 90        timeout: 5000ms\n\n这里演示的是针对 SayHello 接口进行权重配置（当然还有多种匹配规则），90% 的流量会进入 v2 这个 subset，也就是在 k8s-combat-service-istio-mesh service 下的 app: k8s-combat-service-v2 Pod。\nGreeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-**v1**-5b998dc8c8-hkb72, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$ curl &quot;http://127.0.0.1:8081/grpc_client?name=k8s-combat-service-istio-mesh&amp;version=200&quot;Greeting: hostname:k8s-combat-service-v2-5db566fb76-xj7j6, in:world, version:200istio-proxy@k8s-combat-service-v1-5b998dc8c8-hkb72:/$\n经过测试会发现大部分的请求都会按照我们的预期进入 v2 这个分组。\n当然除之外之外我们还可以：\n\n超时时间\n故障注入\n重试具体的配置可以参考 Istio 官方文档：当然在一些云平台也提供了可视化的页面，可以更直观的使用。\n\n\n以上是 阿里云的截图\n\n但他们的管理的资源都偏 kubernetes，一般是由运维或者是 DevOps 来配置，不方便开发使用，所以还需要一个介于云厂商和开发者之间的管理发布平台，可以由开发者以项目维度管理维护这些功能。\n本文的所有源码在这里可以访问：https://github.com/crossoverJie/k8s-combat\n#Blog #Istio \n","categories":["k8s"],"tags":["Istio"]},{"title":"k8s入门到实战-应用配置","url":"/2023/09/26/ob/k8s-configmap/","content":"\n背景在前面三节中已经讲到如何将我们的应用部署到 k8s 集群并提供对外访问的能力，x现在可以满足基本的应用开发需求了。\n现在我们需要更进一步，使用 k8s 提供的一些其他对象来标准化我的应用开发。首先就是 ConfigMap，从它的名字也可以看出这是用于管理配置的对象。\n\nConfigMap不管我们之前是做 Java、Go 还是 Python 开发都会使用到配置文件，而 ConfigMap 的作用可以将我们原本写在配置文件里的内容转存到 k8s 中，然后和我们的 Container 进行绑定。\n存储到环境变量绑定的第一种方式就是将配置直接写入到环境变量，这里我先定义一个 ConfigMap：\napiVersion: v1  kind: ConfigMap  metadata:    name: k8s-combat-configmap  data:    PG_URL: &quot;postgres://postgres:postgres@localhost:5432/postgres?sslmode=disable&quot;\n\n重点是 data 部分，存储的是一个 KV 结构的数据，这里存储的是一个数据库连接。\n\n需要注意，KV 的大小不能超过 1MB\n\n接着可以在容器定义中绑定这个 ConfigMap 的所有 KV 到容器的环境变量：\n# Define all the ConfigMap&#x27;s data as container environment variables envFrom:    - configMapRef:        name: k8s-combat-configmap\n\n我将 ConfigMap 的定义也放在了同一个 deployment 中，直接 apply:\n❯ k apply -f deployment/deployment.yamldeployment.apps/k8s-combat createdconfigmap/k8s-combat-configmap created\n\n此时 ConfigMap 也会被创建，我们可以使用\n❯ k get configmapNAME                   DATA   AGEk8s-combat-configmap   1      3m17s❯ k describe configmap k8s-combat-configmapData====PG_URL:----postgres://postgres:postgres@localhost:5432/postgres?sslmode=disable\n拿到刚才声明的配置信息。\n\n同时我在代码中也读取了这个环境变量：\nhttp.HandleFunc(&quot;/&quot;, func(w http.ResponseWriter, r *http.Request) &#123;     name, _ := os.Hostname()     url := os.Getenv(&quot;PG_URL&quot;)      fmt.Fprint(w, fmt.Sprintf(&quot;%s-%s&quot;, name, url))  &#125;)\n\n访问这个接口便能拿到这个环境变量：\nroot@k8s-combat-7b987bb496-pqt9s:/# curl http://127.0.0.1:8081k8s-combat-7b987bb496-pqt9s-postgres://postgres:postgres@localhost:5432/postgres?sslmode=disableroot@k8s-combat-7b987bb496-pqt9s:/# echo $PG_URLpostgres://postgres:postgres@localhost:5432/postgres?sslmode=disable\n\n存储到文件有些时候我们也需要将这些配置存储到一个文件中，比如在 Java 中可以使用 spring 读取，Go 也可以使用 configor 这些第三方库来读取，所有配置都在一个文件中也更方便维护。\n在 ConfigMap 中新增了一个 key:APP 存放了一个 yaml 格式的数据，然后在容器中使用 volumes 和 volumeMounts 将数据挂载到容器中的指定路径/go/bin/app.yaml\napply 之后我们可以在容器中查看这个文件是否存在：\nroot@k8s-combat-7b987bb496-pqt9s:/# cat /go/bin/app.yamlname: k8s-combatpulsar:  url: &quot;pulsar://localhost:6650&quot;  token: &quot;abc&quot;\n配置已经成功挂载到了这个路径，我们便可以在代码中读取这些数据。\nSecret可以看到 ConfigMap 中是明文存储数据的；\nk describe configmap k8s-combat-configmap\n可以直接查看。\n对一些敏感数据就不够用了，这时我们可以使用 Secret:\napiVersion: v1  kind: Secret  metadata:    name: k8s-combat-secret  type: Opaque  data:    PWD: YWJjCg==---env:    - name: PG_PWD      valueFrom:        secretKeyRef:          name: k8s-combat-secret          key: PWD\n\n这里我新增了一个 Secret 用于存储密码，并在 container 中也将这个 key 写入到环境变量中。\n❯ echo &#x27;abc&#x27; | base64YWJjCg==\nSecret 中的数据需要使用 base64 进行编码，所以我这里存储的是 abc.\napply 之后我们再查看这个 Secret 是不能直接查看原始数据的。\n❯ k describe secret k8s-combat-secretName:         k8s-combat-secretType:  OpaqueData====PWD:  4 bytes\n\nSecret 相比 ConfigMap 多了一个 Type 选项。\n我们现阶段在应用中用的最多的就是这里的 Opaque，其他的暂时还用不上。\n总结在实际开发过程中研发人员基本上是不会直接接触 ConfigMap，一般会给开发者在管理台提供维护配置的页面进行 CRUD。\n由于 ConfigMap 依赖于 k8s 与我们应用的语言无关，所以一些高级特性，比如实时更新就无法实现，每次修改后都得重启应用才能生效。\n类似于 Java 中常见的配置中心：Apollo,Nacos 使用上会有不小的区别，但这些是应用语言强绑定的，如果业务对这些配置中心特性有强烈需求的话也是可以使用的。\n但如果团队本身就是多语言研发，想要降低运维复杂度 ConfigMap 还是不二的选择。\n下一章节会更新大家都很感兴趣的服务网格 Istio，感兴趣的朋友多多点赞转发🙏🏻。\n本文的所有源码和资源文件在这里可以访问：https://github.com/crossoverJie/k8s-combat\n","categories":["k8s"],"tags":["ConfigMap"]},{"title":"在 kubernetes 环境中实现 gRPC 负载均衡","url":"/2023/10/16/ob/k8s-grpc-lb/","content":"\n前言前段时间写过一篇 gRPC 的入门文章，在最后还留了一个坑没有填：也就是 gRPC 的负载均衡问题，因为当时的业务请求量不算大，再加上公司没有对 Istio 这类服务网格比较熟悉的大牛，所以我们也就一直拖着没有解决，依然只是使用了 kubernetes 的 service 进行负载，好在也没有出什么问题。\n\n\n由于现在换了公司后也需要维护公司的服务网格服务，结合公司内部对 Istio 的使用现在终于不再停留在理论阶段了。\n所以也终有机会将这个坑填了。\ngRPC 负载均衡负载不均衡原理先来回顾下背景，为什么会有 gRPC 负载不均衡的问题。由于 gRPC 是基于 HTTP&#x2F;2 协议的，所以客户端和服务端会保持长链接，一旦链接建立成功后就会一直使用这个连接处理后续的请求。\n\n除非我们每次请求之后都新建一个连接，这显然是不合理的。\n所以要解决 gRPC 的负载均衡通常有两种方案：\n\n服务端负载均衡\n客户端负载均衡在 gRPC 这个场景服务端负载均衡不是很合适，所有的请求都需要经过一个负载均衡器，这样它就成为整个系统的瓶颈，所以更推荐使用客户端负载均衡。\n\n客户端负载均衡目前也有两种方案，最常见也是传统方案。这里以 Dubbo 的调用过程为例，调用的时候需要从服务注册中心获取到提供者的节点信息，然后在客户端本地根据一定的负载均衡算法得出一个节点然后发起请求。\n换成 gRPC 也是类似的，这里以 go-zero 负载均衡的原理为例：\ngRPC 官方库也提供了对应的负载均衡接口，但我们依然需要自己维护服务列表然后在客户端编写负载均衡算法，这里有个官方 demo:\nhttps://github.com/grpc/grpc-go/blob/87eb5b7502493f758e76c4d09430c0049a81a557/examples/features/load_balancing&#x2F;client&#x2F;main.go\n但切换到 kubernetes 环境中时再使用以上的方式就不够优雅了，因为我们使用 kubernetes 的目的就是不想再额外的维护这个客户端包，这部分能力最好是由 kubernetes 自己就能提供。\n但遗憾的是 kubernetes 提供的 service 只是基于 L4 的负载，所以我们每次请求的时候都只能将请求发往同一个 Provider 节点。\n测试这里我写了一个小程序来验证负载不均衡的示例：\n// Create gRPC servergo func() &#123;     var port = &quot;:50051&quot;     lis, err := net.Listen(&quot;tcp&quot;, port)     if err != nil &#123;        log.Fatalf(&quot;failed to listen: %v&quot;, err)     &#125;     s := grpc.NewServer()     pb.RegisterGreeterServer(s, &amp;server&#123;&#125;)     if err := s.Serve(lis); err != nil &#123;        log.Fatalf(&quot;failed to serve: %v&quot;, err)     &#125; else &#123;        log.Printf(&quot;served on %s \\n&quot;, port)     &#125;  &#125;()// server is used to implement helloworld.GreeterServer.  type server struct &#123;     pb.UnimplementedGreeterServer  &#125;    // SayHello implements helloworld.GreeterServer  func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) &#123;     log.Printf(&quot;Received: %v&quot;, in.GetName())     name, _ := os.Hostname()     // Return hostname of Server   return &amp;pb.HelloReply&#123;Message: fmt.Sprintf(&quot;hostname:%s, in:%s&quot;, name, in.Name)&#125;, nil  &#125;\n\n使用同一个 gRPC 连接发起一次 gRPC 请求，服务端会返回它的 hostname\nvar (     once sync.Once     c    pb.GreeterClient  )  http.HandleFunc(&quot;/grpc_client&quot;, func(w http.ResponseWriter, r *http.Request) &#123;     once.Do(func() &#123;        service := r.URL.Query().Get(&quot;name&quot;)        conn, err := grpc.Dial(fmt.Sprintf(&quot;%s:50051&quot;, service), grpc.WithInsecure(), grpc.WithBlock())        if err != nil &#123;           log.Fatalf(&quot;did not connect: %v&quot;, err)        &#125;        c = pb.NewGreeterClient(conn)     &#125;)       // Contact the server and print out its response.     name := &quot;world&quot;     ctx, cancel := context.WithTimeout(context.Background(), time.Second)     defer cancel()     g, err := c.SayHello(ctx, &amp;pb.HelloRequest&#123;Name: name&#125;)     if err != nil &#123;        log.Fatalf(&quot;could not greet: %v&quot;, err)     &#125;     fmt.Fprint(w, fmt.Sprintf(&quot;Greeting: %s&quot;, g.GetMessage()))  &#125;)\n\n创建一个 service 用于给 gRPC 提供域名：\napiVersion: v1  kind: Service  metadata:    name: native-tools-2spec:    selector:      app: native-tools-2  ports:      - name: http        port: 8081        targetPort: 8081      - name: grpc        port: 50051        targetPort: 50051\n同时将我们的 gRPC server 部署三个节点，再部署了一个客户端节点：\n❯ k get podNAME                                READY   STATUS    RESTARTSnative-tools-2-d6c454689-52wgd      1/1     Running   0              native-tools-2-d6c454689-67rx4      1/1     Running   0              native-tools-2-d6c454689-zpwxt      1/1     Running   0              native-tools-65c5bd87fc-2fsmc       2/2     Running   0             \n\n\n我们进入客户端节点执行多次 grpc 请求：\nk exec -it native-tools-65c5bd87fc-2fsmc bashGreeting: hostname:native-tools-2-d6c454689-zpwxt, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-d6c454689-zpwxt, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-d6c454689-zpwxt, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-d6c454689-zpwxt, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-d6c454689-zpwxt, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-d6c454689-zpwxt, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-d6c454689-zpwxt, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-d6c454689-zpwxt, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2\n会发现每次请求的都是同一个节点 native-tools-2-d6c454689-zpwxt，这也就证明了在 kubernetes 中直接使用 gRPC 负载是不均衡的，一旦连接建立后就只能将请求发往那个节点。\n使用 IstioIstio 可以拿来解决这个问题，我们换到一个注入了 Istio 的 namespace 下还是同样的 代码，同样的 service 资源进行测试。\n\n关于开启 namespace 的 Istio 注入会在后续更新，现在感兴趣的可以查看下官方文档：https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/\n\nGreeting: hostname:native-tools-2-5fbf46cf54-5m7dl, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-5fbf46cf54-xprjz, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-5fbf46cf54-5m7dl, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-5fbf46cf54-5m7dl, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-5fbf46cf54-xprjz, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-5fbf46cf54-xprjz, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-5fbf46cf54-5m7dl, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-5fbf46cf54-5m7dl, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-5fbf46cf54-nz8h5, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2Greeting: hostname:native-tools-2-5fbf46cf54-nz8h5, in:worldistio-proxy@n:/$ curl http://127.0.0.1:8081/grpc_client?name=native-tools-2\n可以发现同样的请求已经被负载到了多个 server 后端，这样我们就可以不再单独维护一个客户端 SDK 的情况下实现了负载均衡。\n原理其实本质上 Istio 也是客户端负载均衡的一种实现。以 Istio 的架构图为例：\n\n每一个 Pod 下会新增一个 Proxy 的 container，所有的流量入口和出口都会经过它。\n它会从控制平面 Istiod 中拿到服务的注册信息，也就是 kubernetes 中的 service。\n发生请求时由 proxy 容器中的 Envoy 进行最终的负载请求。\n\n可以在使用了 Istio 的 Pod 中查看到具体的容器：\n❯ k get pod native-tools-2-5fbf46cf54-5m7dl -n istio-test-2 -o json | jq &#x27;.spec.containers[].name&#x27;&quot;istio-proxy&quot;&quot;native-tools-2&quot;\n\n可以发现这里存在一个 istio-proxy 的容器，也就是我们常说的 sidecar，这样我们就可以把原本的 SDK 里的功能全部交给 Istio 去处理。\n总结当然 Istio 的功能远不止于此，比如：\n\n统一网关，处理东西、南北向流量。\n灰度发布\n流量控制\n接口粒度的超时配置\n自动重试等\n\n这次只是一个开胃菜，更多关于 Istio 的内容会在后续更新，比如会从如何在 kubernetes 集群中安装 Istio 讲起，带大家一步步使用好 Istio。\n本文相关源码：https://github.com/crossoverJie/k8s-combat\n参考链接：\n\nhttps://istio.io/latest/docs/setup/getting-started/\nhttps://segmentfault.com/a/1190000042295402\nhttps://go-zero.dev/docs/tutorials/service/governance/lb\n\n","categories":["OB","k8s"],"tags":["gRPC"]},{"title":"k8s实战-Istio 网关","url":"/2023/11/13/ob/k8s-istio03/","content":"在上一期 k8s-服务网格实战-配置 Mesh 中讲解了如何配置集群内的 Mesh 请求，Istio 同样也可以处理集群外部流量，也就是我们常见的网关。\n\n\n其实和之前讲到的k8s入门到实战-使用Ingress Ingress 作用类似，都是将内部服务暴露出去的方法。\n只是使用 Istio-gateway 会更加灵活。\n这里有一张功能对比图，可以明显的看出 Istio-gateway 支持的功能会更多，如果是一个中大型企业并且已经用上 Istio 后还是更推荐是有 Istio-gateway，使用同一个控制面就可以管理内外网流量。\n创建 Gateway开始之前首先是创建一个 Istio-Gateway 的资源：\napiVersion: networking.istio.io/v1alpha3  kind: Gateway  metadata:    name: istio-ingress-gateway    namespace: default  spec:    servers:      - port:          number: 80          name: http          protocol: HTTP        hosts:          - &#x27;www.service1.io&#x27;    selector:      app: istio-ingressgateway #与现有的 gateway 关联      istio: ingressgateway\n\n其中的 selector 选择器中匹配的 label 与我们安装 Istio 时候自带的 gateway 关联即可。\n# 查看 gateway 的 labelk get pod -n istio-systemNAME                                    READY   STATUSistio-ingressgateway-649f75b6b9-klljw   1/1     Runningk describe pod istio-ingressgateway-649f75b6b9-klljw -n istio-system |grep LabelsLabels:           app=istio-ingressgateway\n\n\n\n这个 Gateway 在我们第一次安装 Istio 的时候就会安装这个组件。\n\n\n这个配置的含义是网关会代理通过 www.service1.io 这个域名访问的所有请求。\n之后需要使用刚才的 gateway 与我们的服务的 service 进行绑定，这时就需要使用到 VirtualService：\napiVersion: networking.istio.io/v1alpha3  kind: VirtualService  metadata:    name: k8s-combat-istio-http-vs  spec:    gateways:      - istio-ingress-gateway # 绑定刚才创建的 gateway 名称   hosts:      - www.service1.iohttp:- name: default    route:      - destination:          host: k8s-combat-service-istio-mesh  #service 名称        port:            number: 8081          subset: v1\n这个和我们之前讲到的 Mesh 内部流量时所使用到的 VirtualService 配置是一样的。\n这里的含义也是通过 www.service1.io 以及 istio-ingress-gateway 网关的流量会进入这个虚拟服务，但所有的请求都会进入 subset: v1 这个分组。\n这个的分组信息在上一节可以查询到：\napiVersion: networking.istio.io/v1alpha3  kind: DestinationRule  metadata:    name: k8s-combat-service-ds  spec:    host: k8s-combat-service-istio-mesh    subsets:      - name: v1        labels:          app: k8s-combat-service-v1      - name: v2        labels:          app: k8s-combat-service-v2\n\n之后我们访问这个域名即可拿到响应，同时我们打开 k8s-combat-service-istio-mesh service 的 Pod 查看日志，会发现所有的请求都进入了 v1, 如果不需要这个限制条件，将 subset: v1 删除即可。\ncurl  http://www.service1.io/ping\n\n\n本地需要配置下 host: 127.0.0.1 www.service1.io\n\n\n还有一点，我们需要拿到 gateway 的外部IP，才能将 IP 和刚才的域名www.service1.io 进行绑定（host，或者是域名管理台）。\n如果使用的是 docker-desktop 自带的 kubernetes 集群时候直接使用 127.0.0.1 即可，默认就会绑定上。\n如果使用的是 minikube 安装的，那需要使用 minikube tunnel 手动为 service 为LoadBalancer 类型的绑定一个本地 IP，具体可以参考文档：https://minikube.sigs.k8s.io/docs/tasks/loadbalancer\n\n如果是生产环境使用，云服务厂商会自动绑定一个外网 IP。\n\n原理\n这个的访问请求的流程和之前讲到的 kubernetes Ingress 流程是类似的，只是 gateway 是通过 VirtualService 来路由的 service，同时在这个 VirtualService 中可以自定义许多的路由规则。\n总结服务网格 Istio 基本上讲完了，后续还有关于 Telemetry 相关的 trace、log、metrics 会在运维章节更新，也会和 Istio 有所关联。感兴趣的朋友可以持续关注。\n本文的所有源码在这里可以访问：https://github.com/crossoverJie/k8s-combat\n#Blog #Istio \n","categories":["k8s"],"tags":["Istio"]},{"title":"在 kubernetes 环境下如何采集日志","url":"/2024/04/21/ob/k8s-log-collect/","content":"当我们没有使用云原生方案部署应用时采用的日志方案往往是 ELK 技术栈。\n这套技术方案比较成熟，稳定性也很高，所以几乎成为了当时的标配。\n可是随着我们使用 kubernetes 步入云原生的时代后， kubernetes 把以往的操作系统上的许多底层都屏蔽，再由他提供了一些标准接口。\n同时在 kubernetes 中的日志来源也比传统虚拟机多，比如可能有容器、kubernetes 自身的事件、日志等。\n\n\n我们的日志采集方案也得与时俱进，kubernetes 的官方博客有介绍提供一下几种方案：\n节点采集\n第一种方案是在节点中采集日志，我们知道 kubernetes 是分为 master 调度节点以及 worker 工作节点；我们的应用都是运行在 worker 节点中的。\n\n在 kubernetes 环境中更推荐使用标准的 stdout&#x2F;stderr 作为日志输出，这样 kubernetes 更方便做统一处理。\n\n\n以我们的 docker 运行时为例，默认情况下我们的标准输入文件会写入到 /var/log 目录中。\n\n如上图所示：我们可以在 kubernetes 的每一个 worker 节点中部署一个 DaemonSet 类型的采集器（filebeat 等），再由他去采集该节点下 /var/log 的日志，最终由他将日志采集后发往日志处理的后端，比如 elasticsearch 等组件中。\n这种方案的好处是资源占用较低，往往是有多少个 worker 节点就可以部署多少个采集器。\n而且和业务的耦合度低，业务和采集器不管谁进行重启或升级互相都不会产生影响。\n但缺点也比较明显，整个节点的日志采集瓶颈都在这个采集器这里了，如果某些 worker 节点的 Pod 数量不均衡，或者是本身日志产生也不平均时就会出现明显的负债不平衡。\n而且也无法针对某些日志高峰场景进行调优（毕竟所有的 Pod 都是使用的一个日志采集器）。\n所以节点级的日志采集更适用与该 worker 节点负债较低的时候使用，也更容易维护。\nSidecar 代理模式第二种相对于第一种可以理解为由集中式的日志采集分散到各个应用 Pod 中自行采集。\n需要为每一个业务 Pod 挂载一个边车（sidecar）进行日志采集，由于边车和业务 Pod 共享的是一个存储，所以可以很方便的读取到日志。\n由于它是和应用挂载在一起的，所以资源占用自然会比节点采集更多，同理耦合度也增加了，采集组件的升级可能还会影响的业务 Pod。\n但同样的带来好处就是可以针对单个 Pod 更精细的控制采集方案。\n比如对于一些日志写入频繁的应用，可以将 filebeat 的配置提高，甚至还可以将这种高负载的日志单独写入一个 elasticsearch 中，这样可以与普通负载的日志进行资源隔离。\n这个方案更适用与集群规模较大的场景，需要做一些精细化配置。\n\n我们其实也是采用的也是这个方案，不过具体细节稍有不同。\n我们没有直接使用标准输入和输出，原因如下：\n日志格式没法统一，导致最终查询的时候无法做一些标准化的限制（比如我们要求每个日志都需要带业务 id、traceId 等，查询时候有这些业务指标就很容易沉淀一些标准的查询语句。）\n最终我们还是采用了 Java 的老朋友，logback 配置了自己的日志格式，所有的应用都会根据这个模版进行日志输出。\n同时利用日志框架的批量写入、缓冲等特性还更容易进行日志的性能调优。（只使用标准输出时对应用来说是黑盒。）\n最终我们的技术方案是：\n直接写入还有一种方案是直接写入，这个其实和 kubernetes 本身就没有太多关系了。\n由业务自己调用 elasticsearch 或者其他的存储组件的 API 进行写入，这种通常适用于对性能要求较高的场景，略过了中间的采集步骤，直接写入存储端。\n这个我在 VictoriaLogs：一款超低占用的 ElasticSearch 替代方案中介绍过，我需要在 broker 的拦截器中埋点消息信息，从而可以生成一个消息🆔的链路信息。\n\n因为需要拦截消息的发送、消费的各个阶段，加上并发压力较高，所以对日志的写入性能要求还是蛮高的。\n因此就需要在拦截器中直接对写入到日志存储。\n\n这里考虑到我这里的但一场景，以及对资源的消耗，最终选取了 victoriaLog 这个日志存储。\n\n而在发送日志的时候也得用了高性能的日志发生框架，这里选取了aliyun-log-java-producer然后做了一些定制。\n这个库支持以下一些功能：\n\n高性能：批量发送、多线程等\n自动重试\n异步非阻塞\n资源控制（可以对内存、数量进行控制）\n\n因为这是为阿里云日志服务的一个组件，代码里硬编码了只能写入阿里的日志服务。\n所以拿来稍加改造后，现在可以支持自定义发送到任意后端，只需要在初始化时自定义实现发送回调接口即可：\nProducerConfig producerConfig = new ProducerConfig();producerConfig.setSenderArgs(new Object[]&#123;vlogUrl, client&#125;);producerConfig.setSender((batch, args) -&gt; &#123;    StringBuilder body = new StringBuilder();    for (String s : batch.getLogItemsString()) &#123;        body.append(&quot;&#123;\\&quot;create\\&quot;:&#123;&#125;&#125;&quot;);        body.append(&quot;\\n&quot;);        body.append(s);        body.append(&quot;\\n&quot;);    &#125;    RequestBody requestBody =            RequestBody.create(MediaType.parse(&quot;application/json&quot;), body.toString());    Request request =            new Request.Builder()                    .url(String.format(&quot;%s/insert/elasticsearch/_bulk&quot;, args[0]))                    .post(requestBody)                    .build();    OkHttpClient okHttpClient = (OkHttpClient) args[1];    try (Response response = okHttpClient.newCall(request).execute()) &#123;        if (response.isSuccessful()) &#123;        &#125; else &#123;            log.error(&quot;Request failed with error code: &quot; + response);        &#125;    &#125; catch (IOException e) &#123;        log.error(&quot;Send vlogs failed&quot;, e);        throw e;    &#125;&#125;);logProducer = new LogProducer(producerConfig);\n\n\n考虑到这个库只是对阿里云日志服务的一个组件，加上代码已经很久没维护了，所以就没有将这部分代码提交到社区，感兴趣的评论区留言。\n\n日志安全日志是一个非常基础但又很敏感的功能，首先是编码规范上要避免打印一些敏感信息；比如身份证、密码等；同时对日志的访问也要最好权限控制。\n在我们内部的研效平台中，对于日志、监控等功能都是和应用权限挂钩的。\n简单来说就是关闭了统一查询 ES 的入口，只在应用层级提供查询，类似于：\n\n\n图来自于 orbit 产品。\n\nOpenTelemetry当然讲到日志目前自然也逃不过 OpenTelemetry，作为当前云原生可观测性的标准也提供了对应的日志组件。\nOpenTelemetry 也定义了结构化的日志格式：\n&#123;&quot;resourceLogs&quot;:[&#123;&quot;resource&quot;:&#123;&quot;attributes&quot;:[&#123;&quot;key&quot;:&quot;resource-attr&quot;,&quot;value&quot;:&#123;&quot;stringValue&quot;:&quot;resource-attr-val-1&quot;&#125;&#125;]&#125;,&quot;scopeLogs&quot;:[&#123;&quot;scope&quot;:&#123;&#125;,&quot;logRecords&quot;:[&#123;&quot;timeUnixNano&quot;:&quot;1581452773000000789&quot;,&quot;severityNumber&quot;:9,&quot;severityText&quot;:&quot;Info&quot;&quot;body&quot;:&#123;&quot;stringValue&quot;:&quot;This is a log message&quot;&#125;,&quot;attributes&quot;:[&#123;&quot;key&quot;:&quot;app&quot;,&quot;value&quot;:&#123;&quot;stringValue&quot;:&quot;server&quot;&#125;&#125;,&#123;&quot;key&quot;:&quot;instance_num&quot;,&quot;value&quot;:&#123;&quot;intValue&quot;:&quot;1&quot;&#125;&#125;],&quot;droppedAttributesCount&quot;:1,&quot;traceId&quot;:&quot;08040201000000000000000000000000&quot;,&quot;spanId&quot;:&quot;0102040800000000&quot;&#125;,&#123;&quot;timeUnixNano&quot;:&quot;1581452773000000789&quot;,&quot;severityNumber&quot;:9,&quot;severityText&quot;:&quot;Info&quot;,&quot;body&quot;:&#123;&quot;stringValue&quot;:&quot;something happened&quot;&#125;,&quot;attributes&quot;:[&#123;&quot;key&quot;:&quot;customer&quot;,&quot;value&quot;:&#123;&quot;stringValue&quot;:&quot;acme&quot;&#125;&#125;,&#123;&quot;key&quot;:&quot;env&quot;,&quot;value&quot;:&#123;&quot;stringValue&quot;:&quot;dev&quot;&#125;&#125;],&quot;droppedAttributesCount&quot;:1,&quot;traceId&quot;:&quot;&quot;,&quot;spanId&quot;:&quot;&quot;&#125;]&#125;]&#125;]&#125;\n我们可以配置 otel.logs.exporter=otlp (default) 可以将日志输出到 oetl-collector 中，再由他输出到后端存储中。\n虽然这样 otel-collectoer 就成为瓶颈了，但我们也可以部署多个副本来降低压力。\n同时也可以在应用中指定不同的 endpoint(otel.exporter.otlp.endpoint=http://127.0.0.1:4317) 来区分日志的 collector，与其他类型的 collector 做到资源隔离。\n不过目前社区关于日志的实践还比较少，而且由于版本 1.0 版本 release 的时间也不算长，稳定性和之前的 filebeat 相比还得需要时间检验。\n总结理想情况下，我们需要将可观测性的三个重要组件都关联起来才能更好的排查定位问题。\n比如当收到监控系统通过指标变化发出的报警时，可以通过链路追踪定位具体是哪个系统触发的问题。\n之后通过 traceID 定位到具体的日志，再通过日志的上下文列出更多日志信息，这样整个链条就可以串联起来，可以极大的提高效率。\n参考链接：\n\nhttps://github.com/aliyun/aliyun-log-java-producer\nhttps://kubernetes.io/docs/concepts/cluster-administration/logging/\nhttps://coding.net/help/docs/orbit/env/logs-event/intro.html\nhttps://opentelemetry.io/docs/concepts/signals/logs/\n\n","categories":["OB"],"tags":["日志","kubernetes"]},{"title":"k8s 云原生应用如何接入监控","url":"/2025/01/02/ob/k8s-monitor-pod/","content":"前段时间有朋友问我如何在 kubernetes 里搭建监控系统，恰好在公司也在维护内部的可观测平台，正好借这个机会整理下目前常见的自建监控方案。\n一个完整的监控系统通常包含以下的内容：\n\n指标暴露：将系统内部需要关注的指标暴露出去\n指标采集：收集并存储暴露出来的指标\n指标展示：以各种图表展示和分析收集到的数据\n监控告警：当某些关键指标在一定时间周期内出现异常时，可以及时通知相关人员\n\n\n对于 k8s 的监控通常分为两个部分：\n\nk8s 自带的系统组建\n业务 Pod 暴露出来的监控指标\n\n\n系统组建对于 kubernetes 系统组建可以由 cAdvisor 提供监控能力，默认情况下这个功能是开箱即用的，我们只需要在 Prometheus 中配置相关的任务抓取即可：\n- job_name: nodeScrape/monitoring/cadvisor-scrape/0  scrape_interval: 30s  scrape_timeout: 15s  scheme: https  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  tls_config:    insecure_skip_verify: true  relabel_configs:  - source_labels: [__meta_kubernetes_node_name]    target_label: node  - action: replace    source_labels: [__meta_kubernetes_node_name]    separator: ;    target_label: __address__    regex: (.*)    replacement: kubernetes.default.svc:443  - action: replace    source_labels: [__meta_kubernetes_node_name]    separator: ;    target_label: __metrics_path__    regex: (.+)    replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor  kubernetes_sd_configs:  - role: node\n\n这样的话就可以监控 k8s 的内存、CPU 之类的数据。\n具体提供了哪些指标可以参考这里：https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md#prometheus-container-metrics\n也可以找一些常用的监控面板:https://grafana.com/grafana/dashboards/13077-kubernetes-monitoring-dashboard-kubelet-cadvisor-node-exporter/\nk8s 不但提供了 cAdvisor 的数据，还有其他类似的 endpoint: /metrics/resource &amp; /metrics/probes \n具体暴露出来的指标可以参考官方文档：https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/\n业务指标对于业务应用来说第一步也是需要将自己的指标暴露出去，如果是 Java 的话可以使用 Prometheus 提供的库：\n&lt;!-- The client --&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.prometheus&lt;/groupId&gt;    &lt;artifactId&gt;simpleclient&lt;/artifactId&gt;    &lt;version&gt;0.16.0&lt;/version&gt;  &lt;/dependency&gt;  &lt;!-- Hotspot JVM metrics--&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.prometheus&lt;/groupId&gt;    &lt;artifactId&gt;simpleclient_hotspot&lt;/artifactId&gt;    &lt;version&gt;0.16.0&lt;/version&gt;  &lt;/dependency&gt;\n 它会自动将 JVM 相关的指标暴露出去，如果是在 VM 中的应用，那只需要简单的配置下 static_configs 就可以抓取指标了：\n  scrape_configs:  - job_name: &#x27;springboot&#x27;  scrape_interval: 10s  static_configs:  - targets: [&#x27;localhost:8080&#x27;] # Spring Boot ip+port\n\n但在 kubernetes 中这个 IP 是不固定的，每次重建应用的时候都会发生变化，所以我们需要一种服务发现机制来动态的找到 Pod 的 IP。\n- job_name: &#x27;kubernetes-pods&#x27;  kubernetes_sd_configs:  - role: pod  relabel_configs:  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]    action: keep    regex: true  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]    action: replace    target_label: __metrics_path__    regex: (.+)  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]    action: replace    regex: ([^:]+)(?::\\d+)?;(\\d+)    replacement: $1:$2    target_label: __address__  - action: labelmap    regex: __meta_kubernetes_pod_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_pod_label_component]    action: replace    target_label: job  - source_labels: [__meta_kubernetes_pod_name]    action: replace    target_label: kubernetes_pod_name\nPrometheus 提供了一个 kubernetes_sd_configs 的服务发现机制，他会在 kubernetes 中查找 Pod 中是否有配置以下的注解：\ntemplate:  metadata:    annotations:      prometheus.io/path: /metrics      prometheus.io/port: &quot;8082&quot;      prometheus.io/scrape: &quot;true&quot;\n\n都配置成功后我们便可以在 Prometheus 的管理后台查看到具体的服务信息：状态是 UP 则表明抓取数据成功，这样我们就可以在 Prometheus 中查询到数据了。\n\nPrometheus 除了支持 k8s 的服务发现之外还支持各种各样的服务发现，比如你已经使用了  Consul 或者是 Erueka 作为注册中心，也可以直接配置他们的地址然后进行服务发现，这样应用信息发生变化时 Prometheus 也能及时感知到。\n当然 docker/http/docker 等都是支持的，可以按需选择。\nOpenTelemetry随着这两年可观测性标准的完善，许多厂商都在往 OpenTelemetry 上进行迁移，接入 OpenTelemetry 与直接使用 Prometheus 最大的不同是：\n\n不再由 Prometheus 主动抓取应用指标，而是由应用给 OpenTelemetry-Collector 推送标准化的可观测数据（包含日志、trace、指标），再由它远程写入 Prometheus 这类时序数据库中。\n\n整体流程图如下：\n对应用的最大的区别就是可以不再使用刚才提到 Prometheus 依赖，而是只需要挂载一个 javaagent 即可：\njava -javaagent:opentelemetry-javaagent-2.4.0-SNAPSHOT.jar \\  -Dotel.traces.exporter=otlp \\  -Dotel.metrics.exporter=otlp \\  -Dotel.logs.exporter=none \\  -Dotel.service.name=java-demo \\  -Dotel.exporter.otlp.protocol=grpc \\  -Dotel.propagators=tracecontext,baggage \\  -Dotel.exporter.otlp.endpoint=http://127.0.0.1:5317 -jar target/demo-0.0.1-SNAPSHOT.jar\n\n而其中会新增的一个 OpenTelemetry-Collector项目，由它将收到的指标数据转发给 Prometheus，所以在它的配置里会配置 Prometheus 的地址：\nexporters:  otlphttp/prometheus:    endpoint: http://prometheus:9292/api/v1/otlp    tls:      insecure: true\n\n\n之前也写过两篇 OpenTelemetry 和监控相关的文章，可以一起阅读体验更佳：\n\n从 Prometheus 到 OpenTelemetry：指标监控的演进与实践\nOpenTelemetry 实战：从零实现应用指标监控\n\n总结关于 Prometheus 的安装可以参考官方的 operator 或者是 helm：https://github.com/prometheus-operator/kube-prometheus\n当然如果不想使用 Prometheus 也推荐使用 VictoriaMetrics，是一个完全兼容 Prometheus 但是资源占用更少的时序数据库。\n参考链接：\n\nhttps://kubernetes.io/docs/concepts/cluster-administration/system-metrics/\nhttps://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md\nhttps://prometheus.io/docs/prometheus/latest/configuration/configuration/\n\n","categories":["OB","kubernetes"],"tags":["kubernetes"]},{"title":"k8s入门到实战-应用探针","url":"/2023/11/25/ob/k8s-probe/","content":"\n今天进入 kubernetes 的运维部分（并不是运维 kubernetes，而是运维应用），其实日常我们大部分使用 kubernetes 的功能就是以往运维的工作，现在云原生将运维和研发关系变得更紧密了。\n\n\n今天主要讲解 Probe 探针相关的功能，探针最实用的功能就是可以控制应用优雅上线。\n就绪探针举个例子，当我们的 service 关联了多个 Pod 的时候，其中一个 Pod 正在重启但还没达到可以对外提供服务的状态，这时候如果有流量进入。\n那这个请求肯定就会出现异常，从而导致问题，所以我们需要一个和 kubernetes 沟通的渠道，告诉它什么时候可以将流量放进来。比如如图所示的情况，红色 Pod 在未就绪的时候就不会有流量。\n使用就绪探针就可以达到类似的效果：\nreadinessProbe:    failureThreshold: 3    httpGet:      path: /ping      port: 8081      scheme: HTTP    periodSeconds: 3    successThreshold: 1    timeoutSeconds: 1\n这个配置也很直接：\n\n配置一个 HTTP 的 ping 接口\n每三秒检测一次\n失败 3 次则认为检测失败\n成功一次就认为检测成功\n\n\n但没有配置就绪探针时，一旦 Pod 的 Endpoint 加入到 service 中(Pod 进入 Running 状态)，请求就有可能被转发过来，所以配置就绪探针是非常有必要的。\n\n启动探针而启动探针往往是和就绪探针搭配干活的，如果我们一个 Pod 启动时间过长，比如超过上面配置的失败检测次数，此时 Pod 就会被 kubernetes 重启，这样可能会进入无限重启的循环。\n所以启动探针可以先检测一次是否已经启动，直到启动成功后才会做后续的检测。\nstartupProbe:    failureThreshold: 30    httpGet:      path: /ping      port: 8081      scheme: HTTP    periodSeconds: 5    successThreshold: 1    timeoutSeconds: 1\n\n\n我这里两个检测接口是同一个，具体得根据自己是实际业务进行配置；比如应用端口启动之后并不代表业务已经就绪了，可能某些基础数据还没加载到内存中，这个时候就需要自己写其他的接口来配置就绪探针了。\n\n\n所有关于探针相关的日志都可以在 Pod 的事件中查看，比如如果一个应用在启动的过程中频繁重启，那就可以看看是不是某个探针检测失败了。\n存活探针存活探针往往是用于保证应用高可用的，虽然 kubernetes 可以在 Pod 退出后自动重启，比如 Pod OOM；但应用假死他是检测不出来的。\n为了保证这种情况下 Pod 也能被自动重启，就可以配合存活探针使用：\nlivenessProbe:    failureThreshold: 3    httpGet:      path: /ping      port: 8081      scheme: HTTP    periodSeconds: 3    successThreshold: 1    timeoutSeconds: 1\n\n一旦接口响应失败，kubernetes 就会尝试重启。\n\n总结\n以上探针配置最好是可以在研效平台可视化配置，这样维护起来也比较简单。\n探针是维护应用健康的必要手段，强烈推荐大家都进行配置。\n本文的所有源码在这里可以访问：https://github.com/crossoverJie/k8s-combat#Blog \n","categories":["OB"]},{"title":"在 kubernetes 环境下如何优雅扩缩容 Pulsar","url":"/2024/03/27/ob/k8s-pulsar-scale/","content":"背景在整个大环境的降本增效的熏陶下，我们也不得不做好应对方案。\n根据对线上流量、存储以及系统资源的占用，发现我们的 Pulsar 集群有许多的冗余，所以考虑进行缩容从而减少资源浪费，最终也能省一些费用。\n不过在缩容之前很有必要先聊聊扩容，Pulsar 一开始就是存算分离的架构（更多关于 Pulsar 架构的内容本文不做过多介绍，感兴趣的可以自行搜索），天然就非常适合 kubernetes 环境，也可以利用 kubernetes 的能力进行快速扩容。\n\n扩容Pulsar 的扩容相对比较简单，在 kubernetes 环境下只需要修改副本即可。\nBroker当我们的 broker 层出现瓶颈时（比如 CPU、内存负载较高、GC 频繁时）可以考虑扩容。\n\n计算层都扩容了，也需要根据流量计算下存储层是否够用。\n\n如果我们使用的是 helm 安装的 Pulsar 集群，那只需要修改对于的副本数即可。\nbroker:    configuration    component: broker    replicaCount: 3-&gt;5\n\n当我们将副本数从 3 增加到 5 之后 kubernetes 会自动拉起新增的两个 Pod，之后我们啥也不需要做了。\nPulsar 的负载均衡器会自动感知到新增两个 broker 的加入，从而帮我们将一些负载高的节点的流量迁移到新增的节点中。\nBookkeeper在介绍 bookkeeper 扩容前先简单介绍些 Bookkeeper 的一些基本概念。\n\nEnsemble size (E)：当前 Bookkeeper 集群的节点数量\nWrite quorum size (QW)：一条消息需要写入到几个 Bookkeeper 节点中\nACK quorum size (QA)：有多少个 Bookkeeper 节点 ACK 之后表示写入成功\n\n对应到我们在 broker.conf 中的配置如下：\nmanagedLedgerDefaultEnsembleSize: &quot;2&quot;  managedLedgerDefaultWriteQuorum: &quot;2&quot;  managedLedgerDefaultAckQuorum: &quot;2&quot;\n这个三个参数表示一条消息需要同时写入两个 Bookkeeper 节点，同时都返回 ACK 之后才能表示当前消息写入成功。\n从这个配置也可以看出，Bookkeeper 是多副本写入模型，适当的降低 QW 和 QA 的数量可以提高写入吞吐率。\n大部分场景下 Bookkeeper 有三个节点然后 E&#x2F;QW&#x2F;QA 都配置为 2 就可以满足消息多副本写入了。\n\n多副本可以保证当某个节点宕机后，这个节点的消息在其他节点依然有存放，消息读取不会出现问题。\n\n那什么情况下需要扩容 Bookkeeper 了，当然如果单个 Bookkeeper 的负载较高也是可以扩容的。\n但我们当时扩容 Bookkeeper 的场景是想利用 Pulsar 的资源隔离功能。\n因为有部分业务的消息量明显比高于其他的 topic，这样会导致某个 Broker 的负载较高，同时也可能影响到其他正常的 topic。\n最好的方式就将这部分数据用单独的 broker 和 Bookkeeper 来承载，从而实现硬件资源的隔离。\n这样的需求如果使用其他消息队列往往不太好实现，到后来可能就会部署多个集群来实现隔离，但这样也会增加运维的复杂度。\n好在 Pulsar 天然就支持资源隔离，只需要一个集群就可以实现不同 namespace 的流量隔离。\n此时就可以额外扩容几个 Bookkeeper 节点用于特定的 namespace 使用。\n从上图可以看到：我们可以将 broker 和 Bookkeeper 分别进行分组，然后再配置对应的 namespace，这样就能实现资源隔离了。\n\n更多关于资源隔离的细节本文就不过多赘述了。\n\n铺垫了这么多，其实 Bookkeeper 的扩容也蛮简单的：\nbookkeeper:  component: bookie  metadata:    resources:    # requests:    # memory: 4Gi    # cpu: 2  replicaCount: 3-&gt;5\n\n和 broker 扩容类似，提高副本数量后，Pulsar 的元数据中心会感知到新的 Bookkeeper 节点加入，从而更新 broker 中的节点数据，这样就会根据我们配置的隔离策略分配流量。\n缩容其实本文的重点在于缩容，特别是 Bookkeeper 的缩容，这部分内容我在互联网上很少看到有人提及。\nBrokerBroker 的缩容相对简单，因为存算分离的特点：broker 作为计算层是无状态的，并不承载任何的数据。\n\n其实是承载数据的，只是 Pulsar 会自动迁移数据，从而体感上觉得是无状态的。\n\n只是当一个 broker 下线后，它上面所绑定的 topic 会自动转移到其他在线的 broker 中。\n这个过程会导致连接了这个 broker 的 client 触发重连，从而短暂的影响业务。\n\n正因为 broker 的下线会导致 topic 的归属发生转移，所以在下线前最好是先通过监控面板观察需要下线的 broker topic 是否过多，如果过多则可以先手动 unload 一些数据，尽量避免一次性大批量的数据转移。\n\n\n\n观察各个broker 的 topic 数量\n\nBookkeeper而 Bookkeeper 的缩容则没那么容易了，由于它是作为存储层，本身是有状态的，下线后节点上存储的数据是需要迁移到其他的 Bookkeeper 节点中的。\n不然就无法满足之前提到的 Write quorum size (QW) 要求；因此缩容还有一个潜在条件需要满足：\n缩容后的 Bookkeeper 节点数量需要大于broker 中的配置：\nmanagedLedgerDefaultEnsembleSize: &quot;2&quot;  managedLedgerDefaultWriteQuorum: &quot;2&quot;  managedLedgerDefaultAckQuorum: &quot;2&quot;\n\n不然写入会失败，整个集群将变得不可用。\nPulsar 提供了两种 Bookkeeper 的下线方案：\n不需要迁移数据其实两种方案主要区别在于是否需要迁移数据，第一种比较简单，就是不迁移数据的方案。\n首先需要将 Bookkeeper 设置为 read-only 状态，此时该节点将不会接受写请求，直到这个 Bookkeeper 上的数据全部过期被回收后，我们就可以手动下线该节点。\n使用 forceReadOnlyBookie=true 可以强制将 Bookkeeper 设置为只读。\n但这个方案存在几个问题：\n\n下线时间不确定，如果该 Bookkeeper 上存储的数据生命周期较长，则无法预估什么时候可以下线该节点。\n该配置修改后需要重启才能生效，在 kubernetes 环境中这些配置都是写在了 configmap 中，一旦刷新后所有节点都会读取到该配置，无法针对某一个节点生效；所以可能会出现将不该下线的节点设置为了只读状态。\n\n但该方案的好处是不需要迁移数据，人工介入的流程少，同样也就减少了出错的可能。\n比较适合于用虚拟机部署的集群。\n迁移数据第二种就是需要迁移数据的方案，更适用于 kubernetes 环境。\n迁移原理先来看看迁移的原理：\n\n当 bookkeeper 停机后，AutoRecovery Auditor 会检测到 zookeeper 节点/ledger/available 发生变化，将下线节点的 ledger 信息写入到 zookeeper 的 /ledgers/underreplicated 节点中。\nAutoRecovery ReplicationWorker 会检测 /ledgers/underreplicated节点信息，然后轮训这些 ledger 信息从其他在线的 BK 中复制数据到没有该数据的节点，保证 QW 数量不变。\n每复制一条数据后都会删除 /ledgers/underreplicated 节点信息。\n所有 /ledgers/underreplicated 被删除后说明迁移任务完成。\n\n\n执行 bin/bookkeeper shell decommissionbookie 下线命令：\n会等待 /ledgers/underreplicated 全部删除\n 然后删除 zookeeper 中的元数据\n元数据删除后 bookkeeper 才是真正下线成功，此时 broker 才会感知到 Bookkeeper 下线。\n\n\n\nAutoRecovery 是 Bookkeeper 提供的一个自动恢复程序，他会在后台检测是否有数据需要迁移。\n\n简单来说就是当某个Bookkeeper 停机后，它上面所存储的 ledgerID 会被写入到元数据中心，此时会有一个单独的线程来扫描这些需要迁移的数据，最终将这些数据写入到其他在线的 Bookkeeper 节点。\n\nBookkeeper 中的一些关键代码：\n下线步骤下面来看具体的下线流程：\n\n副本数-1\nbin/bookkeeper shell listunderreplicated 检测有多少 ledger 需要被迁移\n\n\n执行远程下线元数据\nnohup bin/bookkeeper shell decommissionbookie -bookieid bkid:3181 &gt; bk.log 2&gt;&amp;1 &amp;\n这个命令会一直后台运行等待数据迁移完成，比较耗时\n\n\n查看下线节点是否已被剔除\nbin/bookkeeper shell listbookies -a\n\n\n循环第一步\n\n第一步是检测一些现在有多少数据需要迁移：bin/bookkeeper shell listunderreplicated 命令查看需要被迁移的 ledger 数据也是来自于 /ledgers/underreplicated节点\n\n正常情况下是 0\n\n第二步的命令会等待数据迁移完成后从 zookeeper 中删除节点信息，这个进程退出后表示下线成功。\n\n\n这个命令最好是后台执行，并输出日志到专门的文件，因为周期较长，很有可能终端会话已经超时了。\n\n我们登录 zookeeper 可以看到需要迁移的 ledger 数据：\nbin/pulsar zookeeper-shell -server pulsar-zookeeper:2181get /ledgers/underreplication/ledgers/0000/0000/0000/0002/urL0000000002replica: &quot;pulsar-test-2-bookie-0.pulsar-test-2-bookie.pulsar-test-2.svc.cluster.local:3181&quot;ctime: 1708507296519\n\nunderreplication 的节点路径中存放了 ledgerId，通过 ledgerId 计算路径：\n注意事项下线过程中我们可以查看 nohup bin/bookkeeper shell decommissionbookie -bookieid bkid:3181 &gt; bk.log 2&gt;&amp;1 &amp;这个命令写入的日志来确认迁移的进度，日志中会打印当前还有多少数量的 ledger 没有迁移。\n同时需要观察 zookeeper、Bookkeeper 的资源占用情况。\n因为迁移过程中写入大量数据到 zookeeper 节点，同时迁移数时也会有大量流量写入 Bookkeeper。\n不要让迁移过程影响到了正常的业务使用。\n根据我的迁移经验来看，通常 2w 的ledger 数据需要 2～3 小时不等的时间，具体情况还得根据你的集群来确认。\n回滚方案当然万一迁移比较耗时，或者影响了业务使用，所以还是要有一个回滚方案：\n这里有一个大的前提：只要 BK 节点元数据、PVC（也就是磁盘中的数据） 没有被删除就可以进行回滚。\n所以只要上述的 decommissionbookie 命令没有完全执行完毕，我们就可以手动 kill 该进程，然后恢复副本数据。\n这样恢复的 Bookkeeper 节点依然可以提供服务，同时数据也还存在；只是浪费了一些 autorecovery 的资源。\n最后当 bookkeeper 成功下线后，我们需要删除 PVC，不然如果今后需要扩容的时候是无法启动 bookkeeper 的，因为在启动过程中会判断挂载的磁盘是否有数据。\n总结总的来说 Pulsar 的扩缩容还是非常简单的，只是对于有状态节点的数据迁移稍微复杂一些，但只要跟着流程走就不会有什么问题。\n参考链接：\n\nhttps://pulsar.apache.org/docs/next/administration-isolation/\nhttps://bookkeeper.apache.org/docs/4.13.0/admin/decomission\nhttps://bookkeeper.apache.org/docs/4.13.0/admin/autorecovery\n\n#Blog #Pulsar \n","categories":["Pulsar","OB"],"tags":["Pulsar"]},{"title":"k8s 常见面试题 01","url":"/2023/08/17/ob/k8s-question-01/","content":"\n前段时间在这个视频中分享了 https://github.com/bregman-arie/devops-exercises 这个知识仓库。\n \n\n这次继续分享里面的内容，本次主要以 k8s 相关的问题为主。\n\n\nk8s 是什么，为什么企业选择使用它k8s 是一个开源应用，给用户提供了管理、部署、扩展容器的能力，以下几个例子更容易理解：\n\n你可以将容器运行在不同的机器或节点中，并且可以将一些变化同步给这些容器，简单来说我们只需要编写 yaml 文件，告诉 k8s 我的预期是什么，其中同步变化的过程全部都交给 k8s 去完成。\n其实就是我们常说的声明式 API\n\n\n第二个特点刚才已经提到了，它可以帮我们一键管理多个容器，同步所有的变更。\n可以根据当前的负载调整应用的副本数，负载高就新创建几个应用实例，低就降低几个，这个可以手动或自动完成。\n\n什么时候使用或者不使用 k8s\n如果主要还是使用物理机这种低级别的基础设施的话，不太建议使用 k8s，这种情况通常是比较传统的业务，没有必要使用 k8s。\n第二种情况是如果是小团队，或者容器规模较小时也不建议使用，除非你想使用 k8s 的滚动发布和自扩容能力，\n不过这些功能运维自己写工具也能实现。\n\n\n\nk8s 有哪些特性\n是自我修复，k8s 对容器有着健康检测，比如使用启动探针、存活探针等，或者是容器 OOM 后也会重启应用尝试修复。\n自带负载均衡，使用 service 可以将流量自动负载到后续 Pod 中，如果 Pod 提供的是 http 服务这个够用了，但如果是 grpc 这样的长链接，就需要使用 istio 这类服务网格，他可以识别出协议类型，从而做到请求级别的负载均衡。\nOperator 自动运维能力：k8s 可以根据应用的运行情况自动调整当前集群的 Pod 数量、存储等，拿 Pulsar 举例，当流量激增后自动新增 broker，磁盘不足时自动扩容等。\n滚动更新能力：当我们发版或者是回滚版本的时候，k8s 会等待新的容器启动之后才会将流量切回来，同时逐步停止老的实例。\n水平扩展能力：可以灵活的新增或者是减少副本的数量，当然也可以自动控制。\n数据加密：使用 secret 可以保存一些敏感的配置或者文件。\n\nk8s 有着哪些对象这个就是考察我们对 k8s 是否是熟悉了，常用的有：\n\nPod\nService\nReplicationController\nDaemonSet\nnamespace\nConfigMap这个其实知道没有太多作用，主要还是得知道在不同场景如何使用不同的组件。\n\n哪些字段是必须的这个问题我也觉得意义不大，只要写过 yaml 就会知道了，metadata, kind, apiVersion\napiVersion: apps/v1  kind: Deployment  metadata:    labels:      app: app  name: app\n\nkubectl 是什么其实就是一个 k8s 的 命令行客户端。\n当你部署应用的时候哪些对象用的比较多\n第一个肯定是 deployment，这应该是最常见的部署方式。\nservice: 可以将流量负载到 Pod 中。\nIngress: 如果需要从集群外访问 Pod 就得需要 Ingress 然后 配合域名访问。\n\n为什么没有 k get containers 这个命令这个问题主要是看对 Pod 的理解，因为在 k8s 中 Pod 就是最小的单位了，如果想要访问容器可以在 Pod 中访问。\n我们可以加上 -c 参数进入具体的容器。\nkubectl exec -it app -c istio-proxy\n\n你认为使用使用 k8s 的最佳实践是什么这个主要是看日常使用时有没有遇到什么坑了：\n\n第一个就是要验证 yaml 内容是否正确，这个确实很重要，一旦执行错了后果很严重，比如使用 helm 的时候最好岂容 dry-run 和 debug，先看看生成的 yaml 是否是预期想要的。\nhelm upgrade app –dry-run –debug\n\n\n第二个限制资源的使用，比如 CPU 和 内存，这个也很重要，如果不设置一旦应用出现 bug 可能导致整个 k8s 集群都受到影响。\n为 Pod，deployment 指定标签，用于分组。\n\n# 资源限制resources:    limits:      cpu: 200m      memory: 200Mi    requests:      cpu: 100m      memory: 100Mi\n\n\n参考来源：https://github.com/bregman-arie/devops-exercises/blob/master/topics/kubernetes/README.md#kubernetes-101\n\n后续部分内容也有出视频版，强烈建议大家关注我的 B 站或者是视频号：\n#Blog #K8s \n","categories":["Interview"],"tags":["k8s"]},{"title":"如何优雅重启 kubernetes 的 Pod","url":"/2023/10/19/ob/k8s-restart-pod/","content":"\n最近在升级服务网格 Istio，升级后有个必要的流程就是需要重启数据面的所有的 Pod，也就是业务的 Pod，这样才能将这些 Pod 的 sidecar 更新为新版本。\n\n\n方案 1因为我们不同环境的 Pod 数不少，不可能手动一个个重启；之前也做过类似的操作：\nkubectl delete --all pods --namespace=dev\n这样可以一键将 dev 这个命名空间下的 Pod 删掉，kubernetes 之后会自动将这些 Pod 重启，保证和应用的可用性。\n但这有个大问题是对 kubernetes 的调度压力较大，一般一个 namespace 下少说也是几百个 Pod，全部需要重新调度启动对 kubernetes 的负载会很高，稍有不慎就会有严重的后果。\n所以当时我的第一版方案是遍历所有的  deployment，删除一个 Pod 后休眠 5 分钟再删下一个，伪代码如下：\ndeployments, err := clientSet.AppsV1().Deployments(ns).List(ctx, metav1.ListOptions&#123;&#125;)  if err != nil &#123;      return err  &#125;for _, deployment := range deployments.Items &#123;\tpodList, err := clientSet.CoreV1().Pods(ns).List(ctx, metav1.ListOptions&#123;  \t    LabelSelector: fmt.Sprintf(&quot;app=%s&quot;, deployment.Name),  \t&#125;)\terr = clientSet.CoreV1().Pods(pod.Namespace).Delete(ctx, pod.Name, metav1.DeleteOptions&#123;&#125;)  \tif err != nil &#123;  \t    return err  \t&#125;  \tlog.Printf(&quot;    Pod %s rebuild success.\\n&quot;, pod.Name)\ttime.Sleep(time.Minute * 5)\t&#125;\n\n\n存在的问题这个方案确实是简单粗暴，但在测试的时候就发现了问题。\n当某些业务只有一个 Pod 的时候，直接删掉之后这个业务就挂了，没有多余的副本可以提供服务了。\n这肯定是不能接受的。\n甚至还有删除之后没有重启成功的：\n\n长期没有重启导致镜像缓存没有了，甚至镜像已经被删除了，这种根本就没法启动成功。\n也有一些 Pod 有 Init-Container 会在启动的时候做一些事情，如果失败了也是没法启动成功的。总之就是有多种情况导致一个 Pod 无法正常启动，这在线上就会直接导致生产问题，所以方案一肯定是不能用的。\n\n方案二为此我就准备了方案二：\n\n\n先将副本数+1，这是会新增一个 Pod，也会使用最新的 sidecar 镜像。\n等待新建的 Pod 重启成功。\n重启成功后删除原有的 Pod。\n再将副本数还原为之前的数量。\n\n这样可以将原有的 Pod 平滑的重启，同时如果新的 Pod 启动失败也不会继续重启其他 Deployment 的 Pod，老的 Pod 也是一直保留的，对服务本身没有任何影响。\n存在的问题看起来是没有什么问题的，就是实现起来比较麻烦，流程很繁琐，这里我贴了部分核心代码：\nfunc RebuildDeploymentV2(ctx context.Context, clientSet kubernetes.Interface, ns string) error &#123;\tdeployments, err := clientSet.AppsV1().Deployments(ns).List(ctx, metav1.ListOptions&#123;&#125;)\tif err != nil &#123;\t\treturn err\t&#125;\tfor _, deployment := range deployments.Items &#123;\t\t// Print each Deployment\t\tlog.Printf(&quot;Ready deployment: %s\\n&quot;, deployment.Name)\t\toriginPodList, err := clientSet.CoreV1().Pods(ns).List(ctx, metav1.ListOptions&#123;\t\t\tLabelSelector: fmt.Sprintf(&quot;app=%s&quot;, deployment.Name),\t\t&#125;)\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t\t// Check if there are any Pods\t\tif len(originPodList.Items) == 0 &#123;\t\t\tlog.Printf(&quot;\tNo pod in %s\\n&quot;, deployment.Name)\t\t\tcontinue\t\t&#125;\t\t// Skip Pods that have already been upgraded\t\tupdateSkip := false\t\tfor _, container := range pod.Spec.Containers &#123;\t\t\tif container.Name == &quot;istio-proxy&quot; &amp;&amp; container.Image == &quot;proxyv2:1.x.x&quot; &#123;\t\t\t\tlog.Printf(&quot;  Pod: %s Container: %s has already upgrade, skip\\n&quot;, pod.Name, container.Name)\t\t\t\tupdateSkip = true\t\t\t&#125;\t\t&#125;\t\tif updateSkip &#123;\t\t\tcontinue\t\t&#125;\t\t// Scale the Deployment, create a new pod.\t\tscale, err := clientSet.AppsV1().Deployments(ns).GetScale(ctx, deployment.Name, metav1.GetOptions&#123;&#125;)\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t\tscale.Spec.Replicas = scale.Spec.Replicas + 1\t\t_, err = clientSet.AppsV1().Deployments(ns).UpdateScale(ctx, deployment.Name, scale, metav1.UpdateOptions&#123;&#125;)\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t\t// Wait for pods to be scaled\t\tfor &#123;\t\t\tpodList, err := clientSet.CoreV1().Pods(ns).List(ctx, metav1.ListOptions&#123;\t\t\t\tLabelSelector: fmt.Sprintf(&quot;app=%s&quot;, deployment.Name),\t\t\t&#125;)\t\t\tif err != nil &#123;\t\t\t\tlog.Fatal(err)\t\t\t&#125;\t\t\tif len(podList.Items) != int(scale.Spec.Replicas) &#123;\t\t\t\ttime.Sleep(time.Second * 10)\t\t\t&#125; else &#123;\t\t\t\tbreak\t\t\t&#125;\t\t&#125;\t\t// Wait for pods to be running\t\tfor &#123;\t\t\tpodList, err := clientSet.CoreV1().Pods(ns).List(ctx, metav1.ListOptions&#123;\t\t\t\tLabelSelector: fmt.Sprintf(&quot;app=%s&quot;, deployment.Name),\t\t\t&#125;)\t\t\tif err != nil &#123;\t\t\t\tlog.Fatal(err)\t\t\t&#125;\t\t\tisPending := false\t\t\tfor _, item := range podList.Items &#123;\t\t\t\tif item.Status.Phase != v1.PodRunning &#123;\t\t\t\t\tlog.Printf(&quot;Deployment: %s Pod: %s Not Running Status: %s\\n&quot;, deployment.Name, item.Name, item.Status.Phase)\t\t\t\t\tisPending = true\t\t\t\t&#125;\t\t\t&#125;\t\t\tif isPending == true &#123;\t\t\t\ttime.Sleep(time.Second * 10)\t\t\t&#125; else &#123;\t\t\t\tbreak\t\t\t&#125;\t\t&#125;\t\t// Remove origin pod\t\tfor _, pod := range originPodList.Items &#123;\t\t\terr = clientSet.CoreV1().Pods(ns).Delete(context.Background(), pod.Name, metav1.DeleteOptions&#123;&#125;)\t\t\tif err != nil &#123;\t\t\t\treturn err\t\t\t&#125;\t\t\tlog.Printf(&quot;\tRemove origin %s success.\\n&quot;, pod.Name)\t\t&#125;\t\t// Recover scale\t\tnewScale, err := clientSet.AppsV1().Deployments(ns).GetScale(ctx, deployment.Name, metav1.GetOptions&#123;&#125;)\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t\tnewScale.Spec.Replicas = newScale.Spec.Replicas - 1\t\tnewScale.ResourceVersion = &quot;&quot;\t\tnewScale.UID = &quot;&quot;\t\t_, err = clientSet.AppsV1().Deployments(ns).UpdateScale(ctx, deployment.Name, newScale, metav1.UpdateOptions&#123;&#125;)\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t\tlog.Printf(&quot;\tDepoloyment %s rebuild success.\\n&quot;, deployment.Name)\t\tlog.Println()\t&#125;\treturn nil&#125;\n\n看的出来代码是比较多的。\n最终方案有没有更简单的方法呢，当我把上述的方案和领导沟通后他人都傻了，这也太复杂了：kubectl 不是有一个直接滚动重启的命令吗。\n❯ k rollout -hManage the rollout of one or many resources.Available Commands:  history       View rollout history  pause         Mark the provided resource as paused  restart       Restart a resource  resume        Resume a paused resource  status        Show the status of the rollout  undo          Undo a previous rollout\n\nkubectl rollout restart deployment/abc使用这个命令可以将 abc 这个 deployment 进行滚动更新，这个更新操作发生在  kubernetes 的服务端，执行的步骤和方案二差不多，只是 kubernetes 实现的比我的更加严谨。\n后来我在查看 Istio 的官方升级指南中也是提到了这个命令：\n\n所以还是得好好看官方文档\n\n整合 kubectl既然有现成的了，那就将这个命令整合到我的脚本里即可，再遍历 namespace 下的 deployment 的时候循环调用就可以了。\n但这个 rollout 命令在 kubernetes 的 client-go 的 SDK 中是没有这个 API 的。\n所以我只有参考 kubectl 的源码，将这部分功能复制过来；不过好在可以直接依赖 kubect 到我的项目里。\nrequire (      k8s.io/api v0.28.2      k8s.io/apimachinery v0.28.2      k8s.io/cli-runtime v0.28.2      k8s.io/client-go v0.28.2      k8s.io/klog/v2 v2.100.1      k8s.io/kubectl v0.28.2  )\n\n源码里使用到的 RestartOptions 结构体是公共访问的，所以我就参考它源码魔改了一下：\nfunc TestRollOutRestart(t *testing.T) &#123;      kubeConfigFlags := defaultConfigFlags()      streams, _, _, _ := genericiooptions.NewTestIOStreams()      ns := &quot;dev&quot;      kubeConfigFlags.Namespace = &amp;ns      matchVersionKubeConfigFlags := cmdutil.NewMatchVersionFlags(kubeConfigFlags)      f := cmdutil.NewFactory(matchVersionKubeConfigFlags)      deploymentName := &quot;deployment/abc&quot;      r := &amp;rollout.RestartOptions&#123;         PrintFlags: genericclioptions.NewPrintFlags(&quot;restarted&quot;).WithTypeSetter(scheme.Scheme),         Resources:  []string&#123;deploymentName&#125;,         IOStreams:  streams,      &#125;      err := r.Complete(f, nil, []string&#123;deploymentName&#125;)      if err != nil &#123;         log.Fatal(err)      &#125;      err = r.RunRestart()      if err != nil &#123;         log.Fatal(err)      &#125;  &#125;\n\n最终在几次 debug 后终于可以运行了，只需要将这部分逻辑移动到循环里，加上 sleep 便可以有规律的重启 Pod 了。\n参考链接：\n\nhttps://istio.io/latest/docs/setup/upgrade/canary/#data-plane\nhttps://github.com/kubernetes/kubectl/blob/master/pkg/cmd/rollout/rollout_restart.go\n\n#Blog #K8s \n","categories":["OB","k8s"],"tags":["client-go"]},{"title":"k8s入门到实战-滚动更新与优雅停机","url":"/2023/11/29/ob/k8s-rollout/","content":"\n当我们在生产环境发布应用时，必须要考虑到当前系统还有用户正在使用的情况，所以尽量需要做到不停机发版。\n\n\n所以在发布过程中理论上之前的 v1 版本依然存在，必须得等待 v2 版本启动成功后再删除历史的 v1 版本。\n\n如果 v2 版本启动失败 v1 版本不会做任何操作，依然能对外提供服务。\n\n滚动更新\n这是我们预期中的发布流程，要在 kubernetes 使用该功能也非常简单，只需要在 spec 下配置相关策略即可：\nspec:  strategy:    rollingUpdate:      maxSurge: 25%      maxUnavailable: 25%    type: RollingUpdate\n这个配置的含义是：\n\n使用滚动更新，当然还有 Recreate 用于删除旧版本的 Pod，我们基本不会用这个策略。\nmaxSurge：滚动更新过程中可以最多超过预期 Pod 数量的百分比，当然也可以填整数。\nmaxUnavailable：滚动更新过程中最大不可用 Pod 数量超过预期的百分比。\n\n这样一旦我们更新了 Pod 的镜像时，kubernetes 就会先创建一个新版本的 Pod 等待他启动成功后再逐步更新剩下的 Pod。\n优雅停机滚动升级过程中不可避免的又会碰到一个优雅停机的问题，毕竟是需要停掉老的 Pod。\n这时我们需要注意两种情况：\n\n停机过程中，已经进入 Pod 的请求需要执行完毕才能退出。\n停机之后不能再有请求路由到已经停机的 Pod\n\n第一个问题如果我们使用的是 Go，可以使用一个钩子来监听  kubernetes 发出的退出信号：\nquit := make(chan os.Signal)  signal.Notify(quit, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT, syscall.SIGPIPE)  go func() &#123;      &lt;-quit      log.Printf(&quot;quit signal received, exit \\n&quot;)      os.Exit(0)  &#125;()\n在这里执行对应的资源释放。\n如果使用的是 spring boot 也有对应的配置：\nserver: \tshutdown: &quot;graceful&quot;spring: \tlifecycle: \t\ttimeout-per-shutdown-phase: &quot;20s&quot;\t\n当应用收到退出信号后，spring boot 将不会再接收新的请求，并等待现有的请求处理完毕。\n但 kubernetes 也不会无限等待应用将 Pod 将任务执行完毕，我们可以在 Pod 中配置\nterminationGracePeriodSeconds: 30\n来定义需要等待多长时间，这里是超过 30s 之后就会强行 kill Pod。\n\n具体值大家可以根据实际情况配置\n\n\nspec:  containers:  - name: example-container    image: example-image    lifecycle:      preStop:        exec:          command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 10&quot;]\n同时我们也可以配置 preStop 做一个 sleep 来确保 kubernetes 将准备删除的 Pod 在 Iptable 中已经更新了之后再删除 Pod。\n这样可以避免第二种情况：已经删除的 Pod 依然还有请求路由过来。具体可以参考 spring boot 文档：https://docs.spring.io/spring-boot/docs/2.4.4/reference/htmlsingle/#cloud-deployment-kubernetes-container-lifecycle\n回滚回滚其实也可以看作是升级的一种，只是升级到了历史版本，在 kubernetes 中回滚应用非常简单。\n# 回滚到上一个版本 k rollout undo deployment/abc# 回滚到指定版本k rollout undo daemonset/abc --to-revision=3\n同时 kubernetes 也能保证是滚动回滚的。\n优雅重启在之前的 如何优雅重启 kubernetes 的 Pod 那篇文章中写过，如果想要优雅重启 Pod 也可以使用 rollout 命令，它也也可以保证是滚动重启。\nk rollout restart deployment/nginx\n\n使用 kubernetes 的滚动更新确实要比我们以往的传统运维简单许多，就几个命令的事情之前得写一些复杂的运维脚本才能实现。\n本文的所有源码在这里可以访问：https://github.com/crossoverJie/k8s-combat#Blog #K8s \n","categories":["OB"]},{"title":"k8s入门到实战--跨服务调用","url":"/2023/09/05/ob/k8s-service/","content":"\n背景在做传统业务开发的时候，当我们的服务提供方有多个实例时，往往我们需要将对方的服务列表保存在本地，然后采用一定的算法进行调用；当服务提供方的列表变化时还得及时通知调用方。\n\n\nstudent:     url:        - 192.168.1.1:8081        - 192.168.1.2:8081\n\n这样自然是对双方都带来不少的负担，所以后续推出的服务调用框架都会想办法解决这个问题。\n以 spring cloud 为例：\n服务提供方会向一个服务注册中心注册自己的服务（名称、IP等信息），客户端每次调用的时候会向服务注册中心获取一个节点信息，然后发起调用。\n但当我们切换到 k8s 后，这些基础设施都交给了 k8s 处理了，所以 k8s 自然得有一个组件来解决服务注册和调用的问题。\n也就是我们今天重点介绍的 service。\nservice在介绍 service 之前我先调整了源码：\nfunc main() &#123;     http.HandleFunc(&quot;/ping&quot;, func(w http.ResponseWriter, r *http.Request) &#123;        name, _ := os.Hostname()        log.Printf(&quot;%s ping&quot;, name)        fmt.Fprint(w, &quot;pong&quot;)     &#125;)     http.HandleFunc(&quot;/service&quot;, func(w http.ResponseWriter, r *http.Request) &#123;        resp, err := http.Get(&quot;http://k8s-combat-service:8081/ping&quot;)        if err != nil &#123;           log.Println(err)           fmt.Fprint(w, err)           return        &#125;        fmt.Fprint(w, resp.Status)     &#125;)       http.ListenAndServe(&quot;:8081&quot;, nil)  &#125;\n新增了一个 /service 的接口，这个接口会通过 service 的方式调用服务提供者的服务，然后重新打包。\nmake docker\n\n同时也新增了一个 deployment-service.yaml:\napiVersion: apps/v1  kind: Deployment  metadata:    labels:      app: k8s-combat-service # 通过标签选择关联    name: k8s-combat-service  spec:    replicas: 1    selector:      matchLabels:        app: k8s-combat-service    template:      metadata:        labels:          app: k8s-combat-service      spec:        containers:          - name: k8s-combat-service            image: crossoverjie/k8s-combat:v1            imagePullPolicy: Always            resources:              limits:                cpu: &quot;1&quot;                memory: 100Mi              requests:                cpu: &quot;0.1&quot;                memory: 10Mi  ---  apiVersion: v1  kind: Service  metadata:    name: k8s-combat-service  spec:    selector:      app: k8s-combat-service # 通过标签选择关联    type: ClusterIP    ports:      - port: 8081        # 本 Service 的端口        targetPort: 8081  # 容器端口        name: app\n\n使用相同的镜像部署一个新的 deployment，名称为 k8s-combat-service，重点是新增了一个kind: Service 的对象。\n这个就是用于声明 service 的组件，在这个组件中也是使用 selector 标签和 deployment 进行了关联。\n也就是说这个 service 用于服务于名称等于 k8s-combat-service 的 deployment。\n下面的两个端口也很好理解，一个是代理的端口， 另一个是  service 自身提供出去的端口。\n至于 type: ClusterIP 是用于声明不同类型的 service，除此之外的类型还有：\n\nNodePort\nLoadBalancer\nExternalName等类型，默认是 ClusterIP，现在不用纠结这几种类型的作用，后续我们在讲到 Ingress 的时候会具体介绍。\n\n负载测试我们先分别将这两个 deployment 部署好：\nk apply -f deployment/deployment.yamlk apply -f deployment/deployment-service.yaml❯ k get podNAME                                  READY   STATUS    RESTARTS   AGEk8s-combat-7867bfb596-67p5m           1/1     Running   0          3h22mk8s-combat-service-5b77f59bf7-zpqwt   1/1     Running   0          3h22m\n\n由于我新增了一个 /service 的接口，用于在 k8s-combat 中通过 service 调用 k8s-combat-service 的接口。\nresp, err := http.Get(&quot;http://k8s-combat-service:8081/ping&quot;)\n\n其中 k8s-combat-service 服务的域名就是他的服务名称。\n\n如果是跨 namespace 调用时，需要指定一个完整名称，在后续的章节会演示。\n\n我们整个的调用流程如下：\n相信大家也看得出来相对于 spring cloud 这类微服务框架提供的客户端负载方式，service 是一种服务端负载，有点类似于 Nginx 的反向代理。\n为了更直观的验证这个流程，此时我将 k8s-combat-service 的副本数增加到 2：\nspec:    replicas: 2\n\n只需要再次执行：\n❯ k apply -f deployment/deployment-service.yamldeployment.apps/k8s-combat-service configuredservice/k8s-combat-service unchanged\n\n\n\n不管我们对 deployment 的做了什么变更，都只需要 apply 这个 yaml  文件即可， k8s 会自动将当前的 deployment 调整为我们预期的状态（比如这里的副本数量增加为 2）；这也就是 k8s 中常说的声明式 API。\n\n可以看到此时 k8s-combat-service 的副本数已经变为两个了。如果我们此时查看这个 service 的描述时：\n❯ k describe svc k8s-combat-service |grep EndpointsEndpoints:         192.168.130.133:8081,192.168.130.29:8081\n会发现它已经代理了这两个 Pod 的 IP。\n此时我进入了 k8s-combat-7867bfb596-67p5m 的容器：\nk exec -it k8s-combat-7867bfb596-67p5m bashcurl http://127.0.0.1:8081/service\n\n并执行两次 /service 接口，发现请求会轮训进入 k8s-combat-service 的代理的 IP 中。\n由于 k8s service 是基于 TCP/UDP 的四层负载，所以在 http1.1  中是可以做到请求级的负载均衡，但如果是类似于 gRPC 这类长链接就无法做到请求级的负载均衡。\n换句话说 service 只支持连接级别的负载。\n如果要支持 gRPC，就得使用 Istio 这类服务网格，相关内容会在后续章节详解。\n总结总的来说 k8s service 提供了简易的服务注册发现和负载均衡功能，当我们只提供 http 服务时是完全够用的。\n相关的源码和 yaml 资源文件都存在这里：https://github.com/crossoverJie/k8s-combat\n","categories":["k8s"],"tags":["Go"]},{"title":"新手如何快速参与开源项目","url":"/2023/08/05/ob/novice-contribute-open-source/","content":"\n前言开源这件事情在软件开发领域一直是一个高频话题，我们工作中不管是使用到的工具还是第三方库都离不开开源的支持。\n近期由于工作的原因，我需要经常和 Apache Pulsar 社区沟通，同时也会将日常碰到的问题反馈给社区，包括一些 bug ，一些我能修的也是顺带就提了一些 PR。\n\n\n之前或多或少我也参与过其他的开源社区，但和现在的还是有些许的不同：\n\n以前我更多的是个人开源项目，偶尔也会有其他开发者向我的仓库贡献代码。\n也参与过其他个人作者或者是社区性质的项目，但流程上没有那么正规或者是标准。\n\n简单来说就是以前就是小打小闹，Pulsar 毕竟是 Apache 社区的顶级项目，参与的整个流程要求也会比较复杂，当然学到的知识也会更多。\n\n这半年时间大大小小提了十几个 PR ，也逐渐捋清楚了一些上手的方法和套路，所以如果你也想参与开源，但苦于不知道如何入门，看完后希望对你有所帮助。\n为什么参与开源首先还是来聊聊参与开源的好处，了解之后也许会让你有路转粉。\n首先最明显的一点就是让你对贡献的这个项目更加深入的了解，我们常常都在面试的时候被问到对 XX 框架的熟悉程度，哪怕你在简历里写的天花乱坠也没有是这个项目  Contributor 更具有说服力。\n其次是沟通交流能力也会得到锻炼，开源社区往往都是以 github issue/PR，或者是 Mailing List 的方式沟通交流，这样的沟通方式和我们常用的微信、QQ 这类及时通讯工具有着本质的区别。\n往往需要我们有了冷静的思考加上清晰的描述才会将自己的观点发布出去，这样不自觉的就会养成自己的总结能力，这个能力对于内容创意内容工作者来说非常重要。\n还有一个更明显的好处就是对个人的能力背书，大家常说的 show me the code，而 GitHub 就是最好的方式。\n当你是某个知名开源项目的 Contributor 甚至是 Committer/PMC 就已经足够证明自己的能力了。\n如何参与如何参与呢，其实也很简单，不外乎有以下几种方式（由易到难）：\n\n一些 typo 类的修复。\n回答社区中用户的问题。\n使用过程中遇到 bug，直接反馈，有兴趣的话最好是自己能修复。\n修复现有 issue 列表中未解决的 bug。\n软件不具备自己需要的功能时提交 feature 提案并实现。\n\n不管是哪种方式我的建议是在准备贡献之前都应该先看看官方提供的贡献指南，通常在官网就能查看。\n\n即便是最简单的修复 typo，因为越是专业的项目每个 PR 的合并都是严谨的，提前了解后可以避免犯一些基本错误从而影响积极性。\n\n这里我以 Pulsar 为例：官网有着详细的贡献指南，包括环境搭建、代码约定、PR/git commit 语义等各种规范。\n这里我重点强调 PR 的语义，一个好的 PR 规范更容易引起社区成员的注意，毕竟我们每一次提交都需要 Committer 的同意才能合并。\n还是以 Pulsar 为例，在提交 PR 前一定得先看看这里的规范要求，不然很可能第一步就会吃瘪。\n可能遇到的问题下面讲讲贡献过程中可能会碰到的问题。\n在上面讲到的难度排序中将修复个人 issue 排在了其他 issue 之前了，这是因为往往对自己提交的 bug 更熟悉，而社区其他人反馈的问题大概率会被老手认领。\n加上自己也不熟悉，可能在自己研究复现的过程中就把自己劝退了。\n认领 issue这里还有个小技巧，当我们准备修复一个不是自己提交的 issue 时，最好是在评论区让 Committer 将这个任务分配给你，这样社区成员就不会做重复工作了。\n类似于这样。\n同时我们在查找可以修复的 issue 时也要注意这个 issue 有没有被认领以及是否有 PR 关联。\n\n有时候 issue 并没有被指定但也有相关 PR 在处理该问题了，这时我们就可以过滤掉这个 issue。\nhelp want也可以找找带有  help want 标签的 issue，这类问题往往会相对简单，修复起来也更容易。\n社区反馈较慢还有一个比较常见的问题是自己提交的 issue 或者是 PR 迟迟没有人处理。\n我们可以先看看这个 issue 对应的代码最近主要是哪些人在维护，这个在 IDE 中配合 GitToolBox 插件就很容易看出来。\n后面的 ID 往往是 PR 号，我们可以通过这个 PR 找到对应的作者，然后尝试在 issue 评论区艾特对方。\n如果依然没有回复，那我们也可以给开发组发送邮件。如果还是没有回复，比如我这个😂\n那也还有一个办法，就是尝试在社交媒体（GitHub 首页、技术群）上找到 Committer 的微信，直接私聊的方式让对方帮忙推进。\n当然也有一些项目长期没有维护了，这种 PR 要做好心里准备，很有可能对方不会理你；这点在国内某个企业的开源项目中比较常见。\n总结总的来说想要做好开源得有耐心和长期坚持，同时给自己带来的好处也是物超所值的，Apache 这类专业的社区我也才参与了半年，后续也会长期坚持下去，也希望哪天可以积累到成为 Committer 后再和大家分享。\n#Pulsar #OpenSource\n","categories":["OpenSource"],"tags":["Pulsar"]},{"title":"如何对 kubernetes 应用做 e2e(端到端) 测试","url":"/2024/05/05/ob/operator-e2e-test/","content":"背景最近在给 opentelemetry-operator提交一个标签选择器的功能时，因为当时修改的函数是私有的，无法添加单测函数，所以社区建议我补充一个 e2e test.\n\n因为在当前的版本下，只要给 deployment 打上了 instrumentation.opentelemetry.io/inject-java: &quot;true&quot; 这类注解就会给该 deployment 注入 agent。但没办法指定不同的 agent 版本（或者不同的环境变量），所以希望可以新增一个选择器，同时可以针对不同的 deployment 维护不同版本的 Instrumentation(是用于控制需要注入 deployment 的资源)；这样就可以灵活控制了。\n\n\n\n\n在这之前我其实也很少做 kubernetes 的 operator 开发，对如何做 kubernetes 的 e2e 测试也比较陌生，好在社区提供了详细的贡献文档。\n\n安装简单来说需要两个关键组件：\n\nkind: kubernetes in docker，是可以在本地利用 docker 启动一个 kubernetes 集群的工具，通常用于在本地进行开发、测试关于 kubernetes 相关的功能。\n安装 kind 的前提是本地已经安装好了 docker。\n\n\nchainsaw: 一个 e2e 测试框架，提供了声明式的方式定义测试用例，也有着丰富断言功能。\n\n他们的安装都很简单，只要本地安装好了 golang，直接使用 go install 即可：\ngo install sigs.k8s.io/kind@v0.22.0go install github.com/kyverno/chainsaw@latest\n\nkind 使用在开始前还是先预习下 kind 的基本使用。\n安装好 kind 之后，使用 create cluster 命令可以在本地创建一个 kubernetes 集群。\nkind create cluster -hCreates a local Kubernetes cluster using Docker container &#x27;nodes&#x27;Usage:  kind create cluster [flags]\n之后只需要等待集群安装成功即可，它会在我们的 cat ~/.kube/config 文件中追加刚才新建集群的连接信息。\nk config get-contextsk config use-context xxx\n\n这样就可以使用这两个命令来查看和切换不同的集群了，虽说是一个本地模拟的 kubernetes 集群，但他的核心功能和一个标准的集群没有什么区别。\nkind delete clusters --all\n使用完成之后可以使用这个命令将所有集群都删除掉。\n准备集群数据在 opentelemetry-operator 中有给我们准备好一个 make 命令: make prepare-e2e ；使用它会帮我们将 operator 的测试环境初始化好。\n大概分为以下几步：\n\n安装 chainsaw\n修改 controller 的镜像为我们本地构建的镜像名称\n本地 docker 镜像打包\n安装 cert-manager\n安装 Operator 需要的 CRD\n部署 Operator deployment\n等待 Operator 启动成功\n\n不过这里的安装过程可能会遇到问题（本质上都是我们的网络问题）：这种情况可以想办法（科学上网）手动先把镜像拉取到本地，然后 kubernetes 就会从本地仓库获取到这个镜像。\ne2e test通常我们需要将同一类的测试功能放到一个文件夹里，比如这样：默认情况下 Chainsaw 会查找目录下名为 chainsaw-test.yaml 作为引导文件。\napiVersion: chainsaw.kyverno.io/v1alpha1  kind: Test  metadata:    creationTimestamp: null    name: instrumentation-java  spec:    steps:    - name: step-00      try:       - command:          entrypoint: kubectl          args:          - annotate          - namespace          - $&#123;NAMESPACE&#125;          - openshift.io/sa.scc.uid-range=1000/1000          - --overwrite      - command:          entrypoint: kubectl          args:          - annotate          - namespace          - $&#123;NAMESPACE&#125;          - openshift.io/sa.scc.supplemental-groups=3000/3000          - --overwrite      - apply:          file: 00-install-collector.yaml      - apply:          file: 00-install-instrumentation-select.yaml    - name: step-01      try:      - apply:          file: 01-install-app-select.yaml      - assert:          file: 01-assert*.yaml      catch:        - podLogs:            selector: app=my-java-select\n\ntests/e2e-instrumentation/instrumentation-select├── 00-install-collector.yaml├── 00-install-instrumentation-select.yaml├── 01-assert-select.yaml├── 01-assert-without-select.yaml├── 01-install-app-select.yaml└── chainsaw-test.yaml\n以我这里的这份文件为例，在其中定义了几个步骤：\n\n初始化环境信息，包含创建 namespace\n安装我们测试所需要的资源\n00-install-collector.yaml：这里主要是安装一个 OpenTelemetry 的 collector\n00-install-instrumentation-select.yaml：安装 Instrumentation 注入资源\n01-install-app-select.yaml：应用一个我们需要测试的 deployment 资源\n01-assert*.yaml：最后对最终生成的 yaml 资源与 assert*.yaml 的进行断言匹配，只有匹配成功后才能测试成功。\n\n\n\n\n这里的测试目的主要是完成一个完整的 Java 应用的 deployment 注入 OpenTelemetry 的 agent 过程还有一些与 OpenTelemetry 相关的环境变量。\n\n以 00-install-instrumentation-select.yaml 文件为例：\napiVersion: opentelemetry.io/v1alpha1  kind: Instrumentation  metadata:    name: java-select  spec:    selector:      matchLabels:        app: my-java-select    env:      - name: OTEL_TRACES_EXPORTER        value: otlp      - name: OTEL_EXPORTER_OTLP_ENDPOINT        value: http://localhost:4317    exporter:      endpoint: http://localhost:4317    propagators:      - jaeger      - b3    sampler:      type: parentbased_traceidratio      argument: &quot;0.25&quot;    java:      env:      - name: OTEL_JAVAAGENT_DEBUG        value: &quot;true&quot;  \n\n它的预期效果是选择 app: my-java-select 的 deployment 将这些环境变量都注入进去，同时默认也会在 deployment 的容器中挂载一个 javaagent.jar:\nls /otel-auto-instrumentation-java/javaagent.jar\n\n而我们的 01-assert-select.yaml:\napiVersion: v1  kind: Pod  metadata:    annotations:      instrumentation.opentelemetry.io/inject-java: &quot;true&quot;      sidecar.opentelemetry.io/inject: &quot;true&quot;    labels:      app: my-java-select  spec:    containers:    - env:      - name: OTEL_JAVAAGENT_DEBUG        value: &quot;true&quot;      - name: JAVA_TOOL_OPTIONS        value: &#x27; -javaagent:/otel-auto-instrumentation-java/javaagent.jar&#x27;      - name: OTEL_TRACES_EXPORTER        value: otlp      - name: OTEL_EXPORTER_OTLP_ENDPOINT        value: http://localhost:4317        - name: OTEL_TRACES_SAMPLER        value: parentbased_traceidratio      - name: OTEL_SERVICE_NAME        value: my-java-select       - name: OTEL_PROPAGATORS        value: jaeger,b3      - name: OTEL_RESOURCE_ATTRIBUTES      name: myapp    - args:      - --config=env:OTEL_CONFIG      name: otc-container    initContainers:    - name: opentelemetry-auto-instrumentation-java  status:    containerStatuses:    - name: myapp      ready: true      started: true    initContainerStatuses:    - name: opentelemetry-auto-instrumentation-java      ready: true    phase: Running\n\n最终就是把实际的 deployment 的 yaml 内容和这份文件进行对比。\n所以这个 e2e 测试就有点类似于集成测试，不会测试具体的功能函数，只需要最终结果能匹配就可以。\n\n当然这个和单元测试也是相辅相成的，缺一不可，不能完全只依赖 e2e 测试，也有可能是概率原因导致最终生成的资源相同；单元测试可以保证函数功能与预期相同。\n\n\n都准备好之后便可以进行测试了，测试的时候也很简单，只需要执行以下命令即可：\nchainsaw test --test-dir ./tests/e2e-multi-instrumentation\n\n这样它就会遍历该目录下的 chainsaw-test.yaml文件进行测试，执行我们上面定义的那些步骤，最终输出测试结果：\n\n同时 Chainsaw 也提供了 Github action，可以方便的让我们和 github CI 进行集成。\njobs:  example:    runs-on: ubuntu-latest    permissions: &#123;&#125;    name: Install Chainsaw    steps:      - name: Install Chainsaw        uses: kyverno/action-install-chainsaw@v0.1.0        with:          release: v0.0.9      - name: Check install        run: chainsaw version\n\n这样我们就可以在 github 中查看我们的测试结果了：\n总结最后不得不感叹作为 CNCF 下面的项目 OpenTelemetry 的开发者体验真好，只要我们跟着贡献者文档一步步操作都能顺利通过 CI 测试，同时还能避免一些 Code Review 过程中的低级错误。\n比如我第一次提 PR 的时候没有添加 changlog 文件，后面在贡献者手册里发现只需要执行 make chlog-new 就会基于当前分支信息帮我们生成一个 changelog 文件模板，然后只需要往里面填写内容即可。\n这些工具链让不同开发者提交的代码和流程都符合规范，同时也降低了贡献难度。\n以上所有的相关源码都可以在 https://github.com/open-telemetry/opentelemetry-operator 中进行查看。\n参考链接：\n\nhttps://github.com/open-telemetry/opentelemetry-operator/pull/2778\nhttps://kind.sigs.k8s.io/\nhttps://kyverno.github.io/chainsaw/latest/\nhttps://github.com/open-telemetry/opentelemetry-operator/blob/main/CONTRIBUTING.md\n\n","categories":["OB"],"tags":["k8s","operator"]},{"title":"实战：如何优雅的从 Skywalking 切换到 OpenTelemetry","url":"/2024/04/07/ob/otel-replace-sw/","content":"\n背景最近公司将我们之前使用的链路工具切换为了 OpenTelemetry.\n\n\n我们的技术栈是：\n        OTLP                               Client──────────►Collect────────►StartRocks(Agent)                               ▲                                          │                                          │                                       Jaeger                                       \n\n其中客户端使用 OpenTelemetry 提供的 Java Agent 进行埋点收集数据，再由 Agent 通过 OTLP(OpenTelemetry Protocol) 协议将数据发往 Collector，在 Collector 中我们可以自行任意处理数据，并决定将这些数据如何存储（这点在以往的 SkyWalking 体系中是很难自定义的）\n这里我们将数据写入 StartRocks 中，供之后的 UI 层进行查看。\n\nOpenTelemetry 是可观测系统的新标准，基于它可以兼容以前使用的 Prometheus、 victoriametrics、skywalking 等系统，同时还可以灵活扩展，不用与任何但一生态或技术栈进行绑定。更多关于 OTel 的内容会在今后介绍。\n\n难点其中有一个关键问题就是：如何在线上进行无缝切换。\n虽然我们内部的发布系统已经支持重新发布后就会切换到新的链路，也可以让业务自行发布然后逐步的切换到新的系统，这样也是最保险的方式。\n但这样会有几个问题：\n\n当存在调用依赖的系统没有全部切换为新链路时，再查询的时候就会出现断层，整个链路无法全部串联起来。\n业务团队没有足够的动力去推动发布，可能切换的周期较长。\n\n所以最好的方式还是由我们在后台统一发布，对外没有任何感知就可以一键全部切换为 OpenTelemetry。\n仔细一看貌似也没什么难的，无非就是模拟用户点击发布按钮而已。\n但这事由我们自动来做就不一样了，用户点击发布的时候会选择他们认为可以发布的分支进行发布，我们不能自作主张的比如选择 main 分支，有可能只是合并了但还不具备发布条件。\n所以保险的方式还是得用当前项目上一次发布时所使用的 git hash 值重新打包发布。\n但这也有几个问题：\n\n重复打包发布太慢了，线上几十上百个项目，每打包发布一次就得几分钟，虽然可以并发，但考虑到 kubernetes 的压力也不能调的太高。\n保不准业务镜像中有单独加入一些环境变量，这样打包可能会漏。\n\n切换方案所以思来想去最保险的方法还是将业务镜像拉取下来，然后手动删除镜像中的 skywalking 包以及 JVM 参数，全部替换为 OpenTelemetry 的包和 JVM 参数。\n整体的方案如下：\n\n遍历 namespace 的 pod ＞0 的 deployment\n遍历 deployment 中的所有 container，获得业务镜像\n跳过 istio 和日志采集 container，获取到业务容器\n判断该容器是否需要替换，其实就是判断环境变量中是否有 skywalking ，如果有就需要替换。\n获取业务容器的镜像\n\n\n基于该 Image 重新构建一个 OpenTelemetry 的镜像   3.1 新的镜像包含新的启动脚本.   3.1.1 新的启动脚本中会删除原有的 skywalking agent   3.2 新镜像会包含 OpenTelemetry 的 jar 包以及我们自定义的 OTel 扩展包   3.3 替换启动命令为新的启动脚本\n修改 deployment 中的 JVM 启动参数\n修改 deployment 的镜像后滚动更新\n开启一个 goroutine 定时检测更新之后是否启动成功\n如果长时间 (比如五分钟) 都没有启动成功，则执行回滚流程\n\n\n\n具体代码因为需要涉及到操作 kubernetes，所以整体就使用 Golang 实现了。\n遍历 deployment 得到需要替换的容器镜像func ProcessDeployment(ctx context.Context, finish []string, deployment v1.Deployment, clientSet kubernetes.Interface) error &#123;\tdeploymentName := deployment.Name\tfor _, s := range finish &#123;\t\tif s == deploymentName &#123;\t\t\tklog.Infof(&quot;Skip finish deployment:%s&quot;, deploymentName)\t\t\treturn nil\t\t&#125;\t&#125;\t// Write finish deployment name to a file\tdefer writeDeploymentName2File(deploymentName, fmt.Sprintf(&quot;finish-%s.log&quot;, deployment.Namespace))\tappName := deployment.GetObjectMeta().GetLabels()[&quot;appName&quot;]\tklog.Infof(&quot;Begin to process deployment:%s, appName:%s&quot;, deploymentName, appName)\tupgrade, err := checkContainIstio(ctx, deployment, clientSet)\tif err != nil &#123;\t\treturn err\t&#125;\tif upgrade == false &#123;\t\tklog.Infof(&quot;Don&#x27;t have istio, No need to upgrade deployment:%s appName:%s&quot;, deploymentName, appName)\t\treturn nil\t&#125;\tfor i, container := range deployment.Spec.Template.Spec.Containers &#123;\t\tif strings.HasPrefix(deploymentName, container.Name) &#123;\t\t\t// Check if container has sw jvm\t\t\tfor _, envVar := range container.Env &#123;\t\t\t\tif envVar.Name == &quot;CATALINA_OPTS&quot; &#123;\t\t\t\t\tif !strings.Contains(envVar.Value, &quot;skywalking&quot;) &#123;\t\t\t\t\t\tklog.Infof(&quot;Skip upgrade don&#x27;t have sw jvm deployment:%s container:%s&quot;, deploymentName, container.Name)\t\t\t\t\t\treturn nil\t\t\t\t\t&#125;\t\t\t\t&#125;\t\t\t&#125;\t\t\tupgrade(container)\t\t\t// Check newDeployment status\t\t\tgo checkNewDeploymentStatus(ctx, clientSet, newDeployment)\t\t\t// delete from image\t\t\tdeleteImage(container.Image)\t\t&#125;\t&#125;\treturn nil&#125;\n\n这个函数需要传入一个 deployment ，同时还有一个已经完成了的列表进来。\n\n已完成列表用于多次运行的时候可以快速跳过已经执行的 deployment。\n\ncheckContainIstio() 函数很简单，判断是否包含了 Istio 容器，如果没有包含说明不是后端应用（可能是前端、大数据之类的任务），就可以直接跳过了。\n\n而判断是否需要替换的前提这事判断环境变量 CATALINA_OPTS 中是否包含了 skywalking 的内容，如果包含则说明需要进行替换。\nUpgrade 核心函数func upgrade(container Container)&#123;\tklog.Infof(&quot;Begin to upgrade deployment:%s container:%s&quot;, deploymentName, container.Name)\tnewImageName := fmt.Sprintf(&quot;%s-otel-%s&quot;, container.Image, generateRandomString(4))\terr := BuildNewOtelImage(container.Image, newImageName)\tif err != nil &#123;\t\treturn err\t&#125;\t// Update deployment jvm ENV\tfor e, envVar := range container.Env &#123;\t\tif envVar.Name == &quot;CATALINA_OPTS&quot; &#123;\t\t\totelJVM := replaceSWAgent2OTel(envVar.Value, appName)\t\t\tdeployment.Spec.Template.Spec.Containers[i].Env[e].Value = otelJVM\t\t&#125;\t&#125;\t// Update deployment image\tdeployment.Spec.Template.Spec.Containers[i].Image = newImageName\tnewDeployment, err := clientSet.AppsV1().Deployments(deployment.Namespace).Update(ctx, &amp;deployment, metav1.UpdateOptions&#123;&#125;)\tif err != nil &#123;\t\treturn err\t&#125;\tklog.Infof(&quot;Finish upgrade deployment:%s container:%s&quot;, deploymentName, container.Name)&#125;\n\n这里一共分为以下几部：\n\n基于老镜像构建新镜像\n更新原有的 CATALINA_OPTS 环境变量，也就是替换 skywalking 的参数\n更新 deployment 镜像，触发滚动更新\n\n构建新镜像\tdockerfile = fmt.Sprintf(`FROM %sCOPY %s /home/admin/%sCOPY otel.tar.gz /home/admin/otel.tar.gzRUN tar -zxvf /home/admin/otel.tar.gz -C /home/adminRUN rm -rf /home/admin/skywalking-agentENTRYPOINT [&quot;/bin/sh&quot;, &quot;/home/admin/start.sh&quot;]`, fromImage, script, script)\tidx := strings.LastIndex(newImageName, &quot;/&quot;) + 1\tdockerFileName := newImageName[idx:]\tcreate, err := os.Create(fmt.Sprintf(&quot;Dockerfile-%s&quot;, dockerFileName))\tif err != nil &#123;\t\treturn err\t&#125;\tdefer func() &#123;\t\tcreate.Close()\t\tos.Remove(create.Name())\t&#125;()\t_, err = create.WriteString(dockerfile)\tif err != nil &#123;\t\treturn err\t&#125;\tcmd := exec.Command(&quot;docker&quot;, &quot;build&quot;, &quot;.&quot;, &quot;-f&quot;, create.Name(), &quot;-t&quot;, newImageName)\tcmd.Stdin = strings.NewReader(dockerfile)\tif err := cmd.Run(); err != nil &#123;\t\treturn err\t&#125;\n\n其实这里的重点就是构建这个新镜像，从这个 dockerfile 中也能看出具体的逻辑，也就是上文提到的删除原有的 skywalking 资源同时将新的 OpenTelemetry 资源打包进去。\n最后再将这个镜像上传到私服。\n其中的替换 JVM 参数也比较简单，直接删除 skywalking 的内容，然后再追加上 OpenTelemetry 需要的参数即可。\n定时检测替换是否成功func checkNewDeploymentStatus(ctx context.Context, clientSet kubernetes.Interface, newDeployment *v1.Deployment) error &#123;\tready := true\ttick := time.Tick(10 * time.Second)\tfor i := 0; i &lt; 30; i++ &#123;\t\t&lt;-tick\t\toriginPodList, err := clientSet.CoreV1().Pods(newDeployment.Namespace).List(ctx, metav1.ListOptions&#123;\t\t\tLabelSelector: metav1.FormatLabelSelector(&amp;metav1.LabelSelector&#123;\t\t\t\tMatchLabels: newDeployment.Spec.Selector.MatchLabels,\t\t\t&#125;),\t\t&#125;)\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t\t// Check if there are any Pods\t\tif len(originPodList.Items) == 0 &#123;\t\t\tklog.Infof(&quot;No Pod in deployment:%s, Skip&quot;, newDeployment.Name)\t\t&#125;\t\tfor _, item := range originPodList.Items &#123;\t\t\t// Check Pod running\t\t\tfor _, status := range item.Status.ContainerStatuses &#123;\t\t\t\tif status.RestartCount &gt; 0 &#123;\t\t\t\t\tready = false\t\t\t\t\tbreak\t\t\t\t&#125;\t\t\t&#125;\t\t&#125;\t\tklog.Infof(&quot;Check deployment:%s namespace:%s status:%t&quot;, newDeployment.Name, newDeployment.Namespace, ready)\t\tif ready == false &#123;\t\t\tbreak\t\t&#125;\t&#125;\tif ready == false &#123;\t\t// rollback\t\tklog.Infof(&quot;=======Rollback deployment:%s namespace:%s&quot;, newDeployment.Name, newDeployment.Namespace)\t\twriteDeploymentName2File(newDeployment.Name, fmt.Sprintf(&quot;rollback-%s.log&quot;, newDeployment.Namespace))\t&#125;\treturn nil&#125;\n\n这里会启动一个 10s 执行一次的定时任务，每次都会检测是否有容器发生了重启（正常情况下是不会出现重启的）\n如果检测了 30 次都没有重启的容器，那就说明本次替换成功了，不然就记录一个日志文件，然后人工处理。\n\n这种通常是原有的镜像与 OpenTelemetry 不兼容，比如里面写死了一些 skywalking 的 API，导致启动失败。\n\n所以替换任务跑完之后我还会检测这个 rollback-$namespace 的日志文件，人工处理这些失败的应用。\n分批处理 deployment最后讲讲如何单个调用刚才的 ProcessDeployment() 函数。\n考虑到不能对 kubernetes 产生影响，所以我们需要限制并发处理 deployment 的数量（我这里的限制是 10 个）。\n所以就得分批进行替换，每次替换 10 个，而且其中有一个执行失败就得暂停后续任务，由人工检测失败原因再决定是否继续处理。\n\n\b毕竟处理的是线上应用，需要小心谨慎。\n\n所以触发的代码如下：\nfunc ProcessDeploymentList(ctx context.Context, data []v1.Deployment, clientSet kubernetes.Interface) error &#123;\tfile, err := os.ReadFile(fmt.Sprintf(&quot;finish-%s.log&quot;, data[0].Namespace))\tif err != nil &#123;\t\treturn err\t&#125;\tsplit := strings.Split(string(file), &quot;\\n&quot;)\tbatchSize := 10\tstart := 0\tfor start &lt; len(data) &#123;\t\tend := start + batchSize\t\tif end &gt; len(data) &#123;\t\t\tend = len(data)\t\t&#125;\t\tbatch := data[start:end]\t\t//等待goroutine结束\t\tvar wg sync.WaitGroup\t\tklog.Infof(&quot;Start process batch size %d&quot;, len(batch))\t\terrs := make(chan error, len(batch))\t\twg.Add(len(batch))\t\tfor _, item := range batch &#123;\t\t\td := item\t\t\tgo func() &#123;\t\t\t\tdefer wg.Done()\t\t\t\tif err := ProcessDeployment(ctx, split, d, clientSet); err != nil &#123;\t\t\t\t\tklog.Errorf(&quot;!!!Process deployment name:%s error: %v&quot;, d.Name, err)\t\t\t\t\terrs &lt;- err\t\t\t\t\treturn\t\t\t\t&#125;\t\t\t&#125;()\t\t&#125;\t\tgo func() &#123;\t\t\twg.Wait()\t\t\tclose(errs)\t\t&#125;()\t\t//任何一个失败就返回\t\tfor err := range errs &#123;\t\t\tif err != nil &#123;\t\t\t\treturn err\t\t\t&#125;\t\t&#125;\t\tstart = end\t\tklog.Infof(&quot;Deal next batch&quot;)\t&#125;\treturn nil&#125;\n\n使用 WaitGroup 来控制一组任务，使用一个 chan 来传递异常；这类分批处理的代码在一些批处理框架中还蛮常见的。\n总结最后只需要查询某个 namespace 下的所有 deployment 列表传入这个批处理函数即可。\n不过整个过程中还是有几个点需要注意：\n\n因为需要替换镜像的前提是要把现有的镜像拉取到本地，所以跑这个任务的客户端需要有充足的磁盘，同时和镜像服务器的网络条件较好。\n不然执行的过程会比较慢，同时磁盘占用满了也会影响任务。\n\n其实这个功能依然有提升空间，考虑到后续会升级 OpenTelemetry  agent 的版本，甚至也需要增减一些 JVM 参数。\n所以最后有一个统一的工具，可以直接升级 Agent，而不是每次我都需要修改这里的代码。\n\n后来在网上看到了得物的相关分享，他们可以远程加载配置来解决这个问题。\n这也是一种解决方案，直到我们看到了 OpenTelemetry 社区提供了 Operator，其中也包含了注入 agent 的功能。\napiVersion: opentelemetry.io/v1alpha1  kind: Instrumentation  metadata:    name: my-instrumentation  spec:    exporter:      endpoint: http://otel-collector:4317    propagators:      - tracecontext      - baggage      - b3    sampler:      type: parentbased_traceidratio      argument: &quot;0.25&quot;    java:      image: private/autoinstrumentation-java:1.32.0-1\n\n我们可以使用他提供的 CRD 来配置我们 agent，只要维护好自己的镜像就好了。\n使用起来也很简单，只要安装好了 OpenTelemetry-operator ，然后再需要注入 Java Agent 的 Pod 中使用注解：\ninstrumentation.opentelemetry.io/inject-java: &quot;true&quot;\n operator 就会自动从刚才我们配置的镜像中读取 agent，然后复制到我们的业务容器。\n再配置上环境变量 $JAVA_TOOL_OPTIONS=/otel/javaagent.java, 这是一个 Java 内置的环境变量，应用启动的时候会自动识别，这样就可以自动注入 agent 了。\nenvJavaToolsOptions   = &quot;JAVA_TOOL_OPTIONS&quot;// set env valueidx := getIndexOfEnv(container.Env, envJavaToolsOptions)  if idx == -1 &#123;      container.Env = append(container.Env, corev1.EnvVar&#123;         Name:  envJavaToolsOptions,         Value: javaJVMArgument,      &#125;)&#125; else &#123;      container.Env[idx].Value = container.Env[idx].Value + javaJVMArgument  &#125;// copy javaagent.jarpod.Spec.InitContainers = append(pod.Spec.InitContainers, corev1.Container&#123;      Name:      javaInitContainerName,      Image:     javaSpec.Image,      Command:   []string&#123;&quot;cp&quot;, &quot;/javaagent.jar&quot;, javaInstrMountPath + &quot;/javaagent.jar&quot;&#125;,      Resources: javaSpec.Resources,      VolumeMounts: []corev1.VolumeMount&#123;&#123;         Name:      javaVolumeName,         MountPath: javaInstrMountPath,      &#125;&#125;,&#125;)\n\n大致的运行原理是当有 Pod 的事件发生了变化（重启、重新部署等），operator 就会检测到变化，此时会判断是否开启了刚才的注解：\ninstrumentation.opentelemetry.io/inject-java: &quot;true&quot;\n\n接着会写入环境变量 JAVA_TOOL_OPTIONS，同时将 jar 包从 InitContainers 中复制到业务容器中。\n\n这里使用到了 kubernetes 的初始化容器，该容器是用于做一些准备工作的，比如依赖安装、配置检测或者是等待其他一些组件启动成功后再启动业务容器。\n\n目前这个 operator 还处于使用阶段，同时部分功能还不满足（比如支持自定义扩展），今后有时间也可以分析下它的运行原理。\n参考链接：\n\nhttps://xie.infoq.cn/article/e6def1e245e9d67735bd00dd5\nhttps://github.com/open-telemetry/opentelemetry-operator/#opentelemetry-auto-instrumentation-injection\n\n","categories":["OB"],"tags":["k8s"]},{"title":"跟着播客学英语-Why I use vim ? part one.","url":"/2023/10/02/ob/podcasts-english-0-vim/","content":"\n最近这段时间在学英语，在网上看到有网友推荐可以听英文播客提高听力水平。\n正好我自己也有听播客的习惯，只不过几乎都是中文，但现在我已经尝试听了一段时间的英文播客，觉得效果还不错。\n大部分都是和 IT 相关的内容，所以一些关键词还能听懂，同时也是自己的感兴趣的内容，如果是一次听不懂我就会反复收听。视频版：\n \n\n\n\n今天来听第一期内容，这位作者是一位资深工程师，讲述他为什么使用 Vim 的过程。https://www.healthyhacker.com/2014/07/29/why-i-use-vim/\n以下是我通过语音转文字的内容\n\n我会精简翻译比较重要的部分，还是推荐大家去收听原始播客。\n\nHealthy hacker episode one. Welcome to the healthy hacker, where we talk about programming, puzzles, memory, fitness, diet, and everything else that you a healthy hacker , find Interesting. I’m Chris Hunt, and on the very first episode episode one, I’m going to answer a question? I get all the time, every conference I go to, every time I start a new job, every time I pair with somebody new. And that is: Chris, why do you use Vim? At of all the text editor on the planet, why do you choose to use Vim?\n\n作者在各种会议和新同事的接触中都会被问到这个问题：为什么你会使用 vim\n\nIt’s so old it looks like crap. Why do you do it? So I’m totally going to tell you I have many various reasons why I love using Vim really excited about it, going to answer that question before we do though, we are going to talk about the workout of the week , all right. The workout of the week is a section that, uh, basically, I’m just going to take a workout I’ve done recently, and tell you about it, and hopefully you find the time this week to try it yourself, because every single one of these workouts you can do, I promise you. \n\n在开始之前先聊聊本周的锻炼\n\nOkay, so this week’s workout is a workout that I’ve been doing for several years. You need barely no equipment. All you need is a floor. I do it at least once, when I travel, sometimes twice, sometimes three times. I think there was a point in my life where I was doing this. Work out, like three or four times a day. This is the only thing I was doing. I don’t recommend that, but you should totally give it a shot at least once this week and let me know how you do, because I’m curious,\n\n这个锻炼已经进行了许多年了，几乎不需要额外的设备，只需要一块地板。旅行的时候也会继续坚持，建议你本周至少得尝试一次。\n\nso let’s get right into it this is  A ten rep pyramid, and I’ll I’ll explain what that means. Basically, you want to do each exercise, one time, then two times, then three times, then four times, all the way up to ten, the top of the pyramid ten times, and then you go back down again to one, so nine, a seven until you finally end with one rep of each exercise, so the two exercises are going to do for this workout is a pushup and a shoulder press with no weight on each of these, you’re just doing body weight.\n\n做一个递增组和递减组，从每组一个动作增加到每组 10 个动作，再由十个动作一组减少到一个动作一组；每组做两个动作，俯卧撑，和坐姿推肩，都不用负重，只需要自重即可。\n\nSo I’m sure, everybody knows what a pushup is. If you don’t check out the show notes, or just do a Google image search, shoulder press you may or may not be as familiar with, but it’s just like it sounds. You take your hands, put them by your shoulders, and then press your hands up into the air again, just to Google. Im in search, you’ll totally get what this is. So the workout is one pushup. One shoulder press, then two pushups, two shoulder presses, then three pushups, then three shoulder presses,\n\n应该都知道俯卧撑怎么做，如果推肩不知道怎么做的话可以去 Google，都是比较简单动作；所以这个训练是一次俯卧撑，一次推肩为一个动作；然后增加到两个俯卧撑+推肩+然后是三个俯卧撑+三个推肩。\n\nthen four, five, six, seven, eight 910, and then go back down again, nine, eight, seven, six, and you finish with one push up, one shoulder, press. Now. The goal with this is to go as fast as you possibly can, but take breaks as necessary. I definitely can’t do this without stopping, especially on the pushups part. So do as fast as you can. When I did this this morning, I did it in seven minutes and 28 seconds, so let me know how you do. I’m super curious if you’ve never done this before, it’s going to seem crazy hard,\n\n以此类推做到十次，再递减到 1 一次，目标是尽可能的快速完成。休息也是非常有必要的，我无法做到不休息全部完成，特别是在俯卧撑这个部分。非常好奇你做完的感受，如果你从来没做过的话，还是比较困难的。\n\nbut I know you can do it. By the time you finished this workout, you will have done 100 pushups and 100 shoulder presses. If it really Really Really is out of your reach, even with breaks, then you can scale this workout, decreasing your pyramid. So instead of doing a ten rep pyramid, do like a six rep pyramid or a Five Rep Pyramid, but with brakes, I know that everybody can do 100 pushups in 100 shoulder presses. It might take you a while, but you can do it, so give it a shot. All right. \n\n但超过你的承受范围时，可以适当减少组数。\n\nSo now it’s time to answer that question: why do I use Vim? Well, for starters, if you’re going to learn any editor vim is a pretty good investment. It’s been around for a long time over 20 years. It’s open source, it runs on everything it’s not like there’s, a company vested in its future you know it doesn’t cost you money, it’s for as long as I’m programming, vim is going to be around, so if I’m going to waste time trying to master a text editor, vim is probably a good choice because it’s not going anywhere\nI’m not going to have to forget everything I’ve learned and start learning a different text editor. I can use vim for the rest of my life for all my text editor so it’s a good investment of your time. Now if you do a Google search, you start looking for books for Vim, you might see Vi and Vi is actually an older editor that Vim is based on Vim stands for Vi improved, and most of them’s functionality comes from Vi, so most of us, don’t use Vi, some of the things I really like about vim, that Vi doesn’t have is improved syntax,\nhighlighting for the languages I like to use mostly Ruby and Javascript. Nowadays, they’re spell-checking, so when you use it for typing an emails or typing up a poll request that comes in handy, there’s splits, so you can view maybe your test and your code at the same time, without having to go back and forth, you have multiple levels of undo and redo them can do diffs, or, as vi can’t do diff, so you can open up two of the same files that are edited at different points of time, and see that diff in red and green it’s pretty nice,\n\n学习 vim 是一个很好的投资，它是开源的免费的，值得花时间去学习。vi 是 vim 的前身，vim 在此基础上进行了改进，比如语法高亮、输入检测等。vim 还可以分屏，可视化对比等\n\nand then you also have scripting Vim script itself, which is, the native script language for Vim is not pretty, but you can also do scripting with other languages, like Perl, Ruby, python and Vim also has a really awesome help system with which vi does not have there’s also some improvements that Vim ads that I don’t really care about, I mean I took notes, obviously because I don’t have all this stuff memorized, and I’ve titled this section dumb stuff, because it’s kind of du I don’t care linefolding is one editing of compressed files I don’t really care about that.\n\nVim 还内置了脚本语言 vimscript，很好用的帮助系统，这些 vi 都没有。\n\nYou can edit files over network connections like ssh ftp http. I could say how that would be useful, but I’ve never wanted or had to do that, and then them also provides a graphical user interface. Usually you open this up using Gvim for graphical Vim, and that provides mouse integration, again, things I don’t care about. One of the main reasons I use Vim is for speed, and not having to touch the mouse, so it’s kind of silly to for me to get excited about that kind of stuff, okay So so that’s the kind of the differences between Vim and Vi and why everybody uses Vim most computers now.\n\nvim 可以通过网络连接来编辑文件，同时也提供了 GUI 界面，可以使用鼠标来操作。不过我对这个并不感兴趣，使用 vim 的主要原因就是因为速度，不需要在去触摸鼠标了（这确实也是大部分人使用 vim 的原因）\n\n生词It’s so old it looks like crap:  它已经很老了，看起来是垃圾。crap:\nif you’re going to learn any editor, vim is a pretty good investment：如果你想学习一个编辑器，Vim 是一个不错的投资。investment:\nI definitely can’t do this without stopping我绝对不能不停下来definitely:\nlet me know how you do, because I’m curious:告诉我你是怎么做的，我很好奇。curious\nYou need barely no equipment.你几乎不需要设备barely\nI don’t have all this stuff memorized:我没有记住所有这些东西stuff\n","categories":["OB","Podcasts"],"tags":["English"]},{"title":"跟着播客学英语-Why I use vim ? part two.","url":"/2023/10/06/ob/podcasts-english-1-vim/","content":"\n在上一期作者讲到了他使用 Vim 的主要原因是提高效率，不需要再去使用鼠标，今天我们继续上次未听完的内容：\n\nif you type Vi, that’s going to be alias to Vim anyway by default there’s, not really a good reason for you to use vi that I can think of. The reason I first started using Vim is kind of a silly one, and that is because my brother used it, uh, my brother, Nick, he climbs mountains for a living. Right now, he lives in Seattle and goes up and down rain near when it gets too cold to climb. Rainer? He goes to the other side of the world and leads, climbs up mountains over there, and then comes back again and starts climb a rain air over and over and over again.\nBefore he did that, though, he used to do quite a bit of programming, and I remember when I would watch him program because he started programming before me, he looked like a real hacker. He looked like the hackers you see on TV. He never touched the mouse. There was text flying out all over the screen files, opening, closing splits. It was awesome. It was like I wanted to be that person you. Know and He was using vim. I didn’t know it was Vim, but that’s what he was using. So I opened Vim. I tried to edit some files I was using Eclipse at the time and doing Java.\n\n作者首次使用 Vim 的 原因有点傻，因为他的哥哥 Nick 做了很长时间的编程，一直使用的都是 Vim，看起来就行是电视里真正的黑客那样，他从不使用鼠标，文字也在屏幕里乱飞，看起来非常酷。而那个时候作者还在使用 eclipse 编写 Java\n\nThis is like my first year of college, I was taking an introductory to programming class. It’s the first programming I’ve ever done, and I thought Eclipse was great. It compiled my code for me. It had buttons for everything, but I didn’t feel like a real hacker when I looked at nick. I mean, granted, he was writing C, which already looks way cooler. He was using Vim as well, and it just impressed me, so I try to start opening up some of my Java files, and I have no idea how to do it. No idea how to edit a file I have no idea how to save a file I have no idea how to type in the document for crying out loud.\nIf I press keys, it does nothing. Sometimes a delete words, sometimes at pacewords, it&#39;s a mess, and this is probably the first experience that everybody has when they start using Vim, because Vim operates completely different than any other text editor I’ve ever used it’s weird them has modes, it has a language, it has objects, subjects, counts, verbs. All that stuff is, like, really weird when you first get started, but once you understand it, it’s a lot of learning upfront everything, just kind of clicks and you’re instantly faster than you’ve ever been before,\n\neclipse 很好用，但看起来没有 Nick 使用 Vim 那么酷，使得作者印象深刻。\n因此他尝试使用 Vim 来打开 Java 文件，但却不知道如何编辑、删除、保存等基本操作，大部分初次使用 Vim 应该都会碰到这些问题，它和我们使用的其他编辑器完全不同，看起来比较奇怪。\n不过一旦你掌握它，那么使用效率将会飞速提高。\n\nfor example, let’s say, we wanted to copy a method from one ruby file and put it in another. If I was using sublime text, I would take my mouse. I would select that method I’d press command c to copy it, and then I’d click over to where I want the method to be, and I’d press command v to paste it. Not very bad that’s pretty fast probably doesn’t take very long, but in Vim, you can do it even faster and without touching your mouse, them has a verb for yanking text it’s not called copying it’s, called yanking.\nIt has a movement called inside, so you can yank inside something, and then it has subjects called text blocks, which, in Ruby, those are methods. Vim understands blocks of text if you’re editing a markdown document like, say, a read me, them’ll know where a paragraph starts and where a paragraph ends, if we’re editing a ruby file it’s going to know where a method begins and where a method ends, or where your class begins or where your class ends, so using the Verb Yank, which happens to be the key Y,\n\n如果我们使用 sublime 这样的编辑器复制一个方法时会比较麻烦，首先要用鼠标选中文本，然后复制再粘贴。但使用 Vim 时不需要使用鼠标，而是被称为 Yanking，当编辑  Markdown 时 Vim 会知道段落的开始和结尾，编辑 Ruby 时可以方法的开始和结尾。\n\nand then the movement inside, which happens to be the key I, and then the subject paragraph, which is vim’s word for a block of text, you can yank a method. So if I put my cursor inside a ruby method and I type Y I P. For yank inside paragraph, it’s going to copy that method to the clipboard. So by understanding the verb, yank the movement inside and the subject paragraph, we can perform actions really, really quickly, and then if we want to pace it somewhere else, we press the peaky for paste,\nand it’ll stick that text down, so everything in Vim is based on those concepts of verbs, movements and subjects. You also have one more thing you can play with, and that is counts, so if you want to perform something multiple times, in most cases, you can stick a number in front of it like one, two, three, five, and it’s going to do it that many times, understanding those basic concepts, gets you a really long way, and then it’s just a matter of understanding which keys correspond to which verbs and which movements and which subjects,\n\n在 vim 中只需要将光标移动到方法中，然后使用 YIP 就可以复制整个方法。所以只要理解了这些基本概念就可以快速提高效率。\n\nand that just comes with time, which is the third reason I like to use Vim. Is there so much stuff to learn whenever I get bored? All I got to do is pull up them help, and I could start learning stuff in every little thing, I learn, every little keyboard shortcut, every movement, every subject, gets me a little bit faster, and it doesn’t seem like a lot like the example I gave earlier, of copying and pasting text and sublime with a mouse that doesn’t take very long. But if you add up all those little bits,\nyou’re saving a ton of time, a good analogy I like to use is lifting weights if I’m squatting 100 pounds, and I take a little, teeny, tiny two and a half pound plate, and I put it on each side of the barbell I probably won’t, even notice it, I mean, I’ll be squatting 105 pounds now, right, that’s not really that big of a difference, but if I add those two and a half pound plates 60 more times, going in once a day, and doing that, I’m now squatting 400 pounds, which is a pretty big difference, so those little changes don’t seem like a lot,\n\n我喜欢使用 Vim 的一个原因是可以学到许多东西，每学一些都可以让自己的效率提高一点。就像是我们深蹲一样，慢慢的加重量，反复尝试最终就能得到巨大的提升。\n\nbut taken as a whole, you’re editing text way faster than people who aren’t using vim. The last reason I like to use Vim is because it runs in the terminal, and this may not seem like a big deal, but it really is cosmetically, it’s nice because you can just pop a terminal open, full screen and have no chrome I use mac os, so I’ve got that big menu bar on top, it’s really nice to just full screen a terminal and not see anything at all, and have your command line tools and your text editor all running in the same window I happen to use tmucks to manage that stuff I won’t talk about Tmux and now,\nbut I definitely will talk about it in another episode, but even out of the cosmetic reasons, it’s nice to have a text editor that runs in the terminal because you’re not always using your own machine right as web developers, I’m a web developer, we’re always connecting to remote machines to edit files. Now, why is it that we should be forced to use a different text editor? The nice thing about Vim is it runs in the terminal, so every machine you connect to will likely already have Vim installed, but even if it doesn’t you can install it,\nand you can put your config files over there and you can make it so that no matter where you’re editing text you’re always in the same environment, which is awesome. Before I started using Vim out, connect a remote machines, and, and be forced to use some textset or I’m not familiar with, and it drove me nuts, but now I sometimes forget I’m on a remote machine because it’s exactly the same as the machine I use at home, so if you’ve never tried them, or maybe you tried it in the past, and it was super confusing or really turned you off.\n\n最后一个使用 Vim 的 原因是它可以在终端中运行，不仅可以使用自己的设备，还可以连接到远程设备去编辑文件，还可以使用相同的配置文件，使得所有的环境配置都是相同的\n\nGive it a second shot there’s, a great stack overflow article that just kind of rehashs a lot of the things I said here, but it’s just it’s also just fun to read it talks about the core concepts of vi, the editor that Vim is based on, and I put a link to that in the shownotes there’s, also, them casts by Drew Neil, which is a screencast series that’s totally free, and he covers a lot of advanced concepts he’s got a little bit of beginner stuff in there as well. You can also just open up a terminal and type vim tutor and Vim’s going to give you a little lesson in how to use Vim,\ngive it a shot that’s all I’ve got for this week, you can find the shownotes at healthy hacker Dot Com, slash one, if you have any questions or feedback, send me a voicemail healthy hacker, Dot Com Slash Voicemail.\n\n在 stack overflow 中有着各种教程，大家可以尝试一下。\n\n生词The reason I first started using Vim is kind of a silly one我第一次使用 Vim 的原因有点傻。\nit&#39;s a mess搞砸了\na good analogy I like to use is lifting weights if I’m squatting 100 pounds,but it really is cosmetically,\n\n","categories":["OB","Podcasts"],"tags":["English"]},{"title":"Pulsar客户端消费模式揭秘：Go 语言实现 ZeroQueueConsumer","url":"/2024/07/29/ob/pulsar-client-zero-consumer/","content":"前段时间在 pulsar-client-go 社区里看到这么一个 issue：\n\nimport &quot;github.com/apache/pulsar-client-go/pulsar&quot;client, err := pulsar.NewClient(pulsar.ClientOptions&#123;    URL: &quot;pulsar://localhost:6650&quot;,&#125;)if err != nil &#123;    log.Fatal(err)&#125;consumer, err := client.Subscribe(pulsar.ConsumerOptions&#123;    Topic:             &quot;persistent://public/default/mq-topic-1&quot;,    SubscriptionName:  &quot;sub-1&quot;,    Type:              pulsar.Shared,    ReceiverQueueSize: 0,&#125;)if err != nil &#123;    log.Fatal(err)&#125;// 小于等于 0 时会设置为 1000const (      defaultReceiverQueueSize = 1000  )if options.ReceiverQueueSize &lt;= 0 &#123;      options.ReceiverQueueSize = defaultReceiverQueueSize  &#125;\n\n他发现手动将 pulsar-client-go 客户端的 ReceiverQueueSize 设置为 0 的时候，客户端在初始化时会再将其调整为 1000.\nif options.ReceiverQueueSize &lt; 0 &#123;      options.ReceiverQueueSize = defaultReceiverQueueSize  &#125;\n\n而如果手动将源码修改为可以设置为 0 时，却不能正常消费，消费者会一直处于 waiting 状态，获取不到任何数据。\n经过我的排查发现是 Pulsar 的  Go  客户端缺少了一个 ZeroQueueConsumerImpl的实现类，这个类主要用于可以精细控制消费逻辑。\n\nIf you’d like to have tight control over message dispatching across consumers, set the consumers’ receiver queue size very low (potentially even to 0 if necessary). Each consumer has a receiver queue that determines how many messages the consumer attempts to fetch at a time. For example, a receiver queue of 1000 (the default) means that the consumer attempts to process 1000 messages from the topic’s backlog upon connection. Setting the receiver queue to 0 essentially means ensuring that each consumer is only doing one thing at a time.\n\nhttps://pulsar.apache.org/docs/next/cookbooks-message-queue/#client-configuration-changes\n正如官方文档里提到的那样，可以将 ReceiverQueueSize 设置为 0；这样消费者就可以一条条的消费数据，而不会将消息堆积在客户端队列里。\n客户端消费逻辑借此机会需要再回顾下 pulsar 客户端的消费逻辑，这样才能理解 ReceiverQueueSize 的作用以及如何在 pulsar-client-go 如何实现这个 ZeroQueueConsumerImpl。\nPulsar 客户端的消费模式是基于推拉结合的：\n如这张图所描述的流程，消费者在启动的时候会主动向服务端发送一个 Flow 的命令，告诉服务端需要下发多少条消息给客户端。\n同时会使用刚才的那个 ReceiverQueueSize参数作为内部队列的大小，将客户端下发的消息存储在内部队列里。\n然后在调用 receive 函数的时候会直接从这个队列里获取数据。\n\n每次消费成功后都会将内部的一个 AvailablePermit+1，直到大于 MaxReceiveQueueSize / 2 就会再次向 broker 发送 flow 命令，告诉 broker 再次下发消息。\n所以这里有一个很关键的事件：就是向 broker 发送 flow 命令，这样才会有新的消息下发给客户端。\n之前经常都会有研发同学让我排查无法消费的问题，最终定位到的原因几乎都是消费缓慢，导致这里的 AvailablePermit 没有增长，从而也就不会触发 broker 给客户端推送新的消息。\n看到的现象就是消费非常缓慢。\nZeroQueueConsumerImpl 原理下面来看看 ZeroQueueConsumerImpl 是如何实现队列大小为 0 依然是可以消费的。\n在构建 consumer 的时候，就会根据队列大小从而来创建普通消费者还是 ZeroQueueConsumerImpl 消费者。\n@Override  protected CompletableFuture&lt;Message&lt;T&gt;&gt; internalReceiveAsync() &#123;      CompletableFuture&lt;Message&lt;T&gt;&gt; future = super.internalReceiveAsync();      if (!future.isDone()) &#123;          // We expect the message to be not in the queue yet          increaseAvailablePermits(cnx());      &#125;      return future;  &#125;\n\n这是 ZeroQueueConsumerImpl 重写的一个消费函数，其中关键的就是 increaseAvailablePermits(cnx());.\nvoid increaseAvailablePermits(ClientCnx currentCnx) &#123;    increaseAvailablePermits(currentCnx, 1);&#125;protected void increaseAvailablePermits(ClientCnx currentCnx, int delta) &#123;    int available = AVAILABLE_PERMITS_UPDATER.addAndGet(this, delta);    while (available &gt;= getCurrentReceiverQueueSize() / 2 &amp;&amp; !paused) &#123;        if (AVAILABLE_PERMITS_UPDATER.compareAndSet(this, available, 0)) &#123;            sendFlowPermitsToBroker(currentCnx, available);            break;        &#125; else &#123;            available = AVAILABLE_PERMITS_UPDATER.get(this);        &#125;    &#125;&#125;\n\n从源码里可以得知这里的逻辑就是将 AvailablePermit 自增，达到阈值后请求 broker 下发消息。\n因为在 ZeroQueueConsumerImpl 中队列大小为 0，所以 available &gt;= getCurrentReceiverQueueSize() / 2永远都会为 true。\n也就是说每消费一条消息都会请求 broker 让它再下发一条消息，这样就达到了每一条消息都精确控制的效果。\npulsar-client-go 中的实现为了在 pulsar-client-go 实现这个需求，我提交了一个 PR 来解决这个问题。\n其实从上面的分析已经得知为啥手动将 ReceiverQueueSize 设置为 0 无法消费消息了。\n根本原因还是在初始化的时候优于队列为 0，导致不会给 broker 发送 flow 命令，这样就不会有消息推送到客户端，也就无法消费到数据了。\n所以我们依然得参考 Java 的 ZeroQueueConsumerImpl 在每次消费的时候都手动增加  availablePermits。\n为此我也新增了一个消费者 zeroQueueConsumer。\n// EnableZeroQueueConsumer, if enabled, the ReceiverQueueSize will be 0.  // Notice: only non-partitioned topic is supported.  // Default is false.  EnableZeroQueueConsumer boolconsumer, err := client.Subscribe(ConsumerOptions&#123;      Topic:                   topicName,      SubscriptionName:        &quot;sub-1&quot;,      Type:                    Shared,      NackRedeliveryDelay:     1 * time.Second,      EnableZeroQueueConsumer: true,  &#125;)if options.EnableZeroQueueConsumer &#123;      options.ReceiverQueueSize = 0  &#125;\n\n在创建消费者的时候需要指定是否开启 ZeroQueueConsumer，当开启后会手动将 ReceiverQueueSize 设置为 0.\n// 可以设置默认值。private int receiverQueueSize = 1000;\n\n\n在 Go 中无法像 Java 那样在结构体初始化化的时候就指定默认值，再加上 Go 的 int 类型具备零值（也就是0），所以无法区分出 ReceiverQueueSize&#x3D;0 是用户主动设置的，还是没有传入这个参数使用的零值。\n\n所以才需要新增一个参数来手动区分是否使用 ZeroQueueConsumer。\n之后在创建 consumer 的时候进行判断，只有使用的是单分区的 topic 并且开启了 EnableZeroQueueConsumer 才能创建  zeroQueueConsumer。\n\n\n\n使用 PARTITIONED_METADATA 命令可以让 broker 返回分区数量。\n\n\nfunc (z *zeroQueueConsumer) Receive(ctx context.Context) (Message, error) &#123;\tif state := z.pc.getConsumerState(); state == consumerClosed || state == consumerClosing &#123;\t\tz.log.WithField(&quot;state&quot;, state).Error(&quot;Failed to ack by closing or closed consumer&quot;)\t\treturn nil, errors.New(&quot;consumer state is closed&quot;)\t&#125;\tz.Lock()\tdefer z.Unlock()\tz.pc.availablePermits.inc()\tfor &#123;\t\tselect &#123;\t\tcase &lt;-z.closeCh:\t\t\treturn nil, newError(ConsumerClosed, &quot;consumer closed&quot;)\t\tcase cm, ok := &lt;-z.messageCh:\t\t\tif !ok &#123;\t\t\t\treturn nil, newError(ConsumerClosed, &quot;consumer closed&quot;)\t\t\t&#125;\t\t\treturn cm.Message, nil\t\tcase &lt;-ctx.Done():\t\t\treturn nil, ctx.Err()\t\t&#125;\t&#125;&#125;\n\n其中的关键代码：z.pc.availablePermits.inc()\n消费时的逻辑其实和 Java 的 ZeroQueueConsumerImpl 逻辑保持了一致，也是每消费一条数据之前就增加一次 availablePermits。\npulsar-client-go 的运行原理与 Java 客户端的类似，也是将消息存放在了一个内部队列里，所以每次消费消息只需要从这个队列 messageCh 里获取即可。\n值得注意的是， pulsar-client-go 版本的 zeroQueueConsumer 就不支持直接读取内部的队列了。\nfunc (z *zeroQueueConsumer) Chan() &lt;-chan ConsumerMessage &#123;      panic(&quot;zeroQueueConsumer cannot support Chan method&quot;)  &#125;\n\n会直接 panic，因为直接消费 channel 在客户端层面就没法帮用户主动发送 flow 命令了，所以这个功能就只能屏蔽掉了，只可以主动的 receive 消息。\n\n许久之前我也画过一个关于 pulsar client 的消费流程图，后续考虑会再写一篇关于 pulsar client 的原理分析文章。\n参考链接：\n\nhttps://github.com/apache/pulsar-client-go/issues/1223\nhttps://cloud.tencent.com/developer/article/2307608\nhttps://pulsar.apache.org/docs/next/cookbooks-message-queue/#client-configuration-changes\nhttps://github.com/apache/pulsar-client-go/pull/1225\n\n","categories":["OB","Pulsar"],"tags":["Pulsar"]},{"title":"一次消息队列异常堆积的排查","url":"/2024/04/29/ob/pulsar-slow-consume/","content":"背景前两天收到业务反馈有一个 topic 的分区消息堆积了：根据之前的经验来看，要么是业务消费逻辑出现问题导致消费过慢，当然也有小概率是消息队列的 Bug（我们使用的是 pulsar）。\n\n排查通过排查，发现确实是在一点多的时候消息堆积了（后面是修复之后堆积开始下降）。\n于是我在刚才堆积处查看了一条堆积消息的列表：\n获取到其中一条消息的 messageId.\n\n这里本质上使用的是 pulsar-admin 的 API。org.apache.pulsar.client.admin.Topics#peekMessages\n\n再通过这条消息的 id （为了演示，这里的 messageId 可能不一样）在我们的 pulsar 消息链路系统中找到了消息的发送链路：通过这个链路会发现消息一直在推送，但就是没有收到客户端的 ACK 响应。\n\n相关的消息链路埋点可以参考这里：如何编写一个 Pulsar Broker Interceptor 插件\n\n简单来说就是在以下几个 broker 提供的拦截器接口加上埋点数据即可：\n\nmessageProduced\nmessageDispatched\nmessageAcked\n\n既然知道了是客户端没有响应 ACK，那就得知道客户端此时在干什么。\n首先排查了 JVM 内存、CPU 等监控情况，发现一切都挺正常的，这段时间没有明显的尖刺。\nArthas 排查于是便准备使用 arthas 查看下线程的运行情况。\n我们进入到对应 Pod 的容器，执行：\njava -jar arthas-boot.jar\n\n因为 JVM 内存都没有啥异常，所以先看看 thread 的运行堆栈，考虑到是 pulsar 消费线程卡住了，所以我们需要加上线程状态已经过滤下线程的名称：\nthread --state WAITING | grep pulsar\n此时就会列出当前 Java 进程中状态为 WATING 并且线程名称以 pulsar 开头的线程。\n\n我在之前的文章 从 Pulsar Client 的原理到它的监控面板 中分析过客户端的原理。\n\n\n可以知道 pulsar 客户端在其中使用的是 pulsar-打头的线程名称，所以这样就列出了我们需要重点关注的线程。\n我们以图中列出的线程 Id：320 为例：\nthread 320\n\n此时便会打印当前线程的堆栈。\n从上述堆栈中会发现线程一直处于 IO 操作中，看起来是在操作数据库。\n我们再往下翻一翻，会发现上层调用的业务代码：查阅代码得知这是一个数据库的写入操作，看起来是在这个环节数据库响应过慢导致的 pulsar 线程被阻塞了；从而导致消息没有及时 ACK。\n为了最终确认是否由数据库引起的，于是继续查询了当前应用的慢 SQL 情况：\n发现其中有一个查询语句调用频次和平均耗时都比较高，而且正好这个表也是刚才在堆栈里操作的那张表。\n经过业务排查发现这个慢 SQL 是由一个定时任务触发的，而这个定时任务由于某些原因一直也没有停止，所以为了快速解决这个问题，我们先尝试将这个定时任务停掉。\n果然停掉没多久后消息就开始快速消费了：从这个时间线也可以看得出来了，在服务端推送了多次之后终于收到了 ACK。\n修复之后业务再去排查优化这个慢 SQL，这样这个问题就得到根本的解决了。\n更多好用技巧当然 arthas 好用的功能还远不止此，我觉得还有以下功能比较好用：\n火焰图profile：可以输出火焰图，在做性能分析的时候非常有用。\n动态修改内存数据还记得之前我们碰到过一个 pulsar 删除 topic 的 Bug，虽然最终修复了问题，但是在发布修复版本的时候为了避免再次触发老版本的 bug，需要在内存中将某个关键字段的值修改掉。\n而且是不能重启应用的情况下修改，此时使用 arthas 就非常的方便：\ncurl -O https://arthas.aliyun.com/arthas-boot.jar &amp;&amp; java -jar arthas-boot.jar 1 -c &quot;vmtool -x 3 --action getInstances --className org.apache.pulsar.broker.ServiceConfiguration  --express &#x27;instances[0].setTopicLevelPoliciesEnabled(false)&#x27;&quot;\n这里使用的是 vmtool 这个子命令来获取对象，最终再使用 express 表达式将其中的值改为了 false。\n当然这是一个高危操作，不到万不得已不推荐这么使用。\nArthas Tunnel &amp; Web Console这是一个方便开发者通过网页就可以连接到 arthas 的功能，避免直接登录到服务器进行操作。\n我们在研效普通也内置了该功能，让开发排查问题更加方便。\nCPU 使用过多cpu 异常使用排查也是一个非常有用的功能，虽然我们可以通过监控得知 JVM 的 cpu 使用情况，但是没法知道具体是哪个线程以及哪行代码造成的 cpu 过高。\nthread -n 3\n\n\n使用以上命令就可以将 cpu 排名前三的线程打印出来，并且列出他的堆栈情况，这样可以很直观的得知 cpu 消耗了在哪些地方了。\n当然还有一些 trace 查询：\ntrace demo.MathGame run &#x27;#cost &gt; 10&#x27;\n比如这是将调用超过 10ms 的函数打印出来，不过如果我们接入了可观测系统（OpenTelemetry、skywalking等）这个功能就用不太上了。\n\n 还可以在运行的时候不停机修改日志级别，这种在线上排查一些疑难杂症的时候非常好用（通常情况下 debug 日志是不打印的），我们可以将日志级别调整为 debug 打印出更加详细的信息：\n[arthas@2062]$ logger --name ROOT --level debugupdate logger level success.\n\n\n如果是在 kubernetes 环境中执行也有可能碰到 Java 进程启动后没有在磁盘中写入 PID 的情况：\n$ java -jar arthas-boot.jar  [INFO] arthas-boot version: 3.6.7  [INFO] Can not find java process. Try to pass &lt;pid&gt; in command line.  Please select an available pid.\n\n导致直接运行的时候无法找到 Java 进程；此时就需要先 ps 拿到 PID 之后再传入 PID 连入 arthas：\n$ java -jar arthas-boot.jar 1\n\n更多关于 arthas 的用法可以参考官网。\n参考链接：\n\nhttps://pulsar.apache.org/docs/3.2.x/admin-api-topics/#peek-messages\nhttps://crossoverjie.top/2023/12/11/ob/Pulsar-Broker-Interceptor/\nhttps://arthas.aliyun.com/\nhttps://crossoverjie.top/2024/01/09/ob/Pulsar-Delete-Topic/\n\n","categories":["OB"],"tags":["Pulsar"]},{"title":"【译】Apache Pulsar 2023 年度回顾","url":"/2024/01/26/ob/translate-pulsar-2023-year-in-review/","content":"原文链接前两天 Pulsar 社区发布了 2023 年年度回顾，去年我也花了一些时间参与社区，所以其中一些内容感受挺明显的，以下就是对一些重点内容的提炼。\n\n\n2023 年是一个重要的里程碑，参与主仓库贡献的开发者达到了 600 位。自从 Pulsar 从 2018 毕业成为 Apache 顶级项目至今一共又 12K+ 的代码提交次数、639 位贡献者、12.2k star、3.5k fork、10k+ 的 slack 用户。\n2023 高光时刻第一个 LTS 3.0 里程碑版本社区发布 Apache Pulsar 3.0，这是第一个长期支持 （LTS） 版本，从 Pulsar 3.0 开始，可以满足不同用户对稳定性和新功能的需求，同时减轻维护历史版本的负担。\n以往的版本发布周期很短，一般是 3～4 个月，为了可以跟上社区新版，往往需要不停的升级，对维护中的负担较大。\n今后的维护时间表如上图，以稳定为主的团队可以选择 LTS 版本，追求新功能的团队可以选择 feature 版本。\n新的官方网站https://pulsar.apache.org/官方网站得到了新的设计。\nPulsar Admin Go Library提供了 Pulsar Admin Go 的客户端，方便 Go 用户管理 Pulsar 资源\n使用 OTel 增强 Pulsar 的可观测系统PIP-264 提案已经获得了社区批准开始开发，它将解决 topic 数量达到 50k~100M 的可观测性问题。同时 Pulsar 社区已经为 OpenTelemetry 提交了两个特性 Near-zero memory allocations metric filtering upon collection 已经作为了 OpenTelemetry 的规范。\n主要事件回顾2023 年，Pulsar 社区在全球范围内举办了一系列活动。\n\nPulsar Summit Europe 2023\nCommunityOverCode Asia 2023\nCommunityOverCode NA 2023\nPulsar Summit NA 2023\n\n社区成长没有贡献者社区很难发展，2023年加入了许多新面孔。\n\n639 位贡献者\n13.4k Github star\n3.5k fork\n新增 8 位 Committers\n新增 6 位 PMC\n10k+ slack 用户\n20M+ docker pulls\n\n项目发布2023年，社区发布了两个 major version 和 12 个 minor version 版本；最大的里程碑依然是发布了首个 LTS 版本 Pulsar3.0。超过了 140 个贡献者提交了大约 1500 次提交。\n同时也带来了一些重要的特性，比如新版本的负载均衡器，大规模的延时消息支持。\n更新了以下一些客户端：\n\nPulsar C++ Client 3.4.2\nPulsar Go Client 0.11.1\nPulsar Node.js Client 1.9.0\nPulsar Python Client 3.3.0\nPulsar Manager 0.4.0\nPulsar Helm Chart 3.1.0\nPulsar dotnet Client 3.1.1\nReactive Client for Apache Pulsar 0.1.0\n\n生态系统2023 年Pulsar 社区也与多个开源项目进行了集成：\n\nQuarkus Extension for Apache Pulsar，通过事件驱动在 Quarkus 使用 Pulsar。\nSpring for Apache Pulsar 提供了 PulsarTemplate 用于生产消息，PulsarListener 注解可以方便的消费消息，在 spring 生态下更容易集成 Pulsar\nOxia:可以使用 Oxia 提到 zookeeper 从而突破 Pulsar 支持 1M topic 的限制。\n\n2024年计划OTel继续推进使用 OpenTelemetry 替换现有的可观测性系统\n限流重构PIP-322 Pulsar Rate Limiting Refactoring限流重构已经被合并，将在 3.2 版本中发布。\n移除 Pulsar SQL 模块将 SQL 模块移除后有效的减少了镜像大小以及构建时间。\n事件2024 年将会继续举办活动，包括 Pulsar Summit North America 和 Pulsar Summit APAC。在这里可以查看以往的活动。\n🔗参考链接：\n\nhttps://youtube.com/playlist?list=PLqRma1oIkcWhOZ6W-g4D_3JNxJzYnwLNX&amp;si=o6G-fRcNgW9zqHGa\nhttps://github.com/apache/pulsar/wiki/Community-Meetings\nhttps://pulsar.apache.org/blog/2024/01/12/pulsar-2023-year-in-review/\n\n","categories":["翻译"],"tags":["Pulsar"]},{"title":"【译】Apache Pulsar 3.2.0 发布","url":"/2024/02/27/ob/translate-pulsar-3.2.0/","content":"原文链接\nPulsar3.2.0 于 2024-02-05 发布，提供了一些新特性和修复了一些 bug ，共有 57 位开发者提交了 88 次 commit。\n以下是一些关键特性介绍.\n\n速率限制在 3.2 中对速率限制做了重构：PIP-322 Pulsar Rate Limiting Refactoring.\n速率限制器是 Pulsar 服务质量（Qos）保证的重要渠道，主要解决了以下问题：\n\n速率限制器的高 CPU 负载\n大量的锁竞争会影响 Netty IO 线程，从而增加其他 topic 的发送延迟\n更好的代码封装\n\nTopic 压缩时会删除 Null-key 消息Pulsar 支持 Topic 压缩，在 3.2 之前的版本中 topic 压缩时会保留 Null key 的消息。\n从 3.2.0 开始将会修改默认行为，默认不会保留，这可以减少存储。如果想要恢复以前的策略可以在 broker.conf 中新增配置：\ntopicCompactionRetainNullKey=true\n具体信息请参考：PIP-318.\nWebSocket 的新特性\n支持多个 topic 消费：PIP-307.\n端对端加密 PIP-290.\n\nCLI 的用户体验改进\nCLI 可以配置内存限制\n允许通过正则或者是文件批量删除 topic\n通过 pulsar-admin clusters list 可以打印当前使用的 cluster\n\n构建系统的改进3.2.0 中引入了PIP-326: Bill of Materials(BOM) 来简化依赖管理。\n参与其中Pulsar 是发展最快的开源项目之一，被 Apache 基金会评选为参与度前五的项目，社区欢迎对开源、消息系统、streaming 感兴趣的参与贡献🎉，可以通过以下资源与社区保持联系：\n\n阅读贡献手册  Apache Pulsar Contribution Guide 开始你的第一个贡献。\n访问 Pulsar GitHub repository, 关注 @apache_pulsar 的 Twitter&#x2F;X , 加入 slack 社区 Pulsar community on Slack.\n\n🔗参考链接：\n\nhttps://github.com/apache/pulsar/blob/master/pip/pip-318.md\nhttps://pulsar.apache.org/docs/3.2.x/concepts-topic-compaction/\nhttps://github.com/apache/pulsar/blob/master/pip/pip-322.md\nhttps://github.com/apache/pulsar/blob/master/pip/pip_307.md\nhttps://github.com/apache/pulsar/blob/master/pip/pip-290.md\nhttps://github.com/apache/pulsar/pull/20663\nhttps://github.com/apache/pulsar/pull/20614\nhttps://github.com/apache/pulsar/blob/master/pip/pip-326.md\nhttps://pulsar.apache.org/contribute/\n\n","categories":["翻译"],"tags":["Pulsar"]},{"title":"深入理解单元测试：技巧与最佳实践","url":"/2024/08/15/ob/unit-test/","content":"之前分享过如何快速上手开源项目以及如何在开源项目里做集成测试，但还没有讲过具体的实操。\n今天来详细讲讲如何写单元测试。\n🤔什么情况下需要单元测试这个大家应该是有共识的，对于一些功能单一、核心逻辑、同时变化不频繁的公开函数才有必要做单元测试。\n对于业务复杂、链路繁琐但也是核心流程的功能通常建议做 e2e 测试，这样可以保证最终测试结果的一致性。\n\n\n💀具体案例我们都知道单测的主要目的是模拟执行你写过的每一行代码，目的就是要覆盖到主要分支，做到自己的每一行代码都心中有数。\n下面以 Apache HertzBeat 的一些单测为例，讲解如何编写一个单元测试。\n先以一个最简单的 org.apache.hertzbeat.collector.collect.udp.UdpCollectImpl#preCheck 函数测试为例。这里的 preCheck 函数就是简单的检测做参数校验。测试时只要我们手动将 metrics 设置为 null 就可以进入这个 if 条件。\n@ExtendWith(MockitoExtension.class)class UdpCollectImplTest &#123;    @InjectMocks    private UdpCollectImpl udpCollect;    @Test    void testPreCheck() &#123;        List&lt;String&gt; aliasField = new ArrayList&lt;&gt;();        aliasField.add(&quot;responseTime&quot;);        Metrics metrics = new Metrics();        metrics.setAliasFields(aliasField);        assertThrows(IllegalArgumentException.class, () -&gt; udpCollect.preCheck(metrics));    &#125;&#125;    \n\n来看具体的单测代码，我们一行行的来看：\n@ExtendWith(MockitoExtension.class) 是 Junit5 提供的一个注解，里面传入的 MockitoExtension.class 是我们单测 mock 常用的框架。\n简单来说就是告诉 Junit5 ，当前的测试类会使用 mockito 作为扩展运行，从而可以 mock 我们运行时的一些对象。\n\n@InjectMocks  private UdpCollectImpl udpCollect;\n\n@InjectMocks 也是 mockito 这个库提供的注解，通常用于声明需要测试的类。\n@InjectMocks  private AbstractCollect udpCollect;\n\n需要注意的是这个注解必须是一个具体的类，不可以是一个抽象类或者是接口。\n其实当我们了解了他的原理就能知道具体的原因：\n当我们 debug 运行时会发现 udpCollect 对象是有值的，而如果我们去掉这个注解 @InjectMocks 再运行就会抛空指针异常。\n\n因为并没有初始化 udpCollect\n\n而使用 @InjectMocks注解后，mockito 框架会自动给 udpCollect 注入一个代理对象；而如果是一个接口或者是抽象类，mockito 框架是无法知道创建具体哪个对象。\n当然在这个简单场景下，我们直接 udpCollect = new UdpCollectImpl() 进行测试也是可以的。\n🔥配合 jacoco 输出单测覆盖率\n在 IDEA 中我们可以以 Coverage 的方式运行，IDEA 就将我们的单测覆盖情况显示在源代码中，绿色的部分就代表在实际在运行时执行到的地方。\n我们也可以在 maven 项目中集成 jacoco，只需要添加一个根目录的 pom.xml 中添加一个 plugin 就可以了。\n&lt;plugin&gt;      &lt;groupId&gt;org.jacoco&lt;/groupId&gt;      &lt;artifactId&gt;jacoco-maven-plugin&lt;/artifactId&gt;      &lt;version&gt;$&#123;jacoco-maven-plugin.version&#125;&lt;/version&gt;      &lt;executions&gt;          &lt;execution&gt;              &lt;goals&gt;                  &lt;goal&gt;prepare-agent&lt;/goal&gt;              &lt;/goals&gt;          &lt;/execution&gt;          &lt;execution&gt;              &lt;id&gt;report&lt;/id&gt;              &lt;phase&gt;test&lt;/phase&gt;              &lt;goals&gt;                  &lt;goal&gt;report&lt;/goal&gt;              &lt;/goals&gt;          &lt;/execution&gt;      &lt;/executions&gt;  &lt;/plugin&gt;\n\n之后运行 mvn test 就会在 target 目录下生成测试报告了。\n\n我们还可以在 GitHub 的 CI 中集成 Codecov，他会直接读取 jacoco 的测试数据，并且在 PR 的评论区加上测试报告。\n\n\n需要从 Codecov 里将你项目的 token 添加到 repo 的 环境变量中即可。\n具体可以参考这个 PR：https://github.com/apache/hertzbeat/pull/1985\n☀️复杂一点的单测刚才展示的是一个非常简单的场景，下面来看看稍微复杂的。\n我们以这个单测为例：org.apache.hertzbeat.collector.collect.redis.RedisClusterCollectImplTest\n@ExtendWith(MockitoExtension.class)public class RedisClusterCollectImplTest &#123;        @InjectMocks    private RedisCommonCollectImpl redisClusterCollect;    @Mock    private StatefulRedisClusterConnection&lt;String, String&gt; connection;    @Mock    private RedisAdvancedClusterCommands&lt;String, String&gt; cmd;    @Mock    private RedisClusterClient client;&#125;\n\n这个单测在刚才的基础上多了一个 @Mock 的注解。\n这是因为我们需要测试的 RedisCommonCollectImpl 类中需要依赖 StatefulRedisClusterConnection/RedisAdvancedClusterCommands/RedisClusterClient 这几个类所提供的服务。\n单测的时候需要使用 mockito 创建一个他们的对象，并且注入到需要被测试的 RedisCommonCollectImpl类中。\n\n不然我们就需要准备单测所需要的资源，比如可以使用的 Redis、MySQL 等。\n\n🚤模拟行为只是注入进去还不够，我们还需要模拟它的行为：\n\n比如调用某个函数可以模拟返回数据\n模拟函数调用抛出异常\n模拟函数调用耗时\n\n这里以最常见的模拟函数返回为例：\n\nString clusterNodes = connection.sync().clusterInfo();\n在源码里看到会使用 connection 的 clusterInfo() 函数返回集群信息。\nString clusterKnownNodes = &quot;2&quot;;String clusterInfoTemp = &quot;&quot;&quot;        cluster_slots_fail:0        cluster_known_nodes:%s        &quot;&quot;&quot;;String clusterInfo = String.format(clusterInfoTemp, clusterKnownNodes);Mockito.when(cmd.clusterInfo()).thenReturn(clusterInfo);        \n\n此时我们就可以使用 Mockito.when().thenReturn() 来模拟这个函数的返回数据。\n而其中的 cmd 自然也是需要模拟返回的：\nMockito.mockStatic(RedisClusterClient.class).when(()-&gt;RedisClusterClient.create(Mockito.any(ClientResources.class),        Mockito.any(RedisURI.class))).thenReturn(client);Mockito.when(client.connect()).thenReturn(connection);Mockito.when(connection.sync()).thenReturn(cmd);Mockito.when(cmd.info(metrics.getName())).thenReturn(info);Mockito.when(cmd.clusterInfo()).thenReturn(clusterInfo);\n\ncmd 是通过 Mockito.when(connection.sync()).thenReturn(cmd);返回的，而 connection 又是从 client.connect() 返回的。\n最终就像是套娃一样，client 在源码中是通过一个静态函数创建的。\n⚡模拟静态函数我依稀记得在我刚接触 mockito 的 16～17 年那段时间还不支持模拟调用静态函数，不过如今已经支持了：\n@Mock  private RedisClusterClient client;Mockito.mockStatic(RedisClusterClient.class).when(()-&gt;RedisClusterClient.create(Mockito.any(ClientResources.class),          Mockito.any(RedisURI.class))).thenReturn(client);\n这样就可以模拟静态函数的返回值了，但前提是返回的 client 需要使用 @Mock 注解。\n💥模拟构造函数有时候我们也需要模拟构造函数，从而可以模拟后续这个对象的行为。\nMockedConstruction&lt;FTPClient&gt; mocked = Mockito.mockConstruction(FTPClient.class,        (ftpClient, context) -&gt; &#123;            Mockito.doNothing().when(ftpClient).connect(ftpProtocol.getHost(),                    Integer.parseInt(ftpProtocol.getPort()));            Mockito.doAnswer(invocationOnMock -&gt; true).when(ftpClient)                    .login(ftpProtocol.getUsername(), ftpProtocol.getPassword());            Mockito.when(ftpClient.changeWorkingDirectory(ftpProtocol.getDirection())).thenReturn(isActive);            Mockito.doNothing().when(ftpClient).disconnect();        &#125;);\n可以使用 Mockito.mockConstruction 来进行模拟，该对象的一些行为就直接写在这个模拟函数内。\n需要注意的是返回的 mocked 对象需要记得关闭。\n不需要 Mock当然也不是所有的场景都需要 mock。\n比如刚才第一个场景，没有依赖任何外部服务时就不需要 mock。\n\n类似于这个 PR 里的测试，只是依赖一个基础的内存缓存组件，就没必要 mock，但如果依赖的是 Redis 缓存组件还是需要 mock 的。https://github.com/apache/hertzbeat/pull/2021\n⚙️修改源码如果有些测试场景下需要获取内部变量方便后续的测试，但是该测试类也没有提供获取变量的函数，我们就只有修改源码来配合测试了。\n比如这个 PR：\n当然如果只是给测试环境下使用的函数或变量，我们可以加上 @VisibleForTesting注解标明一下，这个注解没有其他作用，可以让后续的维护者更清楚的知道这是做什么用的。\n📈集成测试单元测试只能测试一些功能单一的函数，要保证整个软件的质量仅依赖单测是不够的，我们还需要集成测试。\n通常是需要对外提供服务的开源项目都需要集成测试：\n\nPulsar\nKafka\nDubbo 等\n\n以我接触到的服务型应用主要分为两类：一个是 Java 应用一个是 Golang 应用。\n🐳Golang\nGolang 因为工具链没有 Java 那么强大，所以大部分的集成测试的功能都是通过编写 Makefile 和 shell 脚本实现的。\n还是以我熟悉的 Pulsar 的 go-client 为例，它在 GitHub 的集成测试是通过 GitHub action 触发的，定义如下：最终调用的是 Makefile 中的 test 命令，并且把需要测试的 Golang 版本传入进去。\n\nDockerfile：\n这个镜像简单来说就是将 Pulsar 的镜像作为基础运行镜像（这里面包含了 Pulsar 的服务端），然后将这个 pulsar-client-go 的代码复制进去编译。\n接着运行：\ncd /pulsar/pulsar-client-go &amp;&amp; ./scripts/run-ci.sh\n也就是测试脚本。\n\n测试脚本的逻辑也很简单：\n\n启动 pulsar 服务端\n运行测试代码因为所有的测试代码里连接服务端的地址都是 localhost，所以可以直接连接。\n\n通过这里的 action 日志可以跟踪所有的运行情况。\n☕Java\nJava 因为工具链强大，所以集成测试几乎不需要用 Makefile 和脚本配合执行。\n还是以 Pulsar 为例，它的集成测试是需要模拟在本地启动一个服务端（因为 Pulsar 的服务端源码和测试代码都是 Java 写的，更方便做测试），然后再运行测试代码。\n\n这个的好处是任何一个单测都可以在本地直接运行，而  Go 的代码还需要先在本地启动一个服务端，测试起来比较麻烦。\n\n来看看它是如何实现的，我以其中一个 BrokerClientIntegrationTest为例：会在单测启动的时候先启动服务端。\n\n最终会调用 PulsarTestContext 的 build 函数启动 broker（服务端），而执行单测也只需要使用 mvn test 就可以自动触发这些单元测试。只是每一个单测都需要启停服务端，所以要把 Pulsar 的所有单测跑完通常需要 1～2 个小时。\n以上就是日常编写单测可能会碰到的场景，希望对大家有所帮助。\n","categories":["OB"],"tags":["单测"]},{"title":"✅开源项目如何做集成测试","url":"/2024/07/09/ob/%E2%9C%85%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%A6%82%E4%BD%95%E5%81%9A%E9%9B%86%E6%88%90%E6%B5%8B%E8%AF%95/","content":"之前有朋友问如何做集成测试，今天就重点讲讲这个集成测试在开源项目中是如何做的。\n通常是需要对外提供服务的开源项目都需要集成测试：\n\nPulsar\nKafka\nDubbo 等\n\n而只提供本地类库的项目通常只需要编写单元测试即可：\n\nHutool\nApache Commmon\n\n以我接触到的服务型应用主要分为两类：一个是 Java 应用一个是 Golang 应用。\n🐳GolangGolang 因为工具链没有 Java 那么强大，所以大部分的集成测试的功能都是通过编写 Makefile 和 shell 脚本实现的。\n还是以我熟悉的 Pulsar 的 go-client 为例，它在 GitHub 的集成测试是通过 GitHub action 触发的，定义如下：最终调用的是 Makefile 中的 test 命令，并且把需要测试的 Golang 版本传入进去。\n\nDockerfile：\n这个镜像简单来说就是将 Pulsar 的镜像作为基础运行镜像（这里面包含了 Pulsar 的服务端），然后将这个 pulsar-client-go 的代码复制进去编译。\n接着运行：\ncd /pulsar/pulsar-client-go &amp;&amp; ./scripts/run-ci.sh\n也就是测试脚本。\n\n测试脚本的逻辑也很简单：\n\n启动 pulsar 服务端\n运行测试代码因为所有的测试代码里连接服务端的地址都是 localhost，所以可以直接连接。\n\n通过这里的 action 日志可以跟踪所有的运行情况。\n☕JavaJava 因为工具链强大，所以集成测试几乎不需要用 Makefile 和脚本配合执行。\n还是以 Pulsar 为例，它的集成测试是需要模拟在本地启动一个服务端，然后再运行测试代码。\n\n这个的好处是任何一个单测都可以在本地直接运行，而  Go 的代码还需要先在本地启动一个服务端，测试起来比较麻烦。\n\n来看看它是如何实现的，我以其中一个 BrokerClientIntegrationTest为例：会在单测启动的时候先启动服务端。\n\n最终会调用 PulsarTestContext 的 build 函数启动 broker（服务端），而执行单测也只需要使用 mvn 就可以自动触发这些单元测试。只是每一个单测都需要启停服务端，所以要把 Pulsar 的所有单测跑完通常需要 1～2 个小时。\n所以这些集成测试本质上都是先要把测试环境构建出来，再跑对应的测试代码；后续也打算给 cim 加上集成测试实操一下。\n","categories":["OB"]},{"title":"在多语言的分布式系统中如何传递 Trace 信息","url":"/2025/08/13/ob/%E5%9C%A8%E5%A4%9A%E8%AF%AD%E8%A8%80%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BC%A0%E9%80%92%20Trace%20%E4%BF%A1%E6%81%AF/","content":"背景\n前段时间有朋友问我关于 spring cloud 的应用在调用到 Go 的 API 之后出现 Trace 没有串联起来的问题。\n完整的调用流程如下：\n┌──────┐             │Client│             └┬─────┘             ┌▽──────────────────┐│SpringCloud GateWay│└┬──────────────────┘┌▽──────────────┐    │SpringBoot(app)│    └┬──────────────┘    ┌▽──────────┐        │Feign(http)│        └┬──────────┘        ┌▽─────┐             │Go Gin│             └──────┘             \n\n\n根因在解决这个问题之前想要搞清楚 Trace 是如何跨语言以及跨应用传递的。\n其实也可以类比为在分布式系统中如何传递上下文；既然要传递数据那就涉及到系统之间的调用，也就是我们常说的 RPC（remote procedure call)。\n提到 PRC 我们常见的一般有两种协议：\n\n基于 HTTP 协议，简单易读，兼容性好\n基于 TCP 的私有协议，高效性能更佳\n\n基于 TCP 私有协议的又诞生出许多流行的框架，比如：\n\nDubbo\nThrift\ngRPC(基于 HTTP2,严格来说不算私有协议)\n基于 MQ 实现的 RPC（生产消费者模式，本质上这些 MQ 都是私有协议，比如 RocketMQ、Pulsar 等）\n\n但我们需要在 RPC 调用的过程中在上下文里包含 Trace 时，通常都是将 TraceId 作为元数据进行传递。\n对于 HTTP 来说就是 header、而其余的私有 TCP 协议通常也会提供一个元数据的结构用于存放一些非业务数据。\n\n比如在 OpenTelemetry-Go 的 sdk 中，会在一次 RPC 中对 Trace 数据进行埋点。\n最终也是使用 metadata metadata.MD 来获取上下文。\n\n\n在 Pulsar 中是将 TraceId 存放在 properties 中，也相当于是元数据。\n\n┌──────┐│Client│└┬─────┘┌▽─────┐│Pulsar│└┬─────┘┌▽───┐  │gRPC│  └────┘  \n\n\nfunc (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) &#123;      defer apiCounter.Add(ctx, 1)      md, _ := metadata.FromIncomingContext(ctx)      log.Printf(&quot;Received: %v, md: %v&quot;, in.GetName(), md)      name, _ := os.Hostname()      span := trace.SpanFromContext(ctx)      span.SetAttributes(attribute.String(&quot;request.name&quot;, in.Name))      s.span(ctx)      return &amp;pb.HelloReply&#123;Message: fmt.Sprintf(&quot;hostname:%s, in:%s, md:%v&quot;, name, in.Name, md)&#125;, nil  &#125;\n\n\n在这样一次调用中如果我们将 Pulsar 的 properties 和 gRPC meta 打印出来将会看到 TraceID 是如何进行传递的。\n解决回到这个问题本身，Trace 在 Gin Server 端没有关联起来，明显就是 Gin 没有接收到上游的 TraceId，导致它认为是新的一次调用，从而会创建一个 Trace。\n解决起来也很容易，只需要在启动 Gin 的时候传入一个 OTEL 提供的拦截器，在这个拦截器中 OTEL 的 sdk 会自动从 HTTP header 里解析出 TraceId 然后塞入到当前的 context 中，这样两个进程的 Trace 就可以关联起来了。\n相关代码如下：\nr := gin.New()r.Use(otelgin.Middleware(&quot;my-server&quot;))\n\n\n由于 Go 没有提供类似于 Java 的 javaagent 扩展，这类原本可以全自动打桩的代码都需要硬编码实现。\n\n在这个 otelgin 实现的 Middleware 里会使用 HTTP header 来传输 context。\n\n\n本质上是操作 HTTP header 查询和写入 Trace\n\n会首先获取上游的 TraceID，这里的 traceparentHeader 也就是我们刚才看到的 traceparent。\n如果获取到了就会解析里面的 TraceID，并生成当前的 Context，这样这个 context 就会一直往后传递了。\n\n流程与上文提到 gRPC 类似。\n\n这是目前 otel-go-sdk 支持的自动打桩框架，目前看来还不太多，但常用的也都支持了。\n总结如何跨进程调的 Trace 信息都是通过网络传递的，只是对于不同的协议传输的细节也各不相同，但原理都是类似的。\n\n关键就是上面这两张图，进程 1 在调用进程 2 的时候将信息写入进去，进程 2 在收到请求的时候解析出 Trace，这两个步骤缺一不可。#Blog \n","categories":["OpenSource","OpenTelemetry"],"tags":["OpenSource"]},{"title":"如何在平淡的工作中整理出有价值的简历","url":"/2024/12/10/ob/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B9%B3%E6%B7%A1%E7%9A%84%E5%B7%A5%E4%BD%9C%E4%B8%AD%E6%95%B4%E7%90%86%E5%87%BA%E6%9C%89%E4%BB%B7%E5%80%BC%E7%9A%84%E7%AE%80%E5%8E%86/","content":"今天在 HackNews 上看到一个帖子：你们是否很难回忆起在工作中做了哪些贡献？\n\n我觉得挺多人都有类似的问题，通常都是在需要面试或者内部晋升的时候才开始思考这些问题，这时候在想的话难免会有遗漏。\n结合帖子里的回答我整理了以下以下方法。\n\n\n每日记录好记性不如烂笔头，每日做好工作记录，周末再做一次汇总；有部分公司应该就有类似的制度（日报、周报），但那是写给公司看的，这是写给自己整理的；对自己来说只需要整理有用的内容，去掉那些工作中需要的废话。\n这里推荐可以使用 Obsidian 的 daily 插件，每天点击一下日历就会生成一份文档，周末的时候再点击就会自动将周一到周五的内容进行汇总。\n\n建议是在做之前就记录下来，而不是等到今天结束了再记录，此时要么不想记，要么已经忘了。周末汇总的时候可以提炼下，如果是要写到简历里应该怎么写？\n\n同样的观点我在播客代码之外中也有听到，每天在下班的时候进行总结有以下的好处：\n\n更有仪式感，做完这个后一天的工作就结束了。\n可以学会将任务切分，提高对工作的掌控感。\n长期坚持下来可以增强对任务完成时间的准确预估。\n\nGit log 汇总还可以使用 git log --author=&#39;&lt;Your Name&gt; 汇总你对提交记录，然后交给 AI 帮我们总结，这个感觉更适合做工作汇报。\ngit log --pretty=format:&quot;%h - %an, %ar : %s&quot; --author=&#x27;crossoverJie&#x27; | pbcopy\n在 macOS 中可以使用 pbcopy 命令将输出内容复制到粘贴板，然后我们只需要复制到 chatgpt 中就可以帮我们提炼总结了（但这里的前提是自己的每次提交都是有意义的备注）\ngit log --author=&quot;your_username&quot; --since=&quot;2024-01-01&quot; --until=&quot;2024-12-31&quot;\n也可以加上时间筛选，更加精确的统计。\n定时更新简历我个人是建议每个季度都更新一下自己的简历，看看有哪些新的东西可以写上去，这也是回顾自己这段时间工作的有效手段，毕竟简历就是要给人看的美化版自己。\n 这个帖子还有提到面试时不要害怕写自己不熟的技术栈，即便是只在自己的个人项目中使用过（看来国外和我们也类似）\n这个我觉得得是面试情况而定，如果应聘的 1~3 年的初中级岗位，也不是大厂，那可以这么写，但对于业界都知道的一些大厂（比如阿里、字节）这些面试大概率不会只问表面问题，技术栈写的越多对自己也越没有好处。\n本质上就是需要大家多总结，多参考。\n参考链接：\n\nhttps://news.ycombinator.com/item?id=41937892\nhttps://www.xiaoyuzhoufm.com/episode/65954bca6d045a7f5e7a9286\n\n","categories":["OB"]},{"title":"如何选择可以搞钱的技术栈","url":"/2024/11/26/ob/%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E5%8F%AF%E4%BB%A5%E6%90%9E%E9%92%B1%E7%9A%84%E6%8A%80%E6%9C%AF%E6%A0%88/","content":"前言之前在公司主要负责可观测性和 Pulsar 消息队列相关的内容，最近系统比较稳定，只需要做日常运维，所以就抽出时间逐步在接触 OLAP 相关的技术栈。\n我们用的是 StarRocks，也是目前比较流行的 OLAP 数据库；在接触的这段时间以来，让我越发感觉到选对一个靠谱的技术方向的重要性。\n\n\n这里以 Pulsar 举例，Pulsar 也是 Apache 的顶级项目，有一定的技术门槛，同时也解决以往消息队列的一些问题（多租户、低延迟、存算分离等） 但如果把它作为一个商业产品的话，相对来说付费能力还不够强。\n其实也能想得到，就单纯的消息中间件在市场上能讲的故事不多，可以将它融入到其他的生态里，比如数据处理、业务解耦等，但总归是配角，没有它虽然没那么优雅，但也可以玩。\n同理 OpenTelemetry 也是类似的，它用于可观测性系统，主要是就是拿来排查问题的，对于企业来说也谈不上刚需。\n他们两个都有一个共同的特点：小公司不需要（或者自己维护开源版，量小也不容易暴露问题），大公司选择单独的团队自己维护，市场蛋糕较小。\n即便是部分中厂可能选择购买云服务，一般也会选择和自己现有技术栈配套的云厂商，比如已经用了大部分阿里云的产品，这种周边服务也会尽可能的在阿里云上选择替代方案，毕竟这样的风险更小，而且国内的产品大多都写还 ALL IN。\n\n可能更多的还是一些政企、金融等业务会选择这些开源产品的企业版，大部分因为需要私有化部署，不太方便直接使用公有云。\n\n而更刚需的往往都是和数据相关的，比如数据库、MongoDB、ElasticSearch 、kubernetes 等，如果想要提升下非业务水平，倒是可以深入一下这些技术栈。\n举例拿数据库来说，任何公司都需要，即便是小公司也不敢在生产环境自己维护数据库，一般也会购买云产品，或者是招一个 DBA。\n同理还有云原生相关的基础技术栈，比如 kubernetes 以及围绕着 kubernetes 周边的生态。\nk8s 作为云原生的基础底座，只要涉及到上云就离不开它，不管小厂选择云服务还是大厂自己托管都得需要相关技能。\n即便不是直接做 kubernetes 开发也得需要了解相关的知识，对自己理解整个系统是如何运转的很大的帮助。\n除此之外还有也有个简单的方法：就是看看你们公司为哪些服务买单。\n以我最近接触到的 StarRocks 为例，也是和数据处理相关的公司，他们在疫情期间成立的商业化公司，这几年非但没受到影响反而还在增长。\n\n\n看到他们的招聘还蛮活跃。\n\n即便是厂商更倾向于选择云厂商的数据服务，StarRocks 这类原厂公司或多或少也会参与进去提供一些技术支持。\n不过可能有人的第一反应是这些产品的技术门槛较高，上手比较困难，但其实这些往往都是自己给自己上的难度。\n以我最近提交的一个 PR 来说:https://github.com/StarRocks/starrocks/pull/50926\n我之前根本就没有接触过 OLAP 相关的内容，但只要有场景，可以在本地 debug 复现，任何问题都很好解决，即便是这是你不熟的技术栈。\n比如 ES 也是 Java 写的，如果你们公司为它付费了，那不如多花时间研究一下，不一定是需要改它的核心逻辑，上层也有很多东西可以参与的。\n而一旦我们对这些技术栈熟悉之后，今后在换工作时就有着其他人不具备的优势，甚至可以加入这些技术背后的商业公司。\n而这些公司大部分都是满足开发者的喜好：比如远程办公、技术驱动等，大家不妨从现在就可以试试。\n总结不管是哪种技术最终都是要转换为我们到手的收入，所以选择对收入更加敏感的技术栈还是很有必要的。\n以上的内容主要是针对后端开发，当然这里并不包含想要做独立开发的技术栈，主要还是用于求职。\n大家可以看看自己公司以及曾经的公司有对哪些技术付费，或者是一些都需要的刚需通用的技术栈，深入这些技能对搞钱或多或少都有好处。\n","categories":["OB"]},{"title":"推荐一些值得学习的开源项目和框架","url":"/2024/11/20/ob/%E6%8E%A8%E8%8D%90%E4%B8%80%E4%BA%9B%E5%80%BC%E5%BE%97%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%92%8C%E6%A1%86%E6%9E%B6/","content":"今天收到球友的问题，让推荐一些值得看的开源项目，觉得 netty 这些太复杂了不太好上手。\n确实如此，我们日常常用的 Spring、Netty 确实由于发展了多年，看起来比较头大。\n下面我来推荐一些我看过同时觉得不错的项目(几乎都是我参与过的），由易到难，其中也会包含 Java 和 Go 的项目，包含主流的中间件和云原生项目。\n\n\nJava 项目xxl-job难度：🌟🌟推荐指数：🌟🌟🌟\n\nxxl-job 是一个很经典的调度框架，目前在 GitHub 上也有 27k star 的关注，因为功能不复杂所以最近也没有怎么更新了。\n大家日常都会使用这类调度框架，所以理解难度非常低，加上他的实现也比较简单，比如：\n\n使用 MySQL 的锁来简单粗暴的解决分布式锁的问题\n线程池的使用：因为每个任务的调度都需要尽可能的互相不影响，所以里面大量使用了线程池，同时对如何获取异步任务结果也有一些最佳实践。\nRPC 调用：里面内置了一个 RPC 框架，也是作者编写的，其中的实现原理也不复杂，建议看看源码，可以更好的理解我们在工作中用到 rpc 框架。\n\ncim难度：🌟🌟🌟推荐指数：🌟🌟🌟这里夹了一点私货，就是我自己开源的一个分布式即时通讯系统，其实现在来看上一个版本的代码写的挺烂的，不过好在最近发布了 v2.0.0，提升了不少代码质量。\n它具备 IM 即时通讯的基本功能，同时基于它可以实现：\n\n即时通讯\n消息推送\nIOT 消息平台\n\n通过 cim 你可以学习到分布式系统中：\n\n元数据是如何存放和同步的。\nRPC 调用如何实现。\n长链接系统如何实现。\n复杂的分布式系统如何做集成测试等。\n\n详细的介绍可以查看项目首页的 readme，发现有什么需要优化的地方（其实还蛮多 todo 没有做）都欢迎提交 PR。\nPowerJob难度：🌟🌟🌟推荐指数：🌟🌟🌟🌟PowerJob 也是一个调度框架，只是他有后发优势，结合了市面上其他调度系统的优点同时也新增了一些功能，以下是他功能的官方对比图：社区相对于 xxl-job 也更加活跃，目前刚发布了 5.1.0 版本，同时社区也整理许多学习的文章和资料：\n\n它使用了 Akka 来实现远程通信，对这部分内容感兴趣的朋友不容错过，可以看到一些最佳实践。其中的代码写的也很规范，一些类的设计很好，可扩展性很高，比如常用的执行器都是通过一个MapProcessor 扩展而来的。\n推荐大家从任务调度那一块开始看：tech.powerjob.worker.actors.TaskTrackerActor#onReceiveServerScheduleJobReq\nPulsar难度：🌟🌟🌟🌟推荐指数：🌟🌟🌟🌟Pulsar 是目前主流的云原生消息队列中间件，现在使用的公司也非常多，通过他你可以学习到：\n\nAPI 设计：Pulsar 的 client 是直接面向开发者的，在易用性的前提下每次迭代升级还要考虑到兼容性。\n异步调用：Pulsar 里几乎所有的请求都是异步的，所以大量使用了异步➕回调（虽然也有一些坑），可以学到一些高性能代码的编写方式。\nNetty 的最佳用法：消息收发的底层网络框架也是 Netty 支撑的，Pulsar 对它做了封装。\n基于 protocol 的多语言客户端。\n因为 Pulsar 的通信编解码使用的是 protocol，本身是可以基于它生成各种语言的 API，所以在此基础上编写其他语言的客户端就非常方便。\n\n\n\n不过由于 Pulsar 本身的复杂性，上手起来门槛还是不低，推荐先从客户端的代码（Java 和  Go 的都可以）上手。\nStarRocks难度：🌟🌟🌟🌟🌟推荐指数：🌟🌟🌟🌟\nStarRocks 也是我最近才接触到的 OLAP 数据库项目，以前对这个领域的积累几乎为零，所以也是从头学习。\n好在这段时间因为有需求也给它提交了几个 PR，逐渐熟悉起来了。\n我接触下来这些开源项目，发现 StarRocks 这类数据库项目是最有前（钱）景的，毕竟和数据打交道的产品公司的付费意愿会更高一些。\n不过该项目确实对新手不太友好，最好是已经接触过大数据领域再学习会更合适一些，但也不要怕，我就是一个纯小白，没基础就跟着代码 debug，反正都是 Java 写的总能看懂。\n这里推荐先看看我之前写的本地搭建开发环境，这样就可以在 idea 里 debug 了。\nOpenTelemetry难度：🌟🌟🌟🌟推荐指数：🌟🌟🌟🌟🌟OpenTelemetry 现在作为云原生可观测性的事实标准，现在已经逐步成为各大公司必备的技术栈了。\n通过一个 javaagent 就可以自动采集应用的 trace、metrics、logs 等数据，这里先推荐 opentelemetry-java-instrumentation，因为我们日常使用最多的就是基于这个项目打包出来的 javaagent，通过它可以学习到：\n\n如何编写任意函数的拦截器\ntrace 信息是如何在线程和进程之间传递的\n一些常用框架是如何运行的\n比如你需要了解 gRPC 的原理，就可以查看 OpenTelemetry 是如何对他埋点的，从而知晓他的核心原理。\n\n\n优雅的 API 设计\n\n同时 OpenTelemetry 算是我看过最优雅的代码之一了，非常建议大家都看看。\n如果对 OpenTelemetry 还不太熟悉，可以先看看我之前写过的文章。\nGo（云原生项目）cprobe难度：🌟🌟🌟推荐指数：🌟🌟🌟\ncprobe 属于可观测性项目，他的目的是可以把各种 exporter 都整合在一起，比如 kafka_exporter, nginx_exporter, mysql_exporter 等。\n同时还做了上层抽象，可以统一管理各种监控对象的配置，这样就可以部署一个进程监控所有的应用了。\n通过这个项目可以学到：\n\n监控体系的基础知识，比如 Prometheus 和 metrics 等\nGo 语言的基本用法\n\n我之前写过一篇 手把手教你为开源项目贡献代码就是以 cprobe 为例来介绍的。\nVictoriaLogs难度：🌟🌟🌟🌟推荐指数：🌟🌟🌟🌟\n这是一个属于 VictoriaMetrics 的一个子项目，通过这个名字应该会知道他主要用于处理日志，可以把他理解为 ElasticSearch 的简易版，虽然功能简单了但资源消耗也会比 ES 低很多，具体可以看下面的压测图：\n\n通过这个项目可以学到：\n\n数据在磁盘中是如何存储和查询的\nGo 语言中关于 goroutine 和 channel 的一些最佳实践目前的版本还比较早，所以代码都不太复杂，建议大家可以从查询的入口开始看起。\n\n总结以上都是我正经接触过的项目，如果是想长期耕耘同时搞钱的话，推荐 StarRocks，目前也很火。\n如果只是想提升在 Java 领域的水平，那推荐 Pulsar 和 OpenTelemetry，都有很多代码最佳实践。\n如果想要入坑云原生和 Go 项目，那 cprobe 是比较合适的。\n当然不管是哪个项目最主要的还是坚持，很多项目如果只是偶尔看一下很容易忘记，起码要做到真正运行起来然后 debug 过代码。\n参考链接：\n\nhttps://www.yuque.com/powerjob/guidence/wu2e93\nhttps://github.com/VictoriaMetrics/VictoriaMetrics/blob/master/lib/logstorage/storage_search.go\nhttps://crossoverjie.top/tags/OpenTelemetry/\nhttps://crossoverjie.top/2024/10/09/ob/StarRocks-dev-env-build/\n\n","categories":["OpenSource"],"tags":["OpenSource"]},{"title":"🎉我是如何从零到成为 Apache 顶级项目的 Committer","url":"/2024/07/11/ob/%F0%9F%8E%89how-to-be-committer/","content":"\n最近收到了 Apache Pulsar 和 Apache HertzBeat社区的邀请邮件，成为了这两个项目的 Committer。\n\n一路走来我从最开始的打游击战的闲散人员到如今活跃在各个开源项目里的“老兵”，用现在流行的话来说 Apache 的这两个 Committer 就相当于是拿到了编制，进入了正规军。\n下面就分享一下我的个人开源经历，希望对想要参与开源或者已经在其中的开发者有所帮助。\n我的 GitHub 开源故事初识 GitHub我这个 Github 账号是在 15 年 9月份注册的，那时候刚出来参与工作。\n其实在这之前我压根没有听说过 GitHub、对开源也是知之甚少；只是知道老师和同事经常让我在网上可以下载到一些第三方包（现在回想起来几乎都是好 Apache 的提供的包）来解决日常的一些常见需求。\n当时只是觉得非常方便，没想到大部分的工作互联网上都有相关的解决方案。\n直到第二年也就是 16 年我才提交第一行代码，记得当时是需要和同学共享一些代码。\n在学校的时候大家都是把文件打包然后通过 QQ 发送的，因为我之前在 GitHub 上看到大家都是把源码公开的，所以当时的想法是不是可以直接使用 GitHub 把代码发给同学，这样就省去了打包解压的步骤了。\n\n现在想想还好都是一些非业务代码，不然就违反公司安全规定了。\n\n所以其实自己没有任何开源的概念，只是觉得分享代码很方便。\n后续在网上看了 Richard Matthew Stallman 发起的自由软件运动才对开源的由来有了更多的认识，也越发佩服这些参与开源的大佬们。\n托管 Blog\n当时还非常流行在 GitHub 上搭建个人博客，我自然也跟上了这个潮流；直到现在也没有断更。陆续写了 240+ 篇博客。\n\n记得当时最喜欢干的事就是折腾各种主题，可以在 GitHub 免费托管一个博客，对当时的我也是震撼蛮大的。\n\n关于博客的话题还有不少内容可以讲，放到后面继续分享。\n提交第一个项目因为当时在公司刚开始接触到 SSM(spring+springmvc+mybatis)，所以就想把日常学到的东西沉淀下来。\n于是就把一些非业务代码整理后提交了第一个项目，以更新博客的方式陆续更新了各种解决方案：至今已经全部更新完毕，所以我就将它归档了。\n这也是我第一次正儿八经做开源项目，在这个过程中也结实到了不少朋友，沉淀了许多内容；对于刚工作一两年的我来说意义还是很重大的。\n参与正规军(Apache)时间点回到现在，因为工作原因我需要在公司内部维护 Pulsar 消息队列；当时 Pulsar 在公司还有着一些细枝末节的问题需要解决。\n在解决这些问题的过程中就想着看能不能给社区贡献些代码，这样也可以更熟悉整个项目。\n\n其实 20 年左右在之前的公司就有使用 Pulsar，只是当时还没有意识到要向社区贡献代码。\n\n于是我先尝试做一些无关紧要的修改：因为这个还被大佬拒过几个 PR，与此同时我也在持续输出一个 Pulsar 相关的博客，当时也得到了大佬的认可：\n之后我又根据日常工作中遇到的一些问题或者优化持续给社区提交 PR：\n这个过程从第一个 PR 到社区大佬提名我大概经历了一年半的时间。\n越大型、严谨的项目在处理这些 PR 时就是缓慢的，所以如果你真的想深度参与某个项目时就一定要有充分的耐心。\n首先坚持下去，收获自然就来了。\n\nApache HertzBeat今年四月份的时候我在朋友圈还看到另外一个项目：Apache HertzBeat。\n因为当时我也在做一些可观测性的内容，正好这个项目是和监控相关的；于是我就跟着文档走了一遍。\n发现功能很强也很全，当时也是刚加入 Apache 的孵化器，所以还是有许多可以完善的地方。\n我就开始以单测作为切入点尝试贡献源码，社区的响应速度也非常快。\n之后逐渐将我在其他社区学到一些经验也复制到 HertzBeat 中，慢慢的贡献的代码越多，对 HertzBeat 也就更加熟悉了。\n两个多月的时间我贡献了 30 个左右的 PR，后来也受到项目发起者的邀请：\n因为是相对更年轻的项目，才更需要大家群策群力；所以如果你也对监控系统感兴趣，或者比较熟悉前端技术栈（HertzBeat 有后台管理界面）都欢迎前来贡献，后续获得提名的机会要比已经发展稳定的项目更大一些。\n成为 Committer 的好处讲到这里顺便再讲讲成为  Committer 的一些好处了，虽然开源经常和免费白嫖划等号，大部分人都是用爱发电的，但因为也有许多大公司得到了开源的好处，所以也给活跃在社区里的贡献者提供了一些免费福利。\n当然要拿到这些福利肯定是得有一个评判标准，最简单也最直观的就是你是否已经是 Apache 组织的 Committer。\nGithub Copilot首先第一个好处是提供免费个人使用 Copilot，当然这不全是 Committer 的权益，如果你是某个开源项目的活跃贡献者也是可以申请的（不一定能申请过，目前好像没看到通过的标准），只是已经是 Committer 后肯定是能享受这个权益。\nJetbrains 全家桶 IDE\nJB 作为一个和开发者强绑定的公司，也提供了对应的福利，只要使用 Apache 的邮箱就可以免费使用他们的全家桶。\nApache 邮箱提到了邮箱那就不得不提到 Apache 给每个 Committer 都会提供一个专属邮箱：虽然市面上有各种的免费邮箱注册服务，但当你使用 Apache 的邮箱和其他人沟通交流时，大概率对方潜意识里都会对你高看一点。\n这虽然是一些虚无缥缈的东西，但有时候就是会让沟通更加顺畅（比如求职面试时）。\n项目的写权限还有一个好处就是有了项目的写权限，当你参与过开源项目就知道这个的重要性了，有些时候一些 PR 迟迟得不到回复和合并，自己只能干着急。\n有了这个权限之后，只要你的 PR 有人 Approve 之后，在风险可控的情况下不用等着 maintainer 来合并，自己就可以操作。\n同时得益于在社区的活跃程度，你再提交到 PR 会更得到重视，同时也能更好的推进某些 feature；这对于依赖某个开源项目的公司来说受益非常大。\nApache 贡献阶梯相信看到这里应该有不少人对成为 Apache Committer 感兴趣了，也比较好奇什么样的标准才能成为 Committer。\n以下是我根据一些已经是 Committer 的大佬和 Apache 官方给的一个贡献阶梯作为参考总结出来的。\n\n参与开源的人主要分为以下几种角色：\n\n普通用户\n贡献者\nCommitter\nPMC 项目管理人员\n基金会管理人员\n基金会董事\n\n整个路径还是比较清晰的，只是从 PMC 开始到后面的董事难度都是指数级增加。\n\n目前整个国内当选过董事的都是屈指可数。\n\n而关于成为 Committer 的要求某些社区会有明显的标准：\n当然这个标准也不是一成不变的，只要持续的在社区活跃，有脸熟之后自然会有相关的 PMC 为你提名；当然这里的前提条件都是“持续活跃”。\n总结最后再总结下，为爱发电的开源项目也是可以获得回报的；特别是当你合并一个 PR 进入某个项目时带来的愉悦感非常强烈。\n随着时间推进，在之后合并的 PR 可能没有前几次那么强烈，但只要达到一个范围，社区开始提名你为 Committer 时，这个多巴胺又会持续分泌。\n同样的后续成为 PMC、管理人员、董事又会持续带来愉悦，当然难度也一个比一个大。\n后面的层级离我还很远，如果今后有达到的一天再来和大家分享。\n参考链接：\n\nhttps://community.apache.org/contributor-ladder.html\nhttps://hertzbeat.apache.org/zh-cn/docs/community/become_committer\nhttps://zh.wikipedia.org/wiki/%E8%87%AA%E7%94%B1%E8%BD%AF%E4%BB%B6%E8%BF%90%E5%8A%A8\n\n","categories":["OB"]},{"title":"💢线上高延迟请求排查","url":"/2024/10/29/ob/%F0%9F%92%A2%E7%BA%BF%E4%B8%8A%E9%AB%98%E5%BB%B6%E8%BF%9F%E8%AF%B7%E6%B1%82%E6%8E%92%E6%9F%A5/","content":"前几天排查了一个业务接口执行高延迟的问题，也挺有参考意义的，分享一下排查过程。\n现象是业务反馈有一个接口业务逻辑其实很简单，但是调用一次耗时，如下图所示：\n\n排查应用运行状态首先第一步需要查看当时的应用运行状态，包含当时的日志、JVM 的各种监控等。\n因为我们接入了 OpenTelemetry，所以 trace 和日志是可以关联起来的。\n\n点击链路系统旁边的日志按钮可以直接跳转。\n\n可以通过 trace_id 查询到相关日志：\n通过日志可以看出耗时大约在 4s 多一点，然后结合代码发现这两段日志分别是在进入一个核心业务方法之前和方法内打印的。\n\n而第一行日志是在一个自定义限流器中打印的，这个限流器是使用 Guava 的 RateLimiter实现的。\n我的第一反应是不是这个限流器当时限流了，从而导致阻塞了；但查看了当时的 QPS 发现完全低于限流器的配置，所以基本可以排除它的嫌疑了。\nJVM 监控\n\n之后我们查询当时的 JVM 监控发现当时的 GC  频繁，而堆内存也正好发生了一次回收，初步判断是 GC 导致的本次问题。\n但为啥会导致频繁的 GC 呢，还需要继续排查。\n内存排查我们在应用诊断中集成了 Pyroscope的持续剖析，可以实时查看内存的占用情况。\n\n通过内存分析发现有大量的 JSON 序列化占用了大量的内存，同时还发现 Pod 已经被重启好几次了：\n\n查看原因发现是 Pod OOM 导致的。\n因此非常有可能是 GC 导致的，恰好那段时间发生了 GC 内存也有明显变化。\n\n\n\n最后再通过 arthas 确认了 GC 非常频繁，可以确认目前的资源是是非常紧张的，咨询业务之后得知该应用本身占用的资源就比较大，没有太多优化空间，所以最终决定还是加配置。还是提高硬件效率最高，目前运行半个月之后 Pod 内存表现稳定，没有出现一次 OOM 的异常。\n总结虽然最后的处理的方式是简单粗暴的，但其中的过程还是有意义的，遇到不同的情况也有不同的处理方式。\n比如在排查过程中发现内存消耗异常，通过内存分析发现代码可以优化，那就优化代码逻辑。\n如果是堆内存占用不大，但是 Pod 还是 OOM 导致重启，那就要看看 JVM 的内存分配是否合理，应该多预留一些内存给堆外使用。\n但这个过程需要有完善的可观测系统的支撑，比如日志、监控等，如果没有这些数据，再回头排查问题就会比较困难。\n总之这个排查过程才是最主要的，大家还有什么排查问题的小 tips 也欢迎在评论区分享。\n","categories":["问题排查"],"tags":["Java"]},{"title":"🤳如何为复杂的 Java 应用编写集成测试","url":"/2024/09/29/ob/%F0%9F%A4%B3cim-support-integration-test/","content":"最近有时间又把以前开源的 IM 消息系统捡起来继续开发了（确实这些年经常有朋友催更）。\n\n没错，确实是这些年，因为上次发版还是再 2019 年的八月份。\n\n这段时间比较重大的更新就是把元数据中心抽离出来了，以前是和 zookeeper 的代码强耦合在一起的，重构之后可以有多种实现了。\n\n\n今后甚至可以提供一个 jar 包就可以把后端服务全部启动起来用于体验，此时就可以使用一个简单的基于内存的注册中心。\n除此之外做的更多的就是新增了一个集成测试的模块，没有完善的集成测试功能在合并代码的时候都要小心翼翼，基本的功能需求都没法保证。\n加上这几年我也接触了不少优秀的开源项目（比如 Pulsar、OpenTelemetry、HertzBeat 等），他们都有完整的代码合并流程；首先第一点就得把测试流水线跑通过。\n这一点在 OpenTelemetry 社区更为严格：\n\n\n他们的构建测试流程非常多，包括单元测试、集成测试、代码风格、多版本兼容等。\n\n所以在结合了这些优秀项目的经验后我也为 cim 项目新增相关的模块 cim-integration-test，同时也在 github 上配置了相关的 action，最终的效果如下：\n\n在 “Build with Maven” 阶段触发单元测试和集成测试，最终会把测试结果上传到 Codecov，然后会在 PR 的评论区输出测试报告。\n相关的 action 配置如下：\n\n就是配置了几个 Job，重点是这里的：\nmvn -B package --file pom.xml\n\n它会编译并运行项目下面的所有 test 代码。\ncim-integration-test 模块为了方便进行集成测试，我新增了 cim-integration-test 这个模块，这里面没有任何源码，只有测试相关的代码。\n\n类的继承关系图如下：\n\n因为我们做集成测试需要把 cim 所依赖的服务都启动起来，目前主要由以下几个服务：\n\ncim-server: cim 的服务端\ncim-route: 路由服务\ncim-client: 客户端\n\n而 route 服务是依赖于 server 服务，所以 route 继承了 server，client 则是需要 route 和 server 都启动，所以它需要继承 route。\n集成 test container先来看看 server 的测试实现：\npublic abstract class AbstractServerBaseTest &#123;        private static final DockerImageName DEFAULT_IMAGE_NAME = DockerImageName              .parse(&quot;zookeeper&quot;)              .withTag(&quot;3.9.2&quot;);        private static final Duration DEFAULT_STARTUP_TIMEOUT = Duration.ofSeconds(60);        @Container      public final ZooKeeperContainer              zooKeeperContainer = new ZooKeeperContainer(DEFAULT_IMAGE_NAME, DEFAULT_STARTUP_TIMEOUT);        @Getter      private String zookeeperAddr;        public void startServer() &#123;          zooKeeperContainer.start();          zookeeperAddr = String.format(&quot;%s:%d&quot;, zooKeeperContainer.getHost(), zooKeeperContainer.getMappedPort(ZooKeeperContainer.DEFAULT_CLIENT_PORT));          SpringApplication server = new SpringApplication(CIMServerApplication.class);          server.run(&quot;--app.zk.addr=&quot; + zookeeperAddr);      &#125;  &#125;\n\n因为 server 是需要依赖 zookeeper 作为元数据中心，所以在启动之前需要先把 zookeeper 启动起来。\n此时就需要使用 testcontainer 来做支持了，使用它可以在单测的过程中使用 docker 启动任意一个服务，这样在 CI 中做集成测试就很简单了。\n\n我们日常使用的大部分中间件都是支持的，使用起来也很简单。\n先添加相关的依赖：\n&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.postgresql&lt;/groupId&gt;        &lt;artifactId&gt;postgresql&lt;/artifactId&gt;        &lt;version&gt;42.7.3&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;        &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;        &lt;version&gt;1.5.6&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt;        &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt;        &lt;version&gt;5.10.2&lt;/version&gt;        &lt;scope&gt;test&lt;/scope&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;\n\n然后在选择我们需要依赖的服务，比如是 PostgreSQL：\n&lt;dependency&gt;    &lt;groupId&gt;org.testcontainers&lt;/groupId&gt;    &lt;artifactId&gt;postgresql&lt;/artifactId&gt;    &lt;version&gt;1.19.8&lt;/version&gt;    &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;\n\n然后在测试代码中启动相关的服务\nclass CustomerServiceTest &#123;  static PostgreSQLContainer&lt;?&gt; postgres = new PostgreSQLContainer&lt;&gt;(    &quot;postgres:16-alpine&quot;  );  CustomerService customerService;  @BeforeAll  static void beforeAll() &#123;    postgres.start();  &#125;  @AfterAll  static void afterAll() &#123;    postgres.stop();  &#125;  @BeforeEach  void setUp() &#123;    DBConnectionProvider connectionProvider = new DBConnectionProvider(      postgres.getJdbcUrl(),      postgres.getUsername(),      postgres.getPassword()    );    customerService = new CustomerService(connectionProvider);  &#125;\n\n通常情况下我们都是需要获取这些中间件的链接，比如 IP 端口啥的。\norg.testcontainers.containers.ContainerState#getHostorg.testcontainers.containers.ContainerState#getMappedPort\n通常是通过这两个函数来获取对应的 IP 和端口。\n集成@Container  RedisContainer redis = new RedisContainer(DockerImageName.parse(&quot;redis:7.4.0&quot;));    public void startRoute() &#123;      redis.start();      SpringApplication route = new SpringApplication(RouteApplication.class);      String[] args = new String[]&#123;              &quot;--spring.data.redis.host=&quot; + redis.getHost(),              &quot;--spring.data.redis.port=&quot; + redis.getMappedPort(6379),              &quot;--app.zk.addr=&quot; + super.getZookeeperAddr(),      &#125;;        route.setAdditionalProfiles(&quot;route&quot;);      route.run(args);  &#125;\n\n对于 route 来说不但需要 zookeeper 还需要 Redis 来存放用户的路由关系，此时就还需要运行一个 Redis 的容器，使用方法同理。\n最后就需要以 springboot 的方式将这两个应用启动起来，我们直接创建一个 SpringApplication 对象，然后将需要修改的参数通过 --varname=value 的形式将数据传递进去。\n还可以通过 setAdditionalProfiles() 函数指定当前应用运行的 profile，这样我们就可以在测试目录使用对应的配置文件了。\n\nroute.setAdditionalProfiles(&quot;route&quot;);  \n\n比如我们这里设置为 route 就可以使用 application-route.yaml 作为 route 的配置文件启动，就不用每个参数都通过 -- 传递了。\nprivate void login(String userName, int port) throws Exception &#123;      Long userId = super.registerAccount(userName);      SpringApplication client = new SpringApplication(CIMClientApplication.class);      client.setAdditionalProfiles(&quot;client&quot;);      String[] args = new String[]&#123;              &quot;--server.port=&quot; + port,              &quot;--cim.user.id=&quot; + userId,              &quot;--cim.user.userName=&quot; + userName      &#125;;      client.run(args);  &#125;    @Test  public void olu() throws Exception &#123;      super.startServer();      super.startRoute();      this.login(&quot;crossoverJie&quot;, 8082);      this.login(&quot;cj&quot;, 8182);      MsgHandle msgHandle = SpringBeanFactory.getBean(MsgHandle.class);      msgHandle.innerCommand(&quot;:olu&quot;);      msgHandle.sendMsg(&quot;hello&quot;);  &#125;\n\n我们真正要测试的其实是客户端的功能，只要客户端功能正常，说明 server 和 route 也是正常的。\n比如这里的 olu(oline user) 的测试流程是：\n\n启动 server 和 route\n登录注册两个账号\n查询出所有用户\n发送消息\n\n最终的测试结果如下，符合预期。\n\n碰到的问题应用分层不知道大家注意到刚才测试代码存在的问题没有，主要就是没法断言。\n因为客户端、route、server 都是以一个应用的维度去运行的，没法获取到一些关键指标。\n比如输出在线用户，当客户端作为一个应用时，在线用户就是直接打印在了终端，而没有直接暴露一个接口返回在线数据；收发消息也是同理。\n其实在应用内部这些都是有接口的，但是作为一个整体的 springboot 应用就没有提供这些能力了。\n本质上的问题就是这里应该有一个 client-sdk 的模块，client 也是基于这个 sdk 实现的，这样就可以更好的测试相关的功能了。\n之后就准备把 sdk 单独抽离一个模块，这样可以方便基于这个 sdk 实现不同的交互，甚至做一个 UI 界面都是可以的。\n编译失败还有一个问题就是我是直接将 client/route/server 的依赖集成到 integration-test 模块中：\n&lt;dependency&gt;    &lt;groupId&gt;com.crossoverjie.netty&lt;/groupId&gt;    &lt;artifactId&gt;cim-server&lt;/artifactId&gt;    &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt;    &lt;scope&gt;compile&lt;/scope&gt;  &lt;/dependency&gt;    &lt;dependency&gt;    &lt;groupId&gt;com.crossoverjie.netty&lt;/groupId&gt;    &lt;artifactId&gt;cim-forward-route&lt;/artifactId&gt;    &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt;    &lt;scope&gt;compile&lt;/scope&gt;  &lt;/dependency&gt;    &lt;dependency&gt;    &lt;groupId&gt;com.crossoverjie.netty&lt;/groupId&gt;    &lt;artifactId&gt;cim-client&lt;/artifactId&gt;    &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt;    &lt;scope&gt;compile&lt;/scope&gt;  &lt;/dependency&gt;\n\n在 IDEA 里直接点击测试按钮是可以直接运行这里的测试用例的，但是想通过 mvn test 时就遇到了问题。\n\n会在编译期间就是失败了，我排查了很久最终发现是因为这三个模块应用使用了springboot 的构建插件：\n&lt;plugin&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\t&lt;executions&gt;\t\t&lt;execution&gt;\t\t\t&lt;goals&gt;\t\t\t\t&lt;goal&gt;repackage&lt;/goal&gt;\t\t\t&lt;/goals&gt;\t\t&lt;/execution&gt;\t&lt;/executions&gt;&lt;/plugin&gt;\n\n这几个模块最终会被打包成一个 springboot 的 jar 包，从而导致 integration-test 在编译时无法加载进来从而使用里面的类。\n暂时没有找到好的解决办法，我就只有把这几个插件先去掉，需要打包时再手动指定插件。\nmvn clean package spring-boot:repackage -DskipTests=true\n\n其实这里的本质问题也是没有分层的结果，最好还是依赖 route 和 server 的 SDK 进行测试。\n现在因为有了测试的 CI 也欢迎大家来做贡献，可以看看这里的 help want，有一些简单易上手可以先搞起来。\n\nhttps://github.com/crossoverJie/cim/issues/135\n参考链接：\n\nhttps://github.com/crossoverJie/cim/pull/140\nhttps://github.com/crossoverJie/cim/pull/144\n\n","categories":["cim","test"],"tags":["cim"]},{"title":"如何参与一个顶级开源项目","url":"/2019/08/19/open%20source/%20how%20to%20contribute%20open-source%20project/","content":"\n前言最近个人事情比较多（搬家、换工作、短暂休息）所以一直也没有顾得上博客更新，恰好最近收到一封邮件提醒了我。\n\n也是时候写一篇文章来聊聊参与开源项目的事（最近也确实进入了笔荒期）。\nps:第一次收到这样的中秋节礼物，加上 Dubbo 社区的活跃及阿里的重视度，还在做 PRC 或微服务技术选型的朋友可以考虑 Dubbo。\n\n\n参与开源现在具体来聊聊参与开源的事；\n日常几乎所有的开发者都会享受到开源项目所带来的便利甚至是收益，受限于环境早在十几年前甚至几年前开源活动一直都是有国外开发者主导。\n但这几年国内互联网公司逐渐国际化扩大影响力也很大程度的提高了我们的开发水平，以 BAT 为首出现了许多优秀的开源项目。\n现在甚至参与开源项目还能另辟蹊径的拿到大厂 offer，所以其实不少朋友都想参与其中，可能这事给人的第一感觉就不太容易，所以现在还卡在第一步。\n具体步骤以下是以我个人经验总结的几大步骤：\n\n发现问题或自荐 feature 。\nfork 源码。\n本地开发、自测。\n发起 pull request 。\n等待社区 Code Review 。\n跟进社区意见调整代码。\n审核通过，合并进 master 分支，完成本次贡献。\n\n下面我会结合最近一次参与 Dubbo 的流程来具体聊聊。\n发现问题或自荐 feature首先第一步自然要搞清楚自己本次贡献的内容是什么？通常都是解决某个问题或者是提交一个新的 feature ；前者相对起来更加容易一些。\n当然这个问题可以是自己使用过程中发现的，也可以是 Issues 列表中待解决的问题。\n以本次为例，就是我在使用过程中所发现的问题，也提交了相关 Issue 并写了一篇文章记录并解决了该问题：What？一个 Dubbo 服务启动要两个小时！\n值得注意的是在提交 Issue 之前最好是先在 Issue 列表中通过关键字检索下是否已经有相关问题，避免重复。\n同时提交之后也许社区会进行跟进，被打上 invalid 标签认为不是问题，或者是使用姿势不对也是有可能的。\nfork 源码，本地开发当确定这是一个待修复的问题时就可以着手开发了。\n首先第一步自然是将源码拷贝一份到自己仓库中。\n\n接着只需要 clone 自己仓库中的源码到本地进行开发。\n先回顾下我遇到的这个问题。\n\n简单来说就是启动 Dubbo 服务非常缓慢，经过定位是 main 线程阻塞在了获取本机 ip 处。\n所以当时我提出的方案是：在获取本机 ip 时加上超时时间，一旦超时便抛出异常或者是再次重试，但起码得有日志方便用户定位问题。\n问题是主线程会一直阻塞在此处 InetAddress.getLocalHost().getHostAddress()，但又需要知道它阻塞了多久才好判断是否超时。\n所以只能再额外开启一个线程，定时去检测 main 线程是否已经完成任务了，以下便是我第一次 pr 的内容。\n\n这次的重点不是讨论这里具体的技术细节，所以简单说下步骤：\n\n额为声明了大小为 1 的线程池。\n再声明了一个 volatile 标志用于判断主线程是否有完成任务。\n声明了一个 condition 用于新线程做等待。\n最后只需要运行这个线程用于判断这个标志即可。\n\n如何自测开发完成后下一步就是自测，由于这类项目是作为一个基础包依赖于其他的项目才能运行的，所以通常我们还得新建一个项目来配合做全流程测试（单测除外）。\n这里我觉得还是有几个小技巧值得注意。\n第一个是版本号；因为在本地测试，所以需要使用 mvn clean install 将包安装到本地才能在其他项目中依赖进去进行测试。\n但由于我们从官方拉出来的代码版本都已经发布到了 maven 中央仓库中（不管是 release 还是 snapshot），所以我们本地仓库中肯定已经存在这几个版本的 jar 包。\n一旦我们执行 mvn clean install 将自己修改的代码安装到本地时，大概率是会出问题的（也可能是我姿势不对），这样就会导致新建的项目中依赖不了自己新增的代码。\n所以我通常的做法是修改版本号，这个版本号是从来没有被官方发布到中央仓库中的，可以确保自己新增的代码会以一个全新版本安装到本地，这样我们再依赖这个版本进行测试即可。\n\n不过再提交时得注意不要把这个版本号提交上去了。\n\n发起 pull request自测完成后便可发起 pull request 了，不要大意，这里还得有一个地方需要注意，那就是代码换行符的问题。\n一旦换行符与源仓库的不一致时，git 会认为这次修改是删除后重来的，这样会给 code review 带来巨大的麻烦。\n\n就像这样，明明我改动的行数并不多，但 git 确认为你是推翻了重来，导致审核起来根本不知道你改了哪些地方。\n最简单的方法就是设置自己 git 的全局配置，可以参考这里。\n# 提交时转换为LF，检出时转换为CRLFgit config --global core.autocrlf true# 提交时转换为LF，检出时不转换git config --global core.autocrlf input# 提交检出均不转换git config --global core.autocrlf false\n\n\n确认没问题后便可点击这里发起 pull request，后面按照引导执行即可。\n当然各个项目之间还会有自己定制的贡献流程，最好就是查看官方的贡献指南。\nhttp://dubbo.apache.org/en-us/docs/developers/contributor-guide/new-contributor-guide_dev.html\nCode Reviewpr 发起后便可等待社区审核了。\n在这过程中要充分和社区进行交流，有可能你的方案和社区的想法并不一致。\n比如像我这次：\n最终通过沟通加上自己后面的思考觉得还是社区的方案更加轻便合理一些，达成一致之后社区便将这次 pr 合并进 master 中。\n其实整个过程我觉得最有意义的便是 code review 的过程，所有人都可以参与其中头脑风暴，其中也不乏技术大牛，不知不觉便能学到不少东西。\n类似案例虽然我之前的方案没有被采纳，但类似的用法（一个线程监控其他线程）还是不少，正好在 Dubbo 中也有用到。\n便是其中核心的服务调用，默认情况下对使用者来说这看起来是一个同步调用，也就是说消费方会等待 PRC 执行完毕后才会执行后续逻辑。\n但其实在底层这就是一个 TCP 网络包的发送过程，本身就是异步的。\n只是 Dubbo 在你不知道的情况下做了异步转同步，这样看起来就像是一个同步方法。\n\n如图中的红框部分，Dubbo 自身调用了 get() 方法用于同步获取服务提供者的返回结果。\n\n逻辑其实也挺简单，和我上文的方案类似，只是这里的 isDone() 函数返回的是是否已经拿到了服务提供者的返回值而已。\n总结本次总结了参与开源的具体步骤，其实也挺简单；就如官方所说哪怕是提个 Issue，修改一个错别字都算是参与，所以不要想的太难。\n最后还简单分析了 Dubbo 调用过程中的异步转同步的过程，掌握这些操作对自己平时开发也是很有帮助的。\n你的点赞与分享是对我最大的支持\n","categories":["open source"],"tags":["dubbo","thread","多线程"]},{"title":"1.6W star 的 JCSprout 阅读体验大提升","url":"/2018/11/06/personal/1W-star-update/","content":"\n\n万万没想到 JCSprout 截止目前居然有将近1.6W star。真的非常感谢各位大佬的支持。\n\n年初时创建这个 repo 原本只是想根据自己面试与被面试的经历记录一些核心知识点，结果却是越写越多。\n\n\n\n\n在我自己宣传和其他技术大佬(包括阮大)的助攻之下连续两个月都在 GitHub trending Java片区的榜首。\n甚至有一次还一跃到整个 GitHub 的第一，同时还有帮助一些同学拿到了大厂 offer。\n\n扯了这么多进入这次的正题。\n之前有一朋友建议将文档以 gitbook 的形式查看，一直没有时间弄。直到有一天我看到了 docsify 这个项目，瞬间被它的外观，阅读方式所吸引。于是抽了一晚上把所有的文章全部迁移过去。\n现在打开 https://crossoverjie.top/JCSprout/ 即可看到全新的主页，大概长这样：\n\n\n\n确实不管从颜值还是阅读方式来说都非常不错；希望新的界面能让更多的人看的进去学到点东西。\n\n 同时也更新完善了其中的一些内容。比如有些写的早的内容其实并不完善，也优化的处理了。\n\n同时欢迎更多朋友参与进来，不管是提新的点子、修改 bug 都是可以。\n之前的文章也留了不少坑，包括 cicada 还有好几个 bug 待处理、推送的示例代码以及 Kafka 源码的后续更新。\n突然有点像写长篇小说的感觉，还好没有多少人催更🤣。\n不出意外本周会再更新一篇，请持续关注。\n你的点赞与转发是最大的支持。\n","categories":["Person","GitHub"]},{"title":"GitHub 1W star 成就达成！","url":"/2018/08/17/personal/1W-star/","content":"\n起因感谢各位大佬的支持收获了人生第一个（很有可能也是唯一一个）1W star 项目。\n\n从今年一月份创建项目至今 8 个月时间。\n一共关闭了 27 个 issue，47 个 RP，总共有 11 位小伙伴参与维护。\n神奇般的连续两个月上了 GitHub Java 热门榜首。\n\n\n\n\n整个热度走势图也是一路向北：\n\n过程中也有许多朋友反馈得到了帮助，自己确实没想到能起到这么好的作用。\n更名趁这机会我想给项目重新换个名字，因为我发现做到现在这里面并不仅仅包含面试的内容。\n我们也不应该只为了面试而使用该项目，里面所有的技术内容我认为都应该对实际开发起到帮助，让大家少走弯路。\n所有我把项目重命名为：\nJCSprout : Java Core Sprout：处于萌芽阶段的 Java 核心知识库。\n还创建了 logo。\n\n整个技术道路非常漫长，这一点小成就只是一颗小萌芽，希望最后能生根发芽。\n未来今后我依然会持续维护，也希望更多的朋友参与进来。\n还要把之前给自己挖的坑填好：\n\nHTTP \nSpringBoot\nTomcat 等\n\n后期还会继续添加一些源码解析、最佳实践等内容。\n还没有关注的朋友难道不想进来看看嘛？最新地址：\nhttps://github.com/crossoverJie/JCSprout\n你的点赞与转发是最大的支持。\n","categories":["Person","GitHub"]},{"title":"一个学渣的阿里之路","url":"/2018/06/21/personal/Interview-experience/","content":"\n前言最近有些朋友在面试阿里，加上 Java-Interview 项目的原因也有小伙伴和我讨论，近期也在负责部门的招聘，这让我想起年初那段长达三个月的奇葩面试经历🤣。\n本来没想拿出来说的，毕竟最后也没成。\n但由于那几个月的经历让我了解到了大厂的工作方式、对候选同学的考察重点以及面试官的套路等都有了全新的认识。\n当然最重要的是这段时间的查漏补缺也让自己精进不少。\n先交代下背景吧：\n从去年 12 月到今年三月底，我前前后后面了阿里三个部门。\n其中两个部门通过了技术面试，还有一个跪在了三面。\n光看结果还不错，但整个流程堪称曲折。\n下面我会尽量描述流程以及大致的面试题目大纲，希望对想要跳槽、正在面试的同学带来点灵感，帮助可能谈不上，但启发还是能有。\n以下内容较长，请再次备好瓜子板凳。\n\n\n\nA 部门首先是第一次机会，去年 12 月份有位大佬加我，后来才知道是一个部门的技术 Leader 在网上看到我的博客，问我想不想来阿里试试。\n这时距离上次面阿里也过去一年多了，也想看看现在几斤几两，于是便同意了。\n在推荐一周之后收到了杭州打来的电话，说来也巧，那时候我正在机场候机，距离登记还有大概一个小时，心想时间肯定够了。\n那是我时隔一年多第一次面试，还是在机场这样嘈杂的环境里。多多少少还是有些紧张。\n一面以下是我印象比较深刻的内容：\n面试官：\n谈谈你做过项目中印象较深或自认为做的比较好的地方？\n博主：\n我觉得我在 XX 做的不错，用了 XX 需求实现 XX 功能，性能提高了 N 倍。\n面试官：\n你说使用到了 AOP ，能谈谈它的实现原理嘛？\n博主：\n它是依靠动态代理实现的，动态代理又分为 JDK 自身的以及 CGLIB 。。。。\n面试官：\n嗯，能说说他们的不同及优缺点嘛？\n博主：\nJDK 是基于接口实现，而 CGLIB 继承代理类。。。\n就是这样会一直问下去，如果聊的差不多了就开始问一些零散的问题：\n\nJMM 内存模型，如何划分的？分别存储什么内容？线程安全与否？\n类加载机制，谈到双亲委派模型后会问到哪些违反了双亲委派模型？为什么？为什么要双亲委派？好处是什么？\n平时怎么使用多线程？有哪些好处？线程池的几个核心参数的意义？\n线程间通信的方式？\nHashMap 的原理？当谈到线程不安全时自然引申出 ConcurrentHashMap ，它的实现原理？\n分库分表如何设计？垂直拆分、水平拆分？\n业务 ID 的生成规则，有哪些方式？\nSQL 调优？平时使用数据库有哪些注意点？\n当一个应用启动缓慢如何优化？\n\n大概是以上这些，当聊到倒数第二个时我已经登机了。最后不得不提前挂断，结束之前告诉我之后会换一个同事和我沟通，听到这样的回复一面应该是过了，后面也确实证实了这点。\n二面大概过了一周，二面如期而至。\n我听声音很熟，就尝试问下是不是之前一面的面试官，结果真是。\n由于二面的面试官临时有事所以他来替一下。于是我赶紧问他能否把之前答的不好的再说说？的到了肯定的答复后开始了我的表演。\n有了第一次的经验这一次自然也轻车熟路，原本感觉一切尽在掌握却被告知需要笔试突然被激醒。\n笔试是一个在线平台，需要在网页中写代码，会有一个明确的题目：\n\n从一个日志文件中根据关键字读取日志，记录出现的次数，最后按照次数排序打印。\n\n在这过程中切记要和面试官多多交流，因为笔试有时间限制，别到最后发现题目理解错了，这就和高考作文写完发现方向错了一样要命。\n而且在沟通过程中体现出你解题的思路，即使最终结果不对，但说不定思考的过程很符合面试官的胃口哦。这也和今年的高考改卷一样；过程正确得高分，只有结果得低分。\n三面又过了差不多一周的时间接到了三面的电话，一般到了三面会是技术 Leader 之类的角色。\n这个过程中不会过多强调技术细节，更多的考察软件能，比如团队协作、学习能力等。\n但我记得也问了以下一些技术问题：\n\n谈谈你所理解的 HTTP 协议？\n对 TCP 的理解？三次握手？滑动窗口？\n基本算法，Base64 等。\nJava 内存模型，Happen Before 的理解。\n\n一周之后我接到了 HR 助理的电话约了和 HRBP 以及产品技术负责人的视频面试。\n但是我却没有面下去，具体原因得往下看。\nB 部门在 A 部门三面完成后，我等了差不多一星期，这期间我却收到了一封邮件。\n大概内容是他在 GitHub 上看到的我，他们的技术总监对我很感兴趣（我都不敢相信我的眼镜），问我想不想来阿里试试。\n我对比了 A B 部门的区别发现 B 部门在做的事情上确实更加有诱惑力，之后我表达了有一个面试正在流程中的顾虑；对方表示可以私下和我快速的进行三面，如果一切没问题再交由我自行选择。至少对双方都是一个双赢嘛。\n我想也不亏，并且对方很有诚意，就答应试试；于是便有了下面的面试：\n一面面试官：\n对 Java 锁的理解？\n博主：\n我谈到了 synchronize，Lock 接口的应用。\n面试官：\n他们两者的区别以及优缺点呢？\n博主：\nsynchronize 在 JDK1.6 之前称为重量锁，是通过进出对象监视器来实现同步的；1.6 之后做了 XX 优化。。。\n而 ReentrantLock 是利用了一个巧妙数据结构实现的，并且加锁解锁是显式的。。。\n之后又引申到分布式锁，光这块就聊了差不多半个小时。\n之后又聊到了我的开源项目：\n\n是如何想做这个项目的？\n已经有一些关注了后续是如何规划的？\n你今后的学习计划是什么？\n平时看哪些书？\n\n之后技术聊的不是很多，但对于个人发展却聊了不少。\n\n关于锁相关的内容可以参考这里：ReentrantLock 实现原理 synchronize 关键字原理\n\n二面隔了差不多一天的时间，二面很快就来了。\n内容不是很多：\n\n线程间通信的多种方式？\n限流算法？单机限流？分布式限流？\n提到了 Guava Cache ,了解它的实现原理嘛？\n如何定位一个线上问题？\nCPU 高负载？OOM 排查等？\n\n聊完之后表示第二天应该会有三面。\n三面三面的面试官应该是之前邮件中提到的那位总监大佬，以前应该也是一线的技术大牛；聊的问题不是很多：\n\n谈谈对 Netty 的理解？\nNetty 的线程模型？\n写一个 LRU 缓存。\n\n笔试本以为技术面试完了，结果后面告知所有的面试流程都得有笔试了，于是又参与了一次笔试：\n\n交替打印奇偶数\n\n这个相对比较简单，基于锁、等待唤醒机制都是可以的。最后也告知笔试通过。\n之后在推荐我的那位大佬的帮助下戏剧般的通过了整个技术轮（真的很感谢他的认可），并且得知这个消息是在我刚好和 A 部门约好视频面试时间之后。\n也就意味着我必须拒掉一个部门！\n没看错，是我要拒掉一个。这对我来说确实太难了，我压根没想过还有两个机会摆在我面前。\n最后凭着个人的爱好以及 B 部门的热情我很不好意思的拒掉了 A 部门。。。\nHR 面在面这之前我从来没有面过这样大厂的 HR 流程，于是疯狂搜索，希望能弥补点经验。\n也许这就是乐极生悲吧，我确实猜中了 HR 问的大部分问题，但遗憾的是最终依然没能通过。\n后来我在想如果我没有拒掉 A ，会不会结局不一样了？\n但现实就是如此，没有那么多假设，并且每个人也得为自己的选择负责！\n大概的问题是：\n\n为什么想来阿里？\n个人做的最成功最有挑战的事情是什么？\n工作中最难忘的经历？\n对加入我们团队有何期待？\n\nC 部门HR 这关被 Pass 之后没多久我居然又收到了第三个部门的邀约。\n说实话当时我是拒绝的，之前经历了将近两个月的时间却没能如愿我内心是崩溃的。\n我向联系我的大佬表达了我的想法，他倒觉得我最后被 pass 的原因是个小问题，再尝试的话会有很大的几率通过。\n我把这事给朋友说了之后也支持我再试试，反正也没啥损失嘛，而且面试的状态还在。\n所以我又被打了鸡血，才有了下面的面试经过：\n一面面试官：\n服务化框架的选型和差异？\n博主：\n一起探讨了 SpringCloud、Dubbo、Thrift 的差异，优缺点等。\n面试官：\n一致性 Hash 算法的原理？\n博主：\n将数据 Hash 之后落到一个 0 ~ 2^32-1 构成的一个环上。。。。\n面试官：\n谈谈你理解的 Zookeeper？\n博主：\n作为一个分布式协调器。。。\n面试官：\n如何处理 MQ 重复消费？\n博主：\n业务幂等处理。。。。\n面试官：\n客户端负载算法？\n博主：\n轮询、随机、一致性 Hash、故障转移、LRU 等。。\n面试官：\nlong 类型的赋值是否是原子的？\n博主：\n不是。。。\n面试官：\nvolatile 关键字的原理及作用？happen Before？\n博主：\n可见性、一致性。。\n二面一面之后大概一周的时间接到了二面的电话：\n原以为会像之前一样直接进入笔试，这次上来先简单聊了下：\n\n谈谈对微服务的理解，好处以及弊端？\n分布式缓存的设计？热点缓存？\n\n之后才正式进入笔试流程：\n\n这次主要考察设计能力，其实就是对设计模式的理解？能否应对后续的扩展性。\n\n笔试完了之后也和面试官交流，原以为会是算法之类的测试，后来得知他能看到前几轮的笔试情况，特地挑的没有做过的方向。\n所以大家也不用刻意去押题，总有你想不到的，平时多积累才是硬道理。\n三面又过了两周左右，得到 HR 通知；希望能过去杭州参加现场面试。并且阿里包了来回的机票酒店等。\n可见阿里对人才渴望还是舍得下成本的。\n既然都这样了，就当成一次旅游所以去了一趟杭州。\n现场面的时候有别于其他面试，是由两个面试官同时参与：\n\n给一个场景，谈谈你的架构方式。\n\n这就对平时的积累要求较高了。\n还有一个印象较深的是：\n\n在网页上点击一个按钮到服务器的整个流程，尽量完整。\n\n其实之前看过，好像是 Google 的一个面试题。\n完了之后让我回去等通知，没有见到 HR 我就知道凉了，果不其然。\n总结看到这里的朋友应该都是老铁了，我也把上文提到的大多数面试题整理在了 GitHub：\n\n厂库地址：\nhttps://github.com/crossoverJie/Java-Interview\n最后总结下这将近四个月的面试心得：\n\n一定要积极的推销自己，像在 A 部门的三面时，由于基础答得不是很好；所以最后我表达了自己的态度，对工作、技术的积极性。让面试官看到你的潜力值得一个 HC 名额。\n面试过程中遇到自己的不会的可以主动提出，切不可不懂装懂，这一问就露馅。可以将面试官引导到自己擅长的领域。比如当时我正好研究了锁，所以和面试官一聊就是半小时这就是加分项。\n平时要主动积累知识。写博客和参与开源项目就是很好的方式。\n博客可以记录自己踩过的坑，加深印象，而且在写的过程中可以查漏补缺，最后把整个知识体系巩固的比较牢固，良好的内容还可以得到意想不到的收获，比如我第一次面试的机会。\nGitHub 是开发者的一张名片，积极参与开源项目可以和全球大佬头脑风暴，并且在面试过程中绝对是一个加分利器。\n面试官一般最后都会问你有什么要问我的？千万不要问一些公司福利待遇之类的问题。可以问下本次面试的表现？还有哪些需要完善的？从而知道自己答得如何也能补全自己。\n\n还有一点：不要在某次面试失利后否定自己，有时真的不是自己能力不行。这个也讲缘分。\n塞翁失马焉知非福\n我就是个例子，虽然最后没能去成阿里，现在在公司也是一个部门的技术负责人，在我们城市还有个窝，温馨的家，和女朋友一起为想要的生活努力奋斗。\n\n欢迎关注作者公众号于我交流🤗。\n\n","categories":["Interview","Person"]},{"title":"在这个大环境下我是如何找工作的","url":"/2023/06/20/personal/find-job-experience/","content":"蛮久没更新了，本次我想聊聊找工作的事情，相信大家都能感受到从去年开始到现在市场是一天比一天差，特别是在我们互联网 IT 行业。已经过了 18 年之前的高速发展的红利期，能做的互联网应用几乎已经被各大公司做了个遍，现在已经进入稳定的存量市场，所以在这样的大背景下再加上全世界范围内的经济不景气我想每个人都能感受到寒意。\n我还记得大约在 20 年的时候看到网上经常说的一句话：今年将是未来十年最好的一年。\n由于当时我所在的公司业务发展还比较顺利，丝毫没有危机意识，对这种言论总是嗤之以鼻，直到去年国庆节附近。\n\n\n虽然我们做的是海外业务，但是当时受到各方面的原因公司的业务也极速收缩（被收购，资本不看好），所以公司不得不进行裁员；其实到我这里的时候前面已经大概有 2～3 波的优化，我们是最后一波，几乎等于是全军覆没，只留下少数的人维护现有系统。\n这家公司也是我工作这么多年来少数能感受到人情味的公司，虽有不舍，但现实的残酷并不是由我们个人所决定的。\n之后便开始漫长的找工作之旅，到现在也已经入职半年多了；最近看到身边朋友以及网上的一些信息，往往是坏消息多于好消息。\n市场经历半年多的时间，裁员的公司反而增多，岗位也越来越少，所以到现在不管是在职还是离职的朋友或多或少都有所焦虑，我也觉得有必要分享一下我的经历。\n我的预期目标下面重点聊聊找工作的事情；其实刚开始得知要找工作的时候我并不是特别慌，因为当时手上有部分积蓄加上公司有 N+1 的赔偿，同时去年 10 月份的时候岗位相对于现在还是要多一些。\n所以我当时的目标是花一个月的时间找一个我觉得靠谱的工作，至少能长期稳定的工作 3 年以上。\n工作性质可以是纯研发或者是偏管理岗都可以，结合我个人的兴趣纯研发岗的话我希望是可以做纯技术性质的工作，相信大部分做业务研发的朋友都希望能做一些看似“高大上”的内容。这一点我也不例外，所以中间件就和云相关的内容就是我的目标。\n不过这点在重庆这个大洼地中很难找到对口工作，所以我的第二目标是技术 leader，或者说是核心主程之类的，毕竟考虑到 3 年后我也 30+ 了，如果能再积累几年的管理经验后续的路会更好走一些。\n当然还有第三个选项就是远程，不过远程的岗位更少，大部分都是和 web3，区块链相关的工作；我对这块一直比较谨慎所以也没深入了解。\n找工作流水账因为我从入职这家公司到现在其实还没出来面试过，也不太知道市场行情，所以我的想法是先找几家自己不是非去不可的公司练练手。\n\n有一个我个人的偏好忘记讲到，因为最近的一段时间写 Go 会多一些，所以我优先看的是 Go 相关的岗位。\n\n第一家首先第一家是一个 ToB 教育行业的公司，大概的背景是在重庆新成立的研发中心，技术栈也是 Go；\n我现在还记得最后一轮我问研发负责人当初为啥选 Go，他的回答是：\n\nJava 那种臃肿的语言我们首先就不考虑，PHP 也日落西山，未来一定会是 Go 的天下。\n\n由于是新成立的团队，对方发现我之前有管理相关的经验，加上面试印象，所以是期望我过去能做重庆研发 Leader。\n为此还特地帮我申请了薪资调整，因为我之前干过 ToB 业务，所以我大概清楚其中的流程，这种确实得领导特批，所以最后虽然没成但依然很感谢当时的 HR 帮我去沟通。\n第二家第二家主要是偏年轻人的 C 端产品，技术栈也是 Go；给我印象比较深的是，去到公司怎么按电梯都不知道🤣\n\n他们办公室在我们这里的 CBD，我长期在政府赞助的产业园里工作确实受到了小小的震撼，办公环境比较好。\n\n当然面试过程给我留下的印象依然非常深刻，我现在依然记得我坐下后面试官也就是 CTO 给我说的第一句话：\n\n我看过你的简历后就决定今天咱们不聊技术话题了，直接聊聊公司层面和业务上是否感兴趣，以及解答我的疑虑，因为我已经看过你写的很多博客和 GitHub，技术能力方面比较放心。\n\n之后就是常规流程，聊聊公司情况个人意愿等。\n最后我也问了为什么选 Go，这位 CTO 给我的回答和上一家差不多😂\n虽然最终也没能去成，但也非常感谢这位 CTO，他是我碰到为数不多会在面试前认真看你的简历，博客和 GitHub 都会真的点进去仔细阅读👍🏼。\n\n其实这两家我都没怎么讲技术细节，因为确实没怎么聊这部分内容；这时就突出维护自己的技术博客和 GitHub 的优势了，技术博客我从 16 年到现在写了大约 170 篇，GitHub 上开源过一些高 star 项目，也参与过一些开源项目，这些都是没有大厂经历的背书，对招聘者来说也是节约他的时间。\n\n\n当然有好处自然也有“坏处”，这个后续会讲到。\n第三家第三家是找朋友推荐的，在业界算是知名的云原生服务提供商，主要做 ToB 业务；因为主要是围绕着 k8s 社区生态做研发，所以就是纯技术的工作，面试的时候也会问一些技术细节。\n\n我还记得有一轮 leader 面，他说你入职后工作内容和之前完全不同，甚至数据库都不需要安装了。\n\n整体大概 5、6 轮，后面两轮都是 BOSS 面，几乎没有问技术问题，主要是聊聊我的个人项目。\n我大概记得一些技术问题：\n\nk8s 相关的一些组件、Operator\nGo 相关的放射、接口、如何动态修改类实现等等。\nJava 相关就是一些常规的，主要是一些常用特性和 Go 做比较，看看对这两门语言的理解。\n\n其实这家公司是比较吸引我的，几乎就是围绕着开源社区做研发，工作中大部分时间也是在做开源项目，所以可以说是把我之前的业余爱好和工作结合起来了。\n在贡献开源社区的同时还能收到公司的现金奖励，不可谓是双赢。\n对我不太友好的是工作地在成都，入职后得成渝两地跑；而且在最终发 offer 的前两小时，公司突然停止 HC 了，这点确实没想到，所以阴差阳错的我也没有去成。\n第四家第四家也就是我现在入职的公司，当时是我在招聘网站上看到的唯一一家做中间件的岗位，抱着试一试的态度我就投了。面试过程也比较顺利，一轮同事面，一轮 Leader 面。\n技术上也没有聊太多，后来我自己猜测大概率也和我的博客和 Github 有关。\n\n当然整个过程也有不太友好的经历，比如有一家成都的“知名”旅游公司；面试的时候那个面试官给我的感觉是压根没有看我的简历，所有的问题都是在读他的稿子，根本没有上下文联系。\n还有一家更离谱，直接在招聘软件上发了一个加密相关的算法，让我解释下；因为当时我在外边逛街，所以没有注意到消息；后来加上微信后说我为什么没有回复，然后整个面试就在微信上打字进行。\n其中问了一个很具体的问题，我记得好像是 MD5 的具体实现，说实话我不知道，从字里行间我感觉对方的态度并不友好，也就没有必要再聊下去；最后给我说之所以问这些，是因为看了我的博客后觉得我技术实力不错，所以对我期待较高；我只能是地铁老人看手机。\n最终看来八股文确实是绕不开的，我也花了几天时间整理了 Java 和 Go 的相关资料；不过我觉得也有应对的方法。\n首先得看你面试的岗位，如果是常见的业务研发，从招聘的 JD 描述其实是可以看出来的，比如有提到什么 Java 并发、锁、Spring等等，大概率是要问八股的；这个没办法，别人都在背你不背就落后一截了。\n之后我建议自己平时在博客里多记录八股相关的内容，并且在简历上着重标明博客的地址，尽量让面试官先看到；这样先发制人，你想问的我已经总结好了😂。\n但这个的前提是要自己长期记录，不能等到面试的时候才想起去更新，长期维护也能加深自己的印象，按照 “艾宾浩斯遗忘曲线” 进行复习。\n选择这是我当时记录的面试情况，最终根据喜好程度选择了现在这家公司。\n不过也有一点我现在觉得但是考虑漏了，那就是行业前景。\n现在的 C 端业务真的不好做，相对好做的是一些 B 端，回款周期长，同时不太吃现金流；这样的业务相对来说活的会久一些，我现在所在的公司就是纯做 C 端，在我看来也没有形成自己的护城河，只要有人愿意砸钱随时可以把你干下去。\n加上现在的资本也不敢随意投钱，公司哪天不挣钱的话首先就是考虑缩减产研的成本，所以裁员指不定就会在哪一天到来。\n现在庆幸的是入职现在这家公司也没有选错，至少短期内看来不会再裁员，同时我做的事情也是比较感兴趣的；和第三家有些许类似，只是做得是内部的基础架构，也需要经常和开源社区交流。\n面对裁员能做的事情说到裁员，这也是我第一次碰上，只能分享为数不多的经验。\n避免裁员当然第一条是尽量避免进入裁员名单，这个我最近在播客 作为曾经的老板，我们眼中的裁员和那些建议 讲到在当下的市场情况下哪些人更容易进入裁员名单：\n\n年纪大的，这类收入不低，同时收益也没年轻人高，确实更容易进入名单。\n未婚女性，这点确实有点政治不正确，但确实就是现在的事实，这个需要整个社会，政府来一起解决。\n做事本本分分，没有贡献也没出啥事故。\n边缘业务，也容易被优化缩减成本。\n\n那如何避免裁员呢，当然首先尽量别和以上特征重合，一些客观情况避免不了，但我们可以在第三点上主动“卷”一下，当然这个的前提是你还想在这家公司干。\n还有一个方法是提前向公司告知降薪，这点可能很多人不理解，因为我们大部分人的收入都是随着跳槽越来越高的；但这些好处是否是受到前些年互联网过于热门的影响呢？\n当然个人待遇是由市场决定的，现在互联网不可否认的降温了，如果你觉得各方面呆在这家公司都比出去再找一个更好，那这也不失为一个方法；除非你有信心能找到一个更好的，那就另说了。\n未来计划我觉得只要一家公司只要有裁员的风声传出来后，即便是没被裁，你也会处于焦虑之中；要想避免这种焦虑确实也很简单，只要有稳定的被动收入那就无所谓了。\n这个确实也是说起来轻松做起来难，我最近也一直在思考能不能在工作之余做一些小的 side project，这话题就大了，只是我觉得我们程序员先天就有自己做一个产品的机会和能力，与其把生杀大权给别人，不如握在自己手里。\n当然这里得提醒下，在国内的企业，大部分老板都认为签了合同你的 24 小时都是他的，所以这些业务项目最好是保持低调，同时不能影响到本职工作。\n\n欢迎关注作者公众号于我交流🤗。\n\n","categories":["Interview","Person"]},{"title":"如何成为一位「不那么差」的程序员","url":"/2018/08/12/personal/how-to-be-developer/","content":"\n前言已经记不清有多少读者问过：\n\n博主，你是怎么学习的？像我这样的情况有啥好的建议嘛？\n\n也不知道啥时候我居然成人生导师了。当然我不排斥这些问题，和大家交流都是学习的过程。\n因此也许诺会准备一篇关于学习方面的文章；所以本文其实准备了很久，篇幅较长，大家耐心看完希望能有收获。\n\n以下内容仅代表我从业以来所积累的相关经验，我会从硬技能、软实力这些方面尽量阐述我所认为的 “不那么差的程序员” 应当做到哪些技能。\n\n\n\n技能树作为一名码代码的技术工人，怎么说干的还是技术活。\n既然是技术活那专业实力就得过硬，下面我会按照相关类别谈谈我们应该掌握哪些。\n计算机基础一名和电脑打交道的工种，计算机是我们赖以生存的工具。所以一些基础技能是我们应该和必须掌握的。\n\n比如网络相关的知识。\n\n其中就包含了 TCP 协议，它和 UDP 的差异。需要理解 TCP 三次握手的含义，拆、粘包等问题。\n当然上层最常见的 HTTP 也需要了解，甚至是熟悉。\n这块推荐《图解 HTTP》一书。\n\n接着是操作系统相关知识。\n\n由于工作后你写的大部分代码都是运行在 Linux 服务器上，所以对于这个看它脸色行事主你也得熟悉才行。\n比如进程、线程、内存等概念；服务器常见的命令使用，这个没啥窍门就是得平时多敲敲多总结。\n我也是之前兼职了半年运维才算是对这一块比较熟悉。\nLinux 这个自然是推荐业界非常出名的《鸟哥的 Linux 私房菜》。\n当作为一个初学者学习这些东西时肯定会觉得枯燥乏味，大学一般在讲专业课之前都会有这些基础学科。我相信大部分同学应该都没怎么仔细听讲，因为确实这些东西就算是学会了记熟了也没有太多直接的激励。\n但当你工作几年之后会发现，只要你还在做计算机相关的工作，这些都是绕不开的，当哪天这些知识不经意的帮助到你时你会庆幸当初正确的选择。\n数据结构与算法接下来会谈到另一门枯燥的课程：数据结构。\n这块当初在大学时也是最不受待见的一门课程，也是我唯一挂过的科目。\n记得当时每次上课老师就让大家用 C 语言练习书上的习题，看着一个个拆开都认识的字母组合在一起就六亲不认我果断选择了放弃。\n这也造成现在的我每隔一段时间就要看二叉树、红黑树、栈、队列等知识，加深印象。\n算法这个东西我确实没有啥发言权，之前坚持刷了部分 LeetCode 的题目也大多停留在初中级。\n但像基本的查找、排序算法我觉得还是要会的，不一定要手写出来但要理解其思路。\n所以强烈建议还在大学同学们积极参与一些 ACM 比赛，绝对是今后的加分利器。\n这一块内容可能会在应届生校招时发挥较大作用，在工作中如果你的本职工作是 Java Web 开发的话，这一块涉猎的几率还是比较低。\n不过一旦你接触到了模型设计、中间件、高效存储、查询等内容这些也是绕不过的坎。\n这块内容和上面的计算机基础差不多，对于我们 Java 开发来说我觉得平时除了多刷刷 LeetCode 加深印象之外，在日常开发中每选择一个容器存放数据时想想为什么选它？有没有更好的存储方式？写入、查询效率如何？\n同样的坚持下去，今后肯定收货颇丰。\n同时推荐《算法（第4版）》\nJava 基础这里大部分的读者都是 Java 相关，所以这个强相关的技能非常重要。\nJava 基础则是走向 Java 高级的必经之路。\n这里抛开基本语法不谈，重点讨论实际工作中高频次的东西。\n\n基本容器，如：HashMap、ArrayList、HashSet、LinkedList 等，不但要会用还得了解其中的原理。这样才能在不同的场景选择最优的设计。\nIO、NIO 也是需要掌握。日常开发中大部分是在和磁盘、网络（写日志、数据库、Redis）打交道，这些都是 IO 的过程。\n常见的设计模式如：代理、工厂、回调、构建者模式，这对开发灵活、扩展性强的应用有很大帮助。\nJava 多线程是非常重要的特性，日常开发很多。能理解线程模型、多线程优缺点、以及如何避免。\n良好的单测习惯，很多人觉得写单测浪费时间没有意义。但正是有了单测可以提前暴露出许多问题，减少测试返工几率，提高代码质量。\n良好的编程规范，这个可以参考《阿里巴巴 Java 开发手册》以及在它基础上优化的《唯品会 Java 手册》\n\n\n　《Java核心技术·卷 I》值得推荐。\n\n多线程应用有了扎实的基础之后来谈谈多线程、并发相关的内容。\n想让自己的 title 里加上“高级”两字肯定得经过并发的洗礼。\n\n这里谈论的并发主要是指单应用里的场景，多应用的可以看后文的分布式内容。\n\n多线程的出现主要是为了提高 CPU 的利用率、任务的执行效率。但并不是用了多线程就一定能达到这样的效果，因为它同时也带来了一些问题：\n\n上下文切换\n共享资源\n可见性、原子性、有序性等。\n\n一旦使用了多线程那肯定会比单线程的程序要变得复杂和不可控，甚至使用不当还会比单线程慢。所以要考虑清楚是否真的需要多线程。\n会用了之后也要考虑为啥多线程会出现那样的问题，这时就需要理解内存模型、可见性之类的知识点。\n同样的解决方式又有哪些？各自的优缺点也需要掌握。\n谈到多线程就不得不提并发包下面的内容 java.util.concurrent。\n最常用及需要掌握的有：\n\n原子类：用于并发场景的原子操作。\n队列。常用于解耦，需要了解其实现原理。\n并发工具，如 ConcurrentHashMap、CountDownLatch 之类的工具使用以及原理。\n线程池使用，以及相关原理。\n锁相关内容：synchronized、ReentrantLock 的使用及原理。\n\n这一块的内容可以然我们知道写 JDK 大牛处理并发的思路，对我们自己编写高质量的多线程程序也有很多帮助。\n推荐《Java 并发编程的艺术》很好的并发入门书籍。\nJVM 虚拟机想要深入 Java ，JVM 是不可或缺的。对于大部分工作 1~3 年的开发者来说直接接触这一些内容是比较少的。\n到了 3~5 年这个阶段就必须得了解了，以下内容我觉得是必须要掌握的：\n\nJVM 内存划分，知道哪块内存存放哪些内容；线程安全与否；内存不够怎么处理等。\n不同情况的内存溢出、栈溢出，以及定位解决方案。\n分代的垃圾回收策略。\n线上问题定位及相关解决方案。\n一个类的加载、创建对象、垃圾回收、类卸载的整个过程。\n\n掌握这些内容真的对实际分析问题起到巨大帮助。\n\n　对此强力推荐《深入理解Java虚拟机》，这本书反反复复看过好几遍，每个阶段阅读都有不同的收获。\n\n数据库做 WEB 应用开发的同学肯定要和数据库打不少交道，而且通常来说一个系统最先出现瓶颈往往都是数据库，说数据库是压到系统的最后一根稻草一点也不为过。\n所以对数据库的掌握也是非常有必要。拿互联网用的较多的 MySQL 数据库为例，一些必须掌握的知识点：\n\n索引的数据结构及原理、哪些字段应当创建索引。\n针对于一个慢 SQL 的优化思路。\n数据库水平垂直拆分的方案，需要了解业界常用的 MyCAT、sharding-sphere 等中间件。\n\n常规使用可以参考《阿里巴巴 Java 开发手册》中的数据库章节，想要深入了解 MySQL 那肯定得推荐经典的《高性能 MySQL》一书了。\n分布式技术随着互联网的发展，传统的单体应用越来越不适合现有场景。\n因此分布式技术出现了，这块涵盖的内容太多了，经验有限只能列举我日常使用到的一些内容：\n\n首先是一些基础理论如：CAP 定理，知道分布式系统会带来的一些问题以及各个应用权衡的方式。\n了解近些年大热的微服务相关定义、来源以及对比，有条件的可以阅读 martin fowler 的原文 Microservices，或者也可以搜索相关的国内翻译。\n对 Dubbo、SpringCloud 等分布式框架的使用，最好是要了解原理。\n接着要对分布式带来的问题提出解决方案。如分布式锁、分布式限流、分布式事务、分布式缓存、分布式 ID、消息中间件等。\n也要了解一些分布式中的负载算法：权重、Hash、一致性 Hash、故障转移、LRU 等。\n最好能做一个实践如：秒杀架构实践\n\n\n之前有开源一个分布式相关解决组件：\nhttps://github.com/crossoverJie/distributed-redis-tool\n同时推荐一本入门科普《大型网站技术架构》，出版时间有点早，从中可以学习一些思路。\n懂点架构相信大家都有一个架构师的梦想。\n架构师给人的感觉就是画画图纸，搭好架子，下面的人员来添砖加瓦最终产出。\n但其实需要的内功也要非常深厚，就上面列举的样样需要掌握，底层到操作系统、算法；上层到应用、框架都需要非常精通。（PPT 架构师除外）\n我自身参与架构经验也不多，所以只能提供有限的建议。\n首先分布式肯定得掌握，毕竟现在大部分的架构都是基于分布式的。\n这其中就得根据 CAP 理论结合项目情况来选择一致性还是可用性，同时如何做好适合现有团队的技术选型。 \n这里推荐下开涛老师的《亿级流量网站架构核心技术》，列举了很多架构实例，不过网上褒贬不一，但对于刚入门架构的能科普不少知识。\n如何学习谈完了技能树，现在来聊聊如何学习，这也是被问的最多的一个话题。\n而关于学习讨论的最多的也是看视频还是看书？\n视频不得不承认视频是获取知识最便捷的来源，毕竟包含了图、文、声。\n大学几年时间其实我也没好好上专业课，我记得真正入门 Java 还是一个暑假花了两个月的时间天天在家里看 ”马士兵“ 老师的视频教程，当时的资源也很老了，记得好像是 07 年出的视频（用的还是 Google ）。\n那段时间早起晚睡，每天学到东西之后马上实践，心里也很有成就感。后来开学之后一度成为同学们眼中的”学霸“人物。\n\n　现在打开我 12 年的电脑，硬盘里还躺着好几十 G 的教学视频。\n\n看书工作后时间真的很宝贵，完全没有了学生生涯的想学就学的自由。所以现在我主要知识来源还是书籍。\n这些是我最近看的书：\n\n看书又会涉及到电子书和纸质书的区别，我个人比较喜欢纸质书。毕竟我可以方便的记笔记以及可以随时切换章节。最主要的还是从小养成的闻书香的习惯。\n知识付费近几年知识付费越来越流行，许多大佬也加入了这个行列，人们也逐渐在习惯为知识去付费。\n说实话写一好篇文章出一份视频都非常不容易，能有正向的激励，作者才能持续输出更好的内容。\n这块我觉得国内做的比较好我也为之付费的有极客时间、大佬的知识星球等。\n这三点没有绝对的好坏之分，其实可以看出我刚入门的时候看视频，工作之后看书及知识付费内容。\n视频的好处是可以跟着里面老师的思路一步一步往下走，比较有音视频代入感强，就像学校老师讲课一样。\n但由于内容较长使读者没法知晓其中的重点，甚至都不敢快进生怕错过了哪个重要知识，现在由于 IT 越来越火，网上的视频也很多导致质量参差不齐也不成体系。\n而看书可以选择性的浏览自己感兴趣的章节，费解的内容也方便反复阅读\n所以建议刚入门的同学可以看看视频跟着学，参与工作一段时间后可以尝试多看看书。\n当然这不是绝对的，找到适合自己的学习方式就好。但不管是视频还是看书都要多做多实践。\n打造个人品牌个人品牌看似很程序员这个职业不怎么沾边，但在现今的互联网时代对于每个人来说都很重要。\n以往我们在写简历或是评估他人简历的时候往往不会想到去网络搜索他的个人信息，但在这个信息爆炸的时代你在网上留下的一点印记都能被发现。\n博客因此我们需要维护好自己的名片，比如先搭建自己的个人博客。\n博客的好处我也谈过几次了，前期关注人少没关系，重要的是坚持，当你写到 50、100篇文章后你会发现自己在这过程中一定是的到了提高。\nGitHub第二点就和技术人比较相关了：参与维护好自己的 GitHub。\n由于 GitHub 的特殊属性，维护好后可以更好的打造个人品牌。\nTalk is cheap. Show me the code 可不是随便说说的。\n想要维护好可以从几个方面着手：\n\n参与他人的项目，不管是代码库还是知识库都可以，先融入进社区。\n发起自己的开源项目，不管是平时开发过程中的小痛点，还是精心整理的知识点都可以。\n\n但这过程中有几点还是要注意：\n\n我们需要遵守 GitHub 的社交礼仪。能用英文尽量就用英文，特别是在国外厂库中。\n尽量少 push 一些与代码工作无关的内容，我认为这并不能提高自己的品牌。\n别去刷 star。这也是近期才流行起来，不知道为什么总有一些人会钻这种空子，刷起来的热度对自己并没有任何提高。\n\n这里有一篇国外大佬写的 How to build your personal brand as a new developer :\nhttps://medium.freecodecamp.org/building-your-personal-brand-as-a-new-web-developer-f6d4150fd217\nEnglish 挺重要再来谈谈英语的重要性，我记得刚上大学时老师以及一些培训机构都会说：\n\n别怕自己英语差就学不了编程，真正常用的就那些词语。\n\n这句话虽没错，但英语在对 IT 这行来说还是有着极大的加分能力。\n拿常见的 JDK 里的源码注释也是纯英文的，如果英语还不错的话，一些 Spring 的东西完全可以自学，直接去 Spring 官网就可以查看，甚至后面出的 SpringCloud，官方资料就是最好的教程。\n再有就是平时查资料时，有条件的可以尝试用 Google + 英文 搜索，你会发现新的世界。\n不然也不会有面向 Google/Stack Overflow 编程。\n对于英语好的同学自然不怕，那不怎么好的咋办呢？\n比如我，但我在坚持以下几点：\n\n所有的手机、电脑系统统统换成英语语言，养成习惯（不过也有尴尬的连菜单都找不到的情况）。\n订阅一些英语周刊，比如 ”湾区日报“。\n定期去类似于 https://medium.com/ 这样具有影响力的国外社区阅读文章。\n\n虽然现在我也谈不上多好，但目前我也在努力，希望大家也一起坚持。\n推荐一本近期在看的书《程序员的英语》。\n保持竞争力技术这个行业发展迅速、变化太快，每年也都有无数相关行业毕业生加入竞争，稍不留神就会被赶上甚至超越。\n所以我们无时无刻都得保持竞争力。\n多的谈不上，我只能谈下目前我在做的事情：\n\n打好基础。不是学了之后就忘了，需要不停的去看，巩固，基础是万变不离其宗的。\n多看源码，了解原理，不要停留在调参侠的境界。\n关注行业发展、新技术、新动态至少不能落伍了。\n争取每周产出一篇技术相关文章。\n积极参与开源项目。\n\n思维导图\n结合上文产出了一个思维导图更直观些。\n总结本文结合了自身的一些经验列举了一些方法，不一定对每位都有效需要自行判断。\n也反反复复写了差不多一周的时间，希望对在这条路上和正在路上的朋友们起到一些作用。\n大部分都只是谈了个思路，其实每一项单聊都能写很多。每个点都有推荐一本书籍，有更好建议欢迎留言讨论。\n上文大部分的知识点都有维护在 GitHub 上，感兴趣的朋友可以自行查阅：\n\nhttps://github.com/crossoverJie/JCSprout\n","categories":["Person"]},{"title":"一个诡异的 Pulsar InterruptedException 异常","url":"/2023/02/23/pulsar/pulsar-interrupted/","content":"\n背景今天收到业务团队反馈线上有个应用往 Pulsar 中发送消息失败了，经过日志查看得知是发送消息时候抛出了 java.lang.InterruptedException 异常。\n和业务沟通后得知是在一个 gRPC 接口中触发的消息发送，大约持续了半个小时的异常后便恢复正常了，这是整个问题的背景。\n\n\n前置排查拿到该问题后首先排查下是否是共性问题，查看了其他的应用没有发现类似的异常；同时也查看了 Pulsar broker 的监控大盘，在这个时间段依然没有波动和异常；\n这样可以初步排除是 Pulsar 服务端的问题。\n接着便是查看应用那段时间的负载情况，从应用 QPS 到 JVM 的各个内存情况依然没发现有什么明显的变化。\nPulsar 源码排查既然看起来应用本身和 Pulsar broker 都没有问题的话那就只能从异常本身来排查了。\n首先第一步要得知具体使用的是 Pulsar-client 是版本是多少，因为业务使用的是内部基于官方 SDK 封装 springboot starter 所以第一步还得排查这个 starter 是否有影响。\n通过查看源码基本排除了 starter 的嫌疑，里面只是简单的封装了 SDK 的功能而已。\norg.apache.pulsar.client.api.PulsarClientException: java.util.concurrent.ExecutionException: org.apache.pulsar.client.api.PulsarClientException: java.lang.InterruptedException at org.apache.pulsar.client.api.PulsarClientException.unwrap(PulsarClientException.java:1027) at org.apache.pulsar.client.impl.TypedMessageBuilderImpl.send(TypedMessageBuilderImpl.java:91) at java.base/java.lang.Thread.run(Thread.java:834) Caused by: java.util.concurrent.ExecutionException: org.apache.pulsar.client.api.PulsarClientException: java.lang.InterruptedException at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) at org.apache.pulsar.client.impl.TypedMessageBuilderImpl.send(TypedMessageBuilderImpl.java:89) ... 49 common frames omitted Caused by: org.apache.pulsar.client.api.PulsarClientException: java.lang.InterruptedException at org.apache.pulsar.client.impl.ProducerImpl.canEnqueueRequest(ProducerImpl.java:775) at org.apache.pulsar.client.impl.ProducerImpl.sendAsync$original$BWm7PPlZ(ProducerImpl.java:393) at org.apache.pulsar.client.impl.ProducerImpl.sendAsync$original$BWm7PPlZ$accessor$i7NYMN6i(ProducerImpl.java) at org.apache.pulsar.client.impl.ProducerImpl$auxiliary$EfuVvJLT.call(Unknown Source) at org.apache.skywalking.apm.agent.core.plugin.interceptor.enhance.InstMethodsInter.intercept(InstMethodsInter.java:86) at org.apache.pulsar.client.impl.ProducerImpl.sendAsync(ProducerImpl.java) at org.apache.pulsar.client.impl.ProducerImpl.internalSendAsync(ProducerImpl.java:292) at org.apache.pulsar.client.impl.ProducerImpl.internalSendWithTxnAsync(ProducerImpl.java:363) at org.apache.pulsar.client.impl.PartitionedProducerImpl.internalSendWithTxnAsync(PartitionedProducerImpl.java:191) at org.apache.pulsar.client.impl.PartitionedProducerImpl.internalSendAsync(PartitionedProducerImpl.java:167) at org.apache.pulsar.client.impl.TypedMessageBuilderImpl.sendAsync(TypedMessageBuilderImpl.java:103) at org.apache.pulsar.client.impl.TypedMessageBuilderImpl.send(TypedMessageBuilderImpl.java:82) ... 49 common frames omitted Caused by: java.lang.InterruptedException: nullat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343) at java.base/java.util.concurrent.Semaphore.acquire(Semaphore.java:318) at org.apache.pulsar.client.impl.ProducerImpl.canEnqueueRequest(ProducerImpl.java:758)\n\n接下来便只能是分析堆栈了，因为 Pulsar-client 的部分实现源码是没有直接打包到依赖中的，反编译的话许多代码行数对不上，所以需要将官方的源码拉到本地，切换到对于的分支进行查看。\n\n这一步稍微有点麻烦，首先是代码库还挺大的，加上之前如果没有准备好 Pulsar 的开发环境的话估计会劝退一部分人；但其实大部分问题都是网络造成的，只要配置一些 Maven 镜像多试几次总会编译成功。\n\n我这里直接将分支切换到 branch-2.8。\n从堆栈的顶部开始排查 TypedMessageBuilderImpl.java:91：看起来是内部异步发送消息的时候抛了异常。\n接着往下看到这里：\njava.lang.InterruptedException at org.apache.pulsar.client.impl.ProducerImpl.canEnqueueRequest(ProducerImpl.java:775) at\n\n看起来是这里没错，但是代码行数明显不对；因为 2.8 这个分支也是修复过几个版本，所以中间有修改导致代码行数与最新代码对不上也正常。\nsemaphore.get().acquire();\n不过初步来看应该是这行代码抛出的线程终端异常，这里看起来只有他最有可能了。\n为了确认是否是真的是这行代码，这个文件再往前翻了几个版本最终确认了就是这行代码没错了。\n我们点开java.util.concurrent.Semaphore#acquire()的源码，\n/** * &lt;li&gt;has its interrupted status set on entry to this method; or * &lt;li&gt;is &#123;@linkplain Thread#interrupt interrupted&#125; while waiting * for a permit, * &lt;/ul&gt; * then &#123;@link InterruptedException&#125; is thrown and the current thread&#x27;s * interrupted status is cleared. * * @throws InterruptedException if the current thread is interrupted */public void acquire() throws InterruptedException &#123;    sync.acquireSharedInterruptibly(1);&#125;public final void acquireSharedInterruptibly(int arg)    throws InterruptedException &#123;    if (Thread.interrupted() ||        (tryAcquireShared(arg) &lt; 0 &amp;&amp;         acquire(null, arg, true, true, false, 0L) &lt; 0))        throw new InterruptedException();&#125;    \n通过源码会发现 acquire() 函数确实会响应中断，一旦检测到当前线程被中断后便会抛出 InterruptedException 异常。\n定位问题所以问题的原因基本确定了，就是在 Pulsar 的发送消息线程被中断了导致的，但为啥会被中断还需要继续排查。\n我们知道线程中断是需要调用 Thread.currentThread().interrupt(); API的，首先猜测是否 Pulsar 客户端内部有个线程中断了这个发送线程。\n于是我在 pulsar-client 这个模块中搜索了相关代码：排除掉和 producer 不相关的地方，其余所有中断线程的代码都是在有了该异常之后继续传递而已；所以初步来看 pulsar-client 内部没有主动中断的操作。\n既然 Pulsar 自己没有做，那就只可能是业务做的了？\n于是我在业务代码中搜索了一下：\n果然在业务代码中搜到了唯一一处中断的地方，而且通过调用关系得知这段代码是在消息发送前执行的，并且和 Pulsar 发送函数处于同一线程。\n大概的伪代码如下：\n    List.of(1, 2, 3).stream().map(e -&gt; &#123;                return CompletableFuture.supplyAsync(() -&gt; &#123;                    try &#123;                        TimeUnit.MILLISECONDS.sleep(10);                    &#125; catch (InterruptedException ex) &#123;                        throw new RuntimeException(ex);                    &#125;                    return e;                &#125;);            &#125;    ).collect(Collectors.toList()).forEach(f -&gt; &#123;        try &#123;            Integer integer = f.get();            log.info(&quot;====&quot; + integer);            if (integer==3)&#123;                TimeUnit.SECONDS.sleep(10);                Thread.currentThread().interrupt();            &#125;        &#125; catch (InterruptedException e) &#123;            throw new RuntimeException(e);        &#125; catch (ExecutionException e) &#123;            throw new RuntimeException(e);        &#125;    &#125;);MessageId send = producer.newMessage().value(msg.getBytes()).send();\n\n执行这段代码可以完全复现同样的堆栈。\n幸好中断这里还打得有日志：\n\n通过日志搜索发现异常的时间和这个中断的日志时间点完全重合，这样也就知道根本原因了。\n因为业务线程和消息发送线程是同一个，在某些情况下会执行 Thread.currentThread().interrupt();，其实单纯执行这行函数并不会发生什么，只要没有去响应这个中断，也就是 Semaphore 源码中的判断了线程中断的标记：\npublic final void acquireSharedInterruptibly(int arg)    throws InterruptedException &#123;    if (Thread.interrupted() ||        (tryAcquireShared(arg) &lt; 0 &amp;&amp;         acquire(null, arg, true, true, false, 0L) &lt; 0))        throw new InterruptedException();&#125;\n\n但恰好这里业务中断后自己并没有去判断这个标记，导致 Pulsar 内部去判断了，最终抛出了这个异常。\n总结所以归根结底还是这里的代码不合理导致的，首先是自己中断了线程但也没使用，从而导致有被其他基础库使用的可能，所以会造成了一些不可预知的后果。\n再一个是不建议在业务代码中使用 Thread.currentThread().interrupt(); 这类代码，第一眼根本不知道是要干啥，也不易维护。\n其实本质上线程中断也是线程间通信的一种手段，有这类需求完全可以换为内置的 BlockQueue 这类函数来实现。\n","categories":["Pulsar"],"tags":["Pulsar","InterruptedException"]},{"title":"Pulsar负载均衡原理及优化","url":"/2023/02/07/pulsar/pulsar-load-banance/","content":"\n前言前段时间我们在升级 Pulsar 版本的时候发现升级后最后一个节点始终没有流量。\n\n虽然对业务使用没有任何影响，但负载不均会导致资源的浪费。\n\n和同事沟通后得知之前的升级也会出现这样的情况，最终还是人工调用 Pulsar 的 admin API 完成的负载均衡。\n这个问题我尝试在 Google 和 Pulsar 社区都没有找到类似的，不知道是大家都没碰到还是很少升级集群。\n\n我之前所在的公司就是一个版本走到黑😂\n\n\n\nPulsar 负载均衡原理当一个集群可以水平扩展后负载均衡就显得非常重要，根本目的是为了让每个提供服务的节点都能均匀的处理请求，不然扩容就没有意义了。\n在分析这个问题的原因之前我们先看看 Pulsar 负载均衡的实现方案。\n# Enable load balancerloadBalancerEnabled=true\n\n我们可以通过这个 broker 的这个配置来控制负载均衡器的开关，默认是打开。\n但具体使用哪个实现类来作为负载均衡器也可以在配置文件中指定：\n# Name of load manager to useloadManagerClassName=org.apache.pulsar.broker.loadbalance.impl.ModularLoadManagerImpl\n默认使用的是 ModularLoadManagerImpl。\nstatic LoadManager create(final PulsarService pulsar) &#123;    try &#123;        final ServiceConfiguration conf = pulsar.getConfiguration();        // Assume there is a constructor with one argument of PulsarService.        final Object loadManagerInstance = Reflections.createInstance(conf.getLoadManagerClassName(),                Thread.currentThread().getContextClassLoader());        if (loadManagerInstance instanceof LoadManager) &#123;            final LoadManager casted = (LoadManager) loadManagerInstance;            casted.initialize(pulsar);            return casted;        &#125; else if (loadManagerInstance instanceof ModularLoadManager) &#123;            final LoadManager casted = new ModularLoadManagerWrapper((ModularLoadManager) loadManagerInstance);            casted.initialize(pulsar);            return casted;        &#125;    &#125; catch (Exception e) &#123;        LOG.warn(&quot;Error when trying to create load manager: &quot;, e);    &#125;    // If we failed to create a load manager, default to SimpleLoadManagerImpl.    return new SimpleLoadManagerImpl(pulsar);&#125;\n\n当 broker 启动时会从配置文件中读取配置进行加载，如果加载失败会使用 SimpleLoadManagerImpl 作为兜底策略。\n当 broker 是一个集群时，只有 leader 节点的 broker 才会执行负载均衡器的逻辑。\n\nLeader 选举是通过 Zookeeper 实现的。\n\n默然情况下成为 Leader 节点的 broker 会每分钟读取各个 broker 的数据来判断是否有节点负载过高需要做重平衡。\n而是否重平衡的判断依据是由 org.apache.pulsar.broker.loadbalance.LoadSheddingStrategy 接口提供的，它其实只有一个函数：\npublic interface LoadSheddingStrategy &#123;    /**     * Recommend that all of the returned bundles be unloaded.     * @return A map from all selected bundles to the brokers on which they reside.     */    Multimap&lt;String, String&gt; findBundlesForUnloading(LoadData loadData, ServiceConfiguration conf);&#125;\n根据所有 broker 的负载信息计算出一个需要被 unload 的 broker 以及 bundle。\n这里解释下 unload 和 bundle 的概念：\n\nbundle 是一批 topic 的抽象，将 bundle 和 broker 进行关联后客户端才能知道应当连接哪个 broker；而不是直接将 topic 与 broker 绑定，这样才能实现海量 topic 的管理。\nunload 则是将已经与 broker 绑定的 bundle 手动解绑，从而触发负载均衡器选择一台合适的 broker 重新进行绑定；通常是整个集群负载不均的时候触发。\n\nThresholdShedder 原理LoadSheddingStrategy 接口目前有三个实现，这里以官方默认的 ThresholdShedder 为例：\n它的实现算法是根据带宽、内存、流量等各个指标的权重算出每个节点的负载值，之后为整个集群计算出一个平均负载值。\n# 阈值loadBalancerBrokerThresholdShedderPercentage=10\n当集群中有某个节点的负载值超过平均负载值达到一定程度（可配置的阈值）时，就会触发 unload，以上图为例就会将最左边节点中红色部分的 bundle 卸载掉，然后再重新计算一个合适的 broker 进行绑定。\n\n阈值存在的目的是为了避免频繁的 unload，从而影响客户端的连接。\n\n问题原因当某些 topic 的流量突然爆增的时候这种负载策略确实可以处理的很好，但在我们集群升级的情况就不一定了。\n假设我这里有三个节点：\n\nbroker0\nbroker1\nbroker2\n\n集群升级时会从 broker2-&gt;0 进行镜像替换重启，假设在升级前每个 broker 的负载值都是 10。\n\n重启 broker2 时，它所绑定的 bundle 被 broker0&#x2F;1 接管。\n升级 broker1 时，它所绑定的 bundle 又被 broker0&#x2F;2 接管。\n最后升级 broker0, 它所绑定的 bundle 会被broker1&#x2F;2 接管。\n\n只要在这之后没有发生流量激增到触发负载的阈值，那么当前的负载情况就会一直保留下去，也就是 broker0 会一直没有流量。\n经过我反复测试，现象也确实如此。\n./pulsar-perf monitor-brokers --connect-string pulsar-test-zookeeper:2181\n通过这个工具也可以查看各个节点的负载情况\n优化方案这种场景是当前 ThresholdShedder 所没有考虑到的，于是我在我们所使用的版本 2.10.3 的基础上做了简单的优化：\n\n当原有逻辑走完之后也没有获取需要需要卸载的 bundle，同时也存在一个负载极低的 broker 时(emptyBundle)，再触发一次 bundle 查询。\n按照 broker 所绑定的数量排序，选择一个数量最多的 broker 的 第一个 bundle 进行卸载。\n\n修改后打包发布，再走一遍升级流程后整个集群负载就是均衡的了。\n但其实这个方案并不严谨，第二步选择的重点是筛选出负载最高的集群中负载最高的 bundle；这里只是简单的根据数量来判断，并不够准确。\n正当我准备持续优化时，鬼使神差的我想看看 master 上有人修复这个问题没，结果一看还真有人修复了；只是还没正式发版。\nhttps://github.com/apache/pulsar/pull/17456\n\n整体思路是类似的，只是筛选负载需要卸载 bundle 时是根据 bundle 自身的流量来的，这样会更加精准。\n总结不过看社区的进度等这个优化最终能用还不知道得多久，于是我们就自己参考这个思路在管理台做了类似的功能，当升级后出现负载不均衡时人工触发一个逻辑：\n\n系统根据各个节点的负载情况计算出一个负载最高的节点和 bundle 在页面上展示。\n人工二次确认是否要卸载，确认无误后进行卸载。\n\n本质上只是将上述优化的自动负载流程改为人工处理了，经过测试效果是一样的。\nPulsar 整个项目其实非常庞大，有着几十上百个模块，哪怕每次我只改动一行代码准备发布测试时都得经过漫长的编译+ Docker镜像打包+上传私服这些流程，通常需要1~2个小时；但总的来说收获还是很大的，最近也在提一些 issue 和 PR，希望后面能更深入的参与进社区。\n","categories":["Pulsar"],"tags":["Pulsar","消息队列","负载均衡"]},{"title":"Pulsar压测及优化","url":"/2023/01/16/pulsar/pulsar-perf-test/","content":"\n前言这段时间在做 MQ（Pulsar）相关的治理工作，其中一个部分内容关于消息队列的升级，比如：\n\n一键创建一个测试集群。\n运行一批测试用例，覆盖我们线上使用到的功能，并输出测试报告。\n模拟压测，输出测试结果。\n\n本质目的就是想直到新版本升级过程中和升级后对现有业务是否存在影响。\n\n\n一键创建集群和执行测试用例比较简单，利用了 helm 和 k8s client 的 SDK 把整个流程串起来即可。\n压测其实稍微麻烦一点的是压测，Pulsar 官方本身是有提供一个压测工具；只是功能相对比较单一，只能对某批 topic 极限压测，最后输出测试报告。最后参考了官方的压测流程，加入了一些实时监控数据，方便分析整个压测过程中性能的变化。\n客户端 timeout随着压测过程中的压力增大，比如压测时间和线程数的提高，客户端会抛出发送消息 timeout 异常。\norg.apache.pulsar.client.api.PulsarClientException$TimeoutException: The producer pulsar-test-212-20 can not send message to the topic persistent://my-tenant/my-ns/perf-topic-0 within given timeout : createdAt 82.964 seconds ago, firstSentAt 8.348 seconds ago, lastSentAt 8.348 seconds ago, retryCount 1\n\n而这个异常在生产业务环境的高峰期偶尔也出现过，这会导致业务数据的丢失；所以正好这次被我复现出来后想着分析下产生的原因以及解决办法。\n源码分析客户端既然是客户端抛出的异常所以就先看从异常点开始看起，其实整个过程和产生的原因并不复杂，如下图：\n\n客户端流程：\n\n客户端 producer 发送消息时先将消息发往本地的一个 pending 队列。\n待 broker 处理完（写入 bookkeeper) 返回 ACK 时删除该 pending 队列头的消息。\n后台启动一个定时任务，定期扫描队列头（头部的消息是最后写入的）的消息是否已经过期（过期时间可配置，默认30s)。\n如果已经过期（头部消息过期，说明所有消息都已过期）则遍历队列内的消息依次抛出 PulsarClientException$TimeoutException 异常，最后清空该队列。\n\n服务端 broker 流程：\n\n收到消息后调用 bookkeeper API 写入消息。\n写入消息时同时写入回调函数。\n写入成功后执行回调函数，这时会记录一条消息的写入延迟，并通知客户端 ACK。\n通过 broker metric 指标 pulsar_broker_publish_latency 可以获取写入延迟。\n\n从以上流程可以看出，如果客户端不做兜底措施则在第四步会出现消息丢失，这类本质上不算是 broker 丢消息，而是客户端认为当时 broker 的处理能力达到上限，考虑到消息的实时性从而丢弃了还未发送的消息。\n性能分析通过上述分析，特别是 broker 的写入流程得知，整个写入的主要操作便是写入 bookkeeper，所以 bookkeeper 的写入性能便关系到整个集群的写入性能。\n极端情况下，假设不考虑网络的损耗，如果 bookkeeper 的写入延迟是 0ms，那整个集群的写入性能几乎就是无上限；所以我们重点看看在压测过程中 bookkeeper 的各项指标。\nCPU首先是 CPU：\n从图中可以看到压测过程中 CPU 是有明显增高的，所以我们需要找到压测过程中 bookkeeper 的 CPU 大部分损耗在哪里？\n这里不得不吹一波阿里的 arthas 工具，可以非常方便的帮我们生成火焰图。\n\n分析火焰图最简单的一个方法便是查看顶部最宽的函数是哪个，它大概率就是性能的瓶颈。\n在这个图中的顶部并没有明显很宽的函数，大家都差不多，所以并没有明显损耗 CPU 的函数。\n此时在借助云厂商的监控得知并没有得到 CPU 的上限（limit 限制为 8核）。\n\n使用 arthas 过程中也有个小坑，在 k8s 环境中有可能应用启动后没有成功在磁盘写入 pid ，导致查询不到 Java 进程。\n$ java -jar arthas-boot.jar[INFO] arthas-boot version: 3.6.7[INFO] Can not find java process. Try to pass &lt;pid&gt; in command line.Please select an available pid.\n此时可以直接 ps 拿到进程 ID，然后在启动的时候直接传入 pid 即可。\n$ java -jar arthas-boot.jar 1\n通常情况下这个 pid 是 1。\n磁盘既然 CPU 没有问题，那就再看看磁盘是不是瓶颈；\n\n可以看到压测时的 IO 等待时间明显是比日常请求高许多，为了最终确认是否是磁盘的问题，再将磁盘类型换为 SSD 进行测试。\n果然即便是压测，SSD磁盘的 IO 也比普通硬盘的正常请求期间延迟更低。\n既然磁盘 IO 延迟降低了，根据前文的分析理论上整个集群的性能应该会有明显的上升，因此对比了升级前后的消息 TPS 写入指标：\n\n升级后每秒的写入速率由 40k 涨到 80k 左右，几乎是翻了一倍（果然用钱是最快解决问题的方式）；\n\n但即便是这样，极限压测后依然会出现客户端 timeout，这是因为无论怎么提高服务端的处理性能，依然没法做到没有延迟的写入，各个环节都会有损耗。\n\n升级过程中的 timeout还有一个关键的步骤必须要覆盖：模拟生产现场有着大量的生产者和消费者接入收发消息时进行集群升级，对客户端业务的影响。\n根据官方推荐的升级步骤，流程如下：\n\nUpgrade Zookeeper.\nDisable autorecovery.\nUpgrade Bookkeeper.\nUpgrade Broker.\nUpgrade Proxy.\nEnable autorecovery.\n\n其中最关键的是升级 Broker 和 Proxy，因为这两个是客户端直接交互的组件。\n本质上升级的过程就是优雅停机，然后使用新版本的 docker 启动；所以客户端一定会感知到 Broker 下线后进行重连，如果能快速自动重连那对客户端几乎没有影响。\n在我的测试过程中，2000左右的 producer 以 1k 的发送速率进行消息发送，在 30min 内完成所有组件升级，整个过程客户端会自动快速重连，并不会出现异常以及消息丢失。\n而一旦发送频率增加时，在重启 Broker 的过程中便会出现上文提到的 timeout 异常；初步看起来是在默认的 30s 时间内没有重连成功，导致积压的消息已经超时。\n经过分析源码发现关键的步骤如下：\n客户端在与 Broker 的长连接状态断开后会自动重连，而重连到具体哪台 Broker 节点是由 LookUpService 处理的，它会根据使用的 topic 获取到它的元数据。\n\n理论上这个过程如果足够快，对客户端就会越无感。\n\n在元数据中包含有该 topic 所属的 bundle 所绑定的  Broker 的具体 IP+端口，这样才能重新连接然后发送消息。\n\nbundle 是一批 topic 的抽象，用来将一批 topic 与 Broker 绑定。\n\n而在一个 Broker 停机的时会自动卸载它所有的 bundle，并由负载均衡器自动划分到在线的 Broker 中，交由他们处理。\n这里会有两种情况降低 LookUpSerive 获取元数据的速度：\n因为所有的 Broker 都是 stateful 有状态节点，所以升级时是从新的节点开始升级，假设是broker-5，假设升级的那个节点的 bundle 切好被转移 broker-4中，客户端此时便会自动重连到 4 这个Broker 中。\n此时客户端正在讲堆积的消息进行重发，而下一个升级的节点正好是 4，那客户端又得等待 bundle 成功 unload 到新的节点，如果恰好是 3 的话那又得套娃了，这样整个消息的重发流程就会被拉长，直到超过等待时间便会超时。\n还有一种情况是 bundle 的数量比较多，导致上面讲到的 unload 时更新元数据到 zookeeper 的时间也会增加。\n\n所以我在考虑 Broker 在升级过程中时，是否可以将 unload 的 bundle 优先与 Broker-0进行绑定，最后全部升级成功后再做一次负载均衡，尽量减少客户端重连的机会。\n\n解决方案如果我们想要解决这个 timeout 的异常，也有以下几个方案：\n\n将 bookkeeper 的磁盘换为写入时延更低的 SSD，提高单节点性能。\n增加 bookkeeper 节点，不过由于 bookkeeper 是有状态的，水平扩容起来比较麻烦，而且一旦扩容再想缩容也比较困难。\n增加客户端写入的超时时间，这个可以配置。\n客户端做好兜底措施，捕获异常、记录日志、或者入库都可以，后续进行消息重发。\n为 bookkeeper 的写入延迟增加报警。\nSpring 官方刚出炉的 Pulsar-starter 已经内置了 producer 相关的 metrics，客户端也可以对这个进行监控报警。\n\n以上最好实现的就是第四步了，效果好成本低，推荐还没有实现的都尽快 try catch 起来。\n整个测试流程耗费了我一两周的时间，也是第一次全方位的对一款中间件进行测试，其中也学到了不少东西；不管是源码还是架构都对 Pulsar 有了更深入的理解。\n","categories":["Pulsar"],"tags":["Pulsar","消息队列"]},{"title":"通过 Pulsar 源码彻底解决重复消费问题","url":"/2023/02/27/pulsar/pulsar-repeat-consume/","content":"\n背景最近真是和 Pulsar 杠上了，业务团队反馈说是线上有个应用消息重复消费。\n\n而且在测试环境是可以稳定复现的，根据经验来看一般能稳定复现的都比较好解决。\n\n\n定位问题接着便是定位问题了，根据之前的经验让业务按照这几种情况先排查一下：\n通过排查：1,2可以排除了。\n\n没有相关日志\n存在异常，但最外层也捕获了，所以不管有无异常都会 ACK。\n\n第三个也在消费的入口和提交消息出计算了时间，最终发现都是在2s左右 ACK 的。\n伪代码如下：\nConsumer consumer = client.newConsumer()        .subscriptionType(SubscriptionType.Shared)        .enableRetry(true)        .topic(topic)        .ackTimeout(30, TimeUnit.SECONDS)        .subscriptionName(&quot;my-sub&quot;)        .messageListener(new MessageListener&lt;byte[]&gt;() &#123;            @SneakyThrows            @Override            public void received(Consumer&lt;byte[]&gt; consumer, Message&lt;byte[]&gt; msg) &#123;                log.info(&quot;msg_id&#123;&#125;&quot;,msg.getMessageId().toString());                TimeUnit.SECONDS.sleep(2);                consumer.acknowledge(msg);            &#125;        &#125;)        .subscribe();\n\n那这就很奇怪了，因为代码里配置的 ackTimeout 是 30s，理论上来说是不会存在超时导致消息重发的。\n为了排除是否是超时引起的，直接将业务代码注释掉了，等于是消息收到后立即就 ACK，经过测试发现这样确实就没有重复消费了。\n为了再次确认是不是和 ackTimeout 有关，直接将 .ackTimeout(30, TimeUnit.SECONDS) 注释掉后测试，发现也没有重复消费了。\n确认原因既然如此那一定是和这个配置有关了，但看代码确实没有超时，为了定位具体原因只有去看 client 的源码了。\n这里简单梳理下消息的消费的流程：\n\n根据 .receiverQueueSize(1000) 的配置，默认情况下 broker 会直接给客户端推送 1000 条消息。\n客户端将这 1000 条消息保存到内部队列中。\n如果使用同步消费 receive() 时，本质上就是去 take 这个内部队列。\n如果是使用的是 messageListener 异步消费并配置 ackTimeout，每当从队列里获得一条消息后便会把这条消息加入 UnAckedMessageTracker 内部的一个时间轮中，定时检测顶部是否存在消息，如果存在则会触发重新投递。4.1 加入时间轮后，异步调用我们自定义的事件，这个异步操作是提交到一个无界队列中由单个线程依次排队执行（这点是这次问题的关键）\n业务 ACK 的时候会从时间轮中删除消息，所以如果消息 ACK 的足够快，在第四步就不会获取到消息进行重新投递。\n\n\n整体流程如上图，代码细节如下图：\n所以问题的根本原因就是写入时间轮（UnAckedMessageTracker）开始倒计时的线程和回调业务逻辑的不是同一个线程。\n如果业务执行耗时，等到消息从那个单线程的无界队列中取出来的时候很有可能已经过了 ackTimeou 的时间，从而导致了超时重发。\n也就是用户所理解的 ackTimeout 周期（应该进入回调时候开始计时）和 SDK 实现的不一致造成的。\n之后我再次确认同样的代码换为同步消费是没有问题的，不会导致重复消费：\nwhile (true) &#123;Message msg = consumer.receive();            log.info(                    &quot;consumer Message received: &quot; + new String(msg.getData()) + msg.getMessageId().toString());            TimeUnit.SECONDS.sleep(2);            consumer.acknowledge(msg);\t&#125;\n\n查看代码后发现同步代码的获取消息和加入 UnAckedMessageTracker 时间轮是同步的，也就不会出现超时的问题。\n总结所以其实 是messageListener 异步消费的 ackTimeout 的语义是有问题的，需要将加入 UnAckedMessageTracker 处移动到回调函数中同步调用。\n我查看了最新的 2.11.x 版本的代码依然没有修复，正准备提个 PR 切换到 master 时才发现已经有相关的 PR 了，只是还没有发版。\n修复的背景和思路也是类似的，具体参考：\nhttps://github.com/apache/pulsar/pull/18911\n其实业务中并不推荐使用 ackTimeout 这个配置了，不好预估时间从而导致超时，而且我相信大部分业务配置好 ackTImeout 后直到后续出问题的时候才想起来要改。所以干脆一开始就不要使用。\n在 go 版本的 SDK 中直接废弃掉了这个参数，推荐使用 nack API 替换。\n\n","categories":["Pulsar"],"tags":["Pulsar","Consumer"]},{"title":"Pulsar 入门及介绍","url":"/2021/04/18/pulsar/pulsar-start/","content":"\n背景我们最近在做新业务的技术选型，其中涉及到了对消息中间件的选择；结合我们的实际情况希望它能满足以下几个要求：\n\n友好的云原生支持：因为现在的主力语言是 Go，同时在运维上能够足够简单。\n官方支持多种语言的 SDK：还有一些 Python、Java 相关的代码需要维护。\n最好是有一些方便好用的特性，比如：延时消息、死信队列、多租户等。\n\n\n\n\n当然还有一些水平扩容、吞吐量、低延迟这些特性就不用多说了，几乎所有成熟的消息中间件都能满足这些要求。\n基于以上的筛选条件，Pulsar 进入了我们的视野。\n作为 Apache 下的顶级项目，以上特性都能很好的支持。\n下面我们来它有什么过人之处。\n架构\n从官方的架构图中可以看出 Pulsar 主要有以下组件组成：\n\nBroker 无状态组件，可以水平扩展，主要用于生产者、消费者连接；与 Kafka 的 broker 类似，但没有数据存储功能，因此扩展更加轻松。\nBookKeeper 集群：主要用于数据的持久化存储。\nZookeeper 用于存储 broker 与 BookKeeper 的元数据。\n\n整体一看似乎比 Kafka 所依赖的组件还多，这样确实会提供系统的复杂性；但同样的好处也很明显。\nPulsar 的存储于计算是分离的，当需要扩容时会非常简单，直接新增 broker 即可，没有其他的心智负担。\n当存储成为瓶颈时也只需要扩容 BookKeeper，不需要人为的做重平衡，BookKeeper 会自动负载。\n同样的操作，Kafka 就要复杂的多了。\n特性多租户多租户也是一个刚需功能，可以在同一个集群中对不同业务、团队的数据进行隔离。\npersistent://core/order/create-order\n\n以这个 topic 名称为例，在 core 这个租户下有一个 order 的 namespace，最终才是 create-order 的 topic 名称。\n在实际使用中租户一般是按照业务团队进行划分，namespace 则是当前团队下的不同业务；这样便可以很清晰的对 topic 进行管理。\n通常有对比才会有伤害，在没有多租户的消息中间件中是如何处理这类问题的呢：\n\n干脆不分这么细，所有业务线混着用，当团队较小时可能问题不大；一旦业务增加，管理起来会非常麻烦。\n自己在 topic 之前做一层抽象，但其实本质上也是在实现多租户。\n各个业务团队各自维护自己的集群，这样当然也能解决问题，但运维复杂度自然也就提高了。\n\n以上就很直观的看出多租户的重要性了。\nFunction 函数计算Pulsar 还支持轻量级的函数计算，例如需要对某些消息进行数据清洗、转换，然后再发布到另一个 topic 中。\n这类需求就可以编写一个简单的函数，Pulsar 提供了 SDK 可以方便的对数据进行处理，最后使用官方工具发布到 broker 中。\n在这之前这类简单的需求可能也需要自己处理流处理引擎。\n应用除此之外的上层应用，比如生产者、消费者这类概念与使用大家都差不多。\n比如 Pulsar 支持四种消费模式：\n\nExclusive：独占模式，同时只有一个消费者可以启动并消费数据；通过 SubscriptionName 标明是同一个消费者），适用范围较小。\nFailover 故障转移模式：在独占模式基础之上可以同时启动多个 consumer，一旦一个 consumer  挂掉之后其余的可以快速顶上，但也只有一个 consumer 可以消费；部分场景可用。\nShared 共享模式：可以有 N 个消费者同时运行，消息按照 round-robin 轮询投递到每个 consumer 中；当某个 consumer 宕机没有 ack 时，该消息将会被投递给其他消费者。这种消费模式可以提高消费能力，但消息无法做到有序。\nKeyShared 共享模式：基于共享模式；相当于对同一个topic中的消息进行分组，同一分组内的消息只能被同一个消费者有序消费。\n\n第三种共享消费模式应该是使用最多的，当对消息有顺序要求时可以使用 KeyShared 模式。\nSDK\n官方支持的 SDK 非常丰富；我也在官方的 SDK 的基础之上封装了一个内部使用的 SDK。\n因为我们使用了 dig 这样的轻量级依赖注入库，所以使用起来大概是这个样子：\nSetUpPulsar(lookupURL)container := dig.New()container.Provide(func() ConsumerConfigInstance &#123;\treturn NewConsumer(&amp;pulsar.ConsumerOptions&#123;\t\tTopic:            &quot;persistent://core/order/create-order&quot;,\t\tSubscriptionName: &quot;order-sub&quot;,\t\tType:             pulsar.Shared,\t\tName:             &quot;consumer01&quot;,\t&#125;, ConsumerOrder)&#125;)container.Provide(func() ConsumerConfigInstance &#123;\treturn NewConsumer(&amp;pulsar.ConsumerOptions&#123;\t\tTopic:            &quot;persistent://core/order/update-order&quot;,\t\tSubscriptionName: &quot;order-sub&quot;,\t\tType:             pulsar.Shared,\t\tName:             &quot;consumer02&quot;,\t&#125;, ConsumerInvoice)&#125;)container.Invoke(StartConsumer)\n\n其中的两个 container.Provide() 函数用于注入 consumer 对象。\ncontainer.Invoke(StartConsumer)  会从容器中取出所有的 consumer 对象，同时开始消费。\n这时以我有限的 Go 开发经验也在思考一个问题，在 Go 中是否需要依赖注入？\n先来看看使用 Dig 这类库所带来的好处：\n\n对象交由容器管理，很方便的实现单例。\n当各个对象之前依赖关系复杂时，可以减少许多创建、获取对象的代码，依赖关系更清晰。\n\n同样的坏处也有：\n\n跟踪阅读代码时没有那么直观，不能一眼看出某个依赖对象是如何创建的。\n与 Go 所推崇的简洁之道不符。\n\n对于使用过 Spring 的 Java 开发者来说肯定直呼真香，毕竟还是熟悉的味道；但对于完全没有接触过类似需求的 Gopher 来说貌似也不是刚需。\n目前市面上各式各样的 Go 依赖注入库层出不穷，也不乏许多大厂出品，可见还是很有市场的。\n我相信有很多 Gopher 非常反感将 Java 中的一些复杂概念引入到 Go，但我觉得依赖注入本身是不受语言限制，各种语言也都有自己的实现，只是 Java 中的 Spring 不仅仅只是一个依赖注入框架，还有许多复杂功能，让许多开发者望而生畏。\n如果只是依赖注入这个细分需求，实现起来并不复杂，并不会给带来太多复杂度。如果花时间去看源码，在理解概念的基础上很快就能掌握。\n回到 SDK 本身来说，Go 的 SDK 现阶段要比 Java 版本的功能少（准确来说只有 Java 版的功能最丰富），但核心的都有了，并不影响日常使用。\n总结本文介绍了 Pulsar 的一些基本概念与优点，同时顺便讨论一下 Go 的依赖注入；如果大家和我们一样在做技术选型，不妨考虑一下 Pulsar。\n后续会继续分享 Pulsar 的相关内容，有相关经验的朋友也可以在评论区留下自己的见解。\n","categories":["Pulsar"],"tags":["Go","Pulsar","消息队列"]},{"title":"sbc(七)分布式限流","url":"/2018/04/28/sbc/sbc7-Distributed-Limit/","content":"\n前言本文接着上文应用限流进行讨论。\n之前谈到的限流方案只能针对于单个 JVM 有效，也就是单机应用。而对于现在普遍的分布式应用也得有一个分布式限流的方案。\n基于此尝试写了这个组件：\nhttps://github.com/crossoverJie/distributed-redis-tool\nDEMO以下采用的是\nhttps://github.com/crossoverJie/springboot-cloud\n来做演示。\n在 Order 应用提供的接口中采取了限流。首先是配置了限流工具的 Bean:\n@Configurationpublic class RedisLimitConfig &#123;    @Value(&quot;$&#123;redis.limit&#125;&quot;)    private int limit;    @Autowired    private JedisConnectionFactory jedisConnectionFactory;    @Bean    public RedisLimit build() &#123;        RedisClusterConnection clusterConnection = jedisConnectionFactory.getClusterConnection();        JedisCluster jedisCluster = (JedisCluster) clusterConnection.getNativeConnection();        RedisLimit redisLimit = new RedisLimit.Builder&lt;&gt;(jedisCluster)                .limit(limit)                .build();        return redisLimit;    &#125;&#125;\n\n接着在 Controller 使用组件：\n@Autowiredprivate RedisLimit redisLimit ;@Override@CheckReqNopublic BaseResponse&lt;OrderNoResVO&gt; getOrderNo(@RequestBody OrderNoReqVO orderNoReq) &#123;    BaseResponse&lt;OrderNoResVO&gt; res = new BaseResponse();    //限流    boolean limit = redisLimit.limit();    if (!limit)&#123;        res.setCode(StatusEnum.REQUEST_LIMIT.getCode());        res.setMessage(StatusEnum.REQUEST_LIMIT.getMessage());        return res ;    &#125;    res.setReqNo(orderNoReq.getReqNo());    if (null == orderNoReq.getAppId())&#123;        throw new SBCException(StatusEnum.FAIL);    &#125;    OrderNoResVO orderNoRes = new OrderNoResVO() ;    orderNoRes.setOrderId(DateUtil.getLongTime());    res.setCode(StatusEnum.SUCCESS.getCode());    res.setMessage(StatusEnum.SUCCESS.getMessage());    res.setDataBody(orderNoRes);    return res ;&#125;\n\n为了方便使用，也提供了注解:\n@Override@ControllerLimitpublic BaseResponse&lt;OrderNoResVO&gt; getOrderNoLimit(@RequestBody OrderNoReqVO orderNoReq) &#123;    BaseResponse&lt;OrderNoResVO&gt; res = new BaseResponse();    // 业务逻辑    return res ;&#125;\n该注解拦截了 http 请求，会再请求达到阈值时直接返回。\n普通方法也可使用:\n@CommonLimitpublic void doSomething()&#123;&#125;\n\n会在调用达到阈值时抛出异常。\n为了模拟并发，在 User 应用中开启了 10 个线程调用 Order(限流次数为5) 接口(也可使用专业的并发测试工具 JMeter 等)。\n\n\n@Overridepublic BaseResponse&lt;UserResVO&gt; getUserByFeign(@RequestBody UserReqVO userReq) &#123;    //调用远程服务    OrderNoReqVO vo = new OrderNoReqVO();    vo.setAppId(1L);    vo.setReqNo(userReq.getReqNo());    for (int i = 0; i &lt; 10; i++) &#123;        executorService.execute(new Worker(vo, orderServiceClient));    &#125;    UserRes userRes = new UserRes();    userRes.setUserId(123);    userRes.setUserName(&quot;张三&quot;);    userRes.setReqNo(userReq.getReqNo());    userRes.setCode(StatusEnum.SUCCESS.getCode());    userRes.setMessage(&quot;成功&quot;);    return userRes;&#125;private static class Worker implements Runnable &#123;    private OrderNoReqVO vo;    private OrderServiceClient orderServiceClient;    public Worker(OrderNoReqVO vo, OrderServiceClient orderServiceClient) &#123;        this.vo = vo;        this.orderServiceClient = orderServiceClient;    &#125;    @Override    public void run() &#123;        BaseResponse&lt;OrderNoResVO&gt; orderNo = orderServiceClient.getOrderNoCommonLimit(vo);        logger.info(&quot;远程返回:&quot; + JSON.toJSONString(orderNo));    &#125;&#125;    \n\n\n为了验证分布式效果启动了两个 Order 应用。\n\n\n效果如下：\n\n\n实现原理实现原理其实很简单。既然要达到分布式全局限流的效果，那自然需要一个第三方组件来记录请求的次数。\n其中 Redis 就非常适合这样的场景。\n\n每次请求时将当前时间(精确到秒)作为 Key 写入到 Redis 中，超时时间设置为 2 秒，Redis 将该 Key 的值进行自增。\n当达到阈值时返回错误。\n写入 Redis 的操作用 Lua 脚本来完成，利用 Redis 的单线程机制可以保证每个 Redis 请求的原子性。\n\nLua 脚本如下:\n--lua 下标从 1 开始-- 限流 keylocal key = KEYS[1]-- 限流大小local limit = tonumber(ARGV[1])-- 获取当前流量大小local curentLimit = tonumber(redis.call(&#x27;get&#x27;, key) or &quot;0&quot;)if curentLimit + 1 &gt; limit then    -- 达到限流大小 返回    return 0;else    -- 没有达到阈值 value + 1    redis.call(&quot;INCRBY&quot;, key, 1)    redis.call(&quot;EXPIRE&quot;, key, 2)    return curentLimit + 1end\n\nJava 中的调用逻辑:\npublic boolean limit() &#123;    String key = String.valueOf(System.currentTimeMillis() / 1000);    Object result = null;    if (jedis instanceof Jedis) &#123;        result = ((Jedis) this.jedis).eval(script, Collections.singletonList(key), Collections.singletonList(String.valueOf(limit)));    &#125; else if (jedis instanceof JedisCluster) &#123;        result = ((JedisCluster) this.jedis).eval(script, Collections.singletonList(key), Collections.singletonList(String.valueOf(limit)));    &#125; else &#123;        //throw new RuntimeException(&quot;instance is error&quot;) ;        return false;    &#125;    if (FAIL_CODE != (Long) result) &#123;        return true;    &#125; else &#123;        return false;    &#125;&#125;\n\n所以只需要在需要限流的地方调用该方法对返回值进行判断即可达到限流的目的。\n当然这只是利用 Redis 做了一个粗暴的计数器，如果想实现类似于上文中的令牌桶算法可以基于 Lua 自行实现。\nBuilder 构建器在设计这个组件时想尽量的提供给使用者清晰、可读性、不易出错的 API。\n\n比如第一步，如何构建一个限流对象。\n\n最常用的方式自然就是构造函数，如果有多个域则可以采用重叠构造器的方式:\npublic A()&#123;&#125;public A(int a)&#123;&#125;public A(int a,int b)&#123;&#125;\n\n缺点也是显而易见的：如果参数过多会导致难以阅读，甚至如果参数类型一致的情况下客户端颠倒了顺序，但不会引起警告从而出现难以预测的结果。\n第二种方案可以采用 JavaBean 模式，利用 setter 方法进行构建:\nA a = new A();a.setA(a);a.setB(b);\n\n这种方式清晰易读，但却容易让对象处于不一致的状态，使对象处于线程不安全的状态。\n所以这里采用了第三种创建对象的方式，构建器：\npublic class RedisLimit &#123;    private JedisCommands jedis;    private int limit = 200;    private static final int FAIL_CODE = 0;    /**     * lua script     */    private String script;    private RedisLimit(Builder builder) &#123;        this.limit = builder.limit ;        this.jedis = builder.jedis ;        buildScript();    &#125;    /**     * limit traffic     * @return if true     */    public boolean limit() &#123;        String key = String.valueOf(System.currentTimeMillis() / 1000);        Object result = null;        if (jedis instanceof Jedis) &#123;            result = ((Jedis) this.jedis).eval(script, Collections.singletonList(key), Collections.singletonList(String.valueOf(limit)));        &#125; else if (jedis instanceof JedisCluster) &#123;            result = ((JedisCluster) this.jedis).eval(script, Collections.singletonList(key), Collections.singletonList(String.valueOf(limit)));        &#125; else &#123;            //throw new RuntimeException(&quot;instance is error&quot;) ;            return false;        &#125;        if (FAIL_CODE != (Long) result) &#123;            return true;        &#125; else &#123;            return false;        &#125;    &#125;    /**     * read lua script     */    private void buildScript() &#123;        script = ScriptUtil.getScript(&quot;limit.lua&quot;);    &#125;    /**     *  the builder     * @param &lt;T&gt;     */    public static class Builder&lt;T extends JedisCommands&gt;&#123;        private T jedis = null ;        private int limit = 200;        public Builder(T jedis)&#123;            this.jedis = jedis ;        &#125;        public Builder limit(int limit)&#123;            this.limit = limit ;            return this;        &#125;        public RedisLimit build()&#123;            return new RedisLimit(this) ;        &#125;    &#125;&#125;\n\n这样客户端在使用时:\nRedisLimit redisLimit = new RedisLimit.Builder&lt;&gt;(jedisCluster)                .limit(limit)                .build();\n\n更加的简单直接，并且避免了将创建过程分成了多个子步骤。\n这在有多个构造参数，但又不是必选字段时很有作用。\n因此顺便将分布式锁的构建器方式也一并更新了：\nhttps://github.com/crossoverJie/distributed-redis-tool#features\n\n更多内容可以参考 Effective Java\n\nAPI从上文可以看出，使用过程就是调用 limit 方法。\n//限流 boolean limit = redisLimit.limit(); if (!limit)&#123;    //具体限流逻辑 &#125;\n\n为了减少侵入性，也为了简化客户端提供了两种注解方式。\n@ControllerLimit该注解可以作用于 @RequestMapping 修饰的接口中，并会在限流后提供限流响应。\n实现如下：\n@Componentpublic class WebIntercept extends WebMvcConfigurerAdapter &#123;    private static Logger logger = LoggerFactory.getLogger(WebIntercept.class);    @Autowired    private RedisLimit redisLimit;    @Override    public void addInterceptors(InterceptorRegistry registry) &#123;        registry.addInterceptor(new CustomInterceptor())                .addPathPatterns(&quot;/**&quot;);    &#125;    private class CustomInterceptor extends HandlerInterceptorAdapter &#123;        @Override        public boolean preHandle(HttpServletRequest request, HttpServletResponse response,                                 Object handler) throws Exception &#123;            if (redisLimit == null) &#123;                throw new NullPointerException(&quot;redisLimit is null&quot;);            &#125;            if (handler instanceof HandlerMethod) &#123;                HandlerMethod method = (HandlerMethod) handler;                ControllerLimit annotation = method.getMethodAnnotation(ControllerLimit.class);                if (annotation == null) &#123;                    //skip                    return true;                &#125;                boolean limit = redisLimit.limit();                if (!limit) &#123;                    logger.warn(&quot;request has bean limit&quot;);                    response.sendError(500, &quot;request limit&quot;);                    return false;                &#125;            &#125;            return true;        &#125;    &#125;&#125;\n\n其实就是实现了 SpringMVC 中的拦截器，并在拦截过程中判断是否有使用注解，从而调用限流逻辑。\n前提是应用需要扫描到该类，让 Spring 进行管理。\n@ComponentScan(value = &quot;com.crossoverjie.distributed.intercept&quot;)\n\n@CommonLimit当然也可以在普通方法中使用。实现原理则是 Spring AOP (SpringMVC 的拦截器本质也是 AOP)。\n@Aspect@Component@EnableAspectJAutoProxy(proxyTargetClass = true)public class CommonAspect &#123;    private static Logger logger = LoggerFactory.getLogger(CommonAspect.class);    @Autowired    private RedisLimit redisLimit ;    @Pointcut(&quot;@annotation(com.crossoverjie.distributed.annotation.CommonLimit)&quot;)    private void check()&#123;&#125;    @Before(&quot;check()&quot;)    public void before(JoinPoint joinPoint) throws Exception &#123;        if (redisLimit == null) &#123;            throw new NullPointerException(&quot;redisLimit is null&quot;);        &#125;        boolean limit = redisLimit.limit();        if (!limit) &#123;            logger.warn(&quot;request has bean limit&quot;);            throw new RuntimeException(&quot;request has bean limit&quot;) ;        &#125;    &#125;&#125;\n\n很简单，也是在拦截过程中调用限流。\n当然使用时也得扫描到该包:\n@ComponentScan(value = &quot;com.crossoverjie.distributed.intercept&quot;)\n\n总结限流在一个高并发大流量的系统中是保护应用的一个利器，成熟的方案也很多，希望对刚了解这一块的朋友提供一些思路。\n以上所有的源码：\n\nhttps://github.com/crossoverJie/distributed-redis-tool\nhttps://github.com/crossoverJie/springboot-cloud\n\n感兴趣的朋友可以点个 Star 或是提交 PR。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["sbc","Distributed Tools"],"tags":["Distributed Limited"]},{"title":"1K star+ 的项目是如何炼成的？","url":"/2018/05/15/skill/1Kstar/","content":"\n前言首先标题党一下，其实这篇文章主要是记录我的第二个过 1K star 的项目 Java-Interview，顺便分享下其中的过程及经验。\n\n需求选择Java-Interview之所以要做这个项目主要是当时我正在面阿里的两个部门，非常幸运的是技术面都过了。其中的过程真是让我受益匪浅更是印象深刻，所以就想把期间的问题记录下来，加上自己的理解希望能对其他朋友起到帮助。\n正好那段时间也是传说中的金三银四，所以无形中也叫顺势而为吧😏。\n\n\nSSM这个项目的历史就比较悠久了，我看了下第一次提交差不多是两年前。\n从这个名字也可以看出当初还是一个刚入行没多久的小菜鸟，因为之前在学 Java 的时候真的走了很多冤枉路，所以从头开始记录到现在整个过程所学到的东西，踩过的坑。\n由于是面向小白，入门简单，上手较快也取的了一定的关注。\n其实从这两个项目可以看出选择一个方向是很重要的。\n以及该项目解决了什么问题，长期的规划，受众是哪些都要考虑清楚(怎么有点像做产品😅，其实这就是你自己的产品)。\n比如这两个项目的目标：\n\nJava-Interview：持续更新面试问题，希望能让面试者知其然也知其所以然。\nSSM：博主从小白到现在实际开发所遇到的问题记录，以及实战经验，现在逐渐会分享一些难点以及底层。受众大多是小白。\n\n文档很重要既然项目做出来是给人用的，那文档就显得至关重要了。\n就像日常和前端怼接口时，有一个标准的文档输出比在白板上折腾半天要高的多。\n\n其实仔细观察 GitHub 上热门的项目，会发现他们的文档几乎都有一些共同结构：\n\n简单描述项目是干什么的。\n快速启动。\n最近更新。\nQ&#x2F;A 答疑。\n项目截图。\n\n主要目的就是要简单易读，快速上手。\n然后把一些复杂的如系统设计、开发指南等可以放到 wiki 中。\n\n切记不要什么东西都往 README.MD 中写，保持一个简洁的文档可以加分哦。\n\n当然也可以在首页加入一些徽章如：\n\n也能起到一些积极作用。\n积极推荐代码质量这个就不多说了，这应该是最基本的要求。\n俗话说：酒香不怕巷子深。\n但对于做开源项目来说就不太适应了，当你幸辛苦苦做了一个自认为很不错的项目，结果一年过去了都无人问津，这不免会有点打击积极性。\n所以适当的自我推荐就很有必要了。\n\n\n\n上图是我博客、项目的主要流量来源。\n下面是我自身体验比较优质的推荐渠道：\n\n开发者头条：由于截图的时候没有新发文章，之前那篇秒杀架构实践发了之后博客 80% 的流量都是从头条过来的，而且质量很高，不得不点个赞。\n并发编程网: 并发编程网是由阿里大牛清英(买了那本《并发编程的艺术》就被圈粉了)创办的，其中的文章质量普遍较高(导致也会有一点写作门槛)。由于网站的流量也比较高，只要你的文章质量不错肯定会得到好处。\n掘金：掘金这两年也比较火，是专门做开发者内容的，也是网站流量不错。\n开源中国：开源中国的博客也不错，自己也有代码托管，但我还是更喜欢用 GitHub，一般上了编辑推荐都会有不错的访问量。\nV2EX：大名鼎鼎的 V 站，其实受众较少，正因为如此也形成了独有的文化，因此也是我每天比逛(摸鱼)的网站，由于受众大多是开发者所以也能得到很多有用的反馈。\n大佬推荐：最快捷的方式其实就是口口相传，其中当然是大佬的效率最高。之前有个纯洁的微笑、程序猿DD 都投过稿，也能带来不错的流量。\n简书:本来不想推荐简书的（之前的事件以及现在鸡汤太多），但是流量还可以，现在就纯粹当做博客备份的工具了。\n\n\n坚持下来之后会发现：只要自己坚持、保证质量最后会形成自己的阅读圈子，到后面甚至会有其他朋友主动来找你分享，这些都是自我提升的过程。\n\n不忘初心当初做的第一个开源项目就是 SSM，完全受够学习时找资料的痛苦，也得到了很多人的帮助，所以才有了该项目。\n平时工作中或多或少都会用到开源项目，其实我们大部分人也写不出 Spring、Guava 这样的项目，只是再这过程中可以参与进去，收获也是非常丰富的。\n两年前参与开源到现在有收到面试邀请、物质奖励这些都是正面积极的，可以鼓励我们接着做下去。\n但最多的还是在这过程中结识了很多朋友，技术能力提升也很明显，这些都是保持自我可持续发展的必要条件。\n","categories":["小技巧"],"tags":["GitHub"]},{"title":"年轻人的第一篇博客","url":"/2019/05/24/skill/first-blog/","content":"\n前言写这篇文章的前因是有位读者留言提到了相关的话题，其实在之前有一篇《如何成为一位「不那么差」的程序员》时有简要提到但没有细说；这次就借这个机会好好聊聊这个事情的前因后果。\n\n\n\n为什么要写博客为什么要写博客？ 我觉得大部分人应该都知道标准答案。\n\n翻了下记录，我从 16 年四月份至今写了三年的博客，产出了 100 多篇；现在让我回忆当初为啥要写博客，我还记得那时作为一个初入职场的小菜鸟有许多问题、资料都要在网上查找；那时候我就发现不少答案网上已经有现成的了，而且有些大牛还有着酷炫的个人网站。\n这事对我冲击挺大的，主要有以下几点：\n\n我能在别人博客上查到我想要的东西，那是否我也可以输出一些东西被被人搜到呢？\n这样是否对方便记录我自己的问题（高中时的错题本）甚至对今后面试有好处？\n酷炫的博客页面又是一个展示（装B）自己的机会。\n\n我相信大部分人无非也是这几个原因吧，具体是哪个原因我觉得大家首先要想清楚。\n因为如果是前面两个原因，也许后面会因为博客内容帮助到自己的同时也帮助到他人，让自己更有坚持下去的动力。\n如果是最后一个，大概率的会因为自己不想折腾而慢慢放弃。\n我现在主要还是前面两个原因而继续坚持，毕竟我的博客外观已经几年没更新了🤣。\n你适合写博客嘛？目的搞清楚之后再来看看你适合写博客嘛？不过我觉得这个问题改为 你会坚持写博客嘛？ 可能更直接一些。\n因为我认为写博客最大的阻碍就是【坚持】二字了。\n我见过不少博客写的很棒的大牛突然之间停更，至今也没有恢复，其实挺可惜的。\n再讨论这个问题之前首先看两个问题：\n\n做什么事情自己坚持的最久？\n什么原因导致没有坚持下去？\n\n我还记得大学期间我和另外两个小伙伴组建了一个篮球小团伙，每天中午、晚上都会自觉的约上一起练球直到后来毕业。\n现在想想当初为啥会坚持呢？我觉得核心要求有几个：\n\n一个是【热爱】，当初我们是真的热爱这个运动，每天不打场球真的浑身不舒服。\n二是有【产出】，大学那阵由于每天我们都训练进步还是非常明显的，在学校球场基本上是横着走；以致于在球场的知名度也越来越高，当然在虚荣心的驱使下更会拼命练球。\n\n所以回归到写博客这件事情上来，我觉得只要你对这件事是喜欢的，有激情，同时写出的东西对你有正向激励就会让自己有坚持下去的理由。\n\n第二个问题：什么原因导致没有坚持下去？\n就像我去年立下的健身 flag 一样，现在也放弃了。\n归根到底还是因为太懒了，锻炼多麻烦，站着不如坐着，坐着不如躺着；同时效果也不明显导致非常容易放弃。\n换到写博客这事上来也是一样的。\n写一篇不吹水的文章并没有想象那么容易，需要找资料、论证、写提纲、码字等一系列过程。\n和健身一样也是一个长期同时效果不明显的工程，所以弃坑率很高。\n鉴于这些，我觉得你具备这几点作为一个长更（长期更新）型选手是没有问题的。\n\n热爱技术、文字表达，而不是短时间的头脑发热。\n写的过程中自己能得到积累提高，而不是每天记录流水账。\n要有毅力坚持下去，这个就比较玄学了；通常前面两项做的好这个毅力就会随之增强。\n\n所以自己是否适合写博客应该有自己的判断了。\n会遇到哪些问题我还得要给大家泼盆凉水，现在来聊聊写博客过程中可能会碰到哪些问题，不一定都对但至少都是我遇到过的。\n原创、抄袭首先第一个就是 抄袭 问题；\n随着这几年互联网的发展，自媒体也越来越多，这也包括了大量的技术自媒体。\n也许哪天你会收到某平台向你发出申请，想要转载你的文章；自己在得意之余还得要谨慎对待以下几件事情：\n\n对方是否标明出处，包括但不限于二维码、用户ID、网站地址等。\n是否明显标注出处；不要小瞧这个，很多鸡贼的作者确实也会标明出处，但你不拿出老花镜来看是根本发现不了的。（这类撕 B 事件不止听说一两次）\n对于自己（这里的自己指申请者）需要声明原创的要求是否提供了稿费？写一篇原创文章是很费精力的，该拿的一定不能亏了自己。\n\n这些其实都还好，还有一些不知名的平台或作者会神不知鬼不觉的转载你的内容，完全不提出处这件事。\n甚至有些还会修改你的部分内容，转换为他自己的一部分然后发表在一些类似于“头条号、百家号”之类的自媒体平台上；也就是大家常说的“洗搞”。\n我就不止遇到过一次，刚开始还很气愤上去理论；态度好的会道个歉然后删除文章，态度不好的甚至还会和你互喷让你拿出证据证明这是你写的。\n\n经过 N 多次的斗智斗勇后现在即便是有朋友给我说 XX 平台上好像有你的内容，我也非常淡定，啥大风大浪没有见过。\n换个角度想，别人转载不注明出处、甚至抄袭不也侧面证明自己写的还不错嘛？开个玩笑，其实主要原因还是当前的写作环境复杂，没有某一个大的平台来约束所有的产出内容；在这样的客观条件下我们能做的也只有产出优质的内容扩大自己的影响力从而让某些抄袭者不敢轻举乱动。\n产出瓶颈还有一个问题也挺突出，那就是产出问题。\n有段时间我甚至能做到一个月 6 篇原创，当然也有一个月憋出一篇的尴尬。\n这也是没办法的事情，内容产出不像是工地搬砖，花时间就会有效果。\n这个问题我相信 99% 写原创的作者都会碰到，根据我目前的经验还是有几个小 tips 可以提供给大家：\n\n平时一定要多积累，不管是工作中的案例还是业余学到的小技巧都可以；这样不用等到想写点什么的时候没有灵感的尴尬。\n实在不知道写啥时切莫强行输出，也许这一篇文章就能把你之前积累的口碑破坏。\n没有题材时不如多翻翻之前写的东西，也许就有了新的灵感。\n\n能赚钱嘛？问题讲了这么多来看看现实的问题，这事能赚钱嘛？\n有这个想法也没错，毕竟要花费自己大量的业余时间及精力。\n首先可以很明确的告诉大家：做的好是可以有收入的。\n怎么来定义这个做的好呢？直接点说就是有流量，一天多少 PV、UV。\n现在只要有流量那就自然能有转化，我想大家应该也猜到了，这样的转化通常是指推广、广告的转化。\n比如当某天你的个人博客的 PV 达到 1000 或者某个数值后被搜索引擎录入，自然就会有人找你投放广告。\n我这个博客做了三年多现在也才 2000 多点的 PV，虽说也有人找我投放推广，不过我一直觉得博客比较私人还是单纯点，所以一直也没接。\n但是像阮一峰这样的个人博客流量是非常大的，所以也会有专门的广告位出售。\n所以归根结底的问题是如何才能获取到足够多的流量，我觉得还是两点：\n\n足够靠谱的质量，有了好的产出后才会吸引更多人从而形成裂变效应。\n足够的曝光，酒香也怕巷子深；这点会在后面详细说到。\n\n博客平台的选择万事俱备，博客也写好了，如何来发布增加我们刚才提到的曝光呢？\n这时就要看如何选择博客平台了，目前我收集到的有以下方案：\n\n第三方博客平台，如博客园、CSDN、简书、掘金等。\n自建博客 Hexo、Ghost、Wordpress。\nGitHub Issue。\n公众号\n\n下面总结了各自的优缺点：\n\n\n\n\n优点\n缺点\n\n\n\n第三方平台\n通常都有良好的曝光率以及完整的审核体系，优秀的博客官方推荐后可以增加曝光量。\n内容会有审查，同时某些平台的机制比较恶心，有些平台还不支持 Markdown，这里就不点名了。（其实也不算啥缺点）\n\n\n自建博客\n内容随意，自己管控，不受限制。\n除非已经很知名了，不然很难获取到曝光量，自己不做备份数据还容易丢失。同时可能还需要额外的服务器维护费用（GitHub Pages 服务除外）。\n\n\nGitHub Issues\n这个也受到广大开发者的喜欢，对于 GitHub 重度使用者来说没有任何使用负担，符合开发者平时讨论 Issues 的逻辑，同时数据安全可以得到 GitHub 的保证，内容也比较自由。\n毕竟依赖于 GitHub，一旦它除了什么问题那就惨了，同时访问速度也有限制，国内的搜索引擎难以收录。\n\n\n公众号\n这个单独放一类，其实也属于第三方平台；但这几年公众号确实太火了，已经形成了独立的生态圈子。好处：自带流量主，可以对号主带来一些额外收入，同时接推广的几率会高很多\n本身平台是封闭的，很难传播到圈外，不过如果已经在公众号有足够影响力了也不在乎这些。\n\n\n对比了四种方案来看看我是如何使用的。\n其实这几种都用到了。\n第三方平台比如第三方平台，几乎目前市面上叫得上号可以写博客的地方我都会去掺和一下，这样有几个好处：\n\n相当于数据异地容灾了，互相备份。\n曝光率也会得到提升。\n也能降低被抄袭盗版的可能，可以直接在平台举报他。\n\n下面就是我常用的一些平台。\n\n自建博客自建博客目前是使用的是 Hexo,用它来生成静态页面还是非常方便的，依赖于 GitHub Pages 服务甚至都不要额外的服务器，只是访问速度在国内确实有点捉急。\nHexo 其实也有着丰富的主题，现在确实懒得折腾，我这主题都用了几年了。\nGitHub其实我并没有用 GitHub Issues 来搭博客，只是将博客的原始文件用 GitHub 来备份了，由于内容是采用 Markdown 编写的，所以 GitHub 可以直接解析，这样在这个仓库里其实也可以直接当做博客查看。\n类似于这样：\n\n所以也非常推荐大家博文都用 Markdown 来书写，这样还可以一键复制到不同的平台。\n公众号我大概是从去年 3 月份开始做公众号的，其实前期也就是把博客同步一份过来，几乎没怎么运营。\n到现在也算是从 0 到 1 ，不过经验还是比较少，这个要做好还是要花费许多精力，我这刚入门的菜鸟还不能给大家分享太多。\n不过有一说一，公众号做好后接到推广的概率会比其他平台大的多，我也见过好几个大佬离职后专心做公号的（当然自身量得起来）。\n总结扯了这么多，有困难也有好处。\n我也知道不少人都想着下周我一定要写一篇，往往这事在下周还会想一遍；所以别再做死循环递归了，总有一天会 StackOverFlow 的。\n赶紧拿起键盘开码吧，欢迎大家在评论区留下自己的博客😏。\n你的点赞与分享是对我最大的支持\n","categories":["小技巧"],"tags":["博客"]},{"title":"什么样的简历不会被丢进回收站","url":"/2018/08/21/skill/resume/","content":"\n前言从去年校招开始到现在负责部门的面试，从我手上流走的简历多多少少也有上百封了。\n同时最近秋招又要开始了，就想着把我这一年来筛选简历的经验分享给大家，多少还是能提高一些命中率。\n突出优势「简历」自然是突出简单的好，相信大部分面试官都不是全职做面试工作；多数都是工作之余筛选简历。\n就我的情况来说，每天都需要在工作中挤出一部分时间从 10 几份简历中挑选出比较靠谱的。\n总共大概花费 5 分钟的时间，平均算下来差不多一份简历只有 30S。现在我终于相信当初语文老师说：“高考语文作文阅卷只有几十秒的时间”。\n既然时间很短，就需要像写作文一样突出亮点。\n\n\n\n博客、GitHub举个例子，如果我在简历开头的个人介绍栏有看到个人博客、GitHub 链接等，一般都会点进去瞧瞧。\n不知道是否是城市原因，我这里几乎 10 份简历中有两份贴有个人博客、1 份贴有 GitHub 链接。\n哪怕里面的内容不是非常吸引人，但相比来说这样的简历会比其他多花上一些阅读时间，自然印象就更加深刻。\n如果同时内容还非常不错，那就更是加分项了。\n这就和上篇《如何成为一位「不那么差」的程序员》不谋而合：\n\n项目特色通常简历的核心区域就是项目介绍。\n这块我觉得可以适当减少项目具体的业务描述（自然不是不写），因为具体的项目了解一般会在简历评估通过后在面试中详聊。\n所以这里我建议重点描述下自己解决了什么问题，优化了什么地方；比如：\n\n解决了 XX 服务请求超时的问题。\n优化了接口，将 QPS 由 1000 提升到了 5000 等等。\n\n大概是这个方向的介绍。\n需要避免同时简历中也有许多需要注意的地方。\n首先是少用精通的字眼，真的精通也就算了，不然一定会被仔细询问。\n再一个是基本错误尽量少出，比如这样的：\n\n这是当时难得看到贴了 GitHub 地址，名字居然还写错。不过我还是点进去看了，也是没啥营养。\n甚至之前还收到一封简历，最近一次的工作经历竟然是公司 CEO，但一看工作年纪也才 25 岁工作三年而已。\n这样的描述就非常尴尬，建议如果是创业者的身份没什么问题。但这么大一个 title 显然不适合拿来面试。\n还有就是附件格式，建议最好使用 PDF 这样通用的格式在所有的操作系统打开都没问题。\nword 就非常容易出现变形，比如下面这样的我只能看到身高。\n\n1~3 年由于现阶段我主要关注的是 1~5 年这个范围，通常也会分为两个阶段。\n1~3 年多数是初中级岗位，这部分朋友我觉得应当把简历重心放在学习能力、积极主动性上面。\n因为在项目经验并没有那么丰富，所有需要从其他方面体现出自己的优势。比如说扎实的基础。\n3~5 年3~5 年一般是中高级岗位，这时我觉得需要突出自己解决问题的能力、设计产出方案这些技能表现出来。\n同时最好在简历中体现出并发、多线程、分布式相关的经验。\n最怕的就是这个阶段给人的感觉还是 1~3 年的水平，但要的薪资可是 N 倍。\n总结最后推荐一个在线简历模板：http://cv.ftqq.com/ 。（不是广告，我个人也在用。easy 大佬看到了记得给我广告费。）\n以上全是我个人主观感受，欢迎留言讨论。\n你的点赞与转发是最大的支持。\n","categories":["小技巧"],"tags":["简历"]},{"title":"Spring Bean 生命周期","url":"/2018/03/21/spring/spring-bean-lifecycle/","content":"\n前言Spring Bean 的生命周期在整个 Spring 中占有很重要的位置，掌握这些可以加深对 Spring 的理解。\n首先看下生命周期图：\n\n再谈生命周期之前有一点需要先明确：\n\nSpring 只帮我们管理单例模式 Bean 的完整生命周期，对于 prototype 的 bean ，Spring 在创建好交给使用者之后则不会再管理后续的生命周期。\n\n\n\n\n注解方式在 bean 初始化时会经历几个阶段，首先可以使用注解 @PostConstruct, @PreDestroy 来在 bean 的创建和销毁阶段进行调用:\n@Componentpublic class AnnotationBean &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(AnnotationBean.class);    @PostConstruct    public void start()&#123;        LOGGER.info(&quot;AnnotationBean start&quot;);    &#125;    @PreDestroy    public void destroy()&#123;        LOGGER.info(&quot;AnnotationBean destroy&quot;);    &#125;&#125;\n\nInitializingBean, DisposableBean 接口还可以实现 InitializingBean,DisposableBean 这两个接口，也是在初始化以及销毁阶段调用：\n@Servicepublic class SpringLifeCycleService implements InitializingBean,DisposableBean&#123;    private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleService.class);    @Override    public void afterPropertiesSet() throws Exception &#123;        LOGGER.info(&quot;SpringLifeCycleService start&quot;);    &#125;    @Override    public void destroy() throws Exception &#123;        LOGGER.info(&quot;SpringLifeCycleService destroy&quot;);    &#125;&#125;\n\n自定义初始化和销毁方法也可以自定义方法用于在初始化、销毁阶段调用:\n@Configurationpublic class LifeCycleConfig &#123;    @Bean(initMethod = &quot;start&quot;, destroyMethod = &quot;destroy&quot;)    public SpringLifeCycle create()&#123;        SpringLifeCycle springLifeCycle = new SpringLifeCycle() ;        return springLifeCycle ;    &#125;&#125;public class SpringLifeCycle&#123;    private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycle.class);    public void start()&#123;        LOGGER.info(&quot;SpringLifeCycle start&quot;);    &#125;    public void destroy()&#123;        LOGGER.info(&quot;SpringLifeCycle destroy&quot;);    &#125;&#125;\n\n以上是在 SpringBoot 中可以这样配置，如果是原始的基于 XML 也是可以使用:\n&lt;bean class=&quot;com.crossoverjie.spring.SpringLifeCycle&quot; init-method=&quot;start&quot; destroy-method=&quot;destroy&quot;&gt;&lt;/bean&gt;\n\n来达到同样的效果。\n实现 *Aware 接口*Aware 接口可以用于在初始化 bean 时获得 Spring 中的一些对象，如获取 Spring 上下文等。\n@Componentpublic class SpringLifeCycleAware implements ApplicationContextAware &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleAware.class);    private ApplicationContext applicationContext ;    @Override    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123;        this.applicationContext = applicationContext ;        LOGGER.info(&quot;SpringLifeCycleAware start&quot;);    &#125;&#125;\n\n这样在 springLifeCycleAware 这个 bean 初始化会就会调用 setApplicationContext 方法，并可以获得 applicationContext 对象。\nBeanPostProcessor 增强处理器实现 BeanPostProcessor 接口，Spring 中所有 bean 在做初始化时都会调用该接口中的两个方法，可以用于对一些特殊的 bean 进行处理：\n@Componentpublic class SpringLifeCycleProcessor implements BeanPostProcessor &#123;    private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleProcessor.class);    /**     * 预初始化 初始化之前调用     * @param bean     * @param beanName     * @return     * @throws BeansException     */    @Override    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123;        if (&quot;annotationBean&quot;.equals(beanName))&#123;            LOGGER.info(&quot;SpringLifeCycleProcessor start beanName=&#123;&#125;&quot;,beanName);        &#125;        return bean;    &#125;    /**     * 后初始化  bean 初始化完成调用     * @param bean     * @param beanName     * @return     * @throws BeansException     */    @Override    public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123;        if (&quot;annotationBean&quot;.equals(beanName))&#123;            LOGGER.info(&quot;SpringLifeCycleProcessor end beanName=&#123;&#125;&quot;,beanName);        &#125;        return bean;    &#125;&#125;\n\n执行之后观察结果：\n018-03-21 00:40:24.856 [restartedMain] INFO  c.c.s.p.SpringLifeCycleProcessor - SpringLifeCycleProcessor start beanName=annotationBean2018-03-21 00:40:24.860 [restartedMain] INFO  c.c.spring.annotation.AnnotationBean - AnnotationBean start2018-03-21 00:40:24.861 [restartedMain] INFO  c.c.s.p.SpringLifeCycleProcessor - SpringLifeCycleProcessor end beanName=annotationBean2018-03-21 00:40:24.864 [restartedMain] INFO  c.c.s.aware.SpringLifeCycleAware - SpringLifeCycleAware start2018-03-21 00:40:24.867 [restartedMain] INFO  c.c.s.service.SpringLifeCycleService - SpringLifeCycleService start2018-03-21 00:40:24.887 [restartedMain] INFO  c.c.spring.SpringLifeCycle - SpringLifeCycle start2018-03-21 00:40:25.062 [restartedMain] INFO  o.s.b.d.a.OptionalLiveReloadServer - LiveReload server is running on port 357292018-03-21 00:40:25.122 [restartedMain] INFO  o.s.j.e.a.AnnotationMBeanExporter - Registering beans for JMX exposure on startup2018-03-21 00:40:25.140 [restartedMain] INFO  com.crossoverjie.Application - Started Application in 2.309 seconds (JVM running for 3.681)2018-03-21 00:40:25.143 [restartedMain] INFO  com.crossoverjie.Application - start ok!2018-03-21 00:40:25.153 [Thread-8] INFO  o.s.c.a.AnnotationConfigApplicationContext - Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@3913adad: startup date [Wed Mar 21 00:40:23 CST 2018]; root of context hierarchy2018-03-21 00:40:25.155 [Thread-8] INFO  o.s.j.e.a.AnnotationMBeanExporter - Unregistering JMX-exposed beans on shutdown2018-03-21 00:40:25.156 [Thread-8] INFO  c.c.spring.SpringLifeCycle - SpringLifeCycle destroy2018-03-21 00:40:25.156 [Thread-8] INFO  c.c.s.service.SpringLifeCycleService - SpringLifeCycleService destroy2018-03-21 00:40:25.156 [Thread-8] INFO  c.c.spring.annotation.AnnotationBean - AnnotationBean destroy\n\n直到 Spring 上下文销毁时则会调用自定义的销毁方法以及实现了 DisposableBean 的 destroy() 方法。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["Spring"],"tags":["Spring"]},{"title":"SSM(十八) 秒杀架构实践","url":"/2018/05/07/ssm/SSM18-seconds-kill/","content":"\n前言之前在 Java-Interview 中提到过秒杀架构的设计，这次基于其中的理论简单实现了一下。\n\n本次采用循序渐进的方式逐步提高性能达到并发秒杀的效果，文章较长请准备好瓜子板凳(liushuizhang😂)。\n\n本文所有涉及的代码：\n\nhttps://github.com/crossoverJie/SSM\nhttps://github.com/crossoverJie/distributed-redis-tool\n\n最终架构图：\n\n\n\n先简单根据这个图谈下请求的流转，因为后面不管怎么改进这个都是没有变的。\n\n前端请求进入 web 层，对应的代码就是 controller。\n之后将真正的库存校验、下单等请求发往 Service 层（其中 RPC 调用依然采用的 dubbo，只是更新为最新版本，本次不会过多讨论 dubbo 相关的细节，有兴趣的可以查看 基于dubbo的分布式架构）。\nService 层再对数据进行落地，下单完成。\n\n无限制其实抛开秒杀这个场景来说正常的一个下单流程可以简单分为以下几步：\n\n校验库存\n扣库存\n创建订单\n支付\n\n基于上文的架构所以我们有了以下实现：\n先看看实际项目的结构：\n\n还是和以前一样：\n\n提供出一个 API 用于 Service 层实现，以及 web 层消费。\nweb 层简单来说就是一个 SpringMVC。\nService 层则是真正的数据落地。\nSSM-SECONDS-KILL-ORDER-CONSUMER 则是后文会提到的 Kafka 消费。\n\n数据库也是只有简单的两张表模拟下单：\nCREATE TABLE `stock` (  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,  `name` varchar(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;名称&#x27;,  `count` int(11) NOT NULL COMMENT &#x27;库存&#x27;,  `sale` int(11) NOT NULL COMMENT &#x27;已售&#x27;,  `version` int(11) NOT NULL COMMENT &#x27;乐观锁，版本号&#x27;,  PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;CREATE TABLE `stock_order` (  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,  `sid` int(11) NOT NULL COMMENT &#x27;库存ID&#x27;,  `name` varchar(30) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;商品名称&#x27;,  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;,  PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=55 DEFAULT CHARSET=utf8;\n\nweb 层 controller 实现:\n@Autowiredprivate StockService stockService;@Autowiredprivate OrderService orderService;@RequestMapping(&quot;/createWrongOrder/&#123;sid&#125;&quot;)@ResponseBodypublic String createWrongOrder(@PathVariable int sid) &#123;    logger.info(&quot;sid=[&#123;&#125;]&quot;, sid);    int id = 0;    try &#123;        id = orderService.createWrongOrder(sid);    &#125; catch (Exception e) &#123;        logger.error(&quot;Exception&quot;,e);    &#125;    return String.valueOf(id);&#125;\n\n其中 web 作为一个消费者调用看 OrderService 提供出来的 dubbo 服务。\nService 层，OrderService 实现：\n首先是对 API 的实现(会在 API 提供出接口)：\n@Servicepublic class OrderServiceImpl implements OrderService &#123;    @Resource(name = &quot;DBOrderService&quot;)    private com.crossoverJie.seconds.kill.service.OrderService orderService ;    @Override    public int createWrongOrder(int sid) throws Exception &#123;        return orderService.createWrongOrder(sid);    &#125;&#125;\n\n这里只是简单调用了 DBOrderService 中的实现，DBOrderService 才是真正的数据落地，也就是写数据库了。\nDBOrderService 实现：\nTransactional(rollbackFor = Exception.class)@Service(value = &quot;DBOrderService&quot;)public class OrderServiceImpl implements OrderService &#123;    @Resource(name = &quot;DBStockService&quot;)    private com.crossoverJie.seconds.kill.service.StockService stockService;    @Autowired    private StockOrderMapper orderMapper;        @Override    public int createWrongOrder(int sid) throws Exception&#123;        //校验库存        Stock stock = checkStock(sid);        //扣库存        saleStock(stock);        //创建订单        int id = createOrder(stock);        return id;    &#125;        private Stock checkStock(int sid) &#123;        Stock stock = stockService.getStockById(sid);        if (stock.getSale().equals(stock.getCount())) &#123;            throw new RuntimeException(&quot;库存不足&quot;);        &#125;        return stock;    &#125;        private int saleStock(Stock stock) &#123;        stock.setSale(stock.getSale() + 1);        return stockService.updateStockById(stock);    &#125;        private int createOrder(Stock stock) &#123;        StockOrder order = new StockOrder();        order.setSid(stock.getId());        order.setName(stock.getName());        int id = orderMapper.insertSelective(order);        return id;    &#125;                &#125;\n\n\n 预先初始化了 10 条库存。\n\n手动调用下 createWrongOrder/1 接口发现：\n库存表：\n订单表：\n一切看起来都没有问题，数据也正常。\n但是当用 JMeter 并发测试时：\n\n测试配置是：300个线程并发，测试两轮来看看数据库中的结果：\n\n\n\n请求都响应成功，库存确实也扣完了，但是订单却生成了 124 条记录。\n这显然是典型的超卖现象。\n\n其实现在再去手动调用接口会返回库存不足，但为时晚矣。\n\n乐观锁更新怎么来避免上述的现象呢？\n最简单的做法自然是乐观锁了，这里不过多讨论这个，不熟悉的朋友可以看下这篇。\n来看看具体实现：\n\n其实其他的都没怎么改，主要是 Service 层。\n\n@Overridepublic int createOptimisticOrder(int sid) throws Exception &#123;    //校验库存    Stock stock = checkStock(sid);    //乐观锁更新库存    saleStockOptimistic(stock);    //创建订单    int id = createOrder(stock);    return id;&#125;private void saleStockOptimistic(Stock stock) &#123;    int count = stockService.updateStockByOptimistic(stock);    if (count == 0)&#123;        throw new RuntimeException(&quot;并发更新库存失败&quot;) ;    &#125;&#125;\n\n对应的 XML：\n&lt;update id=&quot;updateByOptimistic&quot; parameterType=&quot;com.crossoverJie.seconds.kill.pojo.Stock&quot;&gt;    update stock    &lt;set&gt;        sale = sale + 1,        version = version + 1,    &lt;/set&gt;    WHERE id = #&#123;id,jdbcType=INTEGER&#125;    AND version = #&#123;version,jdbcType=INTEGER&#125;&lt;/update&gt;\n\n同样的测试条件，我们再进行上面的测试 /createOptimisticOrder/1：\n\n\n\n这次发现无论是库存订单都是 OK 的。\n查看日志发现：\n\n很多并发请求会响应错误，这就达到了效果。\n提高吞吐量为了进一步提高秒杀时的吞吐量以及响应效率，这里的 web 和 Service 都进行了横向扩展。\n\nweb 利用 Nginx 进行负载。\nService 也是多台应用。\n\n\n\n再用 JMeter 测试时可以直观的看到效果。\n\n由于我是在阿里云的一台小水管服务器进行测试的，加上配置不高、应用都在同一台，所以并没有完全体现出性能上的优势（ Nginx 做负载转发时候也会增加额外的网络消耗）。\n\nshell 脚本实现简单的 CI由于应用多台部署之后，手动发版测试的痛苦相信经历过的都有体会。\n这次并没有精力去搭建完整的 CI CD，只是写了一个简单的脚本实现了自动化部署，希望对这方面没有经验的同学带来一点启发：\n构建 web#!/bin/bash# 构建 web 消费者#read appnameappname=&quot;consumer&quot;echo &quot;input=&quot;$appnamePID=$(ps -ef | grep $appname | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;)# 遍历杀掉 pidfor var in $&#123;PID[@]&#125;;do    echo &quot;loop pid= $var&quot;    kill -9 $vardoneecho &quot;kill $appname success&quot;cd ..git pullcd SSM-SECONDS-KILLmvn -Dmaven.test.skip=true clean packageecho &quot;build war success&quot;cp /home/crossoverJie/SSM/SSM-SECONDS-KILL/SSM-SECONDS-KILL-WEB/target/SSM-SECONDS-KILL-WEB-2.2.0-SNAPSHOT.war /home/crossoverJie/tomcat/tomcat-dubbo-consumer-8083/webappsecho &quot;cp tomcat-dubbo-consumer-8083/webapps ok!&quot;cp /home/crossoverJie/SSM/SSM-SECONDS-KILL/SSM-SECONDS-KILL-WEB/target/SSM-SECONDS-KILL-WEB-2.2.0-SNAPSHOT.war /home/crossoverJie/tomcat/tomcat-dubbo-consumer-7083-slave/webappsecho &quot;cp tomcat-dubbo-consumer-7083-slave/webapps ok!&quot;sh /home/crossoverJie/tomcat/tomcat-dubbo-consumer-8083/bin/startup.shecho &quot;tomcat-dubbo-consumer-8083/bin/startup.sh success&quot;sh /home/crossoverJie/tomcat/tomcat-dubbo-consumer-7083-slave/bin/startup.shecho &quot;tomcat-dubbo-consumer-7083-slave/bin/startup.sh success&quot;echo &quot;start $appname success&quot;\n\n构建 Service# 构建服务提供者#read appnameappname=&quot;provider&quot;echo &quot;input=&quot;$appnamePID=$(ps -ef | grep $appname | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27;)#if [ $? -eq 0 ]; then#    echo &quot;process id:$PID&quot;#else#    echo &quot;process $appname not exit&quot;#    exit#fi# 遍历杀掉 pidfor var in $&#123;PID[@]&#125;;do    echo &quot;loop pid= $var&quot;    kill -9 $vardoneecho &quot;kill $appname success&quot;cd ..git pullcd SSM-SECONDS-KILLmvn -Dmaven.test.skip=true clean packageecho &quot;build war success&quot;cp /home/crossoverJie/SSM/SSM-SECONDS-KILL/SSM-SECONDS-KILL-SERVICE/target/SSM-SECONDS-KILL-SERVICE-2.2.0-SNAPSHOT.war /home/crossoverJie/tomcat/tomcat-dubbo-provider-8080/webappsecho &quot;cp tomcat-dubbo-provider-8080/webapps ok!&quot;cp /home/crossoverJie/SSM/SSM-SECONDS-KILL/SSM-SECONDS-KILL-SERVICE/target/SSM-SECONDS-KILL-SERVICE-2.2.0-SNAPSHOT.war /home/crossoverJie/tomcat/tomcat-dubbo-provider-7080-slave/webappsecho &quot;cp tomcat-dubbo-provider-7080-slave/webapps ok!&quot;sh /home/crossoverJie/tomcat/tomcat-dubbo-provider-8080/bin/startup.shecho &quot;tomcat-dubbo-provider-8080/bin/startup.sh success&quot;sh /home/crossoverJie/tomcat/tomcat-dubbo-provider-7080-slave/bin/startup.shecho &quot;tomcat-dubbo-provider-8080/bin/startup.sh success&quot;echo &quot;start $appname success&quot;\n\n之后每当我有更新，只需要执行这两个脚本就可以帮我自动构建。\n都是最基础的 Linux 命令，相信大家都看得明白。\n乐观锁更新 + 分布式限流上文的结果看似没有问题，其实还差得远呢。\n这里只是模拟了 300 个并发没有问题，但是当请求达到了 3000 ，3W，300W 呢？\n虽说可以横向扩展可以支撑更多的请求。\n但是能不能利用最少的资源解决问题呢？\n其实仔细分析下会发现：\n\n假设我的商品一共只有 10 个库存，那么无论你多少人来买其实最终也最多只有 10 人可以下单成功。\n\n所以其中会有 99% 的请求都是无效的。\n大家都知道：大多数应用数据库都是压倒骆驼的最后一根稻草。\n通过 Druid 的监控来看看之前请求数据库的情况：\n因为 Service 是两个应用。\n\n\n数据库也有 20 多个连接。\n怎么样来优化呢？其实很容易想到的就是分布式限流。\n我们将并发控制在一个可控的范围之内，然后快速失败这样就能最大程度的保护系统。\ndistributed-redis-tool ⬆️v1.0.3为此还对 https://github.com/crossoverJie/distributed-redis-tool 进行了小小的升级。\n因为加上该组件之后所有的请求都会经过 Redis，所以对 Redis 资源的使用也是要非常小心。\nAPI 更新修改之后的 API 如下：\n@Configurationpublic class RedisLimitConfig &#123;    private Logger logger = LoggerFactory.getLogger(RedisLimitConfig.class);    @Value(&quot;$&#123;redis.limit&#125;&quot;)    private int limit;    @Autowired    private JedisConnectionFactory jedisConnectionFactory;    @Bean    public RedisLimit build() &#123;        RedisLimit redisLimit = new RedisLimit.Builder(jedisConnectionFactory, RedisToolsConstant.SINGLE)                .limit(limit)                .build();        return redisLimit;    &#125;&#125;\n\n这里构建器改用了 JedisConnectionFactory，所以得配合 Spring 来一起使用。\n并在初始化时显示传入 Redis 是以集群方式部署还是单机（强烈建议集群，限流之后对 Redis 还是有一定的压力）。\n限流实现既然 API 更新了，实现自然也要修改：\n/** * limit traffic * @return if true */public boolean limit() &#123;    //get connection    Object connection = getConnection();    Object result = limitRequest(connection);    if (FAIL_CODE != (Long) result) &#123;        return true;    &#125; else &#123;        return false;    &#125;&#125;private Object limitRequest(Object connection) &#123;    Object result = null;    String key = String.valueOf(System.currentTimeMillis() / 1000);    if (connection instanceof Jedis)&#123;        result = ((Jedis)connection).eval(script, Collections.singletonList(key), Collections.singletonList(String.valueOf(limit)));        ((Jedis) connection).close();    &#125;else &#123;        result = ((JedisCluster) connection).eval(script, Collections.singletonList(key), Collections.singletonList(String.valueOf(limit)));        try &#123;            ((JedisCluster) connection).close();        &#125; catch (IOException e) &#123;            logger.error(&quot;IOException&quot;,e);        &#125;    &#125;    return result;&#125;private Object getConnection() &#123;    Object connection ;    if (type == RedisToolsConstant.SINGLE)&#123;        RedisConnection redisConnection = jedisConnectionFactory.getConnection();        connection = redisConnection.getNativeConnection();    &#125;else &#123;        RedisClusterConnection clusterConnection = jedisConnectionFactory.getClusterConnection();        connection = clusterConnection.getNativeConnection() ;    &#125;    return connection;&#125;\n\n如果是原生的 Spring 应用得采用 @SpringControllerLimit(errorCode = 200) 注解。\n实际使用如下：\nweb 端：\n/** * 乐观锁更新库存 限流 * @param sid * @return */@SpringControllerLimit(errorCode = 200)@RequestMapping(&quot;/createOptimisticLimitOrder/&#123;sid&#125;&quot;)@ResponseBodypublic String createOptimisticLimitOrder(@PathVariable int sid) &#123;    logger.info(&quot;sid=[&#123;&#125;]&quot;, sid);    int id = 0;    try &#123;        id = orderService.createOptimisticOrder(sid);    &#125; catch (Exception e) &#123;        logger.error(&quot;Exception&quot;,e);    &#125;    return String.valueOf(id);&#125;\n\nService 端就没什么更新了，依然是采用的乐观锁更新数据库。\n再压测看下效果 /createOptimisticLimitOrderByRedis/1：\n\n\n\n\n\n首先是看结果没有问题，再看数据库连接以及并发请求数都有明显的下降。\n乐观锁更新 + 分布式限流 + Redis 缓存其实仔细观察 Druid 监控数据发现这个 SQL 被多次查询：\n\n其实这是实时查询库存的 SQL，主要是为了在每次下单之前判断是否还有库存。\n这也是个优化点。\n这种数据我们完全可以放在内存中，效率比在数据库要高很多。\n由于我们的应用是分布式的，所以堆内缓存显然不合适，Redis 就非常适合。\n这次主要改造的是 Service 层：\n\n每次查询库存时走 Redis。\n扣库存时更新 Redis。\n需要提前将库存信息写入 Redis（手动或者程序自动都可以）。\n\n主要代码如下：\n@Overridepublic int createOptimisticOrderUseRedis(int sid) throws Exception &#123;    //检验库存，从 Redis 获取    Stock stock = checkStockByRedis(sid);    //乐观锁更新库存 以及更新 Redis    saleStockOptimisticByRedis(stock);    //创建订单    int id = createOrder(stock);    return id ;&#125;private Stock checkStockByRedis(int sid) throws Exception &#123;    Integer count = Integer.parseInt(redisTemplate.opsForValue().get(RedisKeysConstant.STOCK_COUNT + sid));    Integer sale = Integer.parseInt(redisTemplate.opsForValue().get(RedisKeysConstant.STOCK_SALE + sid));    if (count.equals(sale))&#123;        throw new RuntimeException(&quot;库存不足 Redis currentCount=&quot; + sale);    &#125;    Integer version = Integer.parseInt(redisTemplate.opsForValue().get(RedisKeysConstant.STOCK_VERSION + sid));    Stock stock = new Stock() ;    stock.setId(sid);    stock.setCount(count);    stock.setSale(sale);    stock.setVersion(version);    return stock;&#125;    /** * 乐观锁更新数据库 还要更新 Redis * @param stock */private void saleStockOptimisticByRedis(Stock stock) &#123;    int count = stockService.updateStockByOptimistic(stock);    if (count == 0)&#123;        throw new RuntimeException(&quot;并发更新库存失败&quot;) ;    &#125;    //自增    redisTemplate.opsForValue().increment(RedisKeysConstant.STOCK_SALE + stock.getId(),1) ;    redisTemplate.opsForValue().increment(RedisKeysConstant.STOCK_VERSION + stock.getId(),1) ;&#125;    \n\n压测看看实际效果 /createOptimisticLimitOrderByRedis/1：\n\n\n\n\n最后发现数据没问题，数据库的请求与并发也都下来了。\n乐观锁更新 + 分布式限流 + Redis 缓存 + Kafka 异步最后的优化还是想如何来再次提高吞吐量以及性能的。\n我们上文所有例子其实都是同步请求，完全可以利用同步转异步来提高性能啊。\n这里我们将写订单以及更新库存的操作进行异步化，利用 Kafka 来进行解耦和队列的作用。\n每当一个请求通过了限流到达了 Service 层通过了库存校验之后就将订单信息发给 Kafka ，这样一个请求就可以直接返回了。\n消费程序再对数据进行入库落地。\n因为异步了，所以最终需要采取回调或者是其他提醒的方式提醒用户购买完成。\n这里代码较多就不贴了，消费程序其实就是把之前的 Service 层的逻辑重写了一遍，不过采用的是 SpringBoot。\n感兴趣的朋友可以看下。\nhttps://github.com/crossoverJie/SSM/tree/master/SSM-SECONDS-KILL/SSM-SECONDS-KILL-ORDER-CONSUMER\n总结其实经过上面的一顿优化总结起来无非就是以下几点：\n\n尽量将请求拦截在上游。\n还可以根据 UID 进行限流。\n最大程度的减少请求落到 DB。\n多利用缓存。\n同步操作异步化。\nfail fast，尽早失败，保护应用。\n\n码字不易，这应该是我写过字数最多的了，想想当年高中 800 字的作文都憋不出来😂，可想而知是有多难得了。\n以上内容欢迎讨论。\n号外最近在总结一些 Java 相关的知识点，感兴趣的朋友可以一起维护。\n\n地址: https://github.com/crossoverJie/Java-Interview\n\n","categories":["SSM","Distributed Tools"],"tags":["Java","Kafka","Redis","SpringBoot"]},{"title":"StarRocks 升级注意事项","url":"/2025/03/14/starrocks/StarRocks-upgrade/","content":"前段时间升级了生产环境的 StarRocks，从 3.3.3 升级到了 3.3.9，期间还是踩了不少坑所以在这里记录下。\n\n 因为我们的集群使用的是存算分离的版本，也是使用官方提供的 operator 部署在 kubernetes 里的，所以没法按照官方的流程进入虚拟机手动启停对应的服务。\n只能使用 operator 提供的方案手动修改对应组件的镜像版本，后续的升级操作交给 operator 去完成。\n\n\n\n理论上这个升级流程没什么问题，修改镜像版本之后只需要安静等待他滚动更新即可。\n元数据备份与恢复但考虑到之前在社区看到有存算分离集群升级失败导致数据丢失的案例，我们的全量业务已经切换到 StarRocks，如果数据丢失那需要花几天时间进行数据同步，这在业务上是无法接受的，所以我们最好是可以在升级前备份数据，即便是升级失败数据依然还在。\n\n原本官方社区是有提供数据备份与恢复能力的，但是我们使用的存算分离集群不支持😂，而想要获得社区版的支持应该还要等一段时间，即便是支持了我们升级到那个版本依然是需要备份的。\n\n\n好消息，在最新的 3.4.1 版本中已经支持了快照备份了，只是作为一个新 feature，稳定性还有待观察。\n\n所以我们的计划是在当前这个版本（3.3.3）能否自己备份数据，由于我们是存算分离的版本，所以数据主要分为两部分：\n\n存储在所有 FE 节点里的 meta 元数据\n存储在云存储里的业务数据\n\n备份的时候自然就需要备份这两部分的数据。\n备份元数据在元数据里存放了所有的数据库、表、视图等信息，具体在磁盘的结构如下：\n|-- bdb|   |-- 00000000.jdb|   |-- je.config.csv|   |-- je.info.0|   |-- je.info.0.lck|   |-- je.lck|   `-- je.stat.csv|-- image|   |-- ROLE|   |-- VERSION|   |-- image.327375|   |-- starmgr|   |   `-- image.390|   `-- v2|       |-- checksum.327375|       `-- image.327375\n\nbdb 目录主要是用于 leader 选举的，理论上并不需要备份，真正需要的是 image 目录下的 image.327375 等元数据文件。\n\n\n里面是用 JSON 存储的各种类型的元数据，FE 在启动的时候会读取该文件，然后根据不同的类型取不同的偏移量读取其中的元数据加载到内存里。\n我们的 FE 一共有三个节点，需要找到其中的 leader 节点（理论上只需要备份 leader 节点即可，其他节点会在 leader 启动后同步过去），直接将这个 meta 目录备份到本地即可：\n在开始之前需要停掉所有的写入任务，暂停所有的物化视图刷新。\n# inactive 所有的物化视图SELECT CONCAT(&#x27;ALTER MATERIALIZED VIEW &#x27;, TABLE_NAME, &#x27; INACTIVE;&#x27;) FROM information_schema.materialized_views;# 手动创建镜像ALTER SYSTEM CREATE IMAGE;# 找到 leader 节点SHOW FRONTENDS;\n\n然后进入 leader 节点备份元数据：\nk exec -it kube-starrocks-fe-0-n sr -- bashtar -zcvf meta.tar.gz meta/# 下载备份元数据到本地k cp starrocks-fe-0:/opt/starrocks/fe/meta/image.tar.gz image.tar.gz -n starrocks -c fe --retries=5\n\n备份云存储数据云存储的备份就需要结合你使用的云厂商来备份了，通常他们都有提供对应的备份能力。\n要注意的是我们再备份的时候需要记录在存储桶里的目录名称，之后还原的时候名称得保持一致才行。\n恢复元数据当出现极端情况升级失败的时候，我们需要把元数据覆盖回去；但由于我们的应用运行在容器里，不可以在应用启动之后再替换元数据。\n只能在应用启动之前将之前备份的元数据覆盖回去，这里可以使用 kubernetes 中的 initContainers 提前将数据复制到应用容器里。\n在开始之前我们需要先把备份的元数据打包为一个镜像。\nFROM busybox  ADD meta.tar.gz /temp\n\n然后我们需要手动修改 FE 的 statefulset 的资源，创建一个 initContainers。\ninitContainers:    - name: copy-file-init      image: meta:0.0.1      command: [&quot;/bin/sh&quot;, &quot;-c&quot;]      args: [&quot;rm -rf /meta-target/* &amp;&amp; cp -r /temp/meta/. /meta-target&quot;]      volumeMounts:        - name: fe-meta          mountPath: &quot;/meta-target&quot;\n\n原理就是在 initContainers 中挂载原本 FE 的元数据目录，这样就可以直接将之前备份的元数据覆盖过去。\n\n当然也可以直接使用 k8s 的 go client 用代码的方式来修改，会更容易维护。\n\n还原的时候需要先将云存储里的数据先还原之后再还原元数据。\n物化视图刷新策略真正升级的时候倒是没有碰到升级失败的情况，所以没有走恢复流程；但是却碰到了一个更麻烦的事情。\n物化视图作为基表我们在升级前将所有的物化视图设置为了 INACTIVE，升级成功后需要将他们都改为 ACTIVE。\n第一个问题是如果某个物化视图 MV1 的基表也是一个物化视图 MV-base，这样会导致 MV1 的全量刷新。\n我之前在这个 PR 里新增了一个参数：excluded_refresh_tables 可以用于排除基表发生变化的时候刷新物化视图，但是忘记了基表也是物化视图的场景。\n\n所以在这个 PR 中修复了该问题，现在基表是物化视图的时候也可以使用了。\n物化视图手动 ACTIVE前面提到在升级之前需要将所有的物化视图设置为 INACTIVE，升级成功后再手动设置为 ACTIVE。\n我们在手动 ACTIVE 之后发现这些物化视图又在做全量刷新了，于是我们检查了代码。\n\n发现在使用 ALTER MATERIALIZED VIEW order_mv ACTIVE; 修改视图状态的时候会强制刷新物化视图的所有分区。\n\n\nforce: true 的时候会直接跳过基表的分区检查，导致分区的全量刷新。\n\n\n同时会在 ACTIVE 的时候将视图基表的 baseTableVisibleVersionMap 版本号缓存清空，FE 需要在刷新的时候判断当前需要刷新的分区是否存在与缓存中，如果存在的话说明不需要刷新，现在被清空后就一定会被刷新。\n所以我提了一个 PR 可以在 ACTIVE 物化视图的时候人工判断是否需要刷新:\nalter materialized view mv_test1 ACTIVE WITH NO_VALIDATION\n\n这样带上 NO_VALIDATION 参数后就 force=false 也就不会全量刷新了。\n如果在 ACTIVE 物化视图的时候碰到类似场景，可以在这个 PR 发布之后加上 NO_VALIDATION 来跳过刷新。\n参考链接：\n\nhttps://github.com/StarRocks/starrocks/pull/50926\nhttps://github.com/StarRocks/starrocks/pull/56428\nhttps://github.com/StarRocks/starrocks/pull/56864\n\n","categories":["OB"],"tags":["StarRocks"]},{"title":"图床失效了？也许你应该试试这个工具","url":"/2019/05/08/tools/blog-toolbox/","content":"\n前言经过几个小伙伴的提醒，发现个人博客中的许多图片都裂了无法访问；原因就不多说，既然出现问题就得要解决。\n\n\n\n\n原本我的处理方式非常简单粗暴：找到原有的图片重新下载下来上传到新的可用图床再把图片地址替换。\n这样搞了一两篇之后我就绝望了。。。\n之前为了代码能在公众号里也有好的阅读体验，所以能截图的我绝不贴代码，导致一篇文章多的得有十几张图片。\n好在哪位大佬说过“以人肉XX为耻”，这种重复劳动力完全可自动化；于是便有了本次的这个工具。\n它可以一行命令把你所有 Markdown 写的内容中的图片全部替换为新的图床。\n运行效果如下：\n\n\n使用可以直接在这个地址下载 jar 包运行：https://github.com/crossoverJie/blog.toolbox/releases/download/v0.0.1/blog.toolbox-0.0.1-SNAPSHOT.jar\n当然也可以下载源码编译运行：\ngit clone https://github.com/crossoverJie/blog.toolboxmvn clean packagejava -jar nows-0.0.1-SNAPSHOT.jar --app.downLoad.path=/xx/img /xx/xx/path 100\n\n看运行方式也知道，其实就是用 SpringBoot 写了一个工具用于批量下载文中出现的图片同时上传后完成替换。\n\n其中 app.downLoad.path 是用于将下载的图片保存到本地磁盘的目录。\n/xx/xx/path 则是扫描 .md 文件的目录，会递归扫描所有出所有文件。\n100 则是需要替换文件的数量，默认是按照文件修改时间排序。\n\n如果自己的图片较多的话还是有几个坑需要注意下。\n线程数量默认是启动了两个线程去遍历文件、上传下载图片、更新文本等内容，其中的网络 IO 其实挺耗时的，所以其实可以适当的多开些线程来提高任务的执行效率。\n但线程过多也许会触发图床的保护机制，同时也和自己电脑配置有关，这个得结合实际情况考虑了。\n所以可以通过 --app.thread=6 这样的参数来调整线程数量。\n图床限制这个是图片过多一定是大概率出现的，上传请求的频次过高很容易被限流封 IP。\n&#123;&quot;code&quot;:&quot;error&quot;,&quot;msg&quot;:&quot;Upload file count limit. Time left 1027 second.&quot;&#125;\n\n目前来看是封 IP 居多，所以可以通过走代理、换网络的方式来解决。\n当然如果是自搭图床可以无视。\n重试由于我使用的是免费图床，上传过程中偶尔也会出现上传失败的情况，因此默认是有 5 次重试机制的；如果五次都失败了那么大概率是 IP 被封了。\n\n即便是 ip 被封后只要换了新的 ip 重新执行程序它会自动过滤掉已经替换的图片，不会再做无用功，这点可以放心。\n\n图片保存\n默认情况下,下载的图片会保存在本地，我也建议借此机会自己本地都缓存一份，同时名字还和文中的名字一样，避免今后图床彻底挂掉后连恢复的机会都没有。\n总结这个程序的代码就没怎么讲了，确实也挺简单，感兴趣的可以自己下来看看。\n目前功能也很单一，自用完全够了；看后续大家是否还有其他需求再逐渐完善吧，比如：\n\n图床上传失败自动切换到可用图床。\n整体处理效率提升。\n任务执行过程中更好的进度展现等。\n\n再次贴一下源码地址：\nhttps://github.com/crossoverJie/blog.toolbox\n你的点赞与分享是对我最大的支持\n","categories":["工具"],"tags":["Java","策略模式"]},{"title":"【译】Goland 中的隐藏宝石","url":"/2022/07/28/translation/hidden-gems-goland/","content":"原文链接\n\n在日常使用 Goland 时，团队收集了一些可以帮助我们专注于创造的同时减少重复工作的小技巧。如果你是在 IDEA 中使用的 Go 插件，或者其他 IntelliJ 的产品，同样也有这些特性。\n\n\n行排序当你在查看文本文件时，行排序非常有用；按照字母排序后能够帮我们更好的阅读，同时也容易找到重复的行。\n在菜单栏中使用 Edit | Sort Lines or Edit | Reverse Lines可以帮我们快速的对选中的代码或者是整个文件进行排序；或者也可以使用快速命令执行这个操作。\n\n打开对比窗口打开一个对比窗口可以帮助我们对比任何文件、文件夹、文本；举个例子，将复制的内容粘贴到对比窗口中，IDE 会类似于版本控制系统那样展示两者的差异。\n当然也可以用快速指令打开对比窗口（double shift)。\n\n此外你也可以在 IDE 编辑器的任何地方右键鼠标选择与当前粘贴板数据进行对比。\n\n这个功能很棒，可以替换掉以前大部分用 BeyondCompare 的场景了。\n\n暂存文件有时候你需要一个随意的地方来编写一段文本，与当前工作相关的一些记录，也或是与当前项目上下文无关的草稿代码；这时候就需要用到暂存文件了。\n暂存文件可不只是简单的笔记，它支持语法高亮、代码提示以及所有和这个文件类型相关的特性。\n暂存文件与当前项目无关，你可以在任意项目中访问到这些文件，这样你就不需要离开 IDE 到其他地方来保存这些文件了。\n可以在菜单栏中新建暂存文件File | New | Scratch File or，也可以使用快捷键 ⇧ ⌘ N.\n\n\n通常使用这个功能来存放和运行一些测试或者是实例代码。\n\n多行光标多行光标可以让你快速在多个地方同时修改代码，同时它也支持代码提示以及实时模板。\n开启多行光标可以双击 ⌥/Ctrl 后不要释放，然后点击上下箭头键。使用 Escape 键可以退出多行光标。\n\n\n这个在批量修改代码时非常有用。\n\n批量折叠和展开在阅读复杂长篇代码的过程中有时候很难弄懂代码结构，即便是代码是我们自己写的。\n这也容易解决，批量折叠和展开可以快速帮我们浏览代码，快捷键是：macOS:⇧⌘- /⇧⌘+,Windows&#x2F;Linux: Ctrl+Shift+NumPad + / Ctrl+Shift+NumPad。\nIDE 可以帮我们折叠&#x2F;展开选中的代码，如果没有选中则是处理整个文件。\n也可以使用 macOS: ⌥⌘- / ⌥⌘+, Windows&#x2F;Linux:Ctrl+Alt+NumPad + / Ctrl+Alt+NumPad 来递归的处理代码，IDE 将会折叠&#x2F;展开当前代码片段或者是他们包含的片段。\n\n最近文件最近文件可以帮助我们快速跳转到最近经常打开的文件，当我们使用 macOS:⌘+E Windows&#x2F;Linux:Ctrl + E 打开最近文件对话框的时，再使用⌘+E可以再次过滤只显示已经修改过的文件，这样可以帮我们更精准的查找。\n\n这些特性可能有些并不常用，一旦用上一次解决问题后会发现 IntelliJ 的 IDE 功能非常强大，如果你还发现了一些其他有用的特性请在留言区分享。\n","categories":["翻译"],"tags":["IDE"]},{"title":"【译】如何高效的使用 Git","url":"/2018/09/07/translation/how-to-use-git-efficiently/","content":"原文链接\n\n\n代码昨天还是运行好好的今天就不行了。\n\n\n代码被删了。\n\n\n突然出现了一个奇怪的 bug，但是没人知道怎么回事。\n\n如果你出现过上面的任何一种情况，那本篇文章就是为你准备的。\n除了知道 git add, git commit , git push 之外，Git 中还需要其他重要的技术需要掌握。长远来看对我们是有帮助的。这里我将向你展示 Git 的最佳实践。\n\n\nGit 工作流当有多个开发者同时涉及到一个项目时那么就非常有必要正确使用 Git 工作流。\n这里我将介绍一种工作流，它在一个多人大型项目中将非常有用。\n\n前言突然有一天，你成为了一个项目的技术 Leader 并计划做出下一个 Facebook。在这个项目中你有三个开发人员。\n\nAlice：一个开发小白。\nBob：拥有一年工作经验，了解基本开发。\nJohn：三年开发经验，熟练开发技能。\n你：该项目的技术负责人。\n\nGit 开发流程Master 分支\nMaster 分支应该始终和生产环境保持一致。\n由于 master 和生产代码是一致的，所以没有人包括技术负责人能在 master 上直接开发。\n真正的开发代码应当写在其他分支上。\n\nRelease(发布) 分支\n当项目开始时，第一件事情就是创建发布分支。发布分支是基于 master 分支创建而来。\n所有与本项目相关的代码都在发布分支中，这个分支也是一个以 release/ 开头的普通分支。\n比如这次的发布分支名为 release/fb。\n可能有多个项目都基于同一份代码运行，因此对于每一个项目来说都需要创建一个独立的发布分支。假设现在还有一个项目正在并行运行，那就得为这个项目创建一个单独的发布分支比如 release/messenger。\n需要单独的发布分支的原因是：多个并行项目是基于同一份代码运行的，但是项目之间不能有冲突。\n\nFeature(功能分支) branch\n对于应用中的每一个功能都应该创建一个独立的功能分支，这会确保这些功能能被单独构建。\n功能分支也和其他分支一样，只是以 feature/ 开头。\n现在作为技术 Leader，你要求 Alice 去做 Facebook 的登录页面。因此他创建了一个新的功能分支。把他命名为 feature/login。Alice 将会在这个分支上编写所有的登录代码。\n这个功能分支通常是基于 Release(发布) 分支 创建而来。\nBob 的任务为创建添加好友页面，因此他创建了一个名为 feature/friendrequest 的功能分支。\nJohn 则被安排构建消息流，因此创建了一个 feature/newsfeed 的功能分支。\n所有的开发人员都在自己的分支上进行开发，目前为止都很正常。\n现在当 Alice 完成了他的登录开发，他需要将他的功能分支 feature/login 发送给 Release(发布) 分支。这个过程是通过发起一个 pull request 完成的。\n\nPull request首先 pull request 不能和 git pull 搞混了。\n开发人员不能直接向 Release(发布) 分支推送代码，技术 Leader 需要在功能分支合并到 Release(发布) 分支之前做好代码审查。这也是通过 pull request 完成的。\nAlice 能够按照如下 GitHub 方式提交 pull request。\n\n在分支名字的旁边有一个 “New pull request” 按钮，点击之后将会显示如下界面：\n\n\n比较分支是 Alice 的功能分支 feature/login。\nbase 分支则应该是发布分支 release/fb。\n\n点击之后 Alice 需要为这个 pull request 输入名称和描述，最后再点击 “Create Pull Request” 按钮。\n同时 Alice 需要为这个 pull request 指定一个 reviewer。作为技术 Leader 的你被选为本次 pull request 的 reviewer。\n你完成代码审查之后就需要把这个功能分支合并到 Release(发布) 分支。\n现在你已经把 feature/login 分支合并到 release/fb，并且 Alice 非常高兴他的代码被合并了。\n代码冲突 😠\nBob 完成了他的编码工作，同时向 release/fb 分支发起了一个 pull request。\n因为发布分支已经合并了登录的代码，这时代码冲突发生了。解决冲突和合并代码是 reviewer 的责任。在这样的情况下，作为技术 Leader 就需要解决冲突和合并代码了。\n现在 John 也已经完成了他的开发，同时也想把代码合并到发布分支。但 John 非常擅长于解决代码冲突。他将 release/fb 上最新的代码合并到他自己的功能分支 feature/newsfeed （通过 git pull 或 git merge 命令）。同时他解决了所有存在的冲突，现在 feature/newsfeed 已经有了所有发布分支 release/fb 的代码。\n最后 John 创建了一个 pull request，由于 John 已经解决了所有问题，所以本次 pull request 不会再有冲突了。\n\n因此通常有两种方式来解决代码冲突：\n\npull request 的 reviewer 需要解决所有的代码冲突。\n开发人员需要确保将发布分支的最新代码合并到功能分支，并且解决所有的冲突。\n\n还是 Master 分支一旦项目完成，发布分支的代码需要合并回 master 分支，同时需要发布到生产环境。\n因此生产环境中的代码总是和 master 分支保持一致。同时对于今后的任何项目来说都是要确保 master 代码是最新的。\n\n我们现在团队就是按照这样的方式进行开发，确实可以尽可能的减少代码管理上的问题。\n\n题外话像之前那篇《如何成为一位「不那么差」的程序员》说的那样，建议大家都多看看国外的优质博客。\n甚至尝试和作者交流，经过沟通原作者也会在原文中贴上我的翻译链接。大家互惠互利使好的文章转播的更广。\n\n\n你的点赞与转发是最大的支持。\n","categories":["翻译"],"tags":["Git"]},{"title":"【译】Java8 之后对新开发者非常友好的特性盘点","url":"/2022/02/07/translation/new-developer-friendly-features-after-java-8/","content":"原文链接\n\n在这篇文章中，我将描述自 Java8 依赖对开发者来说最重要也最友好的特性，之所以选择 Java8 ，那是因为它依然是目前使用最多的版本。\n具体可见这个调查报告：\n\n\n\nSwitch 表达式 (JDK 12)使用 switch 表达式，你可以定义多个 case 条件，并使用箭头 -&gt; 符号返回值，这个特性在 JDK12 之后启用，它使得 switch 表达式更容易理解了。\npublic String newMultiSwitch(int day) &#123;   return switch (day) &#123;      case 1, 2, 3, 4, 5 -&gt; &quot;workday&quot;;      case 6, 7 -&gt; &quot;weekend&quot;;      default -&gt; &quot;invalid&quot;;   &#125;;&#125;\n在 JDK12 之前，同样的例子要复杂的多：\npublic String oldMultiSwitch(int day) &#123;   switch (day) &#123;      case 1:      case 2:      case 3:      case 4:      case 5:         return &quot;workday&quot;;      case 6:      case 7:         return &quot;weekend&quot;;      default:         return &quot;invalid&quot;;   &#125;&#125;\n\n文本块 (JDK 13)文本块是一个多行字符串，可以避免使用转移字符；从 Java13 之后它成为了预览特性，使用 &quot;&quot;&quot; 符号定义。接下来看看使用它声明一个 JSON 字符串有多简单。\npublic String getNewPrettyPrintJson() &#123;   return &quot;&quot;&quot;          &#123;             &quot;firstName&quot;: &quot;Piotr&quot;,             &quot;lastName&quot;: &quot;Mińkowski&quot;          &#125;          &quot;&quot;&quot;;&#125;\n\nJava13 之前的版本：\npublic String getOldPrettyPrintJson() &#123;   return &quot;&#123;\\n&quot; +          &quot;     \\&quot;firstName\\&quot;: \\&quot;Piotr\\&quot;,\\n&quot; +          &quot;     \\&quot;lastName\\&quot;: \\&quot;Mińkowski\\&quot;\\n&quot; +          &quot;&#125;&quot;;&#125;\n\n新的 Optional Methods (JDK 9&#x2F; JDK 10)Java 9&#x2F;10 版本之后新增了几种可选方法，有意思的是这两个：\n\norElseThrow\nifPresentOrElse\n\n使用 orElseThrow 当数据不存在时你能抛出 NoSuchElementException 异常，相反会返回数据。\npublic Person getPersonById(Long id) &#123;   Optional&lt;Person&gt; personOpt = repository.findById(id);   return personOpt.orElseThrow();&#125;\n\n正因为如此，可以避免在 isPresent 中使用 if 条件。\npublic Person getPersonByIdOldWay(Long id) &#123;   Optional&lt;Person&gt; personOpt = repository.findById(id);   if (personOpt.isPresent())      return personOpt.get();   else      throw new NoSuchElementException();&#125;\n第二个有趣的方法是 ifPresentOrElse ,当数据存在时，会执行带数据参数的函数，相反会执行参数为空的函数。\npublic void printPersonById(Long id) &#123;   Optional&lt;Person&gt; personOpt = repository.findById(id);   personOpt.ifPresentOrElse(      System.out::println,      () -&gt; System.out.println(&quot;Person not found&quot;)   );&#125;\n\n\n在 Java8 中，你需要在 isPresent 方法中使用 if else 语句。\n集合工厂方法(JDK 9)使用 Java9 中的集合工厂方法可以简单的使用预定义数据创建不可变集合。\nList&lt;String&gt; fruits = List.of(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;);Map&lt;Integer, String&gt; numbers = Map.of(1, &quot;one&quot;, 2,&quot;two&quot;, 3, &quot;three&quot;);\n\n在 Java9 之前，你可以使用 Collections ，但肯定是更复杂：\npublic List&lt;String&gt; fruits() &#123;   List&lt;String&gt; fruitsTmp = new ArrayList&lt;&gt;();   fruitsTmp.add(&quot;apple&quot;);   fruitsTmp.add(&quot;banana&quot;);   fruitsTmp.add(&quot;orange&quot;);   return Collections.unmodifiableList(fruitsTmp);&#125;public Map&lt;Integer, String&gt; numbers() &#123;   Map&lt;Integer, String&gt; numbersTmp = new HashMap&lt;&gt;();   numbersTmp.put(1, &quot;one&quot;);   numbersTmp.put(2, &quot;two&quot;);   numbersTmp.put(3, &quot;three&quot;);   return Collections.unmodifiableMap(numbersTmp);&#125;\n\nRecords (JDK 14)使用 Records 你可以定义一个不可变、只能访问数据（只有 getter 方法) 的类，它可以自动创建 toString，equals，hashcode 方法。\npublic record Person(String name, int age) &#123;&#125;\n\n以下效果与  Records  类似：\npublic class PersonOld &#123;    private final String name;    private final int age;    public PersonOld(String name, int age) &#123;        this.name = name;        this.age = age;    &#125;    public String getName() &#123;        return name;    &#125;    public int getAge() &#123;        return age;    &#125;    @Override    public boolean equals(Object o) &#123;        if (this == o) return true;        if (o == null || getClass() != o.getClass()) return false;        PersonOld personOld = (PersonOld) o;        return age == personOld.age &amp;&amp; name.equals(personOld.name);    &#125;    @Override    public int hashCode() &#123;        return Objects.hash(name, age);    &#125;    @Override    public String toString() &#123;        return &quot;PersonOld&#123;&quot; +                &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; +                &quot;, age=&quot; + age +                &#x27;&#125;&#x27;;    &#125;&#125;\n\n\n\n接口中的私有方法 (JDK 9)从 Java8 之后你就可以为接口创建默认方法，但从 Java9 的私有方法你就能充分使用该特性：\npublic interface ExampleInterface &#123;   private void printMsg(String methodName) &#123;      System.out.println(&quot;Calling interface&quot;);      System.out.println(&quot;Interface method: &quot; + methodName);   &#125;   default void method1() &#123;      printMsg(&quot;method1&quot;);   &#125;   default void method2() &#123;      printMsg(&quot;method2&quot;);   &#125;&#125;\n\n局部变量类型推导 (JDK 10 &#x2F; JDK 11)从 Java10 之后你就能使用局部变量类型推导了，只需要使用 var 关键字来代替具体类型；在 Java11 之后你就能在 lambda 表达式中使用类型推导了。\npublic String sumOfString() &#123;   BiFunction&lt;String, String, String&gt; func = (var x, var y) -&gt; x + y;   return func.apply(&quot;abc&quot;, &quot;efg&quot;);&#125;","categories":["翻译"],"tags":["Java"]},{"title":"【译】对于初学者什么是最好的编程语言？","url":"/2018/04/12/translation/translation-What%20Is%20The%20Best%20Programming%20Language%20to%20Start/","content":"原文链接Python？Java？Ruby？JavaScript？有非常多的选择。选择一种编程语言开始你的编码之旅不应该是一件艰巨的任务。\n\n事实上：你将要学习的语言并不是特别重要，更重要的是学习编程的理念。对于任何编程语言来说知识的可传递性都是至关重要的。\n我学习的第一门语言是 Java，学习了循环，while 循环，条件，函数，面向对象编程和许多编程理念。\n然而，选择一门能在编程领域轻松找到工作的语言是更好的选择。对于初学者来说，我这里有一份列表推荐给你：\n\n\nPythonPython 在美国大学里是最受欢迎的入门型语言。\n就像 JavaScript 一样，Python 也非常灵活，现在被用于构建生物信息学的 web 应用。我强烈推荐你学习 Python，它是很棒的入门选择。\nJavaJava 是企业环境中使用最多的语言，根据 TIOBE 统计 Java 长年占据编程语言榜首。同时 Java 是强类型地静态语言，可以更容易地去描述一些编程理念。\nJava 作为最常使用的语言，你可以很轻松地在这段编程之旅中找到 Java 的相关课程和指南来获得帮助。你还可以使用 Java 构建服务端应用、Android APP 等应用程序。\nRubyRuby 是我最喜欢的编程语言，它编写简单，容易理解并且使用顺手。\n就像 JavaScript 一样，它学起来简单但是不易掌握。Ruby 在很多公司中被广泛应用，比如 Airbnb, EBANX, Shopify, Twitter, GitHub 等等。它还有一个超赞的 7*24 小时的在线社区随时提供帮助。Ruby 以  Ruby on Rails 框架著称，它可以帮你很轻松的构建整个 web 应用。\nJavaScriptJavaScript 是我用过的最灵活的语言之一。\n你能用它构建控制台程序，桌面软件，手机 APP，前端开发，后端开发等等。它是一个很不错的编程语言，简单易学但难以掌握。\n我建议你学习并掌握 JavaScript ，但不是作为第一门语言。\n对于初学者来说 JavaScript 很难调试并且不容易学习编程理念比如异步，原型，面向对象等等。\n不要纠结语言你需要通过选择一门语言来学习编程理念，当你学完之后你将花费较小的学习曲线来学习任何其他的语言。\n如果你想要学习如何学习一门新语言的话，可以阅读我的文章 “How to Learn a New Programming Language or Framework”，将会非常有用。\n","categories":["翻译"]},{"title":"【译】你可能不知道但却很有用的 Java 特性","url":"/2022/01/18/translation/useful-unknown-java-features/","content":"原文链接\n\n在这篇文章中你将会学习到一些你可能没听过但有用的 Java 特性，这些是我个人常用的一些特性或者是从其他文章中学习到的，重点是关注 API 而不是语言本身。\n\n\n延迟队列众所周知，在 Java 中有许多类型的集合可以使用，但你听说过 DelayQueue 吗？它是一个特定类型的集合，允许我们基于延时时间对数据排序，这是一个非常有意思的类，它实现了 BlockingQueue 接口，只有当数据过期后才能从队列里取出。\n使用它的第一步，你的 class 需要实现 Delayed 接口中的 getDelay 方法，当然也可以不用声明一个 class，使用 Record 也是可以的。\n\n这是 Java14 的新特性\n\npublic record DelayedEvent(long startTime, String msg) implements Delayed &#123;    public long getDelay(TimeUnit unit) &#123;        long diff = startTime - System.currentTimeMillis();        return unit.convert(diff, TimeUnit.MILLISECONDS);    &#125;    public int compareTo(Delayed o) &#123;        return (int) (this.startTime - ((DelayedEvent) o).startTime);    &#125;&#125;\n\n假设我们需要一个延时 10s 取出的数据，我们只需要放入一个比当前时间多 10s 的任务即可。\nfinal DelayQueue&lt;DelayedEvent&gt; delayQueue = new DelayQueue&lt;&gt;();final long timeFirst = System.currentTimeMillis() + 10000;delayQueue.offer(new DelayedEvent(timeFirst, &quot;1&quot;));log.info(&quot;Done&quot;);log.info(delayQueue.take().msg());\n\n最终输出如下：\n时间格式的日期这个特性可能对大部分人来说没什么用，但老实说我个人非常喜欢；不管怎么说 Java 8 在时间 API 上改进了许多。从这个版本开始或许你不再需要其他任何扩展库了。\n你能想到嘛，从 Java 16 中你甚至可以用标准库表示一天内的日期了，比如 “in the morning” “in the afternoon” ，这是一个新的格式语句 B。\nString s = DateTimeFormatter  .ofPattern(&quot;B&quot;)  .format(LocalDateTime.now());System.out.println(s);\n\n以下是我的输出，具体和你当前时间有关。\n你可能会想为什么会是调用 “B” 呢，这确实看起来不太直观，通过下表也许能解答疑惑：\nStamped Lock在我看来，并发包是 Java 中最有意思的包之一，同时又很少被开发者熟练掌握，特别是长期使用 web 开发框架的开发者。\n有多少人曾经使用过 Lock 呢？相对于 synchronized 来说这是一种更灵活的线程同步机制。\n从 Java8 开始你可以使用一种新的锁：StampedLock.StampedLock，能够替代 ReadWriteLock。\n假设现在有两个线程，一个线程更新金额、一个线程读取余额；更新余额的线程首先需要读取金额，再多线程的情况下需要某种同步机制（不然更新数据会发生错误），第二个线程用乐观锁的方式读取余额。\nStampedLock lock = new StampedLock();Balance b = new Balance(10000);Runnable w = () -&gt; &#123;   long stamp = lock.writeLock();   b.setAmount(b.getAmount() + 1000);   System.out.println(&quot;Write: &quot; + b.getAmount());   lock.unlockWrite(stamp);&#125;;Runnable r = () -&gt; &#123;   long stamp = lock.tryOptimisticRead();   if (!lock.validate(stamp)) &#123;      stamp = lock.readLock();      try &#123;         System.out.println(&quot;Read: &quot; + b.getAmount());      &#125; finally &#123;         lock.unlockRead(stamp);      &#125;   &#125; else &#123;      System.out.println(&quot;Optimistic read fails&quot;);   &#125;&#125;;\n\n现在更新和读取的都用 50 个线程来进行测试，最终的余额将会等于 60000.\nExecutorService executor = Executors.newFixedThreadPool(10);for (int i = 0; i &lt; 50; i++) &#123;   executor.submit(w);   executor.submit(r);&#125;\n\n并发累加器锁并并不是并发包中唯一有意思的特性，并发累加器也同样有趣；它可以根据我们提供的函数更新数据；再多线程更新数据的场景下，LongAccumulator 是比 AtomicLong 更优的选择。\n现在让我们来看看具体如何使用，我们需要两个参数进行初始化；第一个是用于累加计算的函数，通常是一个 sum 函数，第二个参数则是累加计算的初始化值。\n接下来我们用 10000 作为初始值来创建一个 LongAccumulator，最终结果是多少？其实结果与上文相同，都是 60000，但这次我们并没有使用锁。\nLongAccumulator balance = new LongAccumulator(Long::sum, 10000L);Runnable w = () -&gt; balance.accumulate(1000L);ExecutorService executor = Executors.newFixedThreadPool(50);for (int i = 0; i &lt; 50; i++) &#123;   executor.submit(w);&#125;executor.shutdown();if (executor.awaitTermination(1000L, TimeUnit.MILLISECONDS))   System.out.println(&quot;Balance: &quot; + balance.get());assert balance.get() == 60000L;\n\n\n数组的二分查找假设我们想在一个排序列表中插入一个新元素，可以使用 Arrays.binarySearch() 函数，当这个 key 存在时将会返回 key 所在的索引，如果不存在时将会返回插入的位置-(insertion point)-1。\nbinarySearch 是 Java 中非常简单且有效的查询方法。\n下面的这个例子中，对返回结果取反便能的到索引位置。\nint[] t = new int[] &#123;1, 2, 4, 5&#125;;int x = Arrays.binarySearch(t, 3);assert ~x == 2;\n\n\n负数的二进制是以正数的补码表示，对一个数取反+1 就等于补码，所以这里直接取反就等于 Arrays.binarySearch() 不存在时的返回值了。\n\nBit Set如果你需要对二进制数组进行操作你会怎么做？用 boolean[]  布尔数组?\n有一种更高效又更省内存的方式，那就是 BitSet。它允许我们存储和操作 bit 数组，与 boolean[] 相比可省 8 倍的内存；也可以使用 and/or/xor 等逻辑操作。\n假设我们现在有两个 bit 数组，我们需要对他们进行 xor 运算；我们需要创建两个 BitSet 实例，然后调用 xor 函数。\nBitSet bs1 = new BitSet();bs1.set(0);bs1.set(2);bs1.set(4);System.out.println(&quot;bs1 : &quot; + bs1);BitSet bs2 = new BitSet();bs2.set(1);bs2.set(2);bs2.set(3);System.out.println(&quot;bs2 : &quot; + bs2);bs2.xor(bs1);System.out.println(&quot;xor: &quot; + bs2);\n\n最终的输出结果如下：\n\n","categories":["翻译"],"tags":["Java"]},{"title":"SpringCloud Feign 实现动态 URL","url":"/2022/05/23/troubleshoot/SpringCloud-Feign-dynamic-url/","content":"\n背景前段时间同事碰到一个问题，需要在 SpringCloud 的 Feign 调用中使用自定义的 URL；通常情况下是没有这个需求的；毕竟都用了 SpringCloud 的了，那服务之间的调用都是走注册中心的，不会需要自定义 URL 的情况。\n\n\n但也有特殊的，比如我们这里碰到 ToB 场景，需要对每个商户自定义的 URL 进行调用。\n虽说也可以使用原生的 Feign 甚至是自定义一个 OKHTTP Client 实现，但这些方案都得换一种写法；\n打算利用现有的 SpringCloud OpenFeign 来实现，毕竟原生的 Feign 其实是支持该功能的，而 SpringCloud OpenFeign 也只是在这基础上封装了一层。\n\n只需要在接口声明处加上一个 URI 参数即可，这样就可以在每次调用时传递不同的 URI 来实现动态 URL 的目的。\n\n想法很简单，但实践起来却不是那么回事了。伪代码如下：\n@FeignClient(name = &quot;dynamic&quot;)interface DynamicClient &#123;\t@GetMapping(&quot;/&quot;)\tString get(URI uri);&#125;dynamicClient.get(URI.create(&quot;https://github.com&quot;));\t\n\n执行后会抛出负载均衡的异常：\njava.lang.RuntimeException: com.netflix.client.ClientException:Load balancer does not have available server for client: github.com\n\n这个异常也能理解，就是找不到 github 这个服务；找不到也是合理的，毕竟也不是一个内部注册的服务。\n但按照 Feign 的官方介绍，只要接口中声明了 URI 这个参数就能自定义，同时我自己也用原生的 Feign 测试过确实没什么问题。\nDebug那问题只能出在 SpringCloud OpenFeign 的封装上了；经过同事的搜索在网上找到一篇博客解决了这个问题。\nhttps://www.cnblogs.com/syui-terra/p/14386188.html\n\n按照文中的说法，确实只需要加上 URL 参数同时有值就可以了，但原因不明。\n本着打破砂锅问到底的精神，我个人也想知道 OpenFeign 是如何处理的，只要 url 有值就可以，这完全是个黑盒，而且在官方的注释中并没有对这种情况有特殊说明。\n所以我准备从源码中找到答案。\n既然是 url 有值就能正常运行，那一定是在运行过程中获取了这个值；\n但我在源码中查看 url 所使用的地方，并没有在单测之外找到哪里有所应用，说明源码中并没有直接调用 url() 这个函数来获取值。\n但 org.springframework.cloud.openfeign.FeignClient 这个注解总会使用吧，于是我又查询这个注解的使用情况。\n最终在这里查到了使用的痕迹。\n\n这里查阅源码时也有一些小技巧，比如如果我们直接查询时，IDEA 默认的查询范围是整个项目和所有依赖库，会有许多干扰信息。\n\n比如我这里就需要只看项目源码，单测这些都不用看；所以在查询的时候可以过滤一下，这样干扰信息就会少很多。\n\n左边的工具栏还有许多过滤条件，大家可以自行研究一下。\n\n接着从源码中进行阅读，会发现是将 @FeignClient 中的所有数据都写到一个 Map 里进行使用的。最终会发现这个 url 被写入到了 FeignClientFactoryBean 中的 url 成员变量中了。\n查看哪里在使用这个 url 就知道背后的原理了。\n\n在这里打个断点会发现：当 url 为空时会返回一个 LoadBalance 的 client，也就是会从注册中心获取 url 的客户端，而 url 有值时则会获取一个默认的客户端，这样就不会走负载均衡了。\n\n所以我们如果想在 OpenFeign 中使用动态 url 时就得让 @Feign 的 url 有值才行，无论是什么都可以。\n\nFeign 的实现既然已经看到这一步了，我也比较好奇 Feign 是如何做到只要有 URI 参数就使用指定的 URL 呢？\n\n这里也分享一个读源码的小技巧，如果我们跟着程序执行的思路去一步步 debug 的话会非常消耗时间，毕竟这类成熟库的代码量也不小。\n\n这里我们从官方文档中可以得知只要在接口参数中使用了 java.net.URI 便会走自定义的 url，所以我们反过来只要在源码中找到哪里在使用 java.net.URI 便能知道关键源码。\n毕竟使用 java.net.URI 的场景也不会太多。\n\n所以只需要在这个依赖的地方 cmd+shift+f 全局搜索 java.net.URI 就能查到结果，果然不多，只有两处使用。\n\n再结合使用场景猜测大概率是判断参数中是否是有 URL.class 这样的条件，或者是 url 对象；总之我们先用URL 这样关键字在这两个文件中搜索一下，记得勾选匹配大小写；最后会发现的确是判断了参数中是否有 URL 这个类，同时将这个索引位置记录了下来。\n想必后续会通过这个索引位置读取最终的 url 信息。\n\n最终通过这个索引的使用地方查询到了核心源码，如果有值时就取这个 URI 中所指定的地址作为 target。\n到此为止这个问题的背后原理都已经分析完毕了。\n总结其实本文重点是分析了一些 debug 和阅读源码的一些小技巧，特别是在读关于 Spring 相关的代码时一定不能 debug 跟踪到细节中，因为调用链通常是很长的，稍不留神就把自己都绕晕了，只需要知道核心、关键源码是如何处理的即可。\n最后对于 OpenFeign 处理动态 url 的方案确实也有些疑惑，是一个典型的约定大于配置的场景，但问题就在于我们并不知道这个约定是 @Feign  的 url 得有值。\n所以我也提了一个 PR 给 OpenFeign，感兴趣的朋友也可以查看一下：\nhttps://github.com/spring-cloud/spring-cloud-openfeign/pull/713\n","categories":["问题排查","Java 进阶"],"tags":["SpringCloud","Feign"]},{"title":"又一次生产 CPU 高负载排查实践","url":"/2019/06/18/troubleshoot/cpu-percent-100-02/","content":"\n前言前几日早上打开邮箱收到一封监控报警邮件：某某 ip 服务器 CPU 负载较高，请研发尽快排查解决，发送时间正好是凌晨。\n其实早在去年我也处理过类似的问题，并记录下来：《一次生产 CPU 100% 排查优化实践》\n不过本次问题产生的原因却和上次不太一样，大家可以接着往下看。\n\n\n问题分析收到邮件后我马上登陆那台服务器，看了下案发现场还在（负载依然很高）。\n于是我便利用这类问题的排查套路定位一遍。\n\n首先利用 top -c 将系统资源使用情况实时显示出来 （-c 参数可以完整显示命令）。\n接着输入大写 P 将应用按照 CPU 使用率排序，第一个就是使用率最高的程序。\n果不其然就是我们的一个 Java 应用。\n这个应用简单来说就是定时跑一些报表使的，每天凌晨会触发任务调度，正常情况下几个小时就会运行完毕。\n\n常规操作第二步自然是得知道这个应用中最耗 CPU 的线程到底再干嘛。\n利用 top -Hp pid 然后输入 P 依然可以按照 CPU 使用率将线程排序。\n这时我们只需要记住线程的 ID 将其转换为 16 进制存储起来，通过 jstack pid &gt;pid.log 生成日志文件，利用刚才保存的 16 进制进程 ID 去这个线程快照中搜索即可知道消耗 CPU 的线程在干啥了。\n如果你嫌麻烦，我也强烈推荐阿里开源的问题定位神器 arthas 来定位问题。\n比如上述操作便可精简为一个命令 thread -n 3 即可将最忙碌的三个线程快照打印出来，非常高效。\n\n更多关于 arthas 使用教程请参考官方文档。\n\n由于之前忘记截图了，这里我直接得出结论吧：\n最忙绿的线程是一个 GC 线程，也就意味着它在忙着做垃圾回收。\nGC 查看排查到这里，有经验的老司机一定会想到：多半是应用内存使用有问题导致的。\n于是我通过 jstat -gcutil pid 200 50 将内存使用、gc 回收状况打印出来（每隔 200ms 打印 50次）。\n\n从图中可以得到以下几个信息：\n\nEden 区和 old 区都快占满了，可见内存回收是有问题的。\nfgc 回收频次很高，10s 之内发生了 8 次回收（(866493-866485)/ (200 *5)）。\n持续的时间较长，fgc 已经发生了 8W 多次。\n\n内存分析既然是初步定位是内存问题，所以还是得拿一份内存快照分析才能最终定位到问题。\n通过命令 jmap -dump:live,format=b,file=dump.hprof pid 可以导出一份快照文件。\n这时就得借助 MAT 这类的分析工具出马了。\n问题定位\n通过这张图其实很明显可以看出，在内存中存在一个非常大的字符串，而这个字符串正好是被这个定时任务的线程引用着。\n\n大概算了一下这个字符串所占的内存为 258m 左右，就一个字符串来说已经是非常大的对象了。\n那这个字符串是咋产生的呢？\n其实看上图中的引用关系及字符串的内容不难看出这是一个 insert 的 SQL 语句。\n这时不得不赞叹 MAT 这个工具，他还能帮你预测出这个内存快照可能出现问题地方同时给出线程快照。\n\n\n最终通过这个线程快照找到了具体的业务代码：\n他调用一个写入数据库的方法，而这个方法会拼接一个 insert 语句，其中的 values 是循环拼接生成，大概如下：\n&lt;insert id=&quot;insert&quot; parameterType=&quot;java.util.List&quot;&gt;    insert into xx (files)    values    &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; separator=&quot;,&quot;&gt;        xxx    &lt;/foreach&gt;&lt;/insert&gt;\n\n所以一旦这个 list 非常大时，这个拼接的 SQL 语句也会很长。\n\n通过刚才的内存分析其实可以看出这个 List 也是非常大的，也就导致了最终的这个 insert 语句占用的内存巨大。\n优化策略既然找到问题原因那就好解决了，有两个方向：\n\n控制源头 List 的大小，这个 List 也是从某张表中获取的数据，可以分页获取；这样后续的 insert 语句就会减小。\n控制批量写入数据的大小，其实本质还是要把这个拼接的 SQL 长度降下来。\n整个的写入效率需要重新评估。\n\n总结本次问题从分析到解决花的时间并不长，也还比较典型，其中的过程再总结一下：\n\n首先定位消耗 CPU 进程。\n再定位消耗 CPU 的具体线程。\n内存问题 dump 出快照进行分析。\n得出结论，调整代码，测试结果。\n\n最后愿大家都别接到生产告警。\n你的点赞与分享是对我最大的支持\n","categories":["问题排查","Java 进阶"],"tags":["Java","Thread"]},{"title":"一次生产 CPU 100% 排查优化实践","url":"/2018/12/17/troubleshoot/cpu-percent-100/","content":"\n前言到了年底果然都不太平，最近又收到了运维报警：表示有些服务器负载非常高，让我们定位问题。\n还真是想什么来什么，前些天还故意把某些服务器的负载提高（没错，老板让我写个 BUG！），不过还好是不同的环境互相没有影响。\n\n\n定位问题拿到问题后首先去服务器上看了看，发现运行的只有我们的 Java 应用。于是先用 ps 命令拿到了应用的 PID。\n接着使用 top -Hp pid 将这个进程的线程显示出来。输入大写的 P 可以将线程按照 CPU 使用比例排序，于是得到以下结果。\n\n果然某些线程的 CPU 使用率非常高。\n为了方便定位问题我立马使用 jstack pid &gt; pid.log 将线程栈 dump 到日志文件中。\n我在上面 100% 的线程中随机选了一个 pid=194283 转换为 16 进制（2f6eb）后在线程快照中查询：\n\n因为线程快照中线程 ID 都是16进制存放。\n\n\n发现这是 Disruptor 的一个堆栈，前段时间正好解决过一个由于 Disruptor 队列引起的一次 OOM：强如 Disruptor 也发生内存溢出？\n没想到又来一出。\n为了更加直观的查看线程的状态信息，我将快照信息上传到专门分析的平台上。\nhttp://fastthread.io/\n\n其中有一项菜单展示了所有消耗 CPU 的线程，我仔细看了下发现几乎都是和上面的堆栈一样。\n也就是说都是 Disruptor 队列的堆栈，同时都在执行 java.lang.Thread.yield 函数。\n众所周知 yield 函数会让当前线程让出 CPU 资源，再让其他线程来竞争。\n根据刚才的线程快照发现处于 RUNNABLE 状态并且都在执行 yield 函数的线程大概有 30几个。\n因此初步判断为大量线程执行 yield 函数之后互相竞争导致 CPU 使用率增高，而通过对堆栈发现是和使用 Disruptor 有关。\n解决问题而后我查看了代码，发现是根据每一个业务场景在内部都会使用 2 个 Disruptor 队列来解耦。\n假设现在有 7 个业务类型，那就等于是创建 2*7=14 个 Disruptor 队列，同时每个队列有一个消费者，也就是总共有 14 个消费者（生产环境更多）。\n同时发现配置的消费等待策略为 YieldingWaitStrategy 这种等待策略确实会执行 yield 来让出 CPU。\n代码如下：\n\n\n初步看来和这个等待策略有很大的关系。\n\n本地模拟为了验证，我在本地创建了 15 个 Disruptor 队列同时结合监控观察 CPU 的使用情况。\n\n创建了 15 个 Disruptor 队列，同时每个队列都用线程池来往 Disruptor队列 里面发送 100W 条数据。\n消费程序仅仅只是打印一下。\n\n跑了一段时间发现 CPU 使用率确实很高。\n\n\n同时 dump 线程发现和生产的现象也是一致的：消费线程都处于 RUNNABLE 状态，同时都在执行 yield。\n通过查询 Disruptor 官方文档发现：\n\n\nYieldingWaitStrategy 是一种充分压榨 CPU 的策略，使用自旋 + yield的方式来提高性能。当消费线程（Event Handler threads）的数量小于 CPU 核心数时推荐使用该策略。\n\n\n\n同时查阅到其他的等待策略 BlockingWaitStrategy （也是默认的策略），它使用的是锁的机制，对 CPU 的使用率不高。\n于是在和之前同样的条件下将等待策略换为 BlockingWaitStrategy。\n\n\n\n和刚才的 CPU 对比会发现到后面使用率的会有明显的降低；同时 dump 线程后会发现大部分线程都处于 waiting 状态。\n优化解决看样子将等待策略换为 BlockingWaitStrategy 可以减缓 CPU 的使用，\n但留意到官方对 YieldingWaitStrategy 的描述里谈道：当消费线程（Event Handler threads）的数量小于 CPU 核心数时推荐使用该策略。\n而现有的使用场景很明显消费线程数已经大大的超过了核心 CPU 数了，因为我的使用方式是一个 Disruptor 队列一个消费者，所以我将队列调整为只有 1 个再试试(策略依然是 YieldingWaitStrategy)。\n\n\n跑了一分钟，发现 CPU 的使用率一直都比较平稳而且不高。\n总结所以排查到此可以有一个结论了，想要根本解决这个问题需要将我们现有的业务拆分；现在是一个应用里同时处理了 N 个业务，每个业务都会使用好几个 Disruptor 队列。\n由于是在一台服务器上运行，所以 CPU 资源都是共享的，这就会导致 CPU 的使用率居高不下。\n所以我们的调整方式如下：\n\n为了快速缓解这个问题，先将等待策略换为 BlockingWaitStrategy，可以有效降低 CPU 的使用率（业务上也还能接受）。\n第二步就需要将应用拆分（上文模拟的一个 Disruptor 队列），一个应用处理一种业务类型；然后分别单独部署，这样也可以互相隔离互不影响。\n\n当然还有其他的一些优化，因为这也是一个老系统了，这次 dump 线程居然发现创建了 800+ 的线程。\n创建线程池的方式也是核心线程数、最大线程数是一样的，导致一些空闲的线程也得不到回收；这样会有很多无意义的资源消耗。\n所以也会结合业务将创建线程池的方式调整一下，将线程数降下来，尽量的物尽其用。\n本文的演示代码已上传至 GitHub：\nhttps://github.com/crossoverJie/JCSprout\n你的点赞与分享是对我最大的支持\n","categories":["问题排查","Java 进阶"],"tags":["Java","Thread","concurrent","JVM","disruptor"]},{"title":"What？一个 Dubbo 服务启动要两个小时！","url":"/2019/07/05/troubleshoot/dubbo-start-slow/","content":"\n前言前几天在测试环境碰到一个非常奇怪的与 dubbo 相关的问题，事后我在网上搜索了一圈并没有发现类似的帖子或文章，于是便有了这篇。\n希望对还未碰到或正在碰到的朋友有所帮助。\n\n\n现象现象是这样的，有一天测试在测试环境重新部署一个 dubbo 应用的时候发现应用“启动不起来”。\n但过几个小时候之后又能自己慢慢恢复，并能够对外提供 dubbo 服务。\n\n但其实经过我后续排查发现刚开始其实并不是启动不起来，而是启动速度非常缓慢，所以当应用长时间启动后才会对外提供服务。\n\n\n而这个速度慢到居然要花费 2 个小时。\n导致的一个结果是测试完全不敢在测试环境发版验证了，每验证一个功能修复一个 bug 就得等上两个小时，这谁受得了😂。\n\n而且经过多次观察，确实每次都是花费两小时左右应用才能启动起来。\n\n尝试解决最后测试顶不住了，只能让我这个“事故报告撰写专家”来看看。\n当我得知这个问题的现象时其实完全没当一回事：\n\n都不用想，这不就是主线程阻塞了嘛，先看看是否在初始化的时候数据库、Zookeeper 之类的连不上导致阻塞了——-来之多次事故处理的经验告诉我。\n\n于是我把这事打回给测试让他先找运维排查下，不到万不得已不要影响我 Touch fish🐳。\n第二天一早看到测试同学的微信头像跳动时我都已经准备接受又一句 “膜拜大佬👍” 的回复时，却收到 “网络一切正常，没人动过，再不解决就要罢工了🤬”。\n好吧，忽悠不过去了。\n首先这类问题的排查方向应该不会错，就是主线程阻塞了，至于是啥导致的阻塞就不能像之前那样瞎猜了。\n我将应用重启后用 jstack pid 将线程快照打印到终端，直接拉到最后看看 main 线程到底在干啥。\n前几次的快照都是很正常：\n加载 Spring —-&gt;连接 Zookeeper —&gt; 连接 Redis，都是依次执行下来没有阻塞。\n隔了一段后应用确实还没起来，我再次 jstack 后得到如下信息：\n\n翻源码我一直等了十几分钟再多次 jstack 得到的快照得到的信息都是一样的。\n\n如图所示可见主线程是卡在了 dubbo 的某个方法 ServiceConfig.java 的 303 行中。\n于是我找到此处的源码：\n\n简单来说这里的逻辑就是要获取本机的 IP 将其注册到 Zookeeper 中用于其他服务调用。\n\n再往下跟就如堆栈中一样是卡在了 Inet4AddressImpl.getLocalHostName 处。\n但这是一个 native 方法，我们应用也根本干涉不了，最终的现象就是调用这个本地方法非常耗时。\n于是这问题貌似也阻塞在这儿了，没有太多办法。\n最终解决既然这是一个 native 方法，那说明和应用本身没有啥关系（确实也是这样，这个问题是突然间出现的。）\n那是否是服务器本身的问题呢，想到在 native 方法里是获取本机的 hostname，那是否和这个 hostname 有关系呢。\n\n\n这是在我自己的阿里云服务器上测试，真正的测试环境不是这个名字。\n\n拿到服务器 hostname 后再尝试 ping 这个 hostname，奇怪的现象发生了：\n命令刚开始会卡住一段时间（大概几十秒），然后才会输出 hostname 对应的 ip 以及对应的延迟。\n而当我直接 ping 这个 ip 时却能快速响应后面的输出。\n最后我尝试在 &#x2F;etc&#x2F;hosts 配置文件中加入了对应的 host 配置：\nxx.xx.xx.xx(ip) hostname\n\n再次 ping hostname 的效果就和直接 ping ip 一样了。\n于是我再次重启应用，一切都正常了。\n总结最后根据我调整的内容尝试分析下本次问题的原因：\n\n当 Dubbo 在启动获取本地 ip 时，是通过服务器 hostname 从 dns 服务器返回当前的 ip 地址。\n由于 dns 服务器或者是本地服务器与 dns 服务器之间存在网络问题，导致这个过程的时间被拉长（猜测）。\n我在本地的 host 文件中配置后，就相当于本地有一个缓存，优先取本地配置的 ip ，避免了和 dns 服务器交互的过程，所以速度提升了。\n\n虽然问题得到解决了，但还是有几个疑问：\n第一个是为什么和 DNS 服务器的交互会这么慢，即便是慢也没有像应用那样需要 2 个小时才能返回，这里我也没搞得太清楚，有相关经验的朋友可以留言讨论。\n第二就是 Dubbo 在这个依赖外部获取资源时健壮性是否可以做的更好，虽说我这问题估计也几人碰到。\n对于这种长时间没有启动成功的问题是否可以加上提示，比如直接抛出异常退出程序，将问题可能的原因告诉开发者，方便排查问题。\n你的点赞与分享是对我最大的支持\n","categories":["问题排查"],"tags":["Java","Thread","Dubbo"]},{"title":"Pulsar 重复消费?","url":"/2022/03/18/troubleshoot/pulsar-repeat-consume/","content":"\n背景许久没有分享 Java 相关的问题排查了，最近帮同事一起排查了一个问题：\n\n在使用 Pulsar 消费时，发生了同一条消息反复消费的情况。\n\n\n\n排查当他告诉我这个现象的时候我就持怀疑态度，根据之前使用的经验 Pulsar 在官方文档以及 API 中都解释过：\n只有当设置了消费的 ackTimeout 并超时消费时才会重复投递消息，默认情况下是关闭的，查看代码也确实没有开启。\n那会不会是调用了 negativeAcknowledge() 方法呢（调用该方法也会触发重新投递），因为我们使了一个第三方库 https://github.com/majusko/pulsar-java-spring-boot-starter 只有当抛出异常时才会调用该方法。\n查阅代码之后也没有地方抛出异常，甚至整个过程中都没看到异常产生；这就有点诡异了。\n复现为了捋清楚整个事情的来龙去脉，详细了解了他的使用流程；\n其实也就是业务出现了 bug，他在消息消费时 debug 然后进行单步调试，当走完一次调试后，没多久马上又收到了同样的消息。\n但奇怪的是也不是每次 debug 后都能重复消费，我们都说如果一个 bug 能 100% 完全复现，那基本上就解决一大半了。\n所以我们排查的第一步就是完全复现这个问题。\n\n为了排除掉是 IDEA 的问题（虽然极大概率不太可能）既然是 debug 的时候产生的问题，那其实转换到代码也就是 sleep 嘛，所以我们打算在消费逻辑里直接 sleep 一段时间看能否复现。\n经过测试，sleep 几秒到几十秒都无法复现，最后索性 sleep 一分钟，神奇的事情发生了，每次都成功复现！\n既然能成功复现那就好说了，因为我自己的业务代码也有使用到 Pulsar 的地方，为了方便调试就准备在自己的项目里再复现一次。\n结果诡异的事情再次发生，我这里又不能复现了。\n\n虽然这才是符合预期的，但这就没法调了呀。\n\n本着相信现代科学的前提，我们俩唯一的区别就是项目不一样了，为此我对比了两边的代码。\n@PulsarConsumer(        topic = xx,        clazz = Xx.class,        subscriptionType = SubscriptionType.Shared)public void consume(Data msg) &#123;    log.info(&quot;consume msg:&#123;&#125;&quot;, msg.getOrderId());    Lock lock = redisLockRegistry.obtain(msg.getOrderId());    if (lock.tryLock()) &#123;        try &#123;            orderService.do(msg.getOrderId());        &#125; catch (Exception e) &#123;            log.error(&quot;consumer msg:&#123;&#125; err:&quot;, msg.toString(), e);        &#125; finally &#123;            lock.unlock();        &#125;    &#125;&#125;\n\n结果不出所料，同事那边的代码加了锁；一个基于 Redis 的分布式锁，这时我一拍大腿不会是解锁的时候超时了导致抛了异常吧。\n为了验证这个问题，在能复现的基础上我在框架的 Pulsar 消费处打了断点：果然破案了，异常提示已经非常清楚了：加锁已经过了超时时间。\n进入异常后直接 negative 消息，同时异常也被吃掉了，所以之前没有发现。\n查阅了 RedisLockRegistry 的源码，默认超时时间正好是一分钟，所以之前我们 sleep 几十秒也无法复现这个问题。\n总结事后我向同事了解了下为啥这里要加锁，因为我看下来完全没有加锁的必要；结果他是因为从别人那里复制的代码才加上的，压根没想那么多。\n所以这事也能得出一些教训：\n\nctrl C&#x2F;V 虽然方便，但也得充分考虑自己的业务场景。\n使用一些第三方 API 时，需要充分了解其作用、参数。\n\n你的点赞与分享是对我最大的支持\n","categories":["问题排查","Java 进阶"],"tags":["Pulsar","Consumer"]},{"title":"一个线程罢工的诡异事件","url":"/2019/03/12/troubleshoot/thread-gone/","content":"\n背景事情（事故）是这样的，突然收到报警，线上某个应用里业务逻辑没有执行，导致的结果是数据库里的某些数据没有更新。\n虽然是前人写的代码，但作为 Bug maker&amp;killer 只能咬着牙上了。\n\n\n因为之前没有接触过出问题这块的逻辑，所以简单理了下如图：\n\n\n有一个生产线程一直源源不断的往队列写数据。\n消费线程也一直不停的取出数据后写入后续的业务线程池。\n业务线程池里的线程会对每个任务进行入库操作。\n\n整个过程还是比较清晰的，就是一个典型的生产者消费者模型。\n尝试定位接下来便是尝试定位这个问题，首先例行检查了以下几项：\n\n是否内存有内存溢出？\n应用 GC 是否有异常？\n\n通过日志以及监控发现以上两项都是正常的。\n紧接着便 dump 了线程快照查看业务线程池中的线程都在干啥。\n\n结果发现所有业务线程池都处于 waiting 状态，队列也是空的。\n同时生产者使用的队列却已经满了，没有任何消费迹象。\n结合上面的流程图不难发现应该是消费队列的 Consumer 出问题了，导致上游的队列不能消费，下有的业务线程池没事可做。\nreview 代码于是查看了消费代码的业务逻辑，同时也发现消费线程是一个单线程。\n\n结合之前的线程快照，我发现这个消费线程也是处于 waiting 状态，和后面的业务线程池一模一样。\n他做的事情基本上就是对消息解析，之后丢到后面的业务线程池中，没有发现什么特别的地方。\n\n但是由于里面的分支特别多（switch case），看着有点头疼；所以我与写这个业务代码的同学沟通后他告诉我确实也只是入口处解析了一下数据，后续所有的业务逻辑都是丢到线程池中处理的，于是我便带着这个前提去排查了（埋下了伏笔）。\n\n因为这里消费的队列其实是一个 disruptor 队列；它和我们常用的 BlockQueue 不太一样，不是由开发者自定义一个消费逻辑进行处理的；而是在初始化队列时直接丢一个线程池进去，它会在内部使用这个线程池进行消费，同时回调一个方法，在这个方法里我们写自己的消费逻辑。\n所以对于开发者而言，这个消费逻辑其实是一个黑盒。\n于是在我反复 review 了消费代码中的数据解析逻辑发现不太可能出现问题后，便开始疯狂怀疑是不是 disruptor 自身的问题导致这个消费线程罢工了。\n再翻了一阵 disruptor 的源码后依旧没发现什么问题后我咨询对 disruptor 较熟的@咖啡拿铁，在他的帮助下在本地模拟出来和生产一样的情况。\n本地模拟\n本地也是创建了一个单线程的线程池，分别执行了两个任务。\n\n第一个任务没啥好说的，就是简单的打印。\n第二个任务会对一个数进行累加，加到 10 之后就抛出一个未捕获的异常。\n\n接着我们来运行一下。\n\n发现当任务中抛出一个没有捕获的异常时，线程池中的线程就会处于 waiting 状态，同时所有的堆栈都和生产相符。\n\n细心的朋友会发现正常运行的线程名称和异常后处于 waiting 状态的线程名称是不一样的，这个后续分析。\n\n解决问题\n当加入异常捕获后又如何呢？\n\n程序肯定会正常运行。\n\n同时会发现所有的任务都是由一个线程完成的。\n\n虽说就是加了一行代码，但我们还是要搞清楚这里面的门门道道。\n源码分析于是只有直接 debug 线程池的源码最快了；\n\n\n\n通过刚才的异常堆栈我们进入到 ThreadPoolExecutor.java:1142 处。\n\n发现线程池已经帮我们做了异常捕获，但依然会往上抛。\n在 finally 块中会执行 processWorkerExit(w, completedAbruptly) 方法。\n\n\n看过之前《如何优雅的使用和理解线程池》的朋友应该还会有印象。\n线程池中的任务都会被包装为一个内部 Worker 对象执行。\nprocessWorkerExit 可以简单的理解为是把当前运行的线程销毁（workers.remove(w)）、同时新增（addWorker()）一个 Worker 对象接着处理；\n\n就像是哪个零件坏掉后重新换了一个新的接着工作，但是旧零件负责的任务就没有了。\n\n接下来看看 addWorker() 做了什么事情：\n\n只看这次比较关心的部分；添加成功后会直接执行他的 start() 的方法。\n\n由于 Worker 实现了 Runnable 接口，所以本质上就是调用了 runWorker() 方法。\n\n在 runWorker() 其实就是上文 ThreadPoolExecutor 抛出异常时的那个方法。\n\n它会从队列里一直不停的获取待执行的任务，也就是 getTask()；在 getTask 也能看出它会一直从内置的队列取出任务。\n而一旦队列是空的，它就会 waiting 在 workQueue.take()，也就是我们从堆栈中发现的 1067 行代码。\n线程名字的变化\n上文还提到了异常后的线程名称发生了改变，其实在 addWorker() 方法中可以看到 new Worker()时就会重新命名线程的名称，默认就是把后缀的计数+1。\n这样一切都能解释得通了，真相只有一个：\n\n在单个线程的线程池中一但抛出了未被捕获的异常时，线程池会回收当前的线程并创建一个新的 Worker；它也会一直不断的从队列里获取任务来执行，但由于这是一个消费线程，根本没有生产者往里边丢任务，所以它会一直 waiting 在从队列里获取任务处，所以也就造成了线上的队列没有消费，业务线程池没有执行的问题。\n\n总结所以之后线上的那个问题加上异常捕获之后也变得正常了，但我还是有点纳闷的是：\n\n既然后续所有的任务都是在线程池中执行的，也就是纯异步了，那即便是出现异常也不会抛到消费线程中啊。\n\n这不是把我之前储备的知识点推翻了嘛？不信邪！之后我让运维给了加上异常捕获后的线上错误日志。\n结果发现在上文提到的众多 switch case 中，最后一个竟然是直接操作的数据库，导致一个非空字段报错了🤬！！\n这事也给我个教训，还是得眼见为实啊。\n虽然这个问题改动很小解决了，但复盘整个过程还是有许多需要改进的：\n\n消费队列的线程名称竟然和业务线程的前缀一样，导致我光找它就花了许多时间，命名必须得调整。\n开发规范，防御式编程大家需要养成习惯。\n未知的技术栈需要谨慎，比如 disruptor，之前的团队应该只是看了个高性能的介绍就直接使用，并没有深究其原理；导致出现问题后对它拿不准。\n\n实例代码：\nhttps://github.com/crossoverJie/JCSprout/blob/master/src/main/java/com/crossoverjie/thread/ThreadExceptionTest.java\n你的点赞与分享是对我最大的支持\n","categories":["问题排查","Java 进阶"],"tags":["Java","Thread","concurrent","disruptor"]},{"title":"线程池中你不容错过的一些细节","url":"/2019/03/26/troubleshoot/thread-gone2/","content":"\n背景上周分享了一篇《一个线程罢工的诡异事件》，最近也在公司内部分享了这个案例。\n无独有偶，在内部分享的时候也有小伙伴问了之前分享时所提出的一类问题：\n\n\n\n\n\n\n这其实是一类共性问题，我认为主要还是两个原因：\n\n我自己确实也没讲清楚，之前画的那张图还需要再完善，有些误导。\n第二还是大家对线程池的理解不够深刻，比如今天要探讨的内容。\n\n线程池的工作原理首先还是来复习下线程池的基本原理。\n我认为线程池它就是一个调度任务的工具。\n众所周知在初始化线程池会给定线程池的大小，假设现在我们有 1000 个线程任务需要运行，而线程池的大小为 1020，在真正运行任务的过程中他肯定不会创建这1000个线程同时运行，而是充分利用线程池里这 1020 个线程来调度这1000个任务。\n而这里的 10~20 个线程最后会由线程池封装为 ThreadPoolExecutor.Worker 对象，而这个 Worker 是实现了 Runnable 接口的，所以他自己本身就是一个线程。\n深入分析\n这里我们来做一个模拟，创建了一个核心线程、最大线程数、阻塞队列都为2的线程池。\n这里假设线程池已经完成了预热，也就是线程池内部已经创建好了两个线程 Worker。\n当我们往一个线程池丢一个任务会发生什么事呢？\n\n\n第一步是生产者，也就是任务提供者他执行了一个 execute() 方法，本质上就是往这个内部队列里放了一个任务。\n之前已经创建好了的 Worker 线程会执行一个 while 循环 —&gt; 不停的从这个内部队列里获取任务。(这一步是竞争的关系，都会抢着从队列里获取任务，由这个队列内部实现了线程安全。)\n获取得到一个任务后，其实也就是拿到了一个 Runnable 对象(也就是 execute(Runnable task) 这里所提交的任务)，接着执行这个 Runnable 的 **run() 方法，而不是 start()**，这点需要注意后文分析原因。\n\n结合源码来看：\n\n从图中其实就对应了刚才提到的二三两步：\n\nwhile 循环，从 getTask() 方法中一直不停的获取任务。\n拿到任务后，执行它的 run() 方法。\n\n这样一个线程就调度完毕，然后再次进入循环从队列里取任务并不断的进行调度。\n再次解释之前的问题接下来回顾一下我们上一篇文章所提到的，导致一个线程没有运行的根本原因是：\n\n在单个线程的线程池中一但抛出了未被捕获的异常时，线程池会回收当前的线程并创建一个新的 Worker；它也会一直不断的从队列里获取任务来执行，但由于这是一个消费线程，根本没有生产者往里边丢任务，所以它会一直 waiting 在从队列里获取任务处，所以也就造成了线上的队列没有消费，业务线程池没有执行的问题。\n\n结合之前的那张图来看：\n\n这里大家问的最多的一个点是，为什么会没有是根本没有生产者往里边丢任务，图中不是明明画的有一个 product 嘛？\n这里确实是有些不太清楚，再次强调一次：\n图中的 product 是往内部队列里写消息的生产者，并不是往这个 Consumer 所在的线程池中写任务的生产者。\n因为即便 Consumer 是一个单线程的线程池，它依然具有一个常规线程池所具备的所有条件：\n\nWorker 调度线程，也就是线程池运行的线程；虽然只有一个。\n内部的阻塞队列；虽然长度只有1。\n\n再次结合图来看：\n\n所以之前提到的【没有生产者往里边丢任务】是指右图放大后的那一块，也就是内部队列并没有其他线程往里边丢任务执行 execute() 方法。\n而一旦发生未捕获的异常后，Worker1 被回收，顺带的它所调度的线程 task1（这个task1 也就是在执行一个 while 循环消费左图中的那个队列） 也会被回收掉。\n新创建的 Worker2 会取代 Worker1 继续执行 while 循环从内部队列里获取任务，但此时这个队列就一直会是空的，所以也就是处于 Waiting 状态。\n\n我觉得这波解释应该还是讲清楚了，欢迎还没搞明白的朋友留言讨论。\n\n为什是 run() 而不是 start()问题搞清楚后来想想为什么线程池在调度的时候执行的是 Runnable 的 run() 方法，而不是 start() 方法呢？\n我相信大部分没有看过源码的同学心中第一个印象就应该是执行的 start() 方法；\n因为不管是学校老师，还是网上大牛讲的都是只有执行了 start() 方法后操作系统才会给我们创建一个独立的线程来运行，而 run() 方法只是一个普通的方法调用。\n而在线程池这个场景中却恰好就是要利用它只是一个普通方法调用。\n回到我在文初中所提到的：我认为线程池它就是一个调度任务的工具。\n假设这里是调用的 Runnable 的 start 方法，那会发生什么事情。\n如果我们往一个核心、最大线程数为 2 的线程池里丢了 1000 个任务，那么它会额外的创建 1000 个线程，同时每个任务都是异步执行的，一下子就执行完毕了。\n从而没法做到由这两个 Worker 线程来调度这 1000 个任务，而只有当做一个同步阻塞的 run() 方法调用时才能满足这个要求。\n\n这事也让我发现一个奇特的现象：就是网上几乎没人讲过为什么在线程池里是 run 而不是 start，不知道是大家都觉得这是基操还是没人仔细考虑过。\n\n总结针对之前线上事故的总结上次已经写得差不多了，感兴趣的可以翻回去看看。\n这次呢可能更多是我自己的总结，比如写一篇技术博客时如果大部分人对某一个知识点讨论的比较热烈时，那一定是作者要么讲错了，要么没讲清楚。\n这点确实是要把自己作为一个读者的角度来看，不然很容易出现之前的一些误解。\n在这之外呢，我觉得对于线程池把这两篇都看完同时也理解后对于大家理解线程池，利用线程池完成工作也是有很大好处的。\n如果有在面试中加分的记得回来点赞、分享啊。\n你的点赞与分享是对我最大的支持\n","categories":["问题排查","Java 进阶"],"tags":["Java","Thread","concurrent"]},{"title":"VLOG-008:Basketball Day One","url":"/2019/04/21/vlog/Basketball%20Day%20one/","content":"\n周末轻松一下。\n \n\n","categories":["VLOG"]},{"title":"VLOG-004：国产程序员的一天","url":"/2019/02/20/vlog/Chinese-coder-daily/","content":"\nVLOG 近些年非常流行，最近这段时间我也拍了一些来记录生活。\n之前一直想记录自己上班生活的一天；至于为什么标题要加上一个国产两字，是因为之前看到一位国外女程序媛的一天（视频链接见底部），这次是想让大家看看在天朝国情下的反差。\na day in the life of a software engineer:https://www.youtube.com/watch?v=rqX8PFcOpxA\n正片开始 \n\n\n\n08:00\n起床洗漱。\n\n8:20\n出门到轻轨站。\n\n8:30\n到达轻轨站。\n\n9:00\n到达公司开始干活。\n\n10:00\n一个电话远程面试。\n\n11:00\n一个电话远程面试。\n\n中间有一段对自己这些天来面试经历的一些分享，视频加快了但是重点内容都打有字幕。\n12:00\n午饭时间。\n\n15:00\n部门内部会议到 16 点。\n\n17:00\n一个电话远程面试。\n\n19:00\n有时间做自己的撸码工作。\n\n20:00\n准备回家。\n\n22:00\n接女朋友下班。\n\n00:30\n完成一个算法，收工睡觉。\n\n下一次录工作 VLOG 应该是等我当老板咯，希望别是有生之年系列。。。\n","categories":["VLOG"]},{"title":"VLOG-006:一个程序员的周末（上）","url":"/2019/03/21/vlog/Chinese-coder-weekends-01/","content":"\n上次发了一个《VLOG-004：国产程序员的一天》评论和播放量都还不错，这次趁热打铁更新了周末业余生活是怎么样的。\n在这个视频中你将看到：\n\n作为一个居家好男人是如何体现自我价值的？\n我是如何产出一份技术博客？\n女朋友不在家如何正规消遣时间？\n如何撸得代码、下得厨房讨女朋友开心？\n\n正片开始 \n\n\n\n\n这个视频是上部分，主要记录的是在家里的生活。\n起床可能是由于平时早起习惯了，我居然快改掉了多年懒床的毛病，近期周末都是8、9点的样子就会起床。\n起床后磨蹭半天一不小心就到了饭点，由于女朋友上早班一个人的话就点了外卖。\n吃饭的同时会看看一部下饭神剧《武林外传》。\n家务开始洗衣服，洗衣服的同时需要收衣服、叠衣服从而实现个人价值。\n周报\n准备开始写周报，结果一不小心沉浸于电视。。。\n2个小时后周报完成。\nPPT\n完成下周技术分享的 PPT，但受限于设计细胞整个 PPT 将比较难看。\n技术博客\n开始写一篇技术博客，分享了我个人的一些习惯：\n\n先在本子上打好提纲。\n加上一个二次元配图。\n写博客时需要注意的点。\n\n消遣\n玩 switch ，买游戏花的钱已经都可以再买一个 switch 了，果真是买游戏送主机；主要玩：\n\n马里奥赛车8\n喷射乌贼娘\n塞尔达传说\n任天堂明星大乱斗\n\n晚饭\n作为王刚师傅的在线弟子，深的宽油真传，必将料理出感动人心的美味；本次为大家带来的是口味猪肝。\n全程高能，请大家饭前观看（吃完饭后会忍不住想吃）。\nEND看到这里想必你已经对开始的几个问题有了答案了吧，来核对下我的标准答案吧。\n\n作为一个居家好男人是如何体现自我价值的？—-&gt;你的价值就是洗碗、洗衣服收拾家务。\n我是如何产出一份技术博客？—-&gt;提前构思目录、文章配图。\n女朋友不在家如何正规消遣时间？—-&gt;必须是 switch 啊，不然你以为是啥。。\n如何撸得代码、下得厨房讨女朋友开心？—-&gt;看 20 遍王刚师傅的视频自学成才。\n\n下一期将会为大家带来室外篇，作为一个大学毕业到现在成功长膘 20 斤的篮球爱好者，如何科学运动长肉，你值得期待🤫。\n","categories":["VLOG"]},{"title":"VLOG-007:一个程序员的周末（中）","url":"/2019/03/30/vlog/Chinese-coder-weekends-02/","content":"\n承接上期《VLOG-006:一个程序员的周末（上）》\n本次是周末日常的中篇，也是我每周做的最频繁的一件事情【陪女朋友逛街】。\n\n\n导致的结果是我已经清楚的知道所在商圈里大部分门店里的休息位置。\n哪里坐着舒服、哪里的信号比较好、哪家店逛不久会马上走，所以最好是不要坐下。\n可谓是“旱的旱死，涝的涝死”\n正片开始 \n\n\n\n","categories":["VLOG"]},{"title":"「造个轮子」——cicada(轻量级 WEB 框架)","url":"/2018/09/03/wheel/cicada1/","content":"\n前言俗话说 「不要重复造轮子」，关于是否有必要不再本次讨论范围。\n创建这个项目的主要目的还是提升自己，看看和知名类开源项目的差距以及学习优秀的开源方式。\n好了，现在着重来谈谈 cicada 这个项目的核心功能。\n我把他定义为一个快速、轻量级 WEB 框架；没有过多的依赖，核心 jar 包仅 30KB。\n也仅需要一行代码即可启动一个 HTTP 服务。\n\n\n\n\n\n特性现在来谈谈重要的几个特性。\n\n当前版本主要实现了基本的请求、响应、自定义参数以及拦截器功能。\n\n功能虽少，但五脏俱全。\n\n在今后的迭代过程中会逐渐完善上图功能，有好的想法也欢迎提 https://github.com/crossoverJie/cicada/issues。\n快速启动下面来看看如何快速启动一个 HTTP 服务。\n只需要创建一个 Maven 项目，并引入核心包。\n&lt;dependency&gt;    &lt;groupId&gt;top.crossoverjie.opensource&lt;/groupId&gt;    &lt;artifactId&gt;cicada-core&lt;/artifactId&gt;    &lt;version&gt;1.0.1&lt;/version&gt;&lt;/dependency&gt;\n\n如上图所示，再配置一个启动类即可。\npublic class MainStart &#123;    public static void main(String[] args) throws InterruptedException &#123;        CicadaServer.start(MainStart.class,&quot;/cicada-example&quot;) ;    &#125;&#125;\n\n\n配置业务 Action当然我们还需要一个实现业务逻辑的地方。cicada 提供了一个接口，只需要实现该接口即可实现具体逻辑。\n创建业务 Action 实现 top.crossoverjie.cicada.server.action.WorkAction 接口。\n@CicadaAction(value = &quot;demoAction&quot;)public class DemoAction implements WorkAction &#123;    private static final Logger LOGGER = LoggerBuilder.getLogger(DemoAction.class) ;    private static AtomicLong index = new AtomicLong() ;    @Override    public WorkRes&lt;DemoResVO&gt; execute(Param paramMap) throws Exception &#123;        String name = paramMap.getString(&quot;name&quot;);        Integer id = paramMap.getInteger(&quot;id&quot;);        LOGGER.info(&quot;name=[&#123;&#125;],id=[&#123;&#125;]&quot; , name,id);        DemoResVO demoResVO = new DemoResVO() ;        demoResVO.setIndex(index.incrementAndGet());        WorkRes&lt;DemoResVO&gt; res = new WorkRes();        res.setCode(StatusEnum.SUCCESS.getCode());        res.setMessage(StatusEnum.SUCCESS.getMessage());        res.setDataBody(demoResVO) ;        return res;    &#125;&#125;\n\n同时需要再自定义类中加上 @CicadaAction 注解，并需要指定一个 value，该 value 主要是为了在请求路由时能找到业务类。\n这样启动应用并访问 \nhttp://127.0.0.1:7317/cicada-example/demoAction?name=12345&amp;id=10 \n便能执行业务逻辑同时得到服务端的返回。\n\n目前默认支持的是 json 响应，后期也会加上模板解析。\n服务中也会打印相关日志。\n\n灵活的参数配置这里所有的请求参数都封装在 Param 中，可以利用其中的各种 API 获取请求数据。\n之所以是灵活的：我们甚至可以这样请求：\nhttp://127.0.0.1:7317/cicada-example/demoAction?jsonData=&quot;info&quot;: &#123;    &quot;age&quot;: 22,    &quot;name&quot;: &quot;zhangsan&quot;  &#125;\n\n这样就可以传递任意结构的数据，只要业务处理时进行解析即可。\n自定义拦截器拦截器是一个框架的基本功能，可以利用拦截器实现日志记录、事务提交等通用工作。\n为此 cicada 提供一个接口: top.crossoverjie.cicada.server.intercept.CicadaInterceptor。\n我们只需要实现该接口即可编写拦截功能：\n@Interceptor(value = &quot;executeTimeInterceptor&quot;)public class ExecuteTimeInterceptor implements CicadaInterceptor &#123;    private static final Logger LOGGER = LoggerBuilder.getLogger(ExecuteTimeInterceptor.class);    private Long start;    private Long end;    @Override    public void before(Param param) &#123;        start = System.currentTimeMillis();    &#125;    @Override    public void after(Param param) &#123;        end = System.currentTimeMillis();        LOGGER.info(&quot;cast [&#123;&#125;] times&quot;, end - start);    &#125;&#125;\n\n这里演示的是记录所有 action 的执行时间。\n目前默认只实现了 action 的拦截，后期也会加入自定义拦截器。\n拦截适配器虽说在拦截器中提供了 before/after 两个方法，但也不是所有的方法都需要实现。\n因此 cicada 提供了一个适配器：\ntop.crossoverjie.cicada.server.intercept.AbstractCicadaInterceptorAdapter\n我们需要继承他便可按需实现其中的某个方法，如下所示：\n@Interceptor(value = &quot;loggerInterceptor&quot;)public class LoggerInterceptorAbstract extends AbstractCicadaInterceptorAdapter &#123;    private static final Logger LOGGER = LoggerBuilder.getLogger(LoggerInterceptorAbstract.class) ;    @Override    public void before(Param param) &#123;        LOGGER.info(&quot;logger param=[&#123;&#125;]&quot;,param.toString());    &#125;&#125;\n\n性能测试既然是一个 HTTP 服务框架，那性能自然也得保证。\n在测试条件为：300 并发连续压测两轮；1G 内存、单核 CPU、1Mbps。用 Jmeter 压测情况如下：\n\n同样的服务器用 Tomcat 来压测看看结果。\nTomcat 的线程池配置:\n&lt;Executor name=&quot;tomcatThreadPool&quot; namePrefix=&quot;consumer-exec-&quot;        maxThreads=&quot;510&quot; minSpareThreads=&quot;10&quot;/&gt;\n\n\n我这里请求的是 Tomcat 的一个 doc 目录，虽说结果看似 cicada 的性能比 Tomcat  还强。\n但其实这个对比过程中的变量并没有完全控制好，Tomcat 所返回的是 HTML，而 cicada 仅仅返回了 json，当然问题也不止这些。\n但还是能说明 cicada 目前的性能还是不错的。\n总结本文没有过多讨论 cicada 实现原理，感兴趣的可以看看源码，都比较简单。\n在后续的更新中会仔细探讨这块内容。\n同时不出意外 cicada 会持续更新，未来也会加入更多实用的功能。\n甚至我会在适当的时机将它应用于我的生产项目，也希望更多朋友能参与进来一起把这个「轮子」做的更好。\n项目地址：https://github.com/crossoverJie/cicada\n你的点赞与转发是最大的支持。\n","categories":["cicada","轮子"],"tags":["Java","HTTP","Netty"]},{"title":"「造个轮子」——cicada 源码分析","url":"/2018/09/05/wheel/cicada2/","content":"\n前言两天前写了文章《「造个轮子」——cicada(轻量级 WEB 框架)》\n 向大家介绍了 cicada 之后收到很多反馈，也有许多不错的建议。\n同时在 GitHub 也收获了 100 多颗 小♥♥（绝对不是刷的。。）\n\n也有朋友希望能出一个源码介绍，本文就目前的 v1.0.1 版本来一起分析分析。\n\n没有看错，刚发布就修复了一个 bug，想要试用的请升级到 1.0.1 吧。\n\n\n\n技术选型一般在做一个新玩意之前都会有技术选型的过程，但这点在做 cicada 的时候却异常简单。\n因为我的需求是想提供一个高性能的 HTTP 服务，纵观整个开源界其实选择不多。\n加上最近我在做 Netty 相关的开发，所以自然而然就选择了它。\n同时 Netty 自带了对 HTTP 协议的编解码器，可以非常简单快速的开发一个 HTTP 服务器。我只需要把精力放在参数处理、路由等业务处理上即可。\n同时 Netty 也是基于 NIO 实现，性能上也有保证。关于 Netty 相关内容可以参考这里。\n下面来重点分析其中的各个过程。\n路由规则最核心的自然就是 HTTP 的处理 handle，对应的就是 HttpHandle 类。\n\n查看源码其实很容易看出具体的步骤，注释也很明显。\n这里只分析重点功能。\n先来考虑下需求。\n首先作为一个 HTTP 框架，自然是得让使用者能有地方来实现业务代码；就像咱们现在使用 SpringMVC 时写的 controller 一样。\n其实当时考虑过三种方案：\n\n像 SpringMVC 一样定义注解，只要声明了对应注解我就认为这是一个业务类。\n用过 Struts2 的同学应该有印象，它的业务类 Action 都是配置到一个 XML 中；在里面配置接口对应的业务处理类。\n同样的思路，只是把 XML 文件换成 properties 配置文件，在里面编写 JSON 格式的对应关系。\n\n这时就得分析各个方案的优缺点了。\n方案二和三其实就是 XML 和 json 的对比了；XML 会让维护者感到结构清晰，同时便于维护和新增。\nJSON 就不太方便处理了，并且在这样的场景并不用于传输自然也发挥不出优势。\n最后考虑到现在流行的 SpringBoot 都在去 XML，要是再搞一个依赖于 XML 的东西也跟不上大家的使用习惯。\n于是就采用类似于 SpringMVC 这样的注解形式。\n既然采用了注解，那框架怎么知道用户访问某个接口时能对应到业务类呢？\n所以首先第一步自然是需要将加有注解的类全部扫描一遍，放到一个本地缓存中。\n这样才能方便后续的路由定位。\n路由策略其中核心的源码在 routeAction 方法中。\n\n首先会全局扫描使用了 @CicadaAction 的注解，然后再根据请求地址找到对应的业务类。\n全局扫描代码：\n\n首先是获取到项目中自定义的所有类，然后判断是否加有 @CicadaAction 注解。\n是目标类则把他缓存到一个本地 Map 中，方便下次访问时可以不再扫描直接从缓存中获取即可（反射很耗性能）。\n执行完 routeAction 后会获得真正的业务类类型。\nClass&lt;?&gt; actionClazz = routeAction(queryStringDecoder, appConfig);\n传参方式拿到业务类的类类型之后就成功一大半了，只需要反射生成它的对象然后执行方法即可。\n在执行方法之前又要涉及到一个问题，参数我该怎么传递呢？\n考虑到灵活性我采用了最简答 Map 方式。\n因此定义了一个通用的 Param 接口并继承了 Map 接口。\npublic interface Param extends Map&lt;String, Object&gt; &#123;    /**     * get String     * @param param     * @return     */    String getString(String param);    /**     * get Integer     * @param param     * @return     */    Integer getInteger(String param);    /**     * get Long     * @param param     * @return     */    Long getLong(String param);    /**     * get Double     * @param param     * @return     */    Double getDouble(String param);    /**     * get Float     * @param param     * @return     */    Float getFloat(String param);    /**     * get Boolean     * @param param     * @return     */    Boolean getBoolean(String param) ;&#125;\n\n其中封装了几种基本类型的获取方式。\n同时在 buildParamMap() 方法中，将接口中的参数封装到这个 Map 中。\nParam paramMap = buildParamMap(queryStringDecoder);\n\n\n业务执行最后只需要执行业务即可；由于在上文已经获取到业务类的类类型，所以这里通过反射即可调用。\n同时也定义了一个业务类需要实现的一个通用接口 WorkAction，想要实现具体业务只要实现它就行。\n\n而这里的方法参数自然就是刚才定义的参数接口 Param。\n由于所有的业务类都是实现了 WorkAction，所以在反射时都可以定义为 WorkAction 对象。\nWorkAction action = (WorkAction) actionClazz.newInstance();WorkRes execute = action.execute(paramMap);\n\n最后将构建好的参数 map 传入即可。\n响应返回有了请求那自然也得有响应，观察刚才定义的 WorkAction 接口可以发现其实定义了一个 WorkRes 响应类。\n所有的响应数据都需要封装到这个对象中。\n\n这个没啥好说的，都是一些基本数据。\n最后在 responseMsg() 方法中将响应数据编码为 JSON 输出即可。\n\n拦截器设计拦截器也是一个框架基本的功能，用处非常多。\ncicada 的实现原理非常简单，就是在 WorkAction 接口执行业务逻辑之前调用一个方法、执行完毕之后调用另一个方法。\n也是同样的思路需要定义一个接口 CicadaInterceptor，其中有两个方法。\n\n看方法名字自然也能看出具体作用。\n\n同时在这两个方法中执行具体的调用。\n这里重点要看看 interceptorBefore 方法。\n\n其中也是加入了一个缓存，尽量的减少反射操作。\n适配器就这样的拦截器接口是够用了，但并不是所有的业务都需要实现两个接口。\n因此也提供了一个适配器 AbstractCicadaInterceptorAdapter。\n\n它作为一个抽象类实现了 CicadaInterceptor 接口，这样后续的拦截业务也可继承该接口选择性的实现方法即可。\n类似于这样：\n\n总结v1.0.1 版本的 cicada 就介绍完毕了，其中的原理和源码都比较简单。\n大量使用了反射和一些设计模式、多态等应用，这方面经验较少的朋友可以参考下。\n同时也有很多不足；比如传参后续会考虑更加优雅的方式、拦截器目前写的比较死，后续会利用动态代理实现自定义拦截。\n其实 cicada 只是利用周末两天时间做的，bug 肯定少不了；也欢迎大家在 GitHub 上提 issue 参与。\n最后贴下项目地址：\nhttps://github.com/TogetherOS/cicada\n你的点赞与转发是最大的支持。\n","categories":["cicada","轮子"],"tags":["Java","HTTP","Netty"]},{"title":"「造个轮子」——cicada 设计一个配置模块","url":"/2018/09/14/wheel/cicada3/","content":"\n前言在前两次的 cicada 版本中其实还不支持读取配置文件，比如对端口、路由的配置。\n因此我按照自己的想法创建了一个 issue ，也收集到了一些很不错的建议。\n\n\n最终其实还是按照我之前的想法来做了这个配置管理。\n\n同时将 cicada 升级到了 v1.0.2。\n\n \n\n\n目标在做之前是要把需求想好，到底怎样的一个配置管理是对开发人员来说比较友好的？\n我认为有以下几点:\n\n可以自定义配置，并且支持不同的环境（开发、测试、生产）。\n使用灵活。对使用者来说不要有太多的束缚。\n\n理论上来说配置这个东西应当完全独立出来，由一个配置中心来负责管理并且这样可以与应用解耦。\n不过这样的实现和当前 cicada 的定义有些冲突，我想尽量小的依赖第三方组件并可以完全独立运行。\n因此基于这样的情况便有了以下的实现。\n使用在看实现之前先看看基于目前的配置管理如何在业务中使用起来。\n结合现在大家使用 SpringBoot 的习惯，cicada 默认会读取 classpath 下的 application.properties 配置文件。并且会默认读取其中的应用端口以及初始路由地址。\n同时也新增了一个 api。\npublic class MainStart &#123;    public static void main(String[] args) throws Exception &#123;        CicadaServer.start(MainStart.class,&quot;/cicada-example&quot;) ;    &#125;&#125;public class MainStart &#123;    public static void main(String[] args) throws Exception &#123;        CicadaServer.start(MainStart.class) ;    &#125;&#125;\n\n这样在不传默认地址的时候 cicada 会从 application.properties 中读取。\n考虑到后面可维护的情况，cicada 也支持配置各种不同的配置文件。\n使用也比较简单，只需要继承 cicada 提供的一个抽象类即可。\npublic class KafkaConfiguration extends AbstractCicadaConfiguration &#123;    public KafkaConfiguration() &#123;        super.setPropertiesName(&quot;kafka.properties&quot;);    &#125;&#125;public class RedisConfiguration extends AbstractCicadaConfiguration &#123;    public RedisConfiguration() &#123;        super.setPropertiesName(&quot;redis.properties&quot;);    &#125;&#125;\n\n\n按照这样的配置也会默认从 classpath 读取这两个配置文件。\n\n当然这里有个前提：代码里配置的文件名必须得和配置文件名称相同。\n\n那如何在业务中读取这两个配置文件的内容呢？\n这也简单，代码一看就懂：\n\n\n首先需要通过 ConfigurationHolder 获取各自不同配置的管理对象（需要显式指定类类型）。\n通过 get() 方法直接获取配置。\n同时也支持获取 application.properties 里的配置。\n\n同时为了支持在不同环境的使用，当配置了启动参数将会优先读取。\n-Dapplication.properties=/xx/application.properties-Dkafka.properties=/xx/kakfa.properties-Dredis.properties=/xx/redis.properties\n\n这样算是基本实现了上述的配置要求。\n实现要实现以上的功能有几个核心点：\n\n加载所有配置文件。\n将不同的配置文件用不同的对象进行管理。\n提供简易的接口使用。\n\n由于 cicada 需要支持多个配置文件，所有需要定义一个抽象类供所有的配置管理实现。\n\n定义比较简单，其中有两个重要的成员变量：\n\n文件名称：用于初始化时通过名称加载配置文件。\nProperties 其实就是一个 Map 结构的缓存，用于存放所有的配置。当然对外提供的查询是基于它的。\n\n接着就是在初始化时需要找出所有继承了 AbstractCicadaConfiguration 的类。\n\n查询出来之后自然是要进行遍历同时反射创建对象。\n由于之前已经调用了 \nsuper.setPropertiesName(&quot;redis.properties&quot;); \n来赋值配置文件名称，所以还需要在遍历过程中将 Properties 进行赋值。\n同时在这里也体现出优先读取的是 VM 启动参数中的配置文件。\nString systemProperty = System.getProperty(conf.getPropertiesName());\n\n\n需要额外提一点的是：在查找所有用户自定义的配置管理类时需要手动将 cicada 内置的ApplicationConfiguration 加入其中。\n因为使用应用的包名通过反射是查询不出该类的。\n保存自定义配置管理为了方便用户在使用时候可以随意的读取各个配置文件，所以还需要将反射创建的对象保存到一个内部缓存中，核心代码就是上上图中的这段代码：\n// add configuration cacheConfigurationHolder.addConfiguration(aClass.getName(), conf);\n\n其中 ConfigurationHolder 的定义如下。\n\n其实也是利用一个 Map 来存放这些对象。\n这样在使用时候只需要取出即可。\nKafkaConfiguration configuration = (KafkaConfiguration) getConfiguration(KafkaConfiguration.class);String brokerList = configuration.get(&quot;kafka.broker.list&quot;);\n\n重构本次升级同时还重构了部分代码，比如启动类。\n现在看上去要清爽和直接的多：\n\n其中也有一点需要注意的地方。\n大家如果查看日志的话会发现应用启动之后会打印本次的耗时，自然就是在启动时候记录一个时间，初始化完毕之后记录一个即可。\n\n在之前的实现中由于都是在一个方法内，所以直接使用就行了。\n但现在优化之后跨越了不同的方法和类，难道要把时间作为参数在各个方法之前传递嘛？\n\n那未免太不优雅了。\n\n所以 ThreadLocal 就有了发挥余地。\n在初始化的方法中我将当前时间写入：\nThreadLocalHolder.setLocalTime(System.currentTimeMillis());\n\n在最后记录日志的地方直接取出比较即可：\n\n这样使用起来就完全不需要管什么参数传递了。\n同时 ThreadLocalHolder 的定义：\n\n这里还是有一点需要注意，在这种长生命周期的容器中一定得要记得及时清除。\n我这里的时间在查询一次之后就不用了，所以完全放心的在 getLocalTime() 方法中删掉。\n总结这就是本次 v1.0.2 中的升级内容，包含了配置支持以及代码重构。其中有些内容我觉得对接触少的同学来说还是挺有帮助的。\n关于上两次的版本介绍请查看这里：\n\n「造个轮子」——cicada(轻量级 WEB 框架)\n「造个轮子」——cicada 源码分析\n\n还没点关注的朋友可以点波关注：\nhttps://github.com/TogetherOS/cicada\n也欢迎大家参与一起维护！。\n同时后续关于 cicada 的更新会放慢一些。会介绍一些平时实战相关的内容，比如 Kafka 之类的，请持续关注。\n你的点赞与转发是最大的支持。\n","categories":["cicada","轮子"],"tags":["Java","HTTP","Netty"]},{"title":"「造个轮子」——cicada 设计全局上下文","url":"/2018/10/09/wheel/cicada4/","content":"\n前言本次 Cicada 已经更新到了  v1.0.3。\n主要是解决了两个 issue，#9 #8。\n所以本次的主要更新为：\n\nCicada 采用合理的线程分配来处理接入请求线程以及 IO 线程。\n支持多种响应方式（以前只有 json，现在支持 text）。\n为了满足上者引入了 context。\n优雅停机。\n\n其中我觉得最核心也最有用的就是这个 Context，并为此重构了大部分代码。\n\n\n多种响应方式在起初 Cicada 默认只能响应 json，这一点确实不够灵活。加上后续也打算支持模板解析，所以不如直接在 API 中加入可让用户自行选择不同的响应方式。\n因此调整后的 API 如下。\n\n想要输出 text/plain 时。\n\n@CicadaAction(&quot;textAction&quot;)public class TextAction implements WorkAction &#123;    @Override    public void execute(CicadaContext context, Param param) throws Exception &#123;        String url = context.request().getUrl();        String method = context.request().getMethod();        context.text(&quot;hello world url=&quot; + url + &quot; method=&quot; + method);    &#125;&#125;\n\n\n而响应输出 application/json 时只需要把需要响应的对象写入到 json() 方法中.\n\n\n因此原有的业务 action 中也加入了一个上下文的参数：\n/** * abstract execute method * @param context current context * @param param request params * @throws Exception throw exception */void execute(CicadaContext context ,Param param) throws Exception;\n\n下面就来看看这个 Context 是如何完成的。\nCicada Context先看看有了这个上下文之后可以做什么。\n比如有些场景下我们需要拿到本次请求中的头信息，这时就可以通过这个 Context 对象直接获取。\n当然不止是头信息：\n\n获取请求头。\n设置响应头。\n设置 cookie。\n获取请求 URL。\n获取请求的 method（get&#x2F;post）等。\n\n其实通过这些特点可以看出这些信息其实都和一次 请求、响应 密切相关，并且各个请求之间的信息应互不影响。\n这样的特性是不是非常熟悉，没错那就是 ThreadLocal，它可以将每个线程的信息存储起来互不影响。\n\nThreadLocal 的原理本次不做过多分析，只谈它在 Cicada 中的应用。\n\nCicadaContext.class先来看看 CicadaContext 这个类的主要成员变量以及方法。\n\n成员变量是两个接口 CicadaRequest、CicadaResponse，名称就能看出肯定是存放请求和响应数据的。\nHttpDispatcher.class想要存放本次请求的上下文自然是在真正请求分发的地方 HttpDispatcher。\n\n这里改的较大的就是两个红框处，第一部分是做上下文初始化及赋值。\n第二部分自然就是卸载上下文。\n\n先看初始化。\n\nCicadaRequest cicadaRequest = CicadaHttpRequest.init(defaultHttpRequest) ;\n首先是将 request 初始化：\nCicadaHttpRequest 自然是实现了 CicadaRequest 接口：\n\n这里只保存了请求的 URL、method 等信息，后续要加的请求头也存放在此处即可。\nResponse 也是同理的。\n\n\n这两个具体的实现类都私有化了构造函数，防止外部破坏了整体性。\n\n接着将当前请求的上下文保存到了 CicadaContext 中。\nCicadaContext.setContext(new CicadaContext(cicadaRequest,cicadaResponse));\n\n而这个函数本质使用的则是 ThreadLocal 来存放 CicadaContext。\npublic static void setContext(CicadaContext context)&#123;    ThreadLocalHolder.setCicadaContext(context) ;&#125;private static final ThreadLocal&lt;CicadaContext&gt; CICADA_CONTEXT= new ThreadLocal() ;/** * set cicada context * @param context current context */public static void setCicadaContext(CicadaContext context)&#123;    CICADA_CONTEXT.set(context) ;&#125;\n\n处理业务及响应接着就是处理业务，调用不同的 API 做不同响应。\n拿 context.text() 来说：\n\n其实就是设置了对应的响应方式、以及把响应内容写入了 CicadaResponse 的 httpContent 中。\n业务处理完后调用 responseContent() 进行响应：\nresponseContent(ctx,CicadaContext.getResponse().getHttpContent());\n\n其实就是在上下文中拿到的响应方式及响应内容返回给客户端。\n卸载上下文最后有点非常重要，那就是 卸载上下文。\n如果这里不做处理，之后随着请求的增多，ThreadLocal 里存放的数据也越来越多，最终肯定会导致内存溢出。\n所以 CicadaContext.removeContext() 就是为了及时删除当前上下文。\n优雅停机最后还新增了一个停机的方法。\n\n其实也就是利用 Hook 函数实现的。\n由于目前 Cicada 开的线程，占用的资源都不是特别多，所以只是关闭了 Netty 所使用的线程。\n如果后续新增了自身的线程等资源，那也可以全部放到这里来进行释放。\n总结Cicada 已经更新了 4 个版本，雏形都有了。\n后续会重点实现模板解析和注解请求路由完成，把 MVC 中的 view 完成就差不多了。\n还没有了解的朋友可以点击下面链接进入主页了解下😋。\nhttps://github.com/TogetherOS/cicada\n","categories":["cicada","轮子"],"tags":["Java","HTTP","Netty","ThreadLocal"]},{"title":"设计一个可拔插的 IOC 容器","url":"/2018/11/15/wheel/cicada6/","content":"\n前言磨了许久，借助最近的一次通宵上线 cicada 终于更新了 v2.0.0 版本。\n之所以大的版本号变为 2，确实是向下不兼容了；主要表现为：\n\n修复了几个反馈的 bug。\n灵活的路由方式。\n可拔插的 IOC 容器选择。\n\n其中重点是后面两个。\n\n\n新的路由方式先来看第一个：路由方式的更新。\n在之前的版本想要写一个接口必须的实现一个 WorkAction；而且最麻烦的是一个实现类只能做一个接口。\n因此也有朋友给我提过这个 issue。\n\n\n于是改进后的使用方式如下：\n\n\n是否有点似曾相识的感觉😊。\n\n如上图所示，不需要实现某个特定的接口；只需要使用不同的注解即可。\n同时也支持自定义 pojo, cicada 会在调用过程中对参数进行实例化。\n拿这个 getUser 接口为例，当这样请求时这些参数就会被封装进 DemoReq 中.\nhttp://127.0.0.1:5688/cicada-example/routeAction/getUser?id=1234&name=zhangsan\n同时得到响应：\n&#123;&quot;message&quot;:&quot;hello =zhangsan&quot;&#125;\n\n实现过程也挺简单，大家查看源码便会发现；这里贴一点比较核心的步骤。\n\n扫描所有使用 @CicadaAction 注解的类。\n扫描所有使用 @CicadaRoute 注解的方法。\n将他们的映射关系存入 Map 中。\n请求时根据 URL 去 Map 中查找这个关系。\n反射构建参数及方法调用。\n\n扫描类以及写入映射关系\n\n\n请求时查询映射关系\n\n\n反射调用这些方法\n\n是否需要 IOC 容器上面那几个步骤其实我都是一把梭写完的，但当我写到执行具体方法时感觉有点意思了。\n大家都知道反射调用方法有两个重要的参数：\n\n\nobj 方法执行的实例。\nargs.. 自然是方法的参数。\n\n我第一次写的时候是这样的：\nmethod.invoke(method.getDeclaringClass().newInstance(), object);\n\n然后一测试，也没问题。\n当我写完之后 review 代码时发现不对：这样这里每次都会创建一个新的实例，而且反射调用 newInstance() 效率也不高。\n这时我不自觉的想到了 Spring 中 IOC 容器，和这里场景也非常的类似。\n\n在应用初始化时将所有的接口实例化并保存到 bean 容器中，当需要使用时只需要从容器中获取即可。\n\n这样只是会在启动时做很多加载工作，但造福后代啊。\n可拔插的 IOC 容器于是我打算自己实现一个这样的 bean 容器。\n但在实现之前又想到一个 feature:\n\n不如把实现 bean 容器的方案交给使用者选择，可以选择使用 bean 容器，也可以就用之前的每次都创建新的实例，就像 Spring 中的 prototype 作用域一样。\n\n甚至可以自定义容器实现，比如将 bean 存放到数据库、Redis 都行；当然一般人也不会这么干。\n和 SPI 的机制也有点类似。\n要实现上述的需求大致需要以下步骤：\n\n一个通用的接口，包含了注册容器、从容器中获取实例等方法。\nBeanManager 类，由它来管理具体使用哪种 IOC 容器。\n\n所以首先定义了一个接口；CicadaBeanFactory:\n\n包含了注册和获取实例的接口。\n同时分别有两个不同的容器实现方案。\n默认实现；CicadaDefaultBean：\n也就是文中说道的，每次都会创建实例；由于这种方式其实根本就没有 bean 容器，所以也不存在注册了。\n接下来是真正的 IOC 容器；CicadaIoc：\n\n\n它将所有的实例都存放在一个 Map 中。\n\n当然也少不了刚才提到的 CicadaBeanManager，它会在应用启动的时候将所有的实例注册到 bean 容器中。\n\n重点是图中标红的部分：\n\n需要根据用户的选择实例化 CicadaBeanFactory 接口。\n将所有的实例注册到 CicadaBeanFactory 接口中。\n\n同时也提供了一个获取实例的方法：\n\n就是直接调用 CicadaBeanFactory 接口的方法。\n\n然后在上文提到的反射调用方法处就变为：\n\n从 bean 容器中获取实例了；获取的过程可以是每次都创建一个新的对象，也可以是直接从容器中获取实例。这点对于这里的调用者来说并不关心。\n所以这也实现了标题所说的：可拔插。\n为了实现这个目的，我将 CicadaIoc 的实现单独放到一个模块中，以 jar 包的形式提供实现。\n\n所以如果你想要使用 IOC 容器的方式获取实例时只需要在你的应用中额外加入这个 jar 包即可。\n&lt;dependency&gt;    &lt;groupId&gt;top.crossoverjie.opensource&lt;/groupId&gt;    &lt;artifactId&gt;cicada-ioc&lt;/artifactId&gt;    &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;\n\n如果不使用则是默认的 CicadaDefaultBean 实现，也就是每次都会创建对象。\n这样有个好处：\n当你自己想实现一个 IOC 容器时；只需要实现 cicada 提供的 CicadaBeanFactory 接口，并在你的应用中只加入你的 jar 包即可。\n其余所有的代码都不需要改变，便可随意切换不的容器。\n\n当然我是推荐大家使用 IOC 容器的（其实就是单例），牺牲一点应用启动时间带来后续性能的提升是值得的。\n\n总结cicada 的大坑填的差不多了，后续也会做一些小功能的迭代。\n还没有关注的朋友赶紧关注一波：\nhttps://github.com/TogetherOS/cicada\n\nPS：虽然没有仔细分析 Spring IOC 的实现，但相信看完此篇的朋友应该对 Spring IOC 以及 SpringMVC 会有一些自己的理解。\n\n你的点赞与分享是对我最大的支持\n","categories":["cicada","轮子"],"tags":["Java","HTTP","Netty"]},{"title":"利用责任链模式设计一个拦截器","url":"/2018/10/22/wheel/cicada5/","content":"\n前言近期在做 Cicada 的拦截器功能，正好用到了责任链模式。\n这个设计模式在日常使用中频率还是挺高的，借此机会来分析分析。\n责任链模式先来看看什么是责任链模式。\n引用一段维基百科对其的解释：\n\n责任链模式在面向对象程式设计里是一种软件设计模式，它包含了一些命令对象和一系列的处理对象。每一个处理对象决定它能处理哪些命令对象，它也知道如何将它不能处理的命令对象传递给该链中的下一个处理对象。该模式还描述了往该处理链的末尾添加新的处理对象的方法。\n\n光看这段描述可能大家会觉得懵，简单来说就是该设计模式用于对某个对象或者请求进行一系列的处理，这些处理逻辑正好组成一个链条。\n下面来简单演示使用与不使用责任链模式有什么区别和优势。\n\n\n责任链模式的应用传统实现假设这样的场景：传入了一段内容，需要对这段文本进行加工；比如过滤敏感词、错别字修改、最后署上版权等操作。\n常见的写法如下：\npublic class Main &#123;    public static void main(String[] args) &#123;        String msg = &quot;内容内容内容&quot; ;        String result = Process.sensitiveWord()                .typo()                .copyright();    &#125;&#125;\n\n这样看似没啥问题也能解决需求，但如果我还需要为为内容加上一个统一的标题呢？在现有的方式下就不得不新增处理方法，并且是在这个客户端（Process）的基础上进行新增。\n显然这样的扩展性不好。\n责任链模式实现这时候就到了责任链模式发挥作用了。\n该需求非常的符合对某一个对象、请求进行一系列处理的特征。\n于是我们将代码修改：\n这时 Process 就是一个接口了，用于定义真正的处理函数。\npublic interface Process &#123;    /**     * 执行处理     * @param msg     */    void doProcess(String msg) ;&#125;\n\n同时之前对内容的各种处理只需要实现该接口即可：\npublic class SensitiveWordProcess implements Process &#123;    @Override    public void doProcess(String msg) &#123;        System.out.println(msg + &quot;敏感词处理&quot;);    &#125;&#125;public class CopyrightProcess implements Process &#123;    @Override    public void doProcess(String msg) &#123;        System.out.println(msg + &quot;版权处理&quot;);    &#125;&#125;public class CopyrightProcess implements Process &#123;    @Override    public void doProcess(String msg) &#123;        System.out.println(msg + &quot;版权处理&quot;);    &#125;&#125;\n\n然后只需要给客户端提供一个执行入口以及添加责任链的入口即可：\npublic class MsgProcessChain &#123;    private List&lt;Process&gt; chains = new ArrayList&lt;&gt;() ;    /**     * 添加责任链     * @param process     * @return     */    public MsgProcessChain addChain(Process process)&#123;        chains.add(process) ;        return this ;    &#125;    /**     * 执行处理     * @param msg     */    public void process(String msg)&#123;        for (Process chain : chains) &#123;            chain.doProcess(msg);        &#125;    &#125;&#125;\n\n这样使用起来就非常简单：\npublic class Main &#123;    public static void main(String[] args) &#123;        String msg = &quot;内容内容内容==&quot; ;        MsgProcessChain chain = new MsgProcessChain()                .addChain(new SensitiveWordProcess())                .addChain(new TypoProcess())                .addChain(new CopyrightProcess()) ;        chain.process(msg) ;    &#125;&#125;\n\n当我需要再增加一个处理逻辑时只需要添加一个处理单元即可（addChain(Process process)），并对客户端 chain.process(msg) 是无感知的，不需要做任何的改动。\n可能大家没有直接写过责任链模式的相关代码，但不经意间使用到的却不少。\n比如 Netty 中的 pipeline 就是一个典型的责任链模式，它可以让一个请求在整个管道中进行流转。\n\n通过官方图就可以非常清楚的看出是一个责任链模式：\n\n用责任链模式设计一个拦截器对于拦截器来说使用责任链模式再好不过了。\n下面来看看在 Cicada 中的实现：\n首先是定义了和上文 Process 接口类似的 CicadaInterceptor 抽象类：\npublic abstract class CicadaInterceptor &#123;    public boolean before(CicadaContext context,Param param) throws Exception&#123;        return true;    &#125;    public void after(CicadaContext context,Param param) throws Exception&#123;&#125;&#125;\n\n同时定义了一个 InterceptProcess 的客户端：\n\n其中的 loadInterceptors() 会将所有的拦截器加入到责任链中。\n再提供了两个函数分别对应了拦截前和拦截后的入口：\n\n实际应用现在来看看具体是怎么使用的吧。\n\n在请求的 handle 中首先进行加载（loadInterceptors(AppConfig appConfig)），也就是初始化责任链。\n接下来则是客户端的入口；调用拦截前后的入口方法即可。\n\n由于是拦截器，那么在 before 函数中是可以对请求进行拦截的。只要返回 false 就不会继续向后处理。所以这里做了一个返回值的判断。\n\n同时对于使用者来说只需要创建拦截器类继承 CicadaInterceptor 类即可。\n这里做了一个演示，分别有两个拦截器：\n\n记录一个业务 handle 的执行时间。\n在 after 里打印了请求参数。\n同时可在第一个拦截器中返回 false 让请求被拦截。\n\n先来做前两个试验：\n\n\n\n这样当我请求其中一个接口时会将刚才的日志打印出来：\n\n\n接下来我让打印执行时间的拦截器中拦截请求，同时输入向前端输入一段文本：\n\n\n请求接口可以看到如下内容：\n\n\n同时后面的请求参数也没有打印出来，说明请求确实被拦截下来。\n\n同时我也可以调整拦截顺序，只需要在 @Interceptor(order = 1) 注解中定义这个 order 属性即可（默认值是 0，越小越先执行）。\n之前是打印请求参数的拦截器先执行，这次我手动将它的 order 调整为 2，而打印时间的 order 为 1 。\n\n再次请求接口观察后台日志：\n\n发现打印执行时间的拦截器先执行。\n那这个执行执行顺序如何实现自定义配置的呢？\n其实也比较简单，有以下几步：\n\n在加载拦截器时将注解里的 order 保存起来。\n设置拦截器到责任链中时通过反射将 order 的值保存到各个拦截器中。\n最终通过排序重新排列这个责任链的顺序。\n\n贴一些核心代码。\n扫描拦截器时保存 order 值：\n\n\n保存 order 值到拦截器中：\n\n\n重新对责任链排序：\n\n总结整个责任链模式已经讲完，希望对这个设计模式还不了解的朋友带来些帮助。\n上文中的源码如下：\n\nhttps://github.com/TogetherOS/cicada:一个高性能、轻量 HTTP 框架\nhttps://git.io/fxKid\n\n欢迎关注公众号一起交流：\n","categories":["cicada","轮子"],"tags":["Java","HTTP","Netty","责任链"]},{"title":"设计一个全局异常处理器","url":"/2019/07/15/wheel/cicada7-exception-handle/","content":"\n前言最近稍微闲了一点于是把这个半年都没更新的开源项目 cicada 重新捡了起来。\n一些新关注的朋友应该还不知道这项目是干啥的？先来看看官方介绍吧（其实就我自己写的😀）\n\ncicada: 基于 Netty4 实现的快速、轻量级 WEB 框架；没有过多的依赖，核心 jar 包仅 30KB。\n\n\n\n\n针对这个轮子以前也写过相关的介绍，感兴趣的可以再翻回去看看：\n\n「造个轮子」——cicada(轻量级 WEB 框架)\n「造个轮子」——cicada 源码分析\n「造个轮子」——cicada 设计一个配置模块\n「造个轮子」——cicada 设计全局上下文\n利用责任链模式设计一个拦截器\n设计一个可拔插的 IOC 容器\n\n这些都看完了相信对这个小玩意应该会有更多的想法。\n效果广告打完了，回到正题；大家平时最常用的 MVC 框架当属 SpringMVC 了，而在搭建脚手架的时候相信全局异常处理是必不可少的。\nSpring 用法通常我们的做法如下：\n传统 Spring 版本：\n\n实现一个 Spring 自带的接口，重写其中的方法，最后的异常处理便在此处。\n将这个类配置在 Spring 的 xml ，当做一个 bean 注册到 Spring 容器中。\n\npublic class CustomExceptionResolver implements HandlerExceptionResolver &#123;    @Override    public ModelAndView resolveException(HttpServletRequest request,            HttpServletResponse response, Object handler, Exception ex) &#123;\t//自定义处理&#125;\n\n&lt;bean class=&quot;ssm.exception.CustomExceptionResolver&quot;&gt;&lt;/bean&gt; \n\n\n当然现在流行的 SpringBoot 也有对应的简化版本：\n@ControllerAdvicepublic class GlobalExceptionHandler &#123;    @ExceptionHandler(value = Exception.class)    public Object defaultErrorHandler(HttpServletRequest req, Exception e) throws Exception &#123;        //自定义处理    &#125;&#125;\n\n全部都换为注解形式，但本质上还是一样的。\n\n都是要在容器中创建一个特殊的 bean，这个 bean 专门用于处理异常，当系统运行时出现异常，就从容器中找到该 bean，并执行其中的方法即可。\n\n至于这个特殊的 bean 如何标识出来，无非就是实现某个特定接口或者用注解声明，也就对应了传统 Spring 和 SpringBoot 的用法。\ncicada 用法cicada 在设计自己的全局异常处理器时也参考了 Spring 的相关设计，所以最终用法如下：\n@CicadaBeanpublic class ExceptionHandle implements GlobalHandelException &#123;    private final static Logger LOGGER = LoggerBuilder.getLogger(ExceptionHandle.class);    @Override    public void resolveException(CicadaContext context, Exception e) &#123;        LOGGER.error(&quot;Exception&quot;, e);        WorkRes workRes = new WorkRes();        workRes.setCode(&quot;500&quot;);        workRes.setMessage(e.getClass().getName() + &quot;系统运行出现异常&quot;);        context.json(workRes);    &#125;&#125;\n\n自定义一个实现了 GlobalHandelException 接口的类，当请求出现异常时，页面和后台将会如下输出：\n\n设计看得出用法和 Spring 非常类似，也是需要实现一个接口 GlobalHandelException，同时使用 @CicadaBean 注解该类将他加载到 cicada 内置的 IOC 容器内。\n当出现异常时则在这个 IOC 容器中找到该对象调用它的 resolveException 即可。\n其中还可以通过 CicadaContext 全局上下文响应不同的输出（json/text/html）。\n核心原理\n简单画了下流程图，步骤如下：\n\n初始化时会找到实现了 GlobalHandelException 接口的类，将它实例化并注册到 IOC 容器中。\n当发生异常时从容器中获取到异常处理器的对象，执行其中的处理函数即可。\n\n说了半天原理来看看源码是如何实现的。\n\n在初始化 bean 时，如果是一个异常处理器则会将他单独存放（也就相当于前文说的打标识）。\n其中的 GlobalHandelException 本身的定义也非常简单：\n\n\n接下来是运行时：\n\n而当出现异常时则会通过之前的保存的异常处理 bean 进行异常处理，在调用的同时将全局上下文及异常信息传递过去就齐活了。\n这样就可以在这个实现类中实现我们自己的异常处理逻辑了。\n总结万一今后面试官问你们 SpringMVC 的异常处理是如何实现的？你该知道怎么回答了吧😏。\n同时也可以发散一下，是否可以配置一个针对于某一个 controller 的异常处理，这样每个 controller 产生的异常可以单独处理，如果没有配置则进入全局异常；原理也差不多，感兴趣的朋友可以提个 PR 完成该 feature。\n项目源码：\nhttps://github.com/TogetherOS/cicada\n你的点赞与分享是对我最大的支持\n","categories":["cicada","轮子"],"tags":["Java","HTTP","Netty"]},{"title":"Java SPI 的原理与应用","url":"/2020/02/24/wheel/cicada8-spi/","content":"\n前言不知大家现在有没有去公司复工，我已经在家办公将近 3 周了，同时也在家呆了一个多月；还好工作并没有受到任何影响，我个人一直觉得远程工作和 IT 行业是非常契合的，这段时间的工作效率甚至比在办公室还高，同时由于我们公司的业务在海外，所以疫情几乎没有造成太多影响。\n扯远了，这次主要是想和大家分享一下 Java 的 SPI 机制。周末没啥事，我翻了翻我之前的写的博客 《设计一个可拔插的 IOC 容器》，发现当时的实现并不那么优雅。\n还没看过的朋友的我先做个前景提要，当时的需求：\n\n我实现了一个类似于的 SpringMVC 但却很轻量的 http 框架 cicada，其中当然也需要一个 IOC 容器，可以存放所有的单例 bean。\n\n\n这个 IOC 容器的实现我希望可以有多种方式，甚至可以提供一个接口供其他人实现；当然切换这个 IOC 容器的过程肯定是不能存在硬编码的，也就是这里所提到的可拔插。当我想使用 A 的实现方式时，我就引入 A 的 jar 包，使用 B 时就引入 B 的包。\n\n\n\n\n先给大家看看两次实现的区别，先从代码简洁程度来说就是 SPI 更胜一筹。\n什么是 SPI在具体分析之前还是先了解下 SPI 是什么？\n首先它其实是 Service provider interface 的简写，翻译成中文就是服务提供发现接口。\n不过这里不要被这个名词搞混了，这里的服务发现和我们常听到的微服务中的服务发现并不能划等号。\n就如同上文提到的对 IOC 容器的多种实现方式 A、B、C（可以把它们理解为服务），我需要在运行时知道应该使用哪一种具体的实现。\n其实本质上来说这就是一种典型的面向接口编程，这一点在我们刚开始学习编程的时候就被反复强调了。\nSPI 实践接下来我们来如何来利用 SPI 实现刚才提到的可拔插 IOC 容器。\n既然刚才都提到了 SPI 的本质就是面向接口编程，所以自然我们首先需要定义一个接口：\n\n其中包含了一些 Bean 容器所必须的操作：注册、获取、释放 bean。\n为了让其他人也能实现自己的 IOC 容器，所以我们将这个接口单独放到一个 Module 中，可供他人引入实现。\n\n所以当我要实现一个单例的 IOC 容器时，我只需要新建一个 Module 然后引入刚才的模块并实现 CicadaBeanFactory 接口即可。\n当然其中最重要的则是需要在 resources 目录下新建一个 META-INF/services/top.crossoverjie.cicada.base.bean.CicadaBeanFactory 文件，文件名必须得是我们之前定义接口的全限定名（SPI 规范）。\n\n其中的内容便是我们自己实现类的全限定名：\ntop.crossoverjie.cicada.bean.ioc.CicadaIoc\n\n可以想象最终会通过这里的全限定名来反射创建对象。\n只不过这个过程 Java 已经提供 API 屏蔽掉了：\npublic static CicadaBeanFactory getCicadaBeanFactory() &#123;    ServiceLoader&lt;CicadaBeanFactory&gt; cicadaBeanFactories = ServiceLoader.load(CicadaBeanFactory.class);    if (cicadaBeanFactories.iterator().hasNext())&#123;        return cicadaBeanFactories.iterator().next() ;    &#125;    return new CicadaDefaultBean();&#125;\n\n当 classpath 中存在我们刚才的实现类（引入实现类的 jar 包），便可以通过 java.util.ServiceLoader 工具类来找到所有的实现类（可以有多个实现类同时存在，只不过通常我们只需要一个）。\n\n一些都准备好之后，使用自然就非常简单了。\n&lt;dependency&gt;    &lt;groupId&gt;top.crossoverjie.opensource&lt;/groupId&gt;    &lt;artifactId&gt;cicada-ioc&lt;/artifactId&gt;    &lt;version&gt;2.0.4&lt;/version&gt;&lt;/dependency&gt;\n\n我们只需要引入这个依赖便能使用它的实现，当我们想换一种实现方式时只需要更换一个依赖即可。\n这样就做到了不修改一行代码灵活的可拔插选择 IOC 容器了。\nSPI 的一些其他应用虽然平时并不会直接使用到 SPI 来实现业务，但其实我们使用过的绝大多数框架都会提供 SPI 接口方便使用者扩展自己的功能。\n比如 Dubbo 中提供一系列的扩展：\n同类型的 RPC 框架 motan 中也提供了响应的扩展：\n他们的使用方式都和 Java SPI 非常类似，只不过原理略有不同，同时也新增了一些功能。\n比如 motan 的 spi 允许是否为单例等等。\n再比如 MySQL 的驱动包也是利用 SPI 来实现自己的连接逻辑。\n\n总结 Java 自身的 SPI 其实也有点小毛病，比如：\n\n遍历加载所有实现类效率较低。\n当多个 ServiceLoader 同时 load 时会有并发问题（虽然没人这么干）。\n\n最后总结一下，SPI 并不是某项高深的技术，本质就是面向接口编程，而面向接口本身在我们日常开发中也是必备技能，所以了解使用 SPI 也是很用处的。\n本文所有源码：\nhttps://github.com/TogetherOS/cicada\n你的点赞与分享是对我最大的支持\n","categories":["cicada","spi","轮子"],"tags":["Java","HTTP","Netty"]},{"title":"动态代理的实际应用","url":"/2020/03/30/wheel/cicada9-proxy/","content":"\n前言最近在用 Python 的 SQLAlchemy 库时（一个类似于 Hibernate 的 ORM 框架），发现它的 Events 事件还挺好用。\n简单说就是当某张表的数据发生变化（曾、删、改）时会有一个事件回调，这样一些埋点之类的需求都可以实现在这里，同时和业务代码完全解耦，维护起来也很方便。\n例如当订单状态发生变化需要发异步通知这样的需求也可以利用这个实现。\n根据我之前使用 Mybatis 的经验，好像没怎么注意有这个功能，查阅了下发现 Hibernate 是支持的，只是我用得也少，所以也没怎么在意。\n\n\n\n\n 逐渐偏离主题。。。\n\n说这些的主要原因是我打算为之前写的 cicada (轻量的 http 框架)加一个数据库操作包，也实现类似的功能。\n示例最终的使用效果如下：\n\n第一版本还比较粗糙，但功能都具备。\n\n\n第一步：需要实现一个初始化接口，该接口会在应用初始化的时候执行。\n\n紧接着我们需要定义一个 Model：\n@Data@OriginName(&quot;user&quot;)@ToStringpublic class User extends Model &#123;    @PrimaryId    private Integer id ;    private String name ;    private String password ;    @FieldName(value = &quot;city_id&quot;)    private Integer cityId ;    private String description ;&#125;\n\n它所对应的表结构如下：\nCREATE TABLE `user` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `name` varchar(50) DEFAULT NULL,  `password` varchar(100) DEFAULT NULL,  `description` varchar(100) DEFAULT NULL,  `roleId` int(11) DEFAULT NULL COMMENT &#x27;角色ID&#x27;,  `city_id` int(11) DEFAULT NULL,  PRIMARY KEY (`id`))\n\n\n当需要查询数据时：\n便可以这样访问数据库。\n\n当需要更新数据时：\n在初始化 DBHandle 时指定一个回调接口(也就是这里的 UserUpdateListener)，便可以在修改数据的时候拿到本次修改的数据实体。\n@Slf4jpublic class UserUpdateListener implements DataChangeListener &#123;    @Override    public void listener(Object obj) &#123;        log.info(&quot;user update data=&#123;&#125;&quot;, obj.toString());    &#125;&#125;\n\n同时我们可以在控制台看到数据修改时的回调结果：\n\n这样就实现了文初所提到的功能，便可以实现一些数据变化后需要执行的业务逻辑。\n实现下面重点来看看这个功能的实现过程；其实通过生成 DBHandle（数据库增删改的接口）实例的 API 便可以看出些端倪。\nDBHandle handle = (DBHandle) new HandleProxy(DBHandle.class).getInstance(new UserSaveListener());\n\nDBHandel 虽然是个接口，但是它并不是使用一个实现类来实现的，而是通过代理生成。\n那通过代理生成比直接实例化实现类有啥好处呢？\n举个例子，比如现在你想买一个新手机。\n\n第一种方式可以直接在官方旗舰店买一个标配的手机，没有额外的东西只有一个手机。\n当然你也可以在某些第三方经销商那里购买带套餐的，比如套餐一在标配的基础上多了保护壳、贴膜之类的附加属性。\n这个经销商就类似于我们这里的代理类，他可以在原有实现的基础上新增一些东西，至于新增什么全看你自己的需要了。\n而之所以叫动态代理，也是因为这个代理类是在程序运行过程中动态创建的，在编译过程中并不能确定这个类的全限定名。\n\n下面来看看这个代理类是如何生成的：\n主要利用 JDK 自带的 API 实现的，具体参数可以直接参考官方文档：https://docs.oracle.com/javase/8/docs/technotes/guides/reflection/proxy.html\n总之这样便可以创建一个 DBHandler 接口的代理对象，而真正的代理过程是在 InvocationHandler#invoke() 函数中实现的：\n\n这里的实现也是非常简单，在实现完代理对象的业务逻辑后便回调我们传入的事件接口，其中的参数便是当前的数据库 Model 实体对象。\n\n 不过需要注意的是，这个事件回调和业务线程是同一个，所以写在这里的逻辑建议都为异步(Hibernate 和 SQLAlchemy 都存在这个情况)。\n\n总结以上便是整个动态代理实现 ORM 监听机制的全过程，其实可以看出并没有它名称那样看起来高大上，当然本身实现也比较简单。\n同时也不止这一种实现方式，例如:\n\ncglib\njavassist\nASM\n\netc..\n他们的具体实现及优劣就不在本文探讨了，感兴趣的后续我会将这个功能用这几种方式实现一遍。\n同时动态代理的应用也不止于此，比如：\n\nRPC 中无感知的远程调用。\nSpring 中的 AOP、拦截器等。\n\n后续会继续完善这个 ORM 库，甚至可以独立出来作为一个小巧的数据库工具也未尝不可。\n相关源码见此处：https://github.com/TogetherOS/cicada\n你的点赞与分享是对我最大的支持\n","categories":["cicada","动态代理","轮子"],"tags":["Java","HTTP","Netty"]},{"title":"撸了一个 Feign 增强包","url":"/2020/07/28/wheel/feign-plus/","content":"\n前言最近准备将公司的一个核心业务系统用 Java 进行重构，大半年没写 Java ，JDK 都更新到 14 了，考虑到稳定性等问题最终还是选择的 JDK11。\n在整体架构选型时，由于是一个全新的系统，所以没有历史包袱，同时团队中也有多位大牛坐镇，因此我们的选项便大胆起来。\n最终结果就是直接一把梭，直接上未来的大趋势：Service Mesh，直接把什么 SpringCloud、Dubbo 这类分布式框架全部干掉。\n本次的重点不是讨论 Service Mesh 是什么、能解决什么问题、为什么选择它，毕竟我也在学习阶段，啥时候整明白线上也稳定了再和大家来交流。\n\n\n问题既然方向定了就开始实际撸码了，不过刚一开始就验证了”理想很丰满、现实很骨感“；\n由于我们去掉了 SpringCloud 和 Dubbo 这类框架，服务的注册、发现、负载均衡等需求全部都下沉到 Service Mesh 中提供了。\n但对于开发来说依然希望可以调用本地方法的方式来调用远程服务，这在 SpringCloud 这类框架中是很容易实现的，框架本身就有很好的支持。\n回到我们这个场景，需求其实很简单，就是想达到 SpringCloud 中的 Feign 这样的声明式+注解的方式调用。\n\n@Autowiredprivate StoreClient client ;Store store = client.update(1, store)\n\n使用 spring-cloud-openfeign 这个包其实就能实现上述的需求了，但这样会引入一些我们根本不会使用的 SpringCloud 的相关依赖，让人感觉”不干净了“；同时也和 Service Mesh 的理念相反，其中的一大目的就是要降低这类框架的侵入性。\n\n其实 spring-cloud-openfeign 的核心就是 Feign，本身它也是可以开箱即用的，所以便尝试看 Feign 自己是否支持这样的用法。\n\n通过官方文档可以得知：是可以定义接口的形式来调用远程接口的，但它本质上是不依赖其他库便可以使用，所以它本身是没有和 Spring 整合也是合情合理，但也就造成了没有现成库可供我们使用。\n\n我们自然是不想写上图红框处的代码的，希望所有接口直接注入就可以使用。\n\n使用因此结合以上的需求便有了这个库 feign-plus\n它的使用流程其实就是翻版的 spring-cloud-openfeign：\n@FeignPlusClient(name = &quot;github&quot;, url = &quot;$&#123;github.url&#125;&quot;)public interface Github &#123;    @RequestLine(&quot;GET /repos/&#123;owner&#125;/&#123;repo&#125;/contributors&quot;)    List&lt;GitHubRes&gt; contributors(@Param(&quot;owner&quot;) String owner, @Param(&quot;repo&quot;) String repo);&#125;\n\n在 SpringBoot 入口进行扫描：\n@SpringBootApplication@EnableFeignPlusClients(basePackages = &quot;top.crossoverjie.feign.test&quot;)public class DemoApplication &#123;\tpublic static void main(String[] args) &#123;\t\tSpringApplication.run(DemoApplication.class, args);\t&#125;&#125;\n\n在 Spring 上下文中直接注入使用：\n@Autowiredprivate Github github ;List&lt;GitHubRes&gt; contributors = github.contributors(&quot;crossoverJie&quot;, &quot;feign-plus&quot;);logger.info(&quot;contributors=&#123;&#125;&quot;, new Gson().toJson(contributors));    \n\n所以当我们需要调用一些外部第三方接口时（比如支付宝、外部 OpenAPI）便可类似于这样定义一个接口，把所有 HTTP 请求的细节屏蔽掉。\n当然也适合公司内部之间的服务调用，和咱们以前写 SpringCloud 或 Dubbo 时类似；服务提供方提供一个 Client 包，消费方直接依赖便可以调用。其他的负载均衡、容错之类的由 Service Mesh 替我们完成。\n对于内部接口，也可以加上 @RequestMapping(&quot;/path&quot;) 注解：\n\n在请求时便会在 url 后拼接上 /order，这样在配置 feign.order.service.url 时只需要填入服务提供方的域名或 IP 即可。\n\nfeign-plus 也支持切换具体的 httpclient，默认是 okhttp3，通过以下配置便可更改。\n# default(okhttp3)feign.httpclient=http2Client\n\n当然也有其他相关配置：\nfeign.plus.max-idle-connections = 520feign.plus.connect-timeout = 11000feign.plus.read-timeout = 12000\n\n\n\n实现最后简单聊聊是如何完成的吧，其实本质上就是 spring-cloud-openfeign 的浓缩版。\n其中最为核心的便是 top.crossoverjie.feign.plus.factory.FeignPlusBeanFactory 类。\n\n该类实现了 org.springframework.beans.factory.FactoryBean接口，并重写了 getObject() 方法返回一个对象。\n\n这段代码是不是似曾相识，其实就是 Feign 的官方 demo。\n\n这里所返回的对象其实就是我们定义的接口的代理对象，而这个对象本身则是 Feign ，所以再往里说：我们的 http 请求编解码、发起请求等逻辑又被这个 feign 对象所代理了。\n\n这个 HardCodedTarget 则是 Feign 内部用于代理最终请求的对象。\n\n有一个小难受的地方：这样的自己定义 Bean 然后注入对象 Idea 是识别不了的，认为当前上下文没有该 Bean，但是 spring-cloud-openfeign 却可以识别。\n\n\n由于 Feign 支持多个客户端，所以这里的客户端可以通过配置文件动态指定。\n\n利用 SpringBoot 提供的 @ConditionalOnExpression 注解可以根据配置动态的选择使用哪个 httpclient,也就是动态选择生成哪个 Bean。\n总结这个库的逻辑非常简单，本质上就是封装了 Feign 并提供了 SpringBoot 的支持，欢迎有类似需求的朋友下载使用。\nfeign-plus源码：https://github.com/crossoverJie/feign-plus\n你的点赞与分享是对我最大的支持\n","categories":["轮子"],"tags":["Java","SpringBoot","Feign"]},{"title":"撸了一个 Feign 增强包 V2.0 升级版","url":"/2022/05/06/wheel/feign-plus2/","content":"\n前言大概在两年前我写过一篇 撸了一个 Feign 增强包，当时准备是利用 SpringBoot + K8s 构建应用，这个库可以类似于 SpringCloud 那样结合 SpringBoot 使用声明式接口来达到服务间通讯的目的。\n\n\n但后期由于技术栈发生变化（改为 Go），导致该项目只实现了基本需求后就搁置了。\n巧合的时最近内部有部分项目又计划采用 SpringBoot + K8s 开发，于是便着手继续维护；现已经内部迭代了几个版本比较稳定了，也增加了一些实用功能，在此分享给大家。\nhttps://github.com/crossoverJie/feign-plus\n首先是新增了一些 features:\n\n更加统一的 API。\n统一的请求、响应、异常日志记录。\n自定义拦截器。\nMetric 支持。\n异常传递。\n\n示例结合上面提到的一些特性做一些简单介绍，统一的 API 主要是在使用层面：\n在上一个版本中声明接口如下：\n@FeignPlusClient(name = &quot;github&quot;, url = &quot;$&#123;github.url&#125;&quot;)public interface Github &#123;    @RequestLine(&quot;GET /repos/&#123;owner&#125;/&#123;repo&#125;/contributors&quot;)    List&lt;GitHubRes&gt; contributors(@Param(&quot;owner&quot;) String owner, @Param(&quot;repo&quot;) String repo);&#125;\n\n其中的 @RequestLine 等注解都是使用 feign 包所提供的。\n这次更新后改为如下方式：\n@RequestMapping(&quot;/v1/demo&quot;)@FeignPlusClient(name = &quot;demo&quot;, url = &quot;$&#123;feign.demo.url&#125;&quot;, port = &quot;$&#123;feign.demo.port&#125;&quot;)public interface DemoApi &#123;    @GetMapping(&quot;/id&quot;)    String sayHello(@RequestParam(value = &quot;id&quot;) Long id);    @GetMapping(&quot;/id/&#123;id&#125;&quot;)    String id(@PathVariable(value = &quot;id&quot;) Long id);    @PostMapping(&quot;/create&quot;)    Order create(@RequestBody OrderCreateReq req);    @GetMapping(&quot;/query&quot;)    Order query(@SpringQueryMap OrderQueryDTO dto);&#125;\n熟悉的味道，基本都是 Spring 自带的注解，这样在使用上学习成本更低，同时与项目中原本的接口写法保持一致。\n\n@SpringQueryMap(top.crossoverjie.feign.plus.contract.SpringQueryMap) 是由 feign-plus 提供，其实就是从 SpringCloud 中 copy 过来的。\n\n我这里写了两个 demo 来模拟调用：\nprovider： 作为服务提供者提供了一系列接口供消费方调用，并对外提供了一个 api 模块。\n\ndemo：作为服务消费者依赖 provider-api 模块，根据其中声明的接口进行远程调用。配置文件：\nserver:  port: 8181feign:  demo:    url : http://127.0.0.1    port: 8080logging:  level:    top:      crossoverjie: debugmanagement:  endpoints:    web:      base-path: /actuator      exposure:        include: &#x27;*&#x27;  metrics:    distribution:      percentiles:        all: 0.5,0.75,0.95,0.99    export:      prometheus:        enabled: true        step: 1mspring:  application:    name: demo\n\n当我们访问 http://127.0.0.1:8181/hello/2 接口时从控制台可以看到调用结果：\n日志记录从上图中可以看出 feign-plus 会用 debug 记录请求&#x2F;响应结果，如果需要打印出来时需要将该包下的日志级别调整为 debug：\nlogging:  level:    top:      crossoverjie: debug\n\n由于内置了拦截器，也可以自己继承 top.crossoverjie.feign.plus.log.DefaultLogInterceptor 来实现自己的日志拦截记录，或者其他业务逻辑。\n@Component@Slf4jpublic class CustomFeignInterceptor extends DefaultLogInterceptor &#123;    @Override    public void request(String target, String url, String body) &#123;        super.request(target, url, body);        log.info(&quot;request&quot;);    &#125;    @Override    public void exception(String target, String url, FeignException feignException) &#123;        super.exception(target, url, feignException);    &#125;    @Override    public void response(String target, String url, Object response) &#123;        super.response(target, url, response);        log.info(&quot;response&quot;);    &#125;&#125;\n\n监控 metricfeign-plus 会自行记录每个接口之间的调用耗时、异常等情况。访问 http://127.0.0.1:8181/actuator/prometheus 会看到相关埋点信息，通过 feign_call* 的 key 可以自行在 Grafana 配置相关面板，类似于下图：\n异常传递rpc（远程调用）要使用起来真的类似于本地调用，异常传递必不可少。\n// provider\tpublic Order query(OrderQueryDTO dto) &#123;\t\tlog.info(&quot;dto = &#123;&#125;&quot;, dto);\t\tif (dto.getId().equals(&quot;1&quot;)) &#123;\t\t\tthrow new DemoException(&quot;provider test exception&quot;);\t\t&#125;\t\treturn new Order(dto.getId());\t&#125;// consumer        try &#123;            demoApi.query(new OrderQueryDTO(id, &quot;zhangsan&quot;));        &#125; catch (DemoException e) &#123;            log.error(&quot;feignCall:&#123;&#125;, sourceApp:[&#123;&#125;], sourceStackTrace:&#123;&#125;&quot;, e.getMessage(), e.getAppName(), e.getDebugStackTrace(), e);        &#125;\t\n比如 provider 中抛出了一个自定义的异常，在 consumer 中可以通过 try/catch 捕获到该异常。\n为了在 feign-plus 中实现该功能需要几个步骤：\n\n自定义一个通用异常。\n服务提供方需要实现一个全局拦截器，当发生异常时统一对外响应数据。\n服务消费方需要自定义一个异常解码器的 bean。\n\n这里我在 provider 中自定义了一个 DemoException：\n\n通常这个类应该定义在公司内部的通用包中，这里为了演示方便。\n\n接着定义了一个 HttpStatus 的类用于统一对外响应。\n@Data@AllArgsConstructor@NoArgsConstructorpublic class HttpStatus &#123;    private String appName;    private int code;    private String message;    private String debugStackTrace;&#125;\n\n\n这个也应该放在通用包中。\n\n然后在 provider 中定义全局异常处理：\n当出现异常时便会返回一个 http_code&#x3D;500 的数据：\n到这一步又会出现一个引战话题：HTTP 接口返回到底是全部返回 200 然后通过 code 来来判断，还是参考 http_code 进行返回?\n这里不做过多讨论，具体可以参考耗子叔的文章：“一把梭：REST API 全用 POST”\nfeign-plus 默认采用的 http_code !&#x3D;200 才会认为发生了异常。\n而这里的 http_status 也是参考了 Google 的 api 设计：具体可以参考这个链接：https://cloud.google.com/apis/design/errors#propagating_errors\n然后定义一个异常解析器：\n@Configurationpublic class FeignExceptionConfig &#123;    @Bean    public FeignErrorDecoder feignExceptionDecoder() &#123;        return (methodName, response, e) -&gt; &#123;            HttpStatus status = JSONUtil.toBean(response, HttpStatus.class);            return new DemoException(status.getAppName(), status.getCode(), status.getMessage(), status.getDebugStackTrace());        &#125;;    &#125;&#125;\n\n\n通常这块代码也是放在基础包中。\n\n\n这样当服务提供方抛出异常时，消费者便能成功拿到该异常：\n实现原理实现原理其实也比较简单，了解 rpc 原理的话应该会知道，服务提供者返回的异常调用方是不可能接收到的，这和是否由一种语言实现也没关系。\n毕竟两个进程之间的栈是完全不同的，不在一台服务器上，甚至都不在一个地区。\n所以 provider 抛出异常后，消费者只能拿到一串报文，我们只能根据这段报文解析出其中的异常信息，然后再重新创建一个内部自定义的异常（比如这里的 DemoException），也就是我们自定义异常解析器所干的事情。\n下图就是这个异常传递的大致流程：\ncode message 模式由于 feign-plus 默认是采用 http_code != 200 的方式来抛出异常的，所以采用 http_code=200, code message 的方式响应数据将不会传递异常，依然会任务是一次正常调用。\n不过基于该模式传递异常也是可以实现的，但没法做到统一，比如有些团队习惯 code !=0 表示异常，甚至字段都不是 code；再或者异常信息有些是放在 message 或 msg 字段中。\n每个团队、个人习惯都不相同，所以没法抽象出一个标准，因此也就没做相关适配。\n\n这也印证了使用国际标准所带来的好处。\n\n限于篇幅，如果有相关需求的朋友也可以在评论区沟通，实现上会比现在稍微复杂一点点🤏🏻。\n总结项目源码：https://github.com/crossoverJie/feign-plus\n基于2022年云原生这个背景，当然更推荐大家使用 gRPC 来做服务间通信，这样也不需要维护类似于这样的库了。\n不过在一些调用第三方接口而对方也没有提供 SDK 时，这个库也有一定用武之地，虽然使用原生 feign 也能达到相同目的，但使用该库可以使得与 Spring 开发体验一致，同时内置了日志、metric 等功能，避免了重复开发。\n你的点赞与分享是对我最大的支持\n","categories":["轮子"],"tags":["Java","SpringBoot","Feign"]},{"title":"技术阅读周刊第一期","url":"/2023/10/13/ob/newsletter/Newsletter01-20231013/","content":"\n我自己平时有每天阅读文章的习惯，也会将这些文章保存起来并做一些记录，今天在看阮一峰的科技爱好者周刊时突然想到我也可以将这些看过的觉得不错的内容分享出来。\n顺便也可以让大伙参与留下自己觉得不错的内容，互相学习。\n\n\n以下便是第一期的内容：\nIstio 中的负载均衡详解及多集群路由实践 🌟🌟🌟1. 介绍了客户端负载均衡和服务端负载均衡的特点和应用场景。2. 引申出 Istio 使用 Envoy 做客户端负载均衡的方案。3. 介绍 Istio 支持的一些负载均衡算法。4. 如何为具体的服务配置负载均衡，以及如何编写 DestinationRule 和VirtualService\n理解 gRPC 协议🌟🌟🌟🌟1. 首先是介绍了 JSON 编码的缺点，可读性高，但性能差。2. PB 性能好，但可读性差，同时还需要配套工具，比如 .proto 格式 IDL 文件来做额外的接口描述。3. 接口请求是底层依然是 http 协议，不过是 http&#x2F;2 协议，但是请求的映射是直接使用 .proto 文件的描述。4. 消息格式描述了消息体前有五个字节，第一个字节描述了是否压缩，后续四个字节描述了消息大小。5. 因为是 stream 协议的关系，才加了这五个字节，因为每次请求都是同一个连接，为了要区分不同的请求就需要在这五个字节来区分了。\nProtocol Buffers 编码🌟🌟🌟🌟\n配合上一篇一起阅读更加\n\n详细讲解了 PB 编码的原理。1. 定长数据都比较简单，主要是解决变长字符串的问题。2. 以 websocket 为例，websocket 的是三挡变速，而 PB 引入了  VarInts 实现了无级变速，但前提是字段不能太多。3. 使用了 Tag 代替了字段名，但坏处就是解码必须需要 PB 文件，也就是需要通过 PB 文件生成目标语言。4. 同时 Tag 也不能更改，更改后解码端得同步更新。\nThe top 7 software engineering workflow tips I wish I knew earlier 🧰 🌟🌟🌟🌟\n作者使用多年的提高工作效率的七个习惯1. Git 相关，别名，我觉得对我来说是自动补全+历史记录更好用2. 编码相关，别直接使用查找，可以多使用 IDE 快捷键+AI 编程3. 记录学到的知识，比如 Notion，现在我使用 Obsidian4. 使用 Todo 记录自己的灵感，脑子不是拿来存储这些东西的，是拿来做创造力相关的事情（这个我也是使用的 Obsidian 插件 Memos）。5. 可视化沟通，比如使用截图 App，写文档等。6. 使用密码 App，比如 1Password7. 使用窗口管理 App\n\nconc：Go 语言的并发工具库🌟🌟🌟🌟\nBetter structured concurrency for go\n\n这是项目的介绍，简单来说就是封装了一些使用  goroutine 的常用操作：\n\n使用 conc.WaitGroup 替代标准的 sync.WaitGroup，并提供了安全运行的特性，不用担心 panic。\n使用 pool.ResultPool 可以拿到执行的结果，Javaer 是不是似曾相识。\niter.Map/iter.ForEach 可以直接并发 Map 或者是迭代 slice。\n\n这里举了个例子，如果我们想要写出一个安全的 goroutine 程序，大概需要写左边那么多的代码，而使用 conc 会简单很多，也更加易读。\n其实从这里就不难看出，conc 只是将这些代码封装好了，感兴趣的也可以看看源码，代码不多，很快就可以看完。\n\n以上内容和评分纯主观参考，均没有使用类似于 ChatGPT 这类 AI 工具进行总结，绝对是传统人肉阅读归纳，匠心工艺。\n上榜文章都很不错，推荐大家去阅读原文。\n\n文章链接：\n\nhttps://jimmysong.io/blog/demystifying-the-load-balancing-in-istio/\nhttps://github.com/sourcegraph/conc\nhttps://taoshu.in/pb-encoding.html\nhttps://taoshu.in/grpc.html\nhttps://careercutler.substack.com/p/the-top-7-software-engineering-workflow?ref=dailydev\n\n","categories":["OB","Newsletter"],"tags":["Tech"]},{"title":"技术阅读周刊第二期","url":"/2023/10/22/ob/newsletter/Newsletter02-20231022/","content":"\n技术阅读周刊，每周更新。\n历史更新\n20231013：第一期\n\n\n\nGoogle Software Engineer Interview Experience — Offer🌟🌟🌟\n谷歌软件工程师的面试经验。\n\n作者背景：2020 年毕业于 MIT Pune（印度城市），将近 3年的开发经验在这次拿到 Google offer 之前参与过3～4 次 Google 面试，上一次是2019 年。因为有之前的面试经验，所以直接跳过了电话面试，直接准备现场面试。\n作者花了三周时间做准备。\n1st Leetcode Medium Coding(45min)主题：\n\n深度优先查询\n广度优先查询\n哈希表相关。与面试官交流作者的方法，最终顺利过了第一轮。\n\n2nd Leetcode Medium Coding(45min)主题：\n\n滑动窗口\n哈希表\n字符串作者刚开始用了一个暴力解法，经过思考后得出一个最优解。其他的问题都得出了正确的时间复杂度。\n\n3rd Leetcode Hard Coding(45min)主题：\n\n广度优先查询\n堆排序（优先级队列）\n矩阵\n数组这轮面试比较有难度，在面试官的提醒下完成了。\n\n4th 谷歌味面试，领导轮这一轮就是一些行为面试，讨论了一些过去的经历，和假设的一些情况。作者使用了 STAR(Situation, Task, Action, Result) 进行了回答，整个过程比较顺利。\n评论区：\n\n希望可以分享下自己准备的资料\n没有系统设计题吗？\n\n\n看来国际一线大厂对算法的考察占比也是非常高的，但作者可能也有一些考虑没有放出面试的具体题目。\n\nFrom JetBrains to Vim: A modern Vim configuration and plugin set | by devsjc | Sep, 2023 | Medium🌟🌟🌟🌟作者从 JetBrains 切换到 Vim，在 Vim 中主要使用到的插件。作者是由于 Vim 作者的离世才决定花时间研究下 Vim，以下是他常用的一些插件：\nPlugin: kristijanhusak&#x2F;vim-packager从名字可以看出这是一个包管理插件。\nPlugin: junegunn&#x2F;fzf.vim模糊搜索插件。\nPlugin: yegappan&#x2F;lsp\n这个插件功能强大：\n\n自动补全\n变量声明跳转\n重命名变量\n其实就是我们常见 IDE 一些基本功能\n\n\n\nPlugin: dense-analysis&#x2F;ale错误提示插件，可以高亮并修复错误。\nPlugin: 907th&#x2F;vim-auto-save自动保存插件，作者从 IDE 切换到 Vim 后不能自动保存，这个插件可以弥补这个缺失。\nPlugin: jiangmiao&#x2F;auto-pairs自动添加、删除成对的括号，引号等符号。\nPlugin: airblade&#x2F;vim-gitgutter用于显示 git 面板的插件，比较提交记录、对比等。\nPlugin: bluz71&#x2F;vim-mistfly-statusline用于显示一些状态，比如 vim 模式、git 等。\n可以发现大部分的插件功能 IDE 都是自带的，而作者之所以选择 Vim 也是因为他更加的轻量，相对于 Jetbrains 家的 IDE 来说。\nBuilding your brand: How I reached 10k subscribers in 6 months\n作者讲述了他用了半年时间做的 newsletter 个人品牌获得了 1w个订阅的经历：\n\n\n1. 做这个过程中收获了两个好处，如果你想掌握某样技能，那教会别人是最快的，也就是做分享\n2. 可以和有许多不同观点的人交流，获得不同的收获\n3. 和许多人保持了连接，获得赞助、面试机会\n4. 公开学习：公开分享你学到的和你直到的东西。\n5. 长期坚持合适你的方法去学习沉淀一件事情\nHow companies Ship code to Production🌟🌟🌟\n\n企业如何代码交付到生产环境:\n\n1. 产品负责人创建需求\n2. 和研发人员讨论需求\n3. 开发需求\n4. 构建打包，需要通过单元测试、覆盖率、Sonar 扫描等流程\n5. 构建成功后发布到开发环境\n6. 可能会有多个团队在开发不同的分支，所以需要将代码部署到 QA1、QA2\n7. 测试团队会到一个单独的测试环境执行回归测试和性能测试\n8. 测试通过后会部署到 UAT 环境\n9. UAT 测试通过后，将会按照发布计划发布到生产环境\n10. SRE 团队负责对生产环境进行运维和监控\n\n在国内这算是一个中大型团队的交付流程了，需要小型团队可能并不会有单测、代码扫描、多个测试环境、SRE 等流程。\n\nUnderstanding Database Types🌟🌟🌟介绍了目前流行的数据库类型：\n\n关系型数据库\nMySQL\nOracle\nSQL Server\nPG\n\n\n时序数据库\nInfluxDB\nTimeScale DB\nGraphite\nPrometheus\n\n\nNoSQL 数据库\nMongoDB\nCassandra\nRedis\nCouchbase\n\n\n\n\n是一篇科普文章，但因为是一个付费订阅的 Newsletter 我也就没有看过，对这些数据库类型的整理还是比较完善的。\n\n文章链接：\n\nhttps://medium.com/@lahotipranali/google-software-engineer-interview-experience-offer-25e4eb6a0a5c\nhttps://medium.com/@devsjc/from-jetbrains-to-vim-a-modern-vim-configuration-and-plugin-set-d58472a7d53d\nhttps://careercutler.substack.com/p/building-your-brand-how-i-reached\nhttps://blog.bytebytego.com/p/ep81-how-companies-ship-code-to-production?utm_source=post-email-title&amp;publication_id=817132&amp;post_id=137944253&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=4buvd&amp;utm_medium=email\nhttps://blog.bytebytego.com/p/understanding-database-types\n\n#Newletters \n","categories":["OB","Newsletter"],"tags":["Tech"]},{"title":"技术阅读周刊第三期","url":"/2023/10/27/ob/newsletter/Newsletter03-20231027/","content":"\n技术阅读周刊，每周更新。\n历史更新\n20231013：第一期\n20231022：第二期\n\nUnderstanding The Linux TTY SubsystemURL: https://ishuah.com/2021/02/04/understanding-the-linux-tty-subsystem/本文讲解了 Linux TTY 的历史故事和来源。\n\nTTY 是 teletype 的简写，电传打印机；最早是在二次世界大战发挥重要作用。\n60 年代的计算机使用了teletype 作为输入终端。\n后续随着技术进步使用了软件模拟替代了物理的电传打印机。\nTTY 早起位于 Linux 的内核空间，导致缺乏灵活性，后续移动到了用户空间。\n之后又出现了 shell，用户使用 shell 登录系统时会分配一个 TTY 给 shell镜像操作。\nTTY 作为 shell 的硬件运行环境，搭配在一起提供了基础的 Linux 操作环境。\n\n100行代码实现一个模拟终端URL: https://ishuah.com/2021/03/10/build-a-terminal-emulator-in-100-lines-of-go/本文在之前介绍 TTY 背景知识的前提下，讲解使用 100 行代码实现一个模拟终端的 App。最终使用效果如下：\n\n第一步使用了  Go  的一个 GUI 库 fyne 渲染了一个普通文本框\n讲解了一个终端和内核通讯的流程图\nPTY master 获取用户输入发送给 PTY slaver，slave 会执行最终的 shell，并将执行结果返回到 PTY master\n这里使用了  Go  的一个第三方库实现了 PTY 的功能\n使用 fyne 获取键盘事件，缓存用户输入的内容，直到检测到输入了回车键，此时将缓存内容发往 pty\n优化响应结果，目前只能输出最新的一行内容，所以新增一个缓冲区存放历史输出。最终一个简单的模拟终端便实现了，当然功能还很简单，感兴趣的朋友可以在这基础上持续优化。\n\n混沌工程\n最近在复现一个可能的网络问题的时候，接触到了混沌工程，以下是我对它的一些理解\n\n混沌工程（Chaos-Mesh）是一个开源的混沌平台，混沌这词可能不好理解，不过通过他所提供的功能就知道具体是干什么的了。\n核心功能是提供了一些列的故障注入，比如：\n\nPod 故障：重启、OOM等\n网络故障：延迟、丢包、断网\nDNS 故障\nHTTP 故障\n甚至还能模拟 JVM 故障\n\n\n这是对一个 Pod 的 http 请求 80 端口进行中断的配置。\n\n通过这些功能可以看出它是一个模拟故障平台，我们可以在它的管理台进行丰富的故障模拟，可以在开发测试过程中增强我们系统的健壮性。\nHacking Your iTerm. Boost your command line productivity by… | by Chandan Kumar | Better ProgrammingURL: https://betterprogramming.pub/hacking-your-iterm-5d2bdacdaccf\n\n作者分享了它使用终端的一些技巧和工具\n\niTerm推荐使用 iTerm 这个就不多说了\npowerlevel10k\n powerlevel10k 是一个 zsh 主题，提供了许多自定义的选项，可以按照自己的习惯定制。https://github.com/romkatv/powerlevel10k\nSyntax highlighting语法高亮可以让自己知道输入是否正确https://github.com/zsh-users/zsh-syntax-highlighting\nAuto-suggestion自动提示插件，可以类似于 IDE 的方式使用命令行，这个还蛮好用的。https://github.com/zsh-users/zsh-autosuggestions\nAutojump自动跳转，可以更快的跳转到我们尝使用的目录，提高效率。https://github.com/wting/autojump\niTerm 的一些窗口导航快捷键\n新窗口 — ⌘ + T\n关闭窗口 — ⌘ + W \n切换 Tab⌘ + Number Key — ( ⌘2 切换到第二个 Tab)\n垂直分割窗口 (same profile) — ⌘ + D\n水平分割窗口(same profile) — ⌘ + Shift + D \n根据顺序切换窗口 — ⌘ + ] , ⌘ + [\n上一个 tab⌘+ &lt;- — \n下一个窗口⌘+ -&gt;\n最大化窗口⌘ + Shift + Enter\n\n当我们聊 Kubernetes Operator 时，我们在聊些什么_云原生_徐新坤_InfoQ精选文章URL: https://www.infoq.cn/article/SJMUvMg_0H7BS5d99euR\n\n作者讲解了 Docker、Helm、Operator、kubernetes Controller 之间的关系以及一些核心概念。\n\n\nDocker的核心价值是提供了标准化的交付流程，现在几乎没人再交付源码了\nOperator 和 Docker 类似的是标准化了分布式系统的交付流程。\nHelm 也可以做分布式系统交付，但他更侧重于协调多个资源管理，比如可以让 A Pod 启动之后再启动 B Pod。\n但 Helm 无法感知整个系统的运行时状态，而 Operator 则可以，通过这个感知可以自动实现扩容、故障恢复、异常处理等工作，在实现了 Helm 的自动化的同时还实现了智能化。\nOperator 是处于一个第三方视角观察整个系统，所以它可以拿到全局的信息，从而最终达到声明状态的一个目的。\nOperator 可以当做另一种 Controller，可以理解为第三方的 controller，一般是运维我们自己的应用。\nOperator 可以将以往运维的经验沉淀为代码，更利于推进 Dev、Ops 合并为 DevOps。\n\n文章链接：\n\nhttps://ishuah.com/2021/02/04/understanding-the-linux-tty-subsystem/\nhttps://ishuah.com/2021/03/10/build-a-terminal-emulator-in-100-lines-of-go/\nhttps://chaos-mesh.org/zh/docs/\nhttps://betterprogramming.pub/hacking-your-iterm-5d2bdacdaccf\nhttps://www.infoq.cn/article/SJMUvMg_0H7BS5d99euR\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第第四期","url":"/2023/11/03/ob/newsletter/Newsletter04-20231103/","content":"\n技术阅读周刊，每周更新。\n历史更新\n20231013：第一期\n20231022：第二期\n20231027：第三期\n\nTechnology trends for Spring projects : javaURL: https://www.reddit.com/r/java/comments/17ixfzf/technology_trends_for_spring_projects/这是一个 Reddit 帖子，讨论了 Spring 项目的技术趋势，以下是一些点赞较高的回复：在招聘的时候还会 Spring 考虑周边的技术栈：比如 Linux、Docker、kubernetes、Git 等，但如果你具备 Spring Security 和 OpenSSL 的话会更受欢迎。\n\n这点在国内感受貌似并不明显，不确定是不是国外更看重一些。\n\nKotlin 在服务端目前还处于一个较小的规模，特别是如今 Java 发版节奏加快，对于大部分人来说没有足够的理由来摆脱 Java。\nSpring 依然是 Java 领域最流行的服务端框架，不管你是否喜欢。\n不用花太多时间去追寻学习新技术，更重要的是确保你已经打好基础了，比如数据库几乎不会发生变化，Spring 底层依然是基于 Servlet 实现的，理解这些基础技术是如何运作的，从而避免一些常见问题。    现在 Spring 已经没有使用 servlet，而是该用 WebFlux 替代\n\n比较认可这个观点，还是更应该花时间到不会变的技术栈上，上层技术如何改变学起来也会更容易。\n\n其中有两个评论比较有意思：Spring 团队花了大量的时间来接入 Kotlin，比如 coroutines 和一些其他资源    在 Spring3 发布的时候，spring 团队也投入了大量时间到 Groovy确实当下 Groovy 使用的越来越少了，所以技术潮流确实会随时间变化，还是抓住不可变的性价比更高。\nService mesh data plane vs. control plane | by Matt Klein | Envoy ProxyURL: https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc\n服务网格中的数据库面和控制面对比，数据面主要包含以下功能：\n\n健康检测\n路由\n负载均衡\n身份验证\n可观测性\n\n控制面主要包含以下功能：\n\n服务注册发现\n各种控制面的配置管理\n\n常见的数据面产品有：Linkerd, NGINX, HAProxy, Envoy, Traefik控制面产品有：Istio, Nelson, SmartStack数据面和控制面是松耦合的，可以替换不同的数据面产品。\nKubewatch: A Kubernetes Watcher for Observability and Monitoring | by Seifeddine Rajhi | Oct, 2023 | MediumURL: https://medium.com/@seifeddinerajhi/kubewatch-a-kubernetes-watcher-for-observability-and-monitoring-d6dea1dbeb06介绍了一个名为 kubewatch 的开源项目。\n可以将各种资源变化的事件通过 webhook 的方式发出通知，可以让维护者请求 kubernetes 的运行状态。\nnamespaceToWatch: &quot;default&quot;resourcesToWatch:  deployment: false  replicationcontroller: false  replicaset: false  daemonset: false  services: true  pod: true  job: false  node: false  clusterrole: true  clusterrolebinding: true  serviceaccount: true  persistentvolume: false  namespace: false  secret: false  configmap: false  ingress: false  coreevent: false  event: true     \n\n这类开源项目其实还蛮多的，我之前也写过一个用于监听我们应用 Pod 的变化的事件，然后将这些消息发送给 Pulsar，只是这个项目做的更全了。\n\nBye-bye, ChatGPT: AI Tools That Are As Good As ChatGPT (But Few People Use) | by ArticleAce | MediumURL: https://medium.com/@Article_Ace/bye-bye-chatgpt-ai-tools-that-are-as-good-as-chatgpt-but-few-people-use-9df4dcdf5ab0作者分享了一些除了 ChatGPT 之外的工具，大部分是一些垂直应用。\nAuto-GPThttps://github.com/Significant-Gravitas/Auto-GPT\n这个工具已经很流行了，可以自己给自己输入 Prompt 直到完成你的目标为止。\nOpenAI-PlaygroundChatGPT 目前只能让我们选择使用的模型（3.5&#x2F;4）但我们可以使用 playground 来自定义一些输出。https://platform.openai.com/playground\nJasper Jasper 是一个用于为社交媒体、广告、博客、邮件生成内容的 AI 工具，相当于是一些不同的垂直领域。 https://www.jasper.ai/ 作者会用它来生成一些创意、和视频脚本。\nQuillbothttps://quillbot.com/这个也是我自己用的较多的工具，可以用来润色我们的英文表达。但我需要在 GitHub 上提一些比较复杂的 issue 、PR 或者是英文邮件时，就会用这个工具进行润色，效果还是很不错的。\n如何用Go实现一个异步网络库？URL: https://mp.weixin.qq.com/s/UBcDrPwEdFz7JOfj0UP2Uw本文主要讲解了一些场景的网络模型以及 Go 原生网络模型的原理，并对如何实现一个网络库提供了一些思路网络库通常是为了解决两个问题：\n\nC10K&#x2F;100K 问题，如何应对海量的并发连接\n服务端如何在高并发的时候正确响应对此有三种网络模型：\n传统 IO 阻塞模型\nReactor 模型\nProactor 模型\n\n Go 原生模型其实已经很强了，可以把他理解为 Reactor 模型。 \n可以基于以下三层设计进行设计：也可以参考一些开源的网络库：\n\nhttps://github.com/panjf2000/gnet\nhttps://github.com/Allenxuxu/gev\nhttps://github.com/aceld/zinx\n\n文章链接：\n\nhttps://www.reddit.com/r/java/comments/17ixfzf/technology_trends_for_spring_projects/\nhttps://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc\nhttps://medium.com/@seifeddinerajhi/kubewatch-a-kubernetes-watcher-for-observability-and-monitoring-d6dea1dbeb06\nhttps://medium.com/@Article_Ace/bye-bye-chatgpt-ai-tools-that-are-as-good-as-chatgpt-but-few-people-use-9df4dcdf5ab0\nhttps://mp.weixin.qq.com/s/UBcDrPwEdFz7JOfj0UP2Uw\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第第5️⃣期","url":"/2023/11/10/ob/newsletter/Newsletter05-20231110/","content":"\n技术阅读周刊，每周更新。\n历史更新\n20231013：第一期\n20231022：第二期\n20231027：第三期\n20231027：第四期\n\nHow to Use OpenTelemetry in Go. OpenTelemetry is a powerful… | by Akanksha Rana | KloudMateURL: https://blog.kloudmate.com/how-to-use-opentelemetry-in-go-e416ca01c499\n\n作者一步步带你使用 golang 配置了 OpenTelemetry，不过由于 Go 不支持 agent，还是没有 Java 方便，很多地方都需要硬编码。\n\ntp := trace.NewTracerProvider(      trace.WithBatcher(exp),      trace.WithResource(newResource()),  )  defer func() &#123;      if err := tp.Shutdown(context.Background()); err != nil &#123;          l.Fatal(err)      &#125;&#125;()  otel.SetTracerProvider(tp)\n\n\n最终会输出到文件中，适配起来倒也蛮简单的。\nWhat happens when you create a pod in Kubernetes | by Daniele Polencic | ITNEXTURL: https://itnext.io/what-happens-when-you-create-a-pod-in-kubernetes-6b789b6db8a8\n\n本文讲解了一个 Pod 在 kubernetes 中创建的主要流程。\n\n\n\n客户端校验 yaml 格式是否正确\n成功后会将 yaml 写入 etcd.\n之后会将 Pod 信息写入调度队列\n调度队列获取该任务，然后选择一个合适的 Node 节点部署 Pod\n等待 Pod 启动成功通过 Prob 探针校验\n将 Pod 的 IP:Port 信息作为 endpoint 存储在 etcd\n如果有创建 service，会将这个 endpoint 和 service 进行绑定\n之后其余的组件就可以使用这个 service，比如 service mesh、ingress、kube-proxy、coreDNS 等组件。\n\n像Redis作者那样，使用Go实现一个聊天服务器，不到100行代码URL: https://colobu.com/2023/10/29/implement-a-small-chat-server-like-antirez-in-100-lines/\n\n前段时间 Redis 作者用 C 语言写了一个简单的聊天服务器，作者使用 Go 实现了类似的功能，代码量也很少，适合新手联系（ Go +goroutine 确实比 Java 写起来要简单）\n\n有几个核心流程：\n\n每次创建一个连接时，都会将这个连接保存在内存里，使用 conn 作为 key\n每次发送消息时会将消息发到一个内部 chan 中，然后异步读取 chan 并通过 conn 发送消息\n\nFive API Performance Optimization Tricks that Every Java Developer Must Know | by lance | Javarevisited | MediumURL: https://medium.com/javarevisited/five-api-performance-optimization-tricks-that-every-java-developer-must-know-75324ee1d244\n\n作者讲了一些常见的  API 优化手段，不止是 Java 开发者适用。\n\n\n并行调用\n避免长事务：避免 RPC 和查询逻辑与事务代码放在一起，应该拆分。\n添加合适的索引\n数据库返回少量的数据\n加缓存\n\nAre you correctly using Optional, Collections, and Null in your Java code ? | by Abhishek Singh | MediumURL: https://medium.com/@abhisheksinghjava/are-you-correctly-using-optional-collections-and-null-in-your-java-code-5d2b8617d47c\n\nJava 介绍了 Optional 的正确用法\n\n\n入参不需要 Optional\n私有方法返回不需要 Optional\n公共方法返回使用 Optional，因为有些开发者可能不会判断 null。\n集合类数据返回不需要返回 Optional/null, 而是返回空集合。\n\n文章链接：\n\nhttps://blog.kloudmate.com/how-to-use-opentelemetry-in-go-e416ca01c499\nhttps://itnext.io/what-happens-when-you-create-a-pod-in-kubernetes-6b789b6db8a8\nhttps://colobu.com/2023/10/29/implement-a-small-chat-server-like-antirez-in-100-lines/\nhttps://medium.com/javarevisited/five-api-performance-optimization-tricks-that-every-java-developer-must-know-75324ee1d244\nhttps://medium.com/@abhisheksinghjava/are-you-correctly-using-optional-collections-and-null-in-your-java-code-5d2b8617d47c\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第第6️⃣期","url":"/2023/11/17/ob/newsletter/Newsletter06-20231117/","content":"\n技术阅读周刊，每周更新。\n历史更新\n20231013：第一期\n20231022：第二期\n20231027：第三期\n20231103：第四期\n20231007：第五期\n\n5 Skills the Best Engineers I Know Have in CommonURL: https://www.developing.dev/p/5-skills-all-10x-engineers-have?utm_source=post-email-title&amp;publication_id=1340878&amp;post_id=138715343&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=4buvd&amp;utm_medium=email\n\n作者讲述了他身边最好的工程师都具备的五个通用技能\n\n\n\n技术的深度与广度\n对于最好的工程师来说，深度和广度他们都会掌握\n要保持好奇心，好奇心是学习任何新东西的原始动力\n和身边厉害的工程师一起工作，会快速从他们身上学到东西\n\n\n不用权威去影响他人\n我理解的是不是依靠资历、经验来向他人输出观点；而是就事论事，利用知识、技能来输出。\n锻炼写作和口语\n学会销售\n\n\n提升他人\n分享知识，写 WIKI、做分享\n团队协作：codereview、团队讨论等\n构建工具，解决大家遇到的一些共性问题。\n\n\n要有主人公意识\n这些工程师都有主人公意识。\n像老板一样思考问题\n\n\n\nExplaining 9 Types of API TestingURL: https://blog.bytebytego.com/p/ep83-explaining-9-types-of-api-testing?ref=dailydev\n\n介绍了九种常见的 API 测试方法\n\n\n冒烟测试：简单的验证 API 是否可以正常工作\n功能测试：根据需求进行测试，有预期结果进行比较\n集成测试：结合多个 API 完成集成测试，更完善的功能测试\n回归测试：确保新增功能没有影响到原有的 API\n负载测试：模拟不同的负载进行测试，测出系统可支持的最大容量\n压力测试：模拟高负载场景，在这种压力情况下观察 API 行为\n安全测试：模拟外部安全测试\nUI测试：配合 UI 交互进行功能测试\n模糊测试：对 API 进行无效输入，尝试让 API 崩溃\n\n\n实际情况可能并不会分的这么细，往往会将一些步骤合并在一起。\n\nPrometheus 14 点实践经验分享URL: https://mp.weixin.qq.com/s/z2IVP26swYaTeiPTeOMoQw这是一篇 17 年的 Prometheus 使用分享，但放到现在一点也不过时。\n\n使用 USE 理论\b来判断资源是否健康\nUtilization 利用率\nSaturation 饱和率\nErrors 错误\n\n\n使用 RED 理论\nRequest rate 请求速率\nError rate 错误速率\nDuration 持续时间\n\n\n指标命名需要有规范\n通常使用框架生成的都没啥问题\n可以参考 Prometheus 的官方实践 https://prometheus.io/docs/practices/naming/\n\n\n注意指标基数\n避免基数爆炸的，比如不能使用 user_id, trace_id 等作为指标的 label\n\n\n统计失败+总量而不要统计失败+成功量\n告警症状而非原因\n告警规则需要配置持续时间，避免无效告警\n查询时候通常先求 rate() 再求 sum()\n\n程序员可能必读书单推荐（一） - 面向信仰编程URL: https://draveness.me//books-1\n\ndraveness 大佬推荐的都是一些偏低层的，静得下心的可以看看，我觉得我是看不下来的。\n\n\nSICP 《计算机程序的构造和解释》\nCTMCP 《计算机程序设计的概念、技术和模型》\nDDIA 《设计数据密集型应用》\n\nTOP 20 Go最佳实践URL: https://colobu.com/2023/11/17/golang-quick-reference-top-20-best-coding-practices/\n\n都是一些基本套路，各种语言的使用者都推荐掌握\n\n\n适当使用缩进，推荐统一使用 gofmt\n变量和函数名具有意义\n限制行长度，IDE 通常都会有提示\n使用常量代替魔法值\n显示处理错误\n避免使用全局变量\n使用结构体处理复杂逻辑，更易维护\n使用 goroutines 处理并发\n使用 Recover 处理 panic\n避免使用 Init 函数，更容易维护\n使用 Defer 清理资源\n使用复合字面值而非构造函数\n使用显示返回值而非具名返回值，也是代码更易读\n避免变量屏蔽，易读性\n使用接口抽象\n\n文章链接：\n\nhttps://www.developing.dev/p/5-skills-all-10x-engineers-have?utm_source=post-email-title&amp;publication_id=1340878&amp;post_id=138715343&amp;utm_campaign=email-post-title&amp;isFreemail=true&amp;r=4buvd&amp;utm_medium=email\nhttps://blog.bytebytego.com/p/ep83-explaining-9-types-of-api-testing?ref=dailydev\nhttps://mp.weixin.qq.com/s/z2IVP26swYaTeiPTeOMoQw\nhttps://draveness.me//books-1\nhttps://colobu.com/2023/11/17/golang-quick-reference-top-20-best-coding-practices/\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第第7️⃣期","url":"/2023/11/24/ob/newsletter/Newsletter07-20231124/","content":"\n技术阅读周刊，每周更新。\n历史更新\n20231013：第一期\n20231022：第二期\n20231027：第三期\n20231103：第四期\n20231107：第五期\n20231117：第六期\n\nWhat is a JWT? Understanding JSON Web TokensURL: https://supertokens.com/blog/what-is-jwt\n\n本文主要讲了一些 JWT 的基本原理，以及优缺点\n\n\nJWT 的生成规则 &lt;header&gt;.&lt;body&gt;.&lt;signature&gt;\n使用证书+签名算法创建签名 Key\n将 header 和 body 的空格换行都去掉后进行 base64，然后使用 . 拼接起来。\n将刚才拼接的字符串使用 Base64 + HMACSHA256 生成签名。\n最终将 &lt;header&gt;.&lt;body&gt;.&lt;signature&gt; 拼接成 JWT。Base64URLSafe(   HMACSHA256(&quot;eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VySWQiOiJhYmNkMTIzIiwiZXhwaXJ5IjoxNjQ2NjM1NjExMzAxfQ&quot;, &quot;NTNv7j0TuYARvmNMmWXo6fKvM4o6nv/aUi9ryX38ZH+L1bkrnD1ObOQ8JAUmHCBq7Iy7otZcyAagBLHVKvvYaIpmMuxmARQ97jUVG16Jkpkp1wXOPsrF9zwew6TpczyHkHgX5EuLg2MeBuiT/qJACs1J0apruOOJCg/gOtkjB4c=&quot;))Results in:3Thp81rDFrKXr3WrY1MyMnNK8kKoZBX9lg-JwFznR-M\n\n\n验证 JWT\n先获取 header，校验头里的签名类型和算法\n获取 body，然后按照之前的方式 Base64 + HMACSHA256 生成签名\n判断两者签名是否相同，不同则验证失败\n判断过期时间是否过期\n\n\nJWT 的优点\n安全性：使用非对称加密保证数据不被篡改\n高效，无状态：不需要单独使用数据库存储数据，只使用算法就能验证\n\n\n缺点\n因为他的独立性和无状态，除非是 token 过期了，不然很难撤销\n依赖于第一步里生成的签名 Key，一旦这个 Key 被泄露就会被伪造。\n\n\n\nGo 开发中的十大常见陷阱[译]URL: https://tomotoes.com/blog/the-top-10-most-common-mistakes-ive-seen-in-go-projects/\n\n最近在  Reddit 上看到一个帖子，让推荐一本 Go 相关的书籍，大部分都是推荐的 “100 Go Mistakes and How to Avoid Them”，目前还没有中译版本，不过作者之前写过一个十个错误的博客，也可以预先看看。\n\n\n未知的枚举值，将枚举的未知值设置为 0\n自动优化的基准测试\n被转移的指针，日常开发中建议传值，速度会更快。loop:    for &#123;      select &#123;      case &lt;-ch:      // Do something      case &lt;-ctx.Done():        break loop      &#125;    &#125;\n出乎意料的 break，在 select 语句中想要退出 for 循环，可以使用标签。\n正确传递错误上下文，使用 https://github.com/pkg/errors\n扩容切片有性能损耗，如果知道长度可以在初始化时指定长度。\n正确使用 context\n做好函数抽象，可以参考 io.Reader/io.Writer\n在 goroutine 中使用循环调用的时候需要额外赋值，这个在  1.22 已经修复了。\n\nDapr: A Portable, Event-Driven Runtime for Building Distributed Applications | by Seifeddine Rajhi | Nov, 2023 | MediumURL: https://medium.com/@seifeddinerajhi/dapr-a-portable-event-driven-runtime-for-building-distributed-applications-c2ea8254406c\n\n本文介绍了 Dapr 是什么，以及给了一个入门示例\n\n\n\nDapr 是 Distributed Application Runtime 的简称，翻译过来就是分布式应用运行时。\n你可以使用任何语言，任何框架、运行在任何地方构建你的分布式应用程序\nDapr 抽象了我们应用开发中所需要的大部分 API，所有与这些 API 交互的 SDK 都是由 Dapr 提供，所以我们不需要关系他的底层是什么。\n\n3 years managing Kubernetes clusters, my 10 lessons. | by Herve Khg | Nov, 2023 | MediumURL: https://hervekhg.medium.com/3-years-managing-kubernetes-clusters-my-10-lessons-b565a5509f0e\n\n作者描述他三年的 kubernetes 集群管理的十条经验\n\n\n在云环境使用 kubernetes，这会比自己维护要简单很多，即便是自己维护也不会让自己的业务能力得到成长，或者收益性价比不高\n使用代码来部署应用，避免直接在控制台用命令操作，这样难以记录操作。\n避免过度使用 helm，同时要对充分理解其中的配置项；这个也很重要。\n不要直接迁移应用到 kubernetes，往往需要做相关的适配。\n非必要不要使用 Mesh\n避免过多的使用管理工具，kubernetes 的管理工具有很多，但大部分操作就靠 kubectl 就够用了。\n一定要记得定义资源的限制（内存和 CPU），避免程序 bug 导致 kubernetes 集群出现问题\n尽量不要在 Pod 中存储数据，推荐使用 NAS、云存储\n配置 HPA，可以根据负载自动扩容 Pod\n不要畏惧改变，每年需要对 kubernetes 进行升级，升级前需要充分阅读 ReleaseNote.\n\nTen Optimization Tricks to Make Your Java Application Run Faster | by lance | Javarevisited | MediumURL: https://medium.com/javarevisited/ten-optimization-tricks-to-make-your-java-application-run-faster-9742f568ed6f\n\n十个优化让 Java 应用更快\n\n\n循环拼接字符串使用 StringBuilder\n线程池代替自定义线程\n容器类预先分配大小\n用枚举代替常量\n用 NIO 代替传统 IO\n用位移操作\n多使用单例模式\n减少锁的范围\n尽量少使用全局变量\n多使用基础数据类型\n\n文章链接：\n\nhttps://supertokens.com/blog/what-is-jwt\nhttps://tomotoes.com/blog/the-top-10-most-common-mistakes-ive-seen-in-go-projects/\nhttps://medium.com/@seifeddinerajhi/dapr-a-portable-event-driven-runtime-for-building-distributed-applications-c2ea8254406c\nhttps://hervekhg.medium.com/3-years-managing-kubernetes-clusters-my-10-lessons-b565a5509f0e\nhttps://medium.com/javarevisited/ten-optimization-tricks-to-make-your-java-application-run-faster-9742f568ed6f\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第8️⃣期","url":"/2023/12/01/ob/newsletter/Newsletter08-20231201/","content":"\n技术阅读周刊，每周更新。\n\n历史更新\n20231103：第四期\n20231107：第五期\n20231117：第六期\n20231124：第七期\n\nPrometheus vs. VictoriaMetrics (VM) | Last9URL: https://last9.io/blog/prometheus-vs-victoriametrics/?ref=dailydev\n\n对比了 Prometheus 和 VM 的区别考虑到和云原生的环境的兼容性，那 Prometheus 可能更合适些，毕竟是 CNCF 组织下的项目。但如果考虑到性能、存储、\b资源占用性，VM 会更合适一些。\n\n28 - Rust in Action: 10 Project Ideas to Elevate Your SkillsURL: https://rust-trends.com/newsletter/rust-in-action-10-project-ideas-to-elevate-your-skills/?ref=dailydev\n\n这是一个 Rust 的 newsletter，介绍了十个项目 idea 可以提高你的 Rust 的水平，我看了下这些项目也不怎么限制语言，任何语言都可以尝试下。\n\n\n简易版的 grep 命令简单：读取文件根据搜索条件输出搜索结果，涉及到的技术栈：\n文件 IO\n正则表达式\n命令行工具\n\n\n短域名服务中等：接收一个长域名，转换为一个短域名，访问短域名时可以自动重定向到长域名。\nWeb 框架\n数据存储，可以是 SQLite&#x2F;Redis\n生成短链接的字符串算法\n\n\n基于文本的冒险游戏中等：用户可以探索房间，选择物品，解密等。\n输入输出\n游戏结构体定义、以及状态流转。\n\n\n基本的网络爬虫简单：爬取一个网页然后提取指定的信息。\nHTTP\nHTML 解析\n字符串匹配、正则。\n\n\n实时聊天应用中等：支持多个人用户加入房间，可以给每个人发送消息。\n网络编程\n多线程处理客户端连接\n\n\nMarkdown 解析为 HTML中等：\n文本解析\n文件 IO\n\n\n简单的 HTTP 服务中等：支持静态文件服务器，也可以处理 RESTful 请求。\n\nImplementing a Bloom Filter in Go | by Francisco Escher | Nov, 2023 | ITNEXTURL: https://itnext.io/bloom-filters-and-go-1d5ac62557de\n\n多年前我也用 Java 写过一个布隆过滤器，本文作者介绍用 Go 来实现，不过原理都差不多。\n\n布隆过滤器有以下特点：\n\n用极少的内存可以存放大量的数据\n存在误报的可能\n但返回数据不存在时一定不存在\n返回数据存在有一定概率是不存在的\n\n所以基于以上特性就有了下面这些应用场景：\n\n网络安全：可以快速判断 IP 释放在黑名单中\nweb 缓存：判断请求是否在缓存中\n数据库缓存，原理同上\n语法检测：一些文本工具可以快速检测你输入的支付是否在字典里，不存在时进行提示\n区块链认证\n邮件过滤\n\nMastering Concurrency In Go — With Select, Goroutines, and Channels | by Yair Fernando | Better ProgrammingURL: https://betterprogramming.pub/concurrency-with-select-goroutines-and-channels-9786e0c6be3c\n\n使用 select goroutine channel 掌握并发\n\n利用 select 多个 channel，来控制最早完成的线程，同时抛弃其他线程func quickestApiResponse(functions []*Function) &#123;\tvar articles []*Article\tfor _, function := range functions &#123;\t\tfunction.Run()\t&#125;\tselect &#123;\tcase googleNewsResponse := &lt;-google:\t\tfmt.Printf(&quot;Source: %s\\n&quot;, googleNewsResponse.Source)\t\tarticles = googleNewsResponse.Articles\tcase freeNewsReponse := &lt;-free:\t\tfmt.Printf(&quot;Source: %s\\n&quot;, freeNewsReponse.Source)\t\tarticles = freeNewsReponse.Articles\t&#125;\tfmt.Printf(&quot;Articles %v\\n&quot;, articles)&#125;\n\n利用 time.After 返回的 channel，来控制达到超时时间后退出所有的线程func main() &#123;  \tch := make(chan struct&#123;&#125;, 1)  \tgo func() &#123;  \t\tfmt.Println(&quot;do something...&quot;)  \t\ttime.Sleep(4*time.Second)  \t\tch&lt;- struct&#123;&#125;&#123;&#125;  \t&#125;()  \t  \tselect &#123;  \tcase &lt;-ch:  \t\tfmt.Println(&quot;done&quot;)  \tcase &lt;-time.After(3*time.Second):  \t\tfmt.Println(&quot;timeout&quot;)  \t&#125;  &#125;\n\nContext.Withtimeout 来控制超时ch := make(chan string)  timeout, cancel := context.WithTimeout(context.Background(), 3*time.Second)  defer cancel()  go func() &#123;  \ttime.Sleep(time.Second * 4)    \tch &lt;- &quot;done&quot;  &#125;()    select &#123;  case res := &lt;-ch:  \tfmt.Println(res)  case &lt;-timeout.Done():  \tfmt.Println(&quot;timout&quot;, timeout.Err())  &#125;\n文章链接：\n\nhttps://last9.io/blog/prometheus-vs-victoriametrics/?ref=dailydev\nhttps://rust-trends.com/newsletter/rust-in-action-10-project-ideas-to-elevate-your-skills/?ref=dailydev\nhttps://itnext.io/bloom-filters-and-go-1d5ac62557de\nhttps://betterprogramming.pub/concurrency-with-select-goroutines-and-channels-9786e0c6be3c\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第9️⃣期","url":"/2023/12/08/ob/newsletter/Newsletter09-20231208/","content":"\n技术阅读周刊，每周更新。\n\n历史更新\n20231107：第五期\n20231117：第六期\n20231124：第七期\n20231201：第八期\n\n美团技术博客十周年，感谢一路相伴 - 美团技术团队URL: https://tech.meituan.com/2023/12/04/ten-years-of-meituan-technology-blog.html\n\n美团技术博客更新十周年了，这个博客确实在广大开发者心中都是有口皆碑的；记得当初在这里看过 HashMap 的原理分析、动态线程池等技术；现在也有加到订阅列表里，有更新时会第一时间阅读\n\nCompletableFuture原理与实践-外卖商家端API的异步化 - 美团技术团队URL: https://tech.meituan.com/2022/05/12/principles-and-practices-of-completablefuture.html\n\n本文描述了美团对 API 做异步优化的过程，最终选择了 CompletableFuture 的过程CompletableFuture 使用起来的坑还是蛮多的，推荐大家都应该阅读下。\n\n\n明确知道自己的代码运行在哪个线程上，如果不传入线程池那就是公共的 ForkJoinPool 线程池，可能会有阻塞的情况；也可以直接传入自定义的线程池\n线程池循环使用可能会引起死锁，当 A 线程依赖于 B 线程的执行结果时，如果此时是同一个线程池，并且线程池已满，B 线程一直得不到机会执行，那 A 线程也无法运行，从而导致死锁。\nCompletableFuture 的异常往往会被包装为CompletionException，所以最好是要异常工具类进行提取public class ExceptionUtils &#123;    public static Throwable extractRealException(Throwable throwable) &#123;          //这里判断异常类型是否为CompletionException、ExecutionException，如果是则进行提取，否则直接返回。        if (throwable instanceof CompletionException || throwable instanceof ExecutionException) &#123;            if (throwable.getCause() != null) &#123;                return throwable.getCause();            &#125;        &#125;        return throwable;   \n\n没错，数据库确实应该放入 K8s 里！URL: https://mp.weixin.qq.com/s/QJn6-EzPp7PXar-GdMITCA\n\n虽然这是一篇软文，不过其中几个论据确实是有道理的。而 K8s 的控制器则是基于另一种思路：机器能做的事就不应该由人来做。通过 Operator，可以实现24 小时不间断地同步期望状态和实际状态，而这是用 Ansible 很难实现的，你用 Ansible 实现是想写个定时任务嘛？\n\n\n复杂度：\nSealos 提供了一键安装命令，有效降低其复杂度\n\n\n稳定性：\n一个良好的软件架构会不断提升和收敛其鲁棒性，并逐渐减少对人的依赖，比如使用 Oracle 的人喝茶时间一定比用开源 MySQL 的人喝茶时间多\n\n\n性能：\n而且，容器对数据库性能的影响几乎可以忽略不计，真正重要的是磁盘 IO 和网络带宽时延等因素。\n\n\n\n目前市面上大部分云服务厂商所提供的数据库服务也都是跑在  kubernetes 中的。\ndeckarep&#x2F;golang-set: A simple, battle-tested and generic set type for the Go language. Trusted by Docker, 1Password, Ethereum and Hashicorp.URL: https://github.com/deckarep/golang-set\n\n一个泛型的 Go Set 库, 还提供了一些集合常用的操作工具，比如 Contains&#x2F;Difference&#x2F;Intersect 等函数。\n\n已经被这些公司采用了：\n\nEthereum\nDocker\n1Password\nHashicorp\n\n// Syntax example, doesn&#x27;t compile.mySet := mapset.NewSet[T]() // T 是具体的类型// Therefore this code creates an int setmySet := mapset.NewSet[int]()// Or perhaps you want a string setmySet := mapset.NewSet[string]()type myStruct struct &#123;  name string  age uint8&#125;// Alternatively a set of structsmySet := mapset.NewSet[myStruct]()// Lastly a set that can hold anything using the any or empty interface keyword: interface&#123;&#125;. This is effectively removes type safety.mySet := mapset.NewSet[any]()\n\n文章链接：\n\nhttps://tech.meituan.com/2023/12/04/ten-years-of-meituan-technology-blog.html\nhttps://tech.meituan.com/2022/05/12/principles-and-practices-of-completablefuture.html\nhttps://mp.weixin.qq.com/s/QJn6-EzPp7PXar-GdMITCA\nhttps://github.com/deckarep/golang-set\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第十一期","url":"/2023/12/22/ob/newsletter/Newsletter10-20231222/","content":"\n技术阅读周刊，每周更新。\n\n历史更新\n20231124：第七期\n20231201：第八期\n20231215：第十期\n\nA Comprehensive guide to Spring Boot 3.2 with Java 21, Virtual Threads, Spring Security, PostgreSQL, Flyway, Caching, Micrometer, Opentelemetry, JUnit 5, RabbitMQ, Keycloak Integration, and More! (10&#x2F;17) | by Jonathan Chevalier | Nov, 2023 | MediumURL: https://medium.com/@jojoooo/exploring-a-base-spring-boot-application-with-java-21-virtual-thread-spring-security-flyway-c0fde13c1eca#551c\n\n本文讲解了基于最新的 Spring Boot3.2 和 Java 21 所使用到的技术栈\n\n数据库数据库使用 Postgres15 和 flyway 来管理数据库 schema 的迁移。\n异常处理Spring6 实现了新的 RFC9457规范，实现以下接口：\n@Slf4j@ControllerAdvice@RequiredArgsConstructorpublic class GlobalExceptionHandler extends ResponseEntityExceptionHandler &#123;  // Process @Valid  @Override  protected ResponseEntity&lt;Object&gt; handleMethodArgumentNotValid(      @NonNull final MethodArgumentNotValidException ex,      @NonNull final HttpHeaders headers,      @NonNull final HttpStatusCode status,      @NonNull final WebRequest request) &#123;    log.info(ex.getMessage(), ex);    final List&lt;ApiErrorDetails&gt; errors = new ArrayList&lt;&gt;();    for (final ObjectError err : ex.getBindingResult().getAllErrors()) &#123;      errors.add(          ApiErrorDetails.builder()              .pointer(((FieldError) err).getField())              .reason(err.getDefaultMessage())              .build());    &#125;    return ResponseEntity.status(BAD_REQUEST)        .body(this.buildProblemDetail(BAD_REQUEST, &quot;Validation failed.&quot;, errors));  &#125;  private ProblemDetail buildProblemDetail(      final HttpStatus status, final String detail, final List&lt;ApiErrorDetails&gt; errors) &#123;      final ProblemDetail problemDetail =        ProblemDetail.forStatusAndDetail(status, StringUtils.normalizeSpace(detail));        // Adds errors fields on validation errors, following RFC 9457 best practices.    if (CollectionUtils.isNotEmpty(errors)) &#123;      problemDetail.setProperty(&quot;errors&quot;, errors);    &#125;         return problemDetail;  &#125;\n&#123;    &quot;type&quot;: &quot;about:blank&quot;,    &quot;title&quot;: &quot;Bad Request&quot;,    &quot;status&quot;: 400,    &quot;detail&quot;: &quot;Validation failed.&quot;,    &quot;instance&quot;: &quot;/management/companies&quot;,    &quot;errors&quot;: [        &#123;            &quot;pointer&quot;: &quot;name&quot;,            &quot;reason&quot;: &quot;must not be blank&quot;        &#125;,        &#123;            &quot;pointer&quot;: &quot;slug&quot;,            &quot;reason&quot;: &quot;must not be blank&quot;        &#125;    ]&#125;\n应用异常@Getterpublic class RootException extends RuntimeException &#123;  @Serial private static final long serialVersionUID = 6378336966214073013L;  private final HttpStatus httpStatus;  private final List&lt;ApiErrorDetails&gt; errors = new ArrayList&lt;&gt;();  public RootException(@NonNull final HttpStatus httpStatus) &#123;    super();    this.httpStatus = httpStatus;  &#125;  public RootException(@NonNull final HttpStatus httpStatus, final String message) &#123;    super(message);    this.httpStatus = httpStatus;  &#125;&#125;@ExceptionHandler(RootException.class)public ResponseEntity&lt;ProblemDetail&gt; rootException(final RootException ex) &#123;  log.info(ex.getMessage(), ex);    // Uses default message, can be adapted to use ex.getMessage().  final ProblemDetail problemDetail =      this.buildProblemDetail(          ex.getHttpStatus(), API_DEFAULT_REQUEST_FAILED_MESSAGE, ex.getErrors());    return ResponseEntity.status(ex.getHttpStatus()).body(problemDetail);&#125;&#123;    &quot;type&quot;: &quot;about:blank&quot;,    &quot;title&quot;: &quot;Internal Server Error&quot;,    &quot;status&quot;: 500,    &quot;detail&quot;: &quot;Request failed.&quot;,    &quot;instance&quot;: &quot;/back-office/hello-world&quot;&#125;\n异常降级@ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)@ExceptionHandler(Throwable.class)public ProblemDetail handleAllExceptions(final Throwable ex, final WebRequest request) &#123;  log.warn(ex.getMessage(), ex);  this.slack.notify(format(&quot;[API] InternalServerError: %s&quot;, ex.getMessage()));  return this.buildProblemDetail(HttpStatus.INTERNAL_SERVER_ERROR, API_DEFAULT_ERROR_MESSAGE);&#125;&#123;    &quot;type&quot;: &quot;about:blank&quot;,    &quot;title&quot;: &quot;Internal Server Error&quot;,    &quot;status&quot;: 500,    &quot;detail&quot;: &quot;Something went wrong. Please try again later or enter in contact with our service.&quot;,    &quot;instance&quot;: &quot;/back-office/hello-world&quot;&#125;\n当有无法处理的异常时，就需要配置一个兜底的异常。\n缓存&lt;dependency&gt;  \t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  \t&lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;  &lt;/dependency&gt;\n\npublic interface CompanyRepository extends JpaRepository&lt;Company, Long&gt; &#123;  String CACHE_NAME = &quot;company&quot;;  @NonNull  @Cacheable(value = CACHE_NAME, key = &quot;&#123;&#x27;byId&#x27;, #id&#125;&quot;)  @Override  Optional&lt;Company&gt; findById(@NonNull Long id);  @Cacheable(value = CACHE_NAME, key = &quot;&#123;&#x27;bySlug&#x27;, #slug&#125;&quot;)  Optional&lt;Company&gt; findBySlug(String slug);  @Caching(      evict = &#123;        @CacheEvict(value = CACHE_NAME, key = &quot;&#123;&#x27;byId&#x27;, #entity.id&#125;&quot;),        @CacheEvict(value = CACHE_NAME, key = &quot;&#123;&#x27;bySlug&#x27;, #entity.slug&#125;&quot;),      &#125;)  @Override  &lt;S extends Company&gt; @NonNull S save(@NonNull S entity);  /*   * This cache implementation is only valid if the table is not   * frequently updated since it will clear the cache at every update operation   * If you want to be more performant you can use something like https://github.com/ms100/cache-as-multi   * */  @NonNull  @CacheEvict(cacheNames = CACHE_NAME, allEntries = true)  @Override  &lt;S extends Company&gt; List&lt;S&gt; saveAll(@NonNull Iterable&lt;S&gt; entities);  @Caching(      evict = &#123;        @CacheEvict(value = CACHE_NAME, key = &quot;&#123;&#x27;byId&#x27;, #entity.id&#125;&quot;),        @CacheEvict(value = CACHE_NAME, key = &quot;&#123;&#x27;bySlug&#x27;, #entity.slug&#125;&quot;),      &#125;)  @Override  void delete(@NonNull Company entity);  /*   * This cache implementation is only valid if the table is not   * frequently updated since it will clear the cache at every delete operation   * If you want to be more performant you can use something like https://github.com/ms100/cache-as-multi   * */  @CacheEvict(cacheNames = CACHE_NAME, allEntries = true)  @Override  void deleteAll(@NonNull Iterable&lt;? extends Company&gt; entities);&#125;\nSpring 提供了标准的缓存接口，即便是后续需要切换到 Redis，使用的 API 和注解都不会发生改变。\n线程Java21 后支持了虚拟线程，几乎可以无限的实现线程，在 Spring Boot 3.2 需要单独开启。\nspring.threads.virtual.enabled\n\n可观测性&lt;dependency&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;  &lt;groupId&gt;io.micrometer&lt;/groupId&gt;  &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;&lt;/dependency&gt;\n\nspring:  endpoints:    web:      exposure:        include: info, health, prometheus, metrics\n\n\n注意在生成环境不要暴露管理 API\n\nTrace&lt;dependency&gt;  &lt;groupId&gt;io.micrometer&lt;/groupId&gt;  &lt;artifactId&gt;micrometer-tracing-bridge-otel&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;  &lt;groupId&gt;net.ttddyy.observation&lt;/groupId&gt;  &lt;artifactId&gt;datasource-micrometer-spring-boot&lt;/artifactId&gt;  &lt;version&gt;$&#123;datasource-micrometer.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;  &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;  &lt;artifactId&gt;opentelemetry-exporter-otlp&lt;/artifactId&gt;  &lt;version&gt;$&#123;opentelemetry-exporter-otlp.version&#125;&lt;/version&gt;&lt;/dependency&gt;\n\n同步请求的时候每个请求都会带上 traceId 和 spanId ，如果是异步请求时候需要配置：spring.reactor.context-propagation=true\n如果使用 @Async时：\n@Configurationpublic class TaskExecutorConfig &#123;  /*   * Override default SimpleAsyncTaskExecutor to provide context propagation in @Async function   * */  @Bean  public TaskExecutor simpleAsyncTaskExecutor() &#123;    final SimpleAsyncTaskExecutor taskExecutor = new SimpleAsyncTaskExecutor();    taskExecutor.setTaskDecorator(new ContextPropagatingTaskDecorator());    return taskExecutor;  &#125;&#125;\n\n本地测试时候可以使用 Otel Desktop Viewer\nmanagement:    tracing:    sampling:      probability: 1  otlp:    tracing:      endpoint: http://localhost:4317\n\nRust Vs Go: A Hands-On ComparisonURL: https://www.shuttle.rs/blog/2023/09/27/rust-vs-go-comparison\n\n动手比较 Rust 和 Go\n\n\n本文是通过编写一个 web 服务来进行比较的。\n\nGo 更加简单易学，同时标准库非常强大，只需要配合 gin+sqlx 这两个第三方库就能实现一个 web 服务\nRust也可以快速的构建一个安全的 web 服务，但需要依赖许多第三方库，比如http&#x2F;JSON&#x2F;模板引擎&#x2F;时间处理等\n但 Rust 在异常处理方面心智负担更低，代码更容易阅读。\n如果是一个初创小团队，使用 Go 的上手难度确实更低；\n但如果团队愿意花时间投入到 Rust 中，结合他出色的错误处理，和强大的编译检查，长时间来看会得到更好的效果。\n\n为什么要使用 Go 语言？Go 语言的优势在哪里？ - 知乎URL: https://www.zhihu.com/question/21409296/answer/1040884859\n\n图文并茂，讲解了 G-M-P 各自之间的关系，以及调度模型。\n\n\n\nG: Goroutine，用户创建的协程，图中搬运的砖头。\nM: Machine，OS 内核的线程的抽象，代表真正执行的资源；对应到就是图中的地鼠，地鼠不能用户直接创建；得是砖头 G 太多，地鼠 M 本身太少，同时还有空闲的小车 P，此时就会从其他地方借一些地鼠 M 过来直到把小车 P 用完为止。\nP: Processor 处理器，G 只有绑定到 P 才能被调度；P 是图中的小车，由用户设置的 GoMAXPROCS 决定小车的数量。\n\n文章链接：\n\nhttps://blog.canopas.com/golang-14-shorthand-tricks-you-might-not-know-8d8d21954c49\nhttps://medium.com/@jojoooo/exploring-a-base-spring-boot-application-with-java-21-virtual-thread-spring-security-flyway-c0fde13c1eca#551c\nhttps://www.zhihu.com/question/21409296/answer/1040884859\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第🔟期","url":"/2023/12/15/ob/newsletter/Newsletter10-20231215/","content":"\n技术阅读周刊，每周更新。\n\n历史更新\n20231117：第六期\n20231124：第七期\n20231201：第八期\n20231215：第九期\n\nGolang: 14 Shorthand Tricks You Might Not Know! | by Nidhi D | Dec, 2023 | CanopasURL: https://blog.canopas.com/golang-14-shorthand-tricks-you-might-not-know-8d8d21954c49\n同时声明和初始化变量// Long form  var message string  message = &quot;Hello, Golang!&quot;    // Shorthand  message := &quot;Hello, Golang!&quot;\n\n声明和初始化多个变量// Long form  var a, b, c int  a = 1  b = 2  c = 3    // Shorthand  a, b, c := 1, 2, 3\n\n交换变量a, b := 1, 2    // Long form  temp := a  a = b  b = temp    // Shorthand  a, b = b, a\n\nDefer 函数调用// Long form  func cleanup() &#123;  // Cleanup logic  &#125;  defer cleanup()    // Shorthand  defer func() &#123;  // Cleanup logic  &#125;()\n\n检测 Map 中的数据是否存在// Long form  value, exists := myMap[key]  if !exists &#123;      // Key doesn&#x27;t exist in the map  &#125;    // Shorthand  if value, exists := myMap[key]; !exists &#123;      // Key doesn&#x27;t exist in the map  &#125;\n\n使用下标和值迭代切片// Long form  for i := 0; i &lt; len(numbers); i++ &#123;  fmt.Println(i, numbers[i])  &#125;    // Shorthand  for i, value := range numbers &#123;  fmt.Println(i, value)  &#125;\n\n错误检测// Long form  result, err := someFunction()  if err != nil &#123;  // Handle the error  &#125;    // Shorthand  if result, err := someFunction(); err != nil &#123;  // Handle the error  &#125;\n\n创建一个变量的指针// Long form  var x int  ptr := &amp;x    // Shorthand  ptr := new(int)\n\n匿名函数// Long form  func add(x, y int) int &#123;  return x + y  &#125;    // Shorthand  add := func(x, y int) int &#123;  return x + y  &#125;\n\n创建和初始化 Map// Long form  colors := make(map[string]string)  colors[&quot;red&quot;] = &quot;#ff0000&quot;  colors[&quot;green&quot;] = &quot;#00ff00&quot;    // Shorthand  colors := map[string]string&#123;  &quot;red&quot;: &quot;#ff0000&quot;,  &quot;green&quot;: &quot;#00ff00&quot;,  &#125;\n\n声明多个常量// Long form  const pi float64 = 3.14159  const maxAttempts int = 3    // Shorthand  const (  \tpi = 3.14159  \tmaxAttempts = 3  )\n\n\nJava Mastery Unleashed: 12 Essential Tips Every Developer Must EmbraceURL: https://blog.stackademic.com/boost-your-java-skills-12-must-know-programming-tips-for-java-developers-34f8381ec431\n\n一些常用的 Java 技巧\n\n\n善用 Lambda 表达式\n// BeforeList&lt;String&gt; names = new ArrayList&lt;&gt;();  for (Person person : people) &#123;  names.add(person.getName());  &#125;// AfterList&lt;String&gt; names = people.stream()  .map(Person::getName)  .collect(Collectors.toList());\n使用 Optionals 替代 null\nOptional&lt;String&gt; maybeName = Optional.ofNullable(person.getName());  String name = maybeName.orElse(&quot;Unknown&quot;);\n使用 stream 简化集合操作\nList&lt;Integer&gt; evenNumbers = numbers.stream()  .filter(num -&gt; num % 2 == 0)  .collect(Collectors.toList());\nString.format 拼接字符串\nString s1 = &quot;Hello&quot;;  String s2 = &quot; World&quot;;  String s = String.format(&quot;%s%s&quot;, s1, s2);\n\n使用 default method 扩展接口\nimport java.time.LocalDateTime;  public interface TimeClient &#123;  void setTime(int hour, int minute, int second);  void setDate(int day, int month, int year);  void setDateAndTime(int day, int month, int year, int hour, int minute, int second);  LocalDateTime getLocalDateTime();  &#125;\n\n使用枚举替换常量\npublic class Main &#123;    enum Level &#123; LOW, MEDIUM, HIGH &#125;    public static void main(String[] args) &#123;        Level myVar = Level.MEDIUM;        System.out.println(myVar);    &#125;&#125;\n\n使用 try-with-Resource 管理资源\ntry (FileReader fileReader = new FileReader(&quot;example.txt&quot;);     BufferedReader bufferedReader = new BufferedReader(fileReader)) &#123;    String line = bufferedReader.readLine();    // Process the file&#125; catch (IOException e) &#123;    // Handle the exception&#125;\n\nSpringBoot Webflux vs Vert.x: Performance comparison for hello world case | Tech TonicURL: https://medium.com/deno-the-complete-reference/springboot-webflux-vs-vert-x-performance-comparison-for-hello-world-case-41a6bd8e9f8c\n\n本文对比了 SpringBoot Webflux 和 Vert.x 的性能对比\n\n以下是两个框架写的压测接口：\npackage hello;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ConfigurableApplicationContext;import org.springframework.web.reactive.config.EnableWebFlux;import org.reactivestreams.Publisher;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.ResponseBody;import reactor.core.publisher.Mono;@SpringBootApplication@EnableWebFlux@Controllerpublic class Application &#123;    public static void main(String[] args) throws Exception &#123;        SpringApplication.run(Application.class);    &#125;    @GetMapping(&quot;/&quot;)    @ResponseBody    public Publisher&lt;String&gt; handler() &#123;        return Mono.just(&quot;Hello world!&quot;);    &#125;&#125;// Vert.xpackage com.example.starter;    import io.vertx.core.AbstractVerticle;  import io.vertx.core.Promise;  import io.vertx.core.http.HttpServer;  import io.vertx.ext.web.Router;    public class MainVerticle extends AbstractVerticle &#123;      @Override    public void start(Promise&lt;Void&gt; startPromise) throws Exception &#123;      HttpServer server = vertx.createHttpServer();      Router router = Router.router(vertx);        router.get(&quot;/&quot;).respond(ctx -&gt; ctx          .response()          .putHeader(&quot;Content-Type&quot;, &quot;text/plain&quot;)          .end(&quot;hello world!&quot;));        server.requestHandler(router).listen(3000);    &#125;  &#125;\n\n最后直接看对比结果吧：最终作者根据一个计算公式得出两个框架的得分，规则如下：\n\n差距小于 5% 不得分\n5～20 得 1 分\n20～50 得两分\n大于 50，得三分最终是 Vert.x 得分超过 Webflux 55%⬆️\n\n不过个人觉得压测结果再好，套上业务后，比如一个接口查询了多个后端服务，后端服务有依赖于多个数据库，最终出来的 RT 大家都差不多。\n除非是某些对性能极致要求的场景，比如实时数据分析、物联网中间件等和直接业务不太相关领域。\n它的底层依然是 Netty，但比 Netty 提供了跟易用的 API。\nGit Cherry Pick Examples to Apply Hot Fixes and Security Patches — Nick JanetakisURL: https://nickjanetakis.com/blog/git-cherry-pick-examples-to-apply-hot-fixes-and-security-patches?ref=dailydev\n\n讲解了 git cherry-pick 的作用，什么时候该用，什么时候不用。\n\n举个例子：一些大型的开源项目往往都会有一个主分子，同时维护了不同版本的子分支，有些用户可能就会一直使用一些长期维护的子分支，比如 v2.1.0 \\ v2.3.0\n但对于大部分的开发者来说主要会维护主分支，也会在主分支上推进一些新功能，这些新功能不一定会同步到上述提到的两个老版本中。\n但对于一些安全漏洞，重大 bug 等是需要同步到这些子分支的，但又不能把一些不兼容的新特性同步到子分支中。\n此时就可以使用  cherry-pick 这个功能，只将某一个提交给 pick 到目标分支中。\n# Cherry pick more than 1 SHA.## This could be useful if you have a handful of commits that you want to bring over,# you&#x27;ll likely want to order them with the oldest commit being first in the list.git cherry-pick &lt;SHA&gt; &lt;SHA&gt;# Edit the git commit message for the newly applied commit.## This could be useful if want to customize the git commit message with extra context.git cherry-pick &lt;SHA&gt; --edit# Avoid automatically creating the commit which lets you edit the files first.## This could be useful if you need to make manual code adjustments before committing,# such as applying a security patch which uses an older library with a different API.git cherry-pick &lt;SHA&gt; --no-commit\n\n\n\n文章链接：\n\nhttps://blog.canopas.com/golang-14-shorthand-tricks-you-might-not-know-8d8d21954c49\nhttps://blog.stackademic.com/boost-your-java-skills-12-must-know-programming-tips-for-java-developers-34f8381ec431\nhttps://medium.com/deno-the-complete-reference/springboot-webflux-vs-vert-x-performance-comparison-for-hello-world-case-41a6bd8e9f8c\nhttps://nickjanetakis.com/blog/git-cherry-pick-examples-to-apply-hot-fixes-and-security-patches?ref=dailydev\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第十二期","url":"/2023/12/29/ob/newsletter/Newsletter12-20231229/","content":"\n技术阅读周刊，每周更新。\n\n历史更新\n20231201：第八期\n20231215：第十期\n20231122：第十一期\n\nDeno vs Go: Native hello world performance | Tech TonicURL: https://medium.com/deno-the-complete-reference/deno-vs-go-native-hello-world-performance-c57d8fc13c75\n\n使用 Deno 和 Go 进行基本的接口对比\n\n\nMacBook Pro M2 with 16GB of RAM\nDeno v1.38.0\nGo v1.21.3\n\n都是最简单的 httpServer:\nDeno.serve(&#123;  port: 3000,&#125;, (req) =&gt; &#123;  try &#123;    const pathName = new URL(req.url).pathname;    if (pathName !== &quot;/&quot;) &#123;      return new Response(null, &#123; status: 404 &#125;);    &#125;    return new Response(&quot;Hello world!&quot;);  &#125; catch (e) &#123;    return new Response(null, &#123; status: 500 &#125;);  &#125;&#125;);\n\npackage mainimport (  &quot;io&quot;  &quot;net/http&quot;)func main() &#123;  http.HandleFunc(&quot;/&quot;, helloWorld)  http.ListenAndServe(&quot;:3000&quot;, nil)&#125;func helloWorld(w http.ResponseWriter, r *http.Request) &#123;  io.WriteString(w, &quot;Hello world!&quot;)&#125;\n\n\n总的来说 Deno 比 Go 慢了 30% 左右，但 CPU 占有率比 Go 更少，Go 的内存占用更低。\n\n这个对比就图一乐。\n\nTop 7 Spring Boot Design Patterns Unveiled | by Dharmendra Awasthi | Dec, 2023 | StackademicURL: https://blog.stackademic.com/top-7-spring-boot-design-patterns-unveiled-4a2569f8d324\n\n7 个我们可以学习的 Spring Boot 的设计模式\n\nSingleton Pattern 单例模式这个没啥好说的，面试都被讲烂了，依然很经典。当我们使用这些注解声明一个 Bean 时@Component, @Service, @Repository, or @Controller，就会被创建一个单例对象注入到 IOC 容器中。\nFactory Pattern 工厂模式Spring 也提供了工厂模式的接口，我们可以自定义创建逻辑：\nimport org.springframework.beans.factory.FactoryBean;public class MyFactoryBean implements FactoryBean&lt;MyBean&gt; &#123;    @Override    public MyBean getObject() throws Exception &#123;        // Custom logic to create and return MyBean instance        return new MyBean();    &#125;    @Override    public Class&lt;?&gt; getObjectType() &#123;        return MyBean.class;    &#125;    @Override    public boolean isSingleton() &#123;        return true; // Or false, depending on your bean&#x27;s scope    &#125;&#125;\n\nBuilder 创建者模式这个其实不算是 Spring 所提供的，但确实很好用；通常用于创建需要很多可选参数的对象时使用：\nimport lombok.Builder;import lombok.Data;@Data@Builderpublic class User &#123;    private String username;    private String email;    private int age;    // Other fields&#125;User user = User.builder()    .username(&quot;john_doe&quot;)    .email(&quot;john@example.com&quot;)    .age(30)    .build();\n\nProxy 代理模式代理模式在 spring 中通常用于 AOP 切面，可以实现一些通用的非业务逻辑功能；比如日志、缓存、安全检测等：\nimport org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;@Aspect@Componentpublic class LoggingAspect &#123;        @Before(&quot;execution(* com.example.service.*.*(..))&quot;)    public void beforeServiceMethods() &#123;        // Logic to be executed before service methods        System.out.println(&quot;Logging before service methods...&quot;);    &#125;&#125;\nObserve 观察者模式本质上是将业务解耦，生产者发布事件，订阅者接收事件，只是 spring 帮我们封装好了逻辑。\nimport org.springframework.context.ApplicationEvent;public class OrderPlacedEvent extends ApplicationEvent &#123;    private final Order order;    public OrderPlacedEvent(Object source, Order order) &#123;        super(source);        this.order = order;    &#125;    // Getters for order information&#125;import org.springframework.context.ApplicationEventPublisher;import org.springframework.stereotype.Service;@Servicepublic class OrderService &#123;    private final ApplicationEventPublisher eventPublisher;    public OrderService(ApplicationEventPublisher eventPublisher) &#123;        this.eventPublisher = eventPublisher;    &#125;    public void placeOrder(Order order) &#123;        // Logic to place order        // Publish OrderPlacedEvent        eventPublisher.publishEvent(new OrderPlacedEvent(this, order));    &#125;&#125;import org.springframework.context.event.EventListener;import org.springframework.stereotype.Component;@Componentpublic class EmailService &#123;    @EventListener    public void sendEmailOnOrderPlacement(OrderPlacedEvent event) &#123;        Order order = event.getOrder();        // Logic to send email based on the placed order    &#125;&#125;\n\n转转一体化监控系统——Prometheus·Grafana成本治理_TakinTalks稳定性技术交流平台URL: https://news.shulie.io/?p=8229\n链接里有 B 站视频，文字版链接：https://mp.weixin.qq.com/s/FySeVBL7EfihOlNDBvAPpw\n\n转转的监控方案\n\n\n基于 Prometheus 架构（确实已经是监控领域的标准了）\n使用 M3DB 替换了单机的 Prometheus。\n我们使用 VM 替换的 Prometheus，转转没有选择 VM 是因为 M3DB 的压缩率更高。\n\n\n采用 Push 模型推送数据\n由 SDK 进行推送，对业务无感知\n省略了注册中心，改为了数据库存储服务节点信息。\n由于我们使用了 kubernetes，所以是基于 kubernetes 的 SD 实现的服务发现。\n所以我们采用的也是 Pull 拉取模型\n\n\n\n\n重写了告警系统，Prometheus 自带的告警系统存在学习难度大等问题。\n\n文章链接：\n\nhttps://medium.com/deno-the-complete-reference/deno-vs-go-native-hello-world-performance-c57d8fc13c75\nhttps://blog.stackademic.com/top-7-spring-boot-design-patterns-unveiled-4a2569f8d324\nhttps://news.shulie.io/?p=8229\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第十四期：Golang 作者 Rob Pike 在 GopherConAU 上的分享","url":"/2024/01/12/ob/newsletter/Newsletter12-202401012/","content":"\n技术阅读周刊，每周更新。\n\n历史更新\n20231215：第十期\n20231122：第十一期\n20231129：第十二期\n20240105：第十三期：一些提高生产力的终端命令\n\nWhat We Got Right, What We Got WrongURL: https://commandcenter.blogspot.com/2024/01/what-we-got-right-what-we-got-wrong.html?utm_source=changelog-news\n\n本文是 Golang 核心作者之一  Rob Pike 去年底在澳大利亚 GopherConAU 会议上的分享；总结了 Go 语言 14 年来的做对了哪些事情、做错了哪些事情。\n\n\n主要包括：\n\n语言涉及\n社区管理\n项目运营等方面，感兴趣的还可以看油管视频。https://www.youtube.com/watch?v=yE5Tpp2BSGw\n\nTop 10 Platform Engineering Tools You Should Consider in 2024 | by Romaric Philogène | Jan, 2024 | MediumURL: https://medium.com/@rphilogene/top-10-platform-engineering-tools-you-should-consider-in-2024-892e6e211b85\n\n本文介绍了作为一个平台工程师需要掌握的工具。\n\n先定义了什么是平台工程师：为研发人员提供平台资源进行开发，让开发人员可以在云环境中自助完成整个软件生命周期的各个环节，比如基础环境搭建、代码 pipelines、监控等。\n以下是会用到的工具：\n\nkubernetes：这个就不用多讲了。\nCrossplane：用于管理多集群的 kubernetes\nQovery：内部开发者平台\nGithub&#x2F;Gitlab CI&#x2F;CD\nArgoCD：kubernetes 原生提供的持续部署工具。\nDocker\nTerraform： 基础设施自动化工具,可以通过声明式配置文件实现多云基础设施的部署和管理。\nDatadog：监控和日志分析平台，当然也可以使用 Prometheus&#x2F;Grafana 等\n\nLoad Balancing Algorithms Explained VisuallyURL: https://blog.quastor.org/p/load-balancing-algorithms-explained-visually?utm_source=tldrwebdev\n\n本文介绍了一些负载均衡算法以及其优缺点。\n\n\n\n轮询算法(Round Robin):每个请求按顺序分配到不同服务器。实现简单,但不能考虑服务器负载情况。\n\n加权轮询(Weighted Round Robin):考虑服务器性能给各服务器设置权重,请求分配按权重比例进行。仍然不能实时反应服务器负载变化。\n\n最少连接数(Least Connections):实时监测各服务器连接数,将请求分配到连接数最少的服务器上。实现较复杂,需要定期探测各服务器状态。\n\n最短响应时间(Least Response Time):监测各服务器响应时间,分配给响应最快的服务器。\n\n双随机选择(Power of Two Choices):随机选择两台服务器,将请求分配给负载较轻的一台。减少监测开销。\n\n一致哈希(Consistent Hashing):根据请求关键信息计算哈希值,将请求分配给对应的机器范围。解决主机添加和删除问题。\n\n其他算法如根据磁盘、内存利用率进行负载分配等。\n\n\nWhat problem did Go actually solve for GoogleURL: https://www.reddit.com/r/golang/comments/176b5pn/what_problem_did_go_actually_solve_for_google/\n\n这是一个 Reddit 上的帖子，OP 的问题是 Rob 在之前的分享中提到 Golang 创建的原因是要解决 Google 内部的问题，但没有具体讲  Google 到底遇到了什么问题？什么问题是几百种编程语言都无法解决的问题？\n\n以下是一些高赞回答：\n\n更快的本地编译速度\n对新手来说更好理解的代码\n更严格的代码风格，使得大家的代码都差不多。\n更容易编写并发程序\n\n总体来说, Go 主要解决的是在大型分布式系统中如何更高效地进行协作开发、实现高性能又易维护。这正是 Google 当时最关心的问题。\n文章链接：\n\nhttps://commandcenter.blogspot.com/2024/01/what-we-got-right-what-we-got-wrong.html?utm_source=changelog-news\nhttps://medium.com/@rphilogene/top-10-platform-engineering-tools-you-should-consider-in-2024-892e6e211b85\nhttps://blog.quastor.org/p/load-balancing-algorithms-explained-visually?utm_source=tldrwebdev\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第十三期：一些提高生产力的终端命令","url":"/2024/01/05/ob/newsletter/Newsletter12-20240105/","content":"\n技术阅读周刊，每周更新。\n\n历史更新\n20231201：第八期\n20231215：第十期\n20231122：第十一期\n20231129：第十二期\n\n生存还是毁灭？一文带你看懂 Pulsar 的消息保留和过期策略-腾讯云开发者社区-腾讯云URL: https://cloud.tencent.com/developer/article/2245703\n\n\n本文分析了 Pulsar 消息的生命周期，主要是如何保留和回收消息\n\n\nTTL（Time To Live）：使得未 ACK 的消息进入 ACK 状态，供后续回收的时候使用\nRetention保留策略：默认情况下已经被所有订阅者 ACK 的消息会被立即回收掉，配置保留策略可以保留一定时间、一定数量的 ACK 消息，利用这个时间差可以做消息回查\nBacklog: 未被确认消息的集合，也就是积压消息；也可以配置只保留一定时间、数量的消息，从而减少磁盘压力；当超过我们配置的阈值时，有以下几种选择：\n\n这些流行的K8S工具，你都用上了吗URL: https://mp.weixin.qq.com/s/EC-YLm71YB4cMDoTjrdfyg\n\n推荐了一些常用的 kubernetes 管理工具\n\n\n\nHelm: kubernetes 平台的必备的包管理工具\n本地运行的 kubernetes 工具：有时候需要在本地进行开发和测试，这类工具就很有用：\nDocker Desktop\nminikube\nkind\nk3s\n这类工具在ingress、负载均衡、集群外访问等需要单独配置。\n\n\n集群自动缩放器：用于缩放底层节点\n一些云服务厂商自动集成了这类功能，如果是自建集群：\nkubernetes Autoscaler\nKarpenter\n\n\n备份和迁移\n如果部署了有状态的应用，需要进行数据迁移和备份时，可以使用 velero\n\n\n命令行工具\nkube-ps1 用于终端的 kubernetes 命令提示\nkubectx 用于在终端进行 集群、namespace 上下文切换\n\n\nIDE\nOpenLens 一个客户端可视化 app，用于方便管理 kubernetes 集群\n\n\n\n3 Terminal Commands to Increase Your Productivity - DEV CommunityURL: https://dev.to/pankajgupta221b/3-terminal-commands-to-increase-your-productivity-57dm?ref=dailydev\n\n作者介绍了几个常用的可以提高生产力的终端命令\n\n\nalias 别名别名非常好用，以下是我常用的一些别名:\n-=&#x27;cd -&#x27;...=../......=../../.......=../../../........=../../../../..1=&#x27;cd -&#x27;2=&#x27;cd -2&#x27;3=&#x27;cd -3&#x27;4=&#x27;cd -4&#x27;5=&#x27;cd -5&#x27;6=&#x27;cd -6&#x27;7=&#x27;cd -7&#x27;8=&#x27;cd -8&#x27;9=&#x27;cd -9&#x27;dc=dockerjdk11=&#x27;export JAVA_HOME=~/jdk/jdk-11.0.16.1.jdk/Contents/Home&#x27;jdk17=&#x27;export JAVA_HOME=~/Users/chenjie/Documents/dev~/jdk/jdk-17.0.1.jdk/Contents/Home/&#x27;jdk21=&#x27;export JAVA_HOME=~/jdk/jdk-21.0.1.jdk/Contents/Home&#x27;jdk8=&#x27;export JAVA_HOME=&#x27;k=kubectlpp=&#x27;sh hexo-push.sh&#x27;\npbcopy这个在有时候需要 debug 日志或者复制一些长文本到剪贴板里非常有用\ncat xx.properties |grep timeout | pbcopy\n这样就可以把 timeout 这个关键字从文件中复制到粘贴板，我就可以将它复制到其他地方使用。\n反向搜索在终端中使用 ctrl+r 就可以根据关键字在历史命令中查找命令，这个在忘记了一些命令但只记得关键字的时候非常有用。\n我这里使用的终端是 Warp ，交互上更加好用一些。\ncal可以用于显示日历\n文章链接：\n\nhttps://cloud.tencent.com/developer/article/2245703\nhttps://mp.weixin.qq.com/s/EC-YLm71YB4cMDoTjrdfyg\nhttps://dev.to/pankajgupta221b/3-terminal-commands-to-increase-your-productivity-57dm?ref=dailydev\n\n#Newletters \n","categories":["OB","Newsletter"]},{"title":"技术阅读周刊第十四期：常用的 Git 配置","url":"/2024/02/29/ob/newsletter/Newsletter14-20240223/","content":"\n技术阅读周刊，每周更新。\n\n历史更新\n20231122：第十一期\n20231129：第十二期\n20240105：第十三期：一些提高生产力的终端命令\n20240112：第十四期：Golang 作者 Rob Pike 在 GopherConAU 上的分享\n\nHow I write HTTP services in Go after 13 years\n\n使用NewServer函数构建服务实例,利用依赖注入方式将所有的依赖参数包含进来。\n\nfunc NewServer(\tlogger *Logger\tconfig *Config\tcommentStore *commentStore\tanotherStore *anotherStore) http.Handler &#123;\tmux := http.NewServeMux()\taddRoutes(\t\tmux,\t\tLogger,\t\tConfig,\t\tcommentStore,\t\tanotherStore,\t)\tvar handler http.Handler = mux\thandler = someMiddleware(handler)\thandler = someMiddleware2(handler)\thandler = someMiddleware3(handler)\treturn handler&#125;\n\n在routes.go文件中统一定义所有路由函数。\n\nfunc addRoutes(\tmux                 *http.ServeMux,\tlogger              *logging.Logger,\tconfig              Config,\ttenantsStore        *TenantsStore,\tcommentsStore       *CommentsStore,\tconversationService *ConversationService,\tchatGPTService      *ChatGPTService,\tauthProxy           *authProxy) &#123;\tmux.Handle(&quot;/api/v1/&quot;, handleTenantsGet(logger, tenantsStore))\tmux.Handle(&quot;/oauth2/&quot;, handleOAuth2Proxy(logger, authProxy))\tmux.HandleFunc(&quot;/healthz&quot;, handleHealthzPlease(logger))\tmux.Handle(&quot;/&quot;, http.NotFoundHandler())&#125;\n\n主函数只调用run函数来运行服务\nfunc run(ctx context.Context, w io.Writer, args []string) error &#123;\tctx, cancel := signal.NotifyContext(ctx, os.Interrupt)\tdefer cancel()\t// ...&#125;func main() &#123;\tctx := context.Background()\tif err := run(ctx, os.Stdout, os.Args); err != nil &#123;\t\tfmt.Fprintf(os.Stderr, &quot;%s\\n&quot;, err)\t\tos.Exit(1)\t&#125;&#125;\n\n返回闭包 handle\n\n\n// handleSomething handles one of those web requests// that you hear so much about.func handleSomething(logger *Logger) http.Handler &#123;\tthing := prepareThing()\treturn http.HandlerFunc(\t\tfunc(w http.ResponseWriter, r *http.Request) &#123;\t\t\t// use thing to handle request\t\t\tlogger.Info(r.Context(), &quot;msg&quot;, &quot;handleSomething&quot;)\t\t&#125;\t)&#125;\n\n定义通用的encode和decode函数\nfunc encode[T any](w http.ResponseWriter, r *http.Request, status int, v T) error &#123;\tw.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)\tw.WriteHeader(status)\tif err := json.NewEncoder(w).Encode(v); err != nil &#123;\t\treturn fmt.Errorf(&quot;encode json: %w&quot;, err)\t&#125;\treturn nil&#125;func decode[T any](r *http.Request) (T, error) &#123;\tvar v T\tif err := json.NewDecoder(r.Body).Decode(&amp;v); err != nil &#123;\t\treturn v, fmt.Errorf(&quot;decode json: %w&quot;, err)\t&#125;\treturn v, nil&#125;\n\n\n\n提供一个抽象的 Validator 接口用于验证\n// Validator is an object that can be validated.type Validator interface &#123;\t// Valid checks the object and returns any\t// problems. If len(problems) == 0 then\t// the object is valid.\tValid(ctx context.Context) (problems map[string]string)&#125;func decodeValid[T Validator](r *http.Request) (T, map[string]string, error) &#123;\tvar v T\tif err := json.NewDecoder(r.Body).Decode(&amp;v); err != nil &#123;\t\treturn v, nil, fmt.Errorf(&quot;decode json: %w&quot;, err)\t&#125;\tif problems := v.Valid(r.Context()); len(problems) &gt; 0 &#123;\t\treturn v, problems, fmt.Errorf(&quot;invalid %T: %d problems&quot;, v, len(problems))\t&#125;\treturn v, nil, nil&#125;\n\n自定义校验需要实现 Validator 接口。\n\n使用 Once 延迟调用来提高启动性能。func handleTemplate(files string...) http.HandlerFunc &#123;\tvar (\t\tinit    sync.Once\t\ttpl     *template.Template\t\ttplerr  error\t)\treturn func(w http.ResponseWriter, r *http.Request) &#123;\t\tinit.Do(func()&#123;\t\t\ttpl, tplerr = template.ParseFiles(files...)\t\t&#125;)\t\tif tplerr != nil &#123;\t\t\thttp.Error(w, tplerr.Error(), http.StatusInternalServerError)\t\t\treturn\t\t&#125;\t\t// use tpl\t&#125;&#125;\n\nWhat is OpenTelemetry?\n\n这是一篇 OTel 的科普文章\n\nOpenTelemetry 提供一个统一、可扩展的框架，用于收集、分析和观察分布式系统的性能数据。它包括一组API、库、代理和收集器，这些组件可以跨多种编程语言和平台实现对应用程序的监控。\nOpenTelemetry 整合 OpenTracing 和 OpenCensus。\n\n\n2019年，两个社区进行了合并。\n\n同时 OTel 具备以下特征：\n\n统一性：OpenTelemetry 提供了一个统一的API，使得开发者可以在不同的编程语言和框架中以一致的方式实现监控。\n\n可扩展性：可以编写自己的扩展来满足个性化需要\n\n跨平台：OpenTelemetry 支持多种编程语言，如 Java、Python、Go、.NET 等，以及多种云服务和容器平台。\n\n社区驱动：作为一个开源项目，OpenTelemetry 由一个活跃的社区支持，社区成员贡献代码、文档和最佳实践。\n\n与现有工具的兼容性：OpenTelemetry 设计时考虑了与现有监控工具的兼容性，如 Prometheus、Jaeger、Zipkin 等，这使得它可以轻松地集成到现有的监控基础设施中。\n\n\n提供了一种名为：OTLP（OpenTelemetry Protocol）的通讯协议，基于 gRPC。\n使用该协议用于客户端与 Collector 采集器进行交互。\nCollector 是 OpenTelemetry 架构中的一个关键组件，它负责接收、处理和导出数据(Trace&#x2F;log&#x2F;metrics)。\n\n它可以接受从客户端发出的数据进行处理，同时可以导出为不同格式的数据。\n\n总的来说 OTel 是可观测系统的新标准，基于它可以兼容以前使用的 Prometheus、 victoriametrics、skywalking 等系统，同时还可以灵活扩展，不用与任何但一生态或技术栈进行绑定。\n\nPopular git config options\n\n本文总结了一些常用的 git 配置\n\n\npull.ff only 或 pull.rebase true：这两个选项都可以避免在执行git pull时意外创建合并提交，特别是当上游分支已经发生了变化的时候。\n\nmerge.conflictstyle diff3：这个选项使得合并冲突更易于阅读，通过在冲突中显示原始代码版本，帮助用户更好地解决冲突。\n\nrebase.autosquash true 和 rebase.autostash true：这些选项使得修改旧提交变得更容易，并且自动处理stash。\n\npush.default simple 或 push.default current：这些选项告诉git push自动推送当前分支到同名的远程分支。\n\ninit.defaultBranch main：创建新仓库时，默认创建main分支而不是master分支。\n\ncommit.verbose true：在提交时显示整个提交差异。\n\nrerere.enabled true：启用rerere功能，自动解决冲突\n\nhelp.autocorrect：设置自动矫正的级别，以自动运行建议的命令。\n\ncore.pager delta：设置Git使用的分页器，例如使用delta来查看带有语法高亮的diff。\n\ndiff.algorithm histogram：设置Git的diff算法，以改善函数重排时的diff显示。\n\n\n文章链接：\n\nhttps://grafana.com/blog/2024/02/09/how-i-write-http-services-in-go-after-13-years/\nhttps://codeboten.medium.com/what-is-opentelemetry-6a7e5c6901c5\nhttps://jvns.ca/blog/2024/02/16/popular-git-config-options/\n\n#Newletters \n","categories":["OB","Newsletter"]}]